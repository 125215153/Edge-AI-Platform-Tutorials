I1107 01:46:41.795866 34644 deephi_compress.cpp:203] Starting analysis of /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/float.caffemodel
I1107 01:46:41.796025 34644 sens_analyser.cpp:145] Analysis completed 0%
I1107 01:47:26.121862 34644 sens_analyser.cpp:212] Analysing layer [conv1] done
I1107 01:47:26.121904 34644 sens_analyser.cpp:213] Analysis completed 20%
I1107 01:48:06.634673 34644 sens_analyser.cpp:212] Analysing layer [conv2] done
I1107 01:48:06.634702 34644 sens_analyser.cpp:213] Analysis completed 40%
I1107 01:48:47.223513 34644 sens_analyser.cpp:212] Analysing layer [conv3] done
I1107 01:48:47.223541 34644 sens_analyser.cpp:213] Analysis completed 60%
I1107 01:49:27.727144 34644 sens_analyser.cpp:212] Analysing layer [conv4] done
I1107 01:49:27.727172 34644 sens_analyser.cpp:213] Analysis completed 80%
I1107 01:50:08.293376 34644 sens_analyser.cpp:212] Analysing layer [conv5] done
I1107 01:50:08.293407 34644 sens_analyser.cpp:213] Analysis completed 100%
I1107 01:50:08.298638 34644 deephi_compress.cpp:205] Analysis done.
Now you can compress the model with the following command:
deephi_compress compress -config config0.prototxt
I1107 01:50:08.530699 36080 pruning_runner.cpp:190] Sens info found, use it.
I1107 01:50:08.892335 36080 pruning_runner.cpp:217] Start compressing, please wait...
I1107 01:50:14.342483 36080 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 01:50:14.342777 36080 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 01:50:14.343101 36080 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 01:50:14.343271 36080 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 01:50:14.343386 36080 layer_factory.hpp:77] Creating layer data
I1107 01:50:14.343428 36080 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:14.343787 36080 net.cpp:94] Creating Layer data
I1107 01:50:14.343794 36080 net.cpp:409] data -> data
I1107 01:50:14.343803 36080 net.cpp:409] data -> label
I1107 01:50:14.344868 36209 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 01:50:14.344899 36209 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 01:50:14.345073 36080 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 01:50:14.345144 36080 data_layer.cpp:83] output data size: 50,3,227,227
I1107 01:50:14.420029 36080 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:14.420101 36080 net.cpp:144] Setting up data
I1107 01:50:14.420111 36080 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 01:50:14.420115 36080 net.cpp:151] Top shape: 50 (50)
I1107 01:50:14.420119 36080 net.cpp:159] Memory required for data: 30917600
I1107 01:50:14.420125 36080 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 01:50:14.420136 36080 net.cpp:94] Creating Layer label_data_1_split
I1107 01:50:14.420143 36080 net.cpp:435] label_data_1_split <- label
I1107 01:50:14.420150 36080 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 01:50:14.420161 36080 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 01:50:14.420289 36080 net.cpp:144] Setting up label_data_1_split
I1107 01:50:14.420295 36080 net.cpp:151] Top shape: 50 (50)
I1107 01:50:14.420298 36080 net.cpp:151] Top shape: 50 (50)
I1107 01:50:14.420301 36080 net.cpp:159] Memory required for data: 30918000
I1107 01:50:14.420318 36080 layer_factory.hpp:77] Creating layer conv1
I1107 01:50:14.420332 36080 net.cpp:94] Creating Layer conv1
I1107 01:50:14.420337 36080 net.cpp:435] conv1 <- data
I1107 01:50:14.420346 36080 net.cpp:409] conv1 -> conv1
I1107 01:50:14.422021 36080 net.cpp:144] Setting up conv1
I1107 01:50:14.422034 36080 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:14.422036 36080 net.cpp:159] Memory required for data: 88998000
I1107 01:50:14.422062 36080 layer_factory.hpp:77] Creating layer bn1
I1107 01:50:14.422071 36080 net.cpp:94] Creating Layer bn1
I1107 01:50:14.422075 36080 net.cpp:435] bn1 <- conv1
I1107 01:50:14.422080 36080 net.cpp:409] bn1 -> scale1
I1107 01:50:14.422776 36080 net.cpp:144] Setting up bn1
I1107 01:50:14.422783 36080 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:14.422787 36080 net.cpp:159] Memory required for data: 147078000
I1107 01:50:14.422799 36080 layer_factory.hpp:77] Creating layer relu1
I1107 01:50:14.422809 36080 net.cpp:94] Creating Layer relu1
I1107 01:50:14.422813 36080 net.cpp:435] relu1 <- scale1
I1107 01:50:14.422818 36080 net.cpp:409] relu1 -> relu1
I1107 01:50:14.422855 36080 net.cpp:144] Setting up relu1
I1107 01:50:14.422861 36080 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:14.422864 36080 net.cpp:159] Memory required for data: 205158000
I1107 01:50:14.422866 36080 layer_factory.hpp:77] Creating layer pool1
I1107 01:50:14.422874 36080 net.cpp:94] Creating Layer pool1
I1107 01:50:14.422880 36080 net.cpp:435] pool1 <- relu1
I1107 01:50:14.422885 36080 net.cpp:409] pool1 -> pool1
I1107 01:50:14.422930 36080 net.cpp:144] Setting up pool1
I1107 01:50:14.422935 36080 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 01:50:14.422937 36080 net.cpp:159] Memory required for data: 219154800
I1107 01:50:14.422940 36080 layer_factory.hpp:77] Creating layer conv2
I1107 01:50:14.422950 36080 net.cpp:94] Creating Layer conv2
I1107 01:50:14.422955 36080 net.cpp:435] conv2 <- pool1
I1107 01:50:14.422961 36080 net.cpp:409] conv2 -> conv2
I1107 01:50:14.430060 36080 net.cpp:144] Setting up conv2
I1107 01:50:14.430081 36080 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:14.430085 36080 net.cpp:159] Memory required for data: 256479600
I1107 01:50:14.430101 36080 layer_factory.hpp:77] Creating layer bn2
I1107 01:50:14.430127 36080 net.cpp:94] Creating Layer bn2
I1107 01:50:14.430142 36080 net.cpp:435] bn2 <- conv2
I1107 01:50:14.430161 36080 net.cpp:409] bn2 -> scale2
I1107 01:50:14.430783 36080 net.cpp:144] Setting up bn2
I1107 01:50:14.430793 36080 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:14.430797 36080 net.cpp:159] Memory required for data: 293804400
I1107 01:50:14.430805 36080 layer_factory.hpp:77] Creating layer relu2
I1107 01:50:14.430815 36080 net.cpp:94] Creating Layer relu2
I1107 01:50:14.430819 36080 net.cpp:435] relu2 <- scale2
I1107 01:50:14.430826 36080 net.cpp:409] relu2 -> relu2
I1107 01:50:14.430851 36080 net.cpp:144] Setting up relu2
I1107 01:50:14.430861 36080 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:14.430865 36080 net.cpp:159] Memory required for data: 331129200
I1107 01:50:14.430868 36080 layer_factory.hpp:77] Creating layer pool2
I1107 01:50:14.430876 36080 net.cpp:94] Creating Layer pool2
I1107 01:50:14.430887 36080 net.cpp:435] pool2 <- relu2
I1107 01:50:14.430893 36080 net.cpp:409] pool2 -> pool2
I1107 01:50:14.430927 36080 net.cpp:144] Setting up pool2
I1107 01:50:14.430932 36080 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:14.430936 36080 net.cpp:159] Memory required for data: 339782000
I1107 01:50:14.430939 36080 layer_factory.hpp:77] Creating layer conv3
I1107 01:50:14.430950 36080 net.cpp:94] Creating Layer conv3
I1107 01:50:14.430956 36080 net.cpp:435] conv3 <- pool2
I1107 01:50:14.430964 36080 net.cpp:409] conv3 -> conv3
I1107 01:50:14.441447 36080 net.cpp:144] Setting up conv3
I1107 01:50:14.441467 36080 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:14.441469 36080 net.cpp:159] Memory required for data: 352761200
I1107 01:50:14.441493 36080 layer_factory.hpp:77] Creating layer relu3
I1107 01:50:14.441500 36080 net.cpp:94] Creating Layer relu3
I1107 01:50:14.441504 36080 net.cpp:435] relu3 <- conv3
I1107 01:50:14.441510 36080 net.cpp:409] relu3 -> relu3
I1107 01:50:14.441532 36080 net.cpp:144] Setting up relu3
I1107 01:50:14.441536 36080 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:14.441540 36080 net.cpp:159] Memory required for data: 365740400
I1107 01:50:14.441541 36080 layer_factory.hpp:77] Creating layer conv4
I1107 01:50:14.441552 36080 net.cpp:94] Creating Layer conv4
I1107 01:50:14.441558 36080 net.cpp:435] conv4 <- relu3
I1107 01:50:14.441565 36080 net.cpp:409] conv4 -> conv4
I1107 01:50:14.457365 36080 net.cpp:144] Setting up conv4
I1107 01:50:14.457430 36080 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:14.457448 36080 net.cpp:159] Memory required for data: 378719600
I1107 01:50:14.457479 36080 layer_factory.hpp:77] Creating layer relu4
I1107 01:50:14.457502 36080 net.cpp:94] Creating Layer relu4
I1107 01:50:14.457520 36080 net.cpp:435] relu4 <- conv4
I1107 01:50:14.457538 36080 net.cpp:409] relu4 -> relu4
I1107 01:50:14.457597 36080 net.cpp:144] Setting up relu4
I1107 01:50:14.457613 36080 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:14.457629 36080 net.cpp:159] Memory required for data: 391698800
I1107 01:50:14.457641 36080 layer_factory.hpp:77] Creating layer conv5
I1107 01:50:14.457664 36080 net.cpp:94] Creating Layer conv5
I1107 01:50:14.457679 36080 net.cpp:435] conv5 <- relu4
I1107 01:50:14.457697 36080 net.cpp:409] conv5 -> conv5
I1107 01:50:14.466989 36080 net.cpp:144] Setting up conv5
I1107 01:50:14.467015 36080 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:14.467016 36080 net.cpp:159] Memory required for data: 400351600
I1107 01:50:14.467026 36080 layer_factory.hpp:77] Creating layer relu5
I1107 01:50:14.467036 36080 net.cpp:94] Creating Layer relu5
I1107 01:50:14.467041 36080 net.cpp:435] relu5 <- conv5
I1107 01:50:14.467058 36080 net.cpp:409] relu5 -> relu5
I1107 01:50:14.467095 36080 net.cpp:144] Setting up relu5
I1107 01:50:14.467103 36080 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:14.467105 36080 net.cpp:159] Memory required for data: 409004400
I1107 01:50:14.467108 36080 layer_factory.hpp:77] Creating layer pool5
I1107 01:50:14.467119 36080 net.cpp:94] Creating Layer pool5
I1107 01:50:14.467125 36080 net.cpp:435] pool5 <- relu5
I1107 01:50:14.467133 36080 net.cpp:409] pool5 -> pool5
I1107 01:50:14.467169 36080 net.cpp:144] Setting up pool5
I1107 01:50:14.467175 36080 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 01:50:14.467178 36080 net.cpp:159] Memory required for data: 410847600
I1107 01:50:14.467181 36080 layer_factory.hpp:77] Creating layer fc6
I1107 01:50:14.467191 36080 net.cpp:94] Creating Layer fc6
I1107 01:50:14.467197 36080 net.cpp:435] fc6 <- pool5
I1107 01:50:14.467206 36080 net.cpp:409] fc6 -> fc6
I1107 01:50:14.778128 36080 net.cpp:144] Setting up fc6
I1107 01:50:14.778152 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.778156 36080 net.cpp:159] Memory required for data: 411666800
I1107 01:50:14.778163 36080 layer_factory.hpp:77] Creating layer relu6
I1107 01:50:14.778172 36080 net.cpp:94] Creating Layer relu6
I1107 01:50:14.778187 36080 net.cpp:435] relu6 <- fc6
I1107 01:50:14.778203 36080 net.cpp:409] relu6 -> relu6
I1107 01:50:14.778244 36080 net.cpp:144] Setting up relu6
I1107 01:50:14.778249 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.778251 36080 net.cpp:159] Memory required for data: 412486000
I1107 01:50:14.778254 36080 layer_factory.hpp:77] Creating layer drop6
I1107 01:50:14.778261 36080 net.cpp:94] Creating Layer drop6
I1107 01:50:14.778264 36080 net.cpp:435] drop6 <- relu6
I1107 01:50:14.778270 36080 net.cpp:409] drop6 -> drop6
I1107 01:50:14.778301 36080 net.cpp:144] Setting up drop6
I1107 01:50:14.778306 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.778307 36080 net.cpp:159] Memory required for data: 413305200
I1107 01:50:14.778311 36080 layer_factory.hpp:77] Creating layer fc7
I1107 01:50:14.778334 36080 net.cpp:94] Creating Layer fc7
I1107 01:50:14.778337 36080 net.cpp:435] fc7 <- drop6
I1107 01:50:14.778344 36080 net.cpp:409] fc7 -> fc7
I1107 01:50:14.912243 36080 net.cpp:144] Setting up fc7
I1107 01:50:14.912269 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.912271 36080 net.cpp:159] Memory required for data: 414124400
I1107 01:50:14.912294 36080 layer_factory.hpp:77] Creating layer bn7
I1107 01:50:14.912313 36080 net.cpp:94] Creating Layer bn7
I1107 01:50:14.912319 36080 net.cpp:435] bn7 <- fc7
I1107 01:50:14.912324 36080 net.cpp:409] bn7 -> scale7
I1107 01:50:14.912804 36080 net.cpp:144] Setting up bn7
I1107 01:50:14.912811 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.912816 36080 net.cpp:159] Memory required for data: 414943600
I1107 01:50:14.912823 36080 layer_factory.hpp:77] Creating layer relu7
I1107 01:50:14.912830 36080 net.cpp:94] Creating Layer relu7
I1107 01:50:14.912834 36080 net.cpp:435] relu7 <- scale7
I1107 01:50:14.912840 36080 net.cpp:409] relu7 -> relu7
I1107 01:50:14.912878 36080 net.cpp:144] Setting up relu7
I1107 01:50:14.912883 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.912884 36080 net.cpp:159] Memory required for data: 415762800
I1107 01:50:14.912887 36080 layer_factory.hpp:77] Creating layer drop7
I1107 01:50:14.912894 36080 net.cpp:94] Creating Layer drop7
I1107 01:50:14.912897 36080 net.cpp:435] drop7 <- relu7
I1107 01:50:14.912902 36080 net.cpp:409] drop7 -> drop7
I1107 01:50:14.912930 36080 net.cpp:144] Setting up drop7
I1107 01:50:14.912935 36080 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:14.912937 36080 net.cpp:159] Memory required for data: 416582000
I1107 01:50:14.912940 36080 layer_factory.hpp:77] Creating layer fc8
I1107 01:50:14.912947 36080 net.cpp:94] Creating Layer fc8
I1107 01:50:14.912950 36080 net.cpp:435] fc8 <- drop7
I1107 01:50:14.912956 36080 net.cpp:409] fc8 -> fc8
I1107 01:50:14.913828 36080 net.cpp:144] Setting up fc8
I1107 01:50:14.913841 36080 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:14.913843 36080 net.cpp:159] Memory required for data: 416582400
I1107 01:50:14.913851 36080 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 01:50:14.913858 36080 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 01:50:14.913864 36080 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 01:50:14.913870 36080 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 01:50:14.913879 36080 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 01:50:14.913913 36080 net.cpp:144] Setting up fc8_fc8_0_split
I1107 01:50:14.913918 36080 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:14.913921 36080 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:14.913923 36080 net.cpp:159] Memory required for data: 416583200
I1107 01:50:14.913925 36080 layer_factory.hpp:77] Creating layer loss
I1107 01:50:14.913933 36080 net.cpp:94] Creating Layer loss
I1107 01:50:14.913936 36080 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 01:50:14.913941 36080 net.cpp:435] loss <- label_data_1_split_0
I1107 01:50:14.913947 36080 net.cpp:409] loss -> loss
I1107 01:50:14.913956 36080 layer_factory.hpp:77] Creating layer loss
I1107 01:50:14.914031 36080 net.cpp:144] Setting up loss
I1107 01:50:14.914036 36080 net.cpp:151] Top shape: (1)
I1107 01:50:14.914038 36080 net.cpp:154]     with loss weight 1
I1107 01:50:14.914052 36080 net.cpp:159] Memory required for data: 416583204
I1107 01:50:14.914055 36080 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 01:50:14.914062 36080 net.cpp:94] Creating Layer accuracy-top1
I1107 01:50:14.914068 36080 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 01:50:14.914073 36080 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 01:50:14.914079 36080 net.cpp:409] accuracy-top1 -> top-1
I1107 01:50:14.914089 36080 net.cpp:144] Setting up accuracy-top1
I1107 01:50:14.914093 36080 net.cpp:151] Top shape: (1)
I1107 01:50:14.914098 36080 net.cpp:159] Memory required for data: 416583208
I1107 01:50:14.914100 36080 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 01:50:14.914115 36080 net.cpp:220] loss needs backward computation.
I1107 01:50:14.914119 36080 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 01:50:14.914122 36080 net.cpp:220] fc8 needs backward computation.
I1107 01:50:14.914126 36080 net.cpp:220] drop7 needs backward computation.
I1107 01:50:14.914130 36080 net.cpp:220] relu7 needs backward computation.
I1107 01:50:14.914134 36080 net.cpp:220] bn7 needs backward computation.
I1107 01:50:14.914139 36080 net.cpp:220] fc7 needs backward computation.
I1107 01:50:14.914142 36080 net.cpp:220] drop6 needs backward computation.
I1107 01:50:14.914145 36080 net.cpp:220] relu6 needs backward computation.
I1107 01:50:14.914149 36080 net.cpp:220] fc6 needs backward computation.
I1107 01:50:14.914153 36080 net.cpp:220] pool5 needs backward computation.
I1107 01:50:14.914157 36080 net.cpp:220] relu5 needs backward computation.
I1107 01:50:14.914161 36080 net.cpp:220] conv5 needs backward computation.
I1107 01:50:14.914165 36080 net.cpp:220] relu4 needs backward computation.
I1107 01:50:14.914170 36080 net.cpp:220] conv4 needs backward computation.
I1107 01:50:14.914173 36080 net.cpp:220] relu3 needs backward computation.
I1107 01:50:14.914177 36080 net.cpp:220] conv3 needs backward computation.
I1107 01:50:14.914180 36080 net.cpp:220] pool2 needs backward computation.
I1107 01:50:14.914185 36080 net.cpp:220] relu2 needs backward computation.
I1107 01:50:14.914188 36080 net.cpp:220] bn2 needs backward computation.
I1107 01:50:14.914192 36080 net.cpp:220] conv2 needs backward computation.
I1107 01:50:14.914196 36080 net.cpp:220] pool1 needs backward computation.
I1107 01:50:14.914201 36080 net.cpp:220] relu1 needs backward computation.
I1107 01:50:14.914203 36080 net.cpp:220] bn1 needs backward computation.
I1107 01:50:14.914207 36080 net.cpp:220] conv1 needs backward computation.
I1107 01:50:14.914211 36080 net.cpp:222] label_data_1_split does not need backward computation.
I1107 01:50:14.914219 36080 net.cpp:222] data does not need backward computation.
I1107 01:50:14.914222 36080 net.cpp:264] This network produces output loss
I1107 01:50:14.914225 36080 net.cpp:264] This network produces output top-1
I1107 01:50:14.914254 36080 net.cpp:284] Network initialization done.
W1107 01:50:14.914630 36080 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:14.915740 36080 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:14.957620 36080 net.cpp:860] Force copying param 4 weights from layer 'bn7'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I1107 01:50:14.957686 36080 caffe_interface.cpp:363] Running for 80 iterations.
I1107 01:50:15.000377 36080 caffe_interface.cpp:125] Batch 0, loss = 0.299505
I1107 01:50:15.000398 36080 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I1107 01:50:15.020017 36080 caffe_interface.cpp:125] Batch 1, loss = 0.263073
I1107 01:50:15.020036 36080 caffe_interface.cpp:125] Batch 1, top-1 = 0.96
I1107 01:50:15.038054 36080 caffe_interface.cpp:125] Batch 2, loss = 0.276373
I1107 01:50:15.038072 36080 caffe_interface.cpp:125] Batch 2, top-1 = 0.9
I1107 01:50:15.057287 36080 caffe_interface.cpp:125] Batch 3, loss = 0.332689
I1107 01:50:15.057303 36080 caffe_interface.cpp:125] Batch 3, top-1 = 0.82
I1107 01:50:15.075012 36080 caffe_interface.cpp:125] Batch 4, loss = 0.307378
I1107 01:50:15.075028 36080 caffe_interface.cpp:125] Batch 4, top-1 = 0.92
I1107 01:50:15.094748 36080 caffe_interface.cpp:125] Batch 5, loss = 0.174317
I1107 01:50:15.094763 36080 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 01:50:15.112730 36080 caffe_interface.cpp:125] Batch 6, loss = 0.271658
I1107 01:50:15.112746 36080 caffe_interface.cpp:125] Batch 6, top-1 = 0.96
I1107 01:50:15.133018 36080 caffe_interface.cpp:125] Batch 7, loss = 0.2581
I1107 01:50:15.133035 36080 caffe_interface.cpp:125] Batch 7, top-1 = 0.94
I1107 01:50:15.152684 36080 caffe_interface.cpp:125] Batch 8, loss = 0.257099
I1107 01:50:15.152705 36080 caffe_interface.cpp:125] Batch 8, top-1 = 0.96
I1107 01:50:15.172005 36080 caffe_interface.cpp:125] Batch 9, loss = 0.357233
I1107 01:50:15.172034 36080 caffe_interface.cpp:125] Batch 9, top-1 = 0.9
I1107 01:50:15.191522 36080 caffe_interface.cpp:125] Batch 10, loss = 0.233861
I1107 01:50:15.191542 36080 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I1107 01:50:15.209842 36080 caffe_interface.cpp:125] Batch 11, loss = 0.282014
I1107 01:50:15.209861 36080 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I1107 01:50:15.228307 36080 caffe_interface.cpp:125] Batch 12, loss = 0.29246
I1107 01:50:15.228325 36080 caffe_interface.cpp:125] Batch 12, top-1 = 0.92
I1107 01:50:15.246771 36080 caffe_interface.cpp:125] Batch 13, loss = 0.264075
I1107 01:50:15.246788 36080 caffe_interface.cpp:125] Batch 13, top-1 = 0.92
I1107 01:50:15.266191 36080 caffe_interface.cpp:125] Batch 14, loss = 0.255462
I1107 01:50:15.266221 36080 caffe_interface.cpp:125] Batch 14, top-1 = 0.96
I1107 01:50:15.284430 36080 caffe_interface.cpp:125] Batch 15, loss = 0.262524
I1107 01:50:15.284440 36080 caffe_interface.cpp:125] Batch 15, top-1 = 0.94
I1107 01:50:15.304416 36080 caffe_interface.cpp:125] Batch 16, loss = 0.23143
I1107 01:50:15.304433 36080 caffe_interface.cpp:125] Batch 16, top-1 = 0.96
I1107 01:50:15.324268 36080 caffe_interface.cpp:125] Batch 17, loss = 0.258211
I1107 01:50:15.324286 36080 caffe_interface.cpp:125] Batch 17, top-1 = 0.98
I1107 01:50:15.342686 36080 caffe_interface.cpp:125] Batch 18, loss = 0.27243
I1107 01:50:15.342702 36080 caffe_interface.cpp:125] Batch 18, top-1 = 0.98
I1107 01:50:15.362143 36080 caffe_interface.cpp:125] Batch 19, loss = 0.239107
I1107 01:50:15.362159 36080 caffe_interface.cpp:125] Batch 19, top-1 = 0.96
I1107 01:50:15.380434 36080 caffe_interface.cpp:125] Batch 20, loss = 0.244046
I1107 01:50:15.380450 36080 caffe_interface.cpp:125] Batch 20, top-1 = 0.98
I1107 01:50:15.399533 36080 caffe_interface.cpp:125] Batch 21, loss = 0.190298
I1107 01:50:15.399549 36080 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I1107 01:50:15.418045 36080 caffe_interface.cpp:125] Batch 22, loss = 0.22044
I1107 01:50:15.418062 36080 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I1107 01:50:15.437345 36080 caffe_interface.cpp:125] Batch 23, loss = 0.230986
I1107 01:50:15.437363 36080 caffe_interface.cpp:125] Batch 23, top-1 = 0.98
I1107 01:50:15.455870 36080 caffe_interface.cpp:125] Batch 24, loss = 0.224851
I1107 01:50:15.455888 36080 caffe_interface.cpp:125] Batch 24, top-1 = 0.98
I1107 01:50:15.475342 36080 caffe_interface.cpp:125] Batch 25, loss = 0.250732
I1107 01:50:15.475360 36080 caffe_interface.cpp:125] Batch 25, top-1 = 0.96
I1107 01:50:15.493070 36080 caffe_interface.cpp:125] Batch 26, loss = 0.247892
I1107 01:50:15.493088 36080 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I1107 01:50:15.512322 36080 caffe_interface.cpp:125] Batch 27, loss = 0.229635
I1107 01:50:15.512349 36080 caffe_interface.cpp:125] Batch 27, top-1 = 0.98
I1107 01:50:15.530591 36080 caffe_interface.cpp:125] Batch 28, loss = 0.297295
I1107 01:50:15.530607 36080 caffe_interface.cpp:125] Batch 28, top-1 = 0.9
I1107 01:50:15.549877 36080 caffe_interface.cpp:125] Batch 29, loss = 0.218297
I1107 01:50:15.549895 36080 caffe_interface.cpp:125] Batch 29, top-1 = 0.98
I1107 01:50:15.567975 36080 caffe_interface.cpp:125] Batch 30, loss = 0.223006
I1107 01:50:15.567991 36080 caffe_interface.cpp:125] Batch 30, top-1 = 1
I1107 01:50:15.587718 36080 caffe_interface.cpp:125] Batch 31, loss = 0.239136
I1107 01:50:15.587734 36080 caffe_interface.cpp:125] Batch 31, top-1 = 0.96
I1107 01:50:15.606681 36080 caffe_interface.cpp:125] Batch 32, loss = 0.234131
I1107 01:50:15.606698 36080 caffe_interface.cpp:125] Batch 32, top-1 = 0.94
I1107 01:50:15.625914 36080 caffe_interface.cpp:125] Batch 33, loss = 0.288774
I1107 01:50:15.625929 36080 caffe_interface.cpp:125] Batch 33, top-1 = 0.94
I1107 01:50:15.644004 36080 caffe_interface.cpp:125] Batch 34, loss = 0.332802
I1107 01:50:15.644037 36080 caffe_interface.cpp:125] Batch 34, top-1 = 0.88
I1107 01:50:15.662761 36080 caffe_interface.cpp:125] Batch 35, loss = 0.206282
I1107 01:50:15.662777 36080 caffe_interface.cpp:125] Batch 35, top-1 = 0.98
I1107 01:50:15.680696 36080 caffe_interface.cpp:125] Batch 36, loss = 0.250466
I1107 01:50:15.680709 36080 caffe_interface.cpp:125] Batch 36, top-1 = 0.92
I1107 01:50:15.699380 36080 caffe_interface.cpp:125] Batch 37, loss = 0.223119
I1107 01:50:15.699395 36080 caffe_interface.cpp:125] Batch 37, top-1 = 0.94
I1107 01:50:15.717483 36080 caffe_interface.cpp:125] Batch 38, loss = 0.248352
I1107 01:50:15.717499 36080 caffe_interface.cpp:125] Batch 38, top-1 = 0.98
I1107 01:50:15.736708 36080 caffe_interface.cpp:125] Batch 39, loss = 0.204509
I1107 01:50:15.736717 36080 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I1107 01:50:15.754745 36080 caffe_interface.cpp:125] Batch 40, loss = 0.313407
I1107 01:50:15.754755 36080 caffe_interface.cpp:125] Batch 40, top-1 = 0.92
I1107 01:50:15.774170 36080 caffe_interface.cpp:125] Batch 41, loss = 0.216859
I1107 01:50:15.774179 36080 caffe_interface.cpp:125] Batch 41, top-1 = 0.98
I1107 01:50:15.792012 36080 caffe_interface.cpp:125] Batch 42, loss = 0.250739
I1107 01:50:15.792021 36080 caffe_interface.cpp:125] Batch 42, top-1 = 1
I1107 01:50:15.811059 36080 caffe_interface.cpp:125] Batch 43, loss = 0.29553
I1107 01:50:15.811067 36080 caffe_interface.cpp:125] Batch 43, top-1 = 0.9
I1107 01:50:15.828917 36080 caffe_interface.cpp:125] Batch 44, loss = 0.252843
I1107 01:50:15.828924 36080 caffe_interface.cpp:125] Batch 44, top-1 = 0.94
I1107 01:50:15.847858 36080 caffe_interface.cpp:125] Batch 45, loss = 0.300529
I1107 01:50:15.847867 36080 caffe_interface.cpp:125] Batch 45, top-1 = 0.88
I1107 01:50:15.866082 36080 caffe_interface.cpp:125] Batch 46, loss = 0.271036
I1107 01:50:15.866091 36080 caffe_interface.cpp:125] Batch 46, top-1 = 0.92
I1107 01:50:15.885562 36080 caffe_interface.cpp:125] Batch 47, loss = 0.239405
I1107 01:50:15.885571 36080 caffe_interface.cpp:125] Batch 47, top-1 = 0.98
I1107 01:50:15.903403 36080 caffe_interface.cpp:125] Batch 48, loss = 0.260583
I1107 01:50:15.903414 36080 caffe_interface.cpp:125] Batch 48, top-1 = 0.96
I1107 01:50:15.922858 36080 caffe_interface.cpp:125] Batch 49, loss = 0.233383
I1107 01:50:15.922866 36080 caffe_interface.cpp:125] Batch 49, top-1 = 1
I1107 01:50:15.940706 36080 caffe_interface.cpp:125] Batch 50, loss = 0.264916
I1107 01:50:15.940714 36080 caffe_interface.cpp:125] Batch 50, top-1 = 0.96
I1107 01:50:15.960155 36080 caffe_interface.cpp:125] Batch 51, loss = 0.313994
I1107 01:50:15.960165 36080 caffe_interface.cpp:125] Batch 51, top-1 = 0.92
I1107 01:50:15.977988 36080 caffe_interface.cpp:125] Batch 52, loss = 0.256033
I1107 01:50:15.977998 36080 caffe_interface.cpp:125] Batch 52, top-1 = 0.94
I1107 01:50:15.997064 36080 caffe_interface.cpp:125] Batch 53, loss = 0.22557
I1107 01:50:15.997073 36080 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I1107 01:50:16.015379 36080 caffe_interface.cpp:125] Batch 54, loss = 0.215439
I1107 01:50:16.015388 36080 caffe_interface.cpp:125] Batch 54, top-1 = 0.98
I1107 01:50:16.034637 36080 caffe_interface.cpp:125] Batch 55, loss = 0.287229
I1107 01:50:16.034646 36080 caffe_interface.cpp:125] Batch 55, top-1 = 0.92
I1107 01:50:16.053078 36080 caffe_interface.cpp:125] Batch 56, loss = 0.255445
I1107 01:50:16.053087 36080 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 01:50:16.072417 36080 caffe_interface.cpp:125] Batch 57, loss = 0.238802
I1107 01:50:16.072424 36080 caffe_interface.cpp:125] Batch 57, top-1 = 0.94
I1107 01:50:16.090620 36080 caffe_interface.cpp:125] Batch 58, loss = 0.27426
I1107 01:50:16.090628 36080 caffe_interface.cpp:125] Batch 58, top-1 = 0.94
I1107 01:50:16.110064 36080 caffe_interface.cpp:125] Batch 59, loss = 0.279506
I1107 01:50:16.110074 36080 caffe_interface.cpp:125] Batch 59, top-1 = 0.88
I1107 01:50:16.127888 36080 caffe_interface.cpp:125] Batch 60, loss = 0.271357
I1107 01:50:16.127897 36080 caffe_interface.cpp:125] Batch 60, top-1 = 0.9
I1107 01:50:16.147110 36080 caffe_interface.cpp:125] Batch 61, loss = 0.271174
I1107 01:50:16.147120 36080 caffe_interface.cpp:125] Batch 61, top-1 = 0.94
I1107 01:50:16.165314 36080 caffe_interface.cpp:125] Batch 62, loss = 0.223811
I1107 01:50:16.165323 36080 caffe_interface.cpp:125] Batch 62, top-1 = 0.98
I1107 01:50:16.184546 36080 caffe_interface.cpp:125] Batch 63, loss = 0.283941
I1107 01:50:16.184554 36080 caffe_interface.cpp:125] Batch 63, top-1 = 0.96
I1107 01:50:16.203171 36080 caffe_interface.cpp:125] Batch 64, loss = 0.383686
I1107 01:50:16.203179 36080 caffe_interface.cpp:125] Batch 64, top-1 = 0.78
I1107 01:50:16.222928 36080 caffe_interface.cpp:125] Batch 65, loss = 0.256617
I1107 01:50:16.222937 36080 caffe_interface.cpp:125] Batch 65, top-1 = 0.98
I1107 01:50:16.241864 36080 caffe_interface.cpp:125] Batch 66, loss = 0.304979
I1107 01:50:16.241873 36080 caffe_interface.cpp:125] Batch 66, top-1 = 0.92
I1107 01:50:16.260403 36080 caffe_interface.cpp:125] Batch 67, loss = 0.272395
I1107 01:50:16.260412 36080 caffe_interface.cpp:125] Batch 67, top-1 = 0.94
I1107 01:50:16.280321 36080 caffe_interface.cpp:125] Batch 68, loss = 0.2689
I1107 01:50:16.280330 36080 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I1107 01:50:16.298537 36080 caffe_interface.cpp:125] Batch 69, loss = 0.225662
I1107 01:50:16.298545 36080 caffe_interface.cpp:125] Batch 69, top-1 = 0.96
I1107 01:50:16.318236 36080 caffe_interface.cpp:125] Batch 70, loss = 0.195394
I1107 01:50:16.318244 36080 caffe_interface.cpp:125] Batch 70, top-1 = 0.98
I1107 01:50:16.336697 36080 caffe_interface.cpp:125] Batch 71, loss = 0.236778
I1107 01:50:16.336706 36080 caffe_interface.cpp:125] Batch 71, top-1 = 0.98
I1107 01:50:16.355360 36080 caffe_interface.cpp:125] Batch 72, loss = 0.242571
I1107 01:50:16.355368 36080 caffe_interface.cpp:125] Batch 72, top-1 = 0.94
I1107 01:50:16.373184 36080 caffe_interface.cpp:125] Batch 73, loss = 0.230019
I1107 01:50:16.373193 36080 caffe_interface.cpp:125] Batch 73, top-1 = 0.96
I1107 01:50:16.392271 36080 caffe_interface.cpp:125] Batch 74, loss = 0.228658
I1107 01:50:16.392279 36080 caffe_interface.cpp:125] Batch 74, top-1 = 0.98
I1107 01:50:16.410516 36080 caffe_interface.cpp:125] Batch 75, loss = 0.253417
I1107 01:50:16.410524 36080 caffe_interface.cpp:125] Batch 75, top-1 = 0.94
I1107 01:50:16.429407 36080 caffe_interface.cpp:125] Batch 76, loss = 0.278414
I1107 01:50:16.429415 36080 caffe_interface.cpp:125] Batch 76, top-1 = 0.94
I1107 01:50:16.447427 36080 caffe_interface.cpp:125] Batch 77, loss = 0.267461
I1107 01:50:16.447435 36080 caffe_interface.cpp:125] Batch 77, top-1 = 0.9
I1107 01:50:16.466729 36080 caffe_interface.cpp:125] Batch 78, loss = 0.265593
I1107 01:50:16.466737 36080 caffe_interface.cpp:125] Batch 78, top-1 = 0.94
I1107 01:50:16.485146 36080 caffe_interface.cpp:125] Batch 79, loss = 0.20082
I1107 01:50:16.485152 36080 caffe_interface.cpp:125] Batch 79, top-1 = 0.98
I1107 01:50:16.485155 36080 caffe_interface.cpp:130] Loss: 0.257515
I1107 01:50:16.485160 36080 caffe_interface.cpp:142] loss = 0.257515 (* 1 = 0.257515 loss)
I1107 01:50:16.485164 36080 caffe_interface.cpp:142] top-1 = 0.94675
I1107 01:50:16.725997 36080 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/sparse.caffemodel
I1107 01:50:16.726020 36080 pruning_runner.cpp:320] summary of REGULAR compression with rate 0:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.946749866    | 0              |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 3764995        | 0%             |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 2153918368     | 0%             |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config0.prototxt
I1107 01:50:16.961256 36240 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/net_finetune.prototxt
I1107 01:50:17.140511 36240 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 01:50:17.141016 36240 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24907481088, dev_info[0]: total=25620447232 free=24907481088
I1107 01:50:17.141028 36240 caffe_interface.cpp:493] Using GPUs 0
I1107 01:50:17.141276 36240 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 01:50:17.728754 36240 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/net_finetune.prototxt"
type: "Adam"
I1107 01:50:17.728860 36240 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/net_finetune.prototxt
I1107 01:50:17.729091 36240 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 01:50:17.729109 36240 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 01:50:17.729259 36240 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 01:50:17.729331 36240 layer_factory.hpp:77] Creating layer data
I1107 01:50:17.729460 36240 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:17.730088 36240 net.cpp:94] Creating Layer data
I1107 01:50:17.730099 36240 net.cpp:409] data -> data
I1107 01:50:17.730113 36240 net.cpp:409] data -> label
I1107 01:50:17.731422 36279 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 01:50:17.731465 36279 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 01:50:17.731720 36240 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 01:50:17.731791 36240 data_layer.cpp:83] output data size: 256,3,227,227
I1107 01:50:18.100697 36240 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:18.100772 36240 net.cpp:144] Setting up data
I1107 01:50:18.100782 36240 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 01:50:18.100787 36240 net.cpp:151] Top shape: 256 (256)
I1107 01:50:18.100790 36240 net.cpp:159] Memory required for data: 158298112
I1107 01:50:18.100798 36240 layer_factory.hpp:77] Creating layer conv1
I1107 01:50:18.100831 36240 net.cpp:94] Creating Layer conv1
I1107 01:50:18.100836 36240 net.cpp:435] conv1 <- data
I1107 01:50:18.100857 36240 net.cpp:409] conv1 -> conv1
I1107 01:50:18.102741 36240 net.cpp:144] Setting up conv1
I1107 01:50:18.102754 36240 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 01:50:18.102757 36240 net.cpp:159] Memory required for data: 455667712
I1107 01:50:18.102790 36240 layer_factory.hpp:77] Creating layer bn1
I1107 01:50:18.102803 36240 net.cpp:94] Creating Layer bn1
I1107 01:50:18.102807 36240 net.cpp:435] bn1 <- conv1
I1107 01:50:18.102814 36240 net.cpp:409] bn1 -> scale1
I1107 01:50:18.104099 36240 net.cpp:144] Setting up bn1
I1107 01:50:18.104105 36240 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 01:50:18.104109 36240 net.cpp:159] Memory required for data: 753037312
I1107 01:50:18.104120 36240 layer_factory.hpp:77] Creating layer relu1
I1107 01:50:18.104126 36240 net.cpp:94] Creating Layer relu1
I1107 01:50:18.104131 36240 net.cpp:435] relu1 <- scale1
I1107 01:50:18.104136 36240 net.cpp:409] relu1 -> relu1
I1107 01:50:18.104216 36240 net.cpp:144] Setting up relu1
I1107 01:50:18.104223 36240 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 01:50:18.104226 36240 net.cpp:159] Memory required for data: 1050406912
I1107 01:50:18.104230 36240 layer_factory.hpp:77] Creating layer pool1
I1107 01:50:18.104238 36240 net.cpp:94] Creating Layer pool1
I1107 01:50:18.104241 36240 net.cpp:435] pool1 <- relu1
I1107 01:50:18.104248 36240 net.cpp:409] pool1 -> pool1
I1107 01:50:18.104285 36240 net.cpp:144] Setting up pool1
I1107 01:50:18.104290 36240 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 01:50:18.104293 36240 net.cpp:159] Memory required for data: 1122070528
I1107 01:50:18.104297 36240 layer_factory.hpp:77] Creating layer conv2
I1107 01:50:18.104306 36240 net.cpp:94] Creating Layer conv2
I1107 01:50:18.104310 36240 net.cpp:435] conv2 <- pool1
I1107 01:50:18.104316 36240 net.cpp:409] conv2 -> conv2
I1107 01:50:18.118978 36240 net.cpp:144] Setting up conv2
I1107 01:50:18.119005 36240 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 01:50:18.119010 36240 net.cpp:159] Memory required for data: 1313173504
I1107 01:50:18.119025 36240 layer_factory.hpp:77] Creating layer bn2
I1107 01:50:18.119040 36240 net.cpp:94] Creating Layer bn2
I1107 01:50:18.119045 36240 net.cpp:435] bn2 <- conv2
I1107 01:50:18.119055 36240 net.cpp:409] bn2 -> scale2
I1107 01:50:18.119626 36240 net.cpp:144] Setting up bn2
I1107 01:50:18.119634 36240 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 01:50:18.119638 36240 net.cpp:159] Memory required for data: 1504276480
I1107 01:50:18.119650 36240 layer_factory.hpp:77] Creating layer relu2
I1107 01:50:18.119668 36240 net.cpp:94] Creating Layer relu2
I1107 01:50:18.119673 36240 net.cpp:435] relu2 <- scale2
I1107 01:50:18.119680 36240 net.cpp:409] relu2 -> relu2
I1107 01:50:18.119709 36240 net.cpp:144] Setting up relu2
I1107 01:50:18.119715 36240 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 01:50:18.119720 36240 net.cpp:159] Memory required for data: 1695379456
I1107 01:50:18.119724 36240 layer_factory.hpp:77] Creating layer pool2
I1107 01:50:18.119735 36240 net.cpp:94] Creating Layer pool2
I1107 01:50:18.119738 36240 net.cpp:435] pool2 <- relu2
I1107 01:50:18.119756 36240 net.cpp:409] pool2 -> pool2
I1107 01:50:18.119788 36240 net.cpp:144] Setting up pool2
I1107 01:50:18.119796 36240 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 01:50:18.119801 36240 net.cpp:159] Memory required for data: 1739681792
I1107 01:50:18.119805 36240 layer_factory.hpp:77] Creating layer conv3
I1107 01:50:18.119817 36240 net.cpp:94] Creating Layer conv3
I1107 01:50:18.119822 36240 net.cpp:435] conv3 <- pool2
I1107 01:50:18.119828 36240 net.cpp:409] conv3 -> conv3
I1107 01:50:18.129686 36240 net.cpp:144] Setting up conv3
I1107 01:50:18.129724 36240 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 01:50:18.129729 36240 net.cpp:159] Memory required for data: 1806135296
I1107 01:50:18.129741 36240 layer_factory.hpp:77] Creating layer relu3
I1107 01:50:18.129755 36240 net.cpp:94] Creating Layer relu3
I1107 01:50:18.129760 36240 net.cpp:435] relu3 <- conv3
I1107 01:50:18.129787 36240 net.cpp:409] relu3 -> relu3
I1107 01:50:18.129846 36240 net.cpp:144] Setting up relu3
I1107 01:50:18.129868 36240 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 01:50:18.129886 36240 net.cpp:159] Memory required for data: 1872588800
I1107 01:50:18.129909 36240 layer_factory.hpp:77] Creating layer conv4
I1107 01:50:18.129932 36240 net.cpp:94] Creating Layer conv4
I1107 01:50:18.129946 36240 net.cpp:435] conv4 <- relu3
I1107 01:50:18.129962 36240 net.cpp:409] conv4 -> conv4
I1107 01:50:18.160538 36240 net.cpp:144] Setting up conv4
I1107 01:50:18.160604 36240 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 01:50:18.160617 36240 net.cpp:159] Memory required for data: 1939042304
I1107 01:50:18.160646 36240 layer_factory.hpp:77] Creating layer relu4
I1107 01:50:18.160667 36240 net.cpp:94] Creating Layer relu4
I1107 01:50:18.160681 36240 net.cpp:435] relu4 <- conv4
I1107 01:50:18.160706 36240 net.cpp:409] relu4 -> relu4
I1107 01:50:18.160758 36240 net.cpp:144] Setting up relu4
I1107 01:50:18.160778 36240 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 01:50:18.160790 36240 net.cpp:159] Memory required for data: 2005495808
I1107 01:50:18.160804 36240 layer_factory.hpp:77] Creating layer conv5
I1107 01:50:18.160826 36240 net.cpp:94] Creating Layer conv5
I1107 01:50:18.160841 36240 net.cpp:435] conv5 <- relu4
I1107 01:50:18.160858 36240 net.cpp:409] conv5 -> conv5
I1107 01:50:18.198796 36240 net.cpp:144] Setting up conv5
I1107 01:50:18.198843 36240 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 01:50:18.198858 36240 net.cpp:159] Memory required for data: 2049798144
I1107 01:50:18.198877 36240 layer_factory.hpp:77] Creating layer relu5
I1107 01:50:18.198894 36240 net.cpp:94] Creating Layer relu5
I1107 01:50:18.198905 36240 net.cpp:435] relu5 <- conv5
I1107 01:50:18.198920 36240 net.cpp:409] relu5 -> relu5
I1107 01:50:18.198983 36240 net.cpp:144] Setting up relu5
I1107 01:50:18.198999 36240 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 01:50:18.199007 36240 net.cpp:159] Memory required for data: 2094100480
I1107 01:50:18.199013 36240 layer_factory.hpp:77] Creating layer pool5
I1107 01:50:18.199033 36240 net.cpp:94] Creating Layer pool5
I1107 01:50:18.199041 36240 net.cpp:435] pool5 <- relu5
I1107 01:50:18.199054 36240 net.cpp:409] pool5 -> pool5
I1107 01:50:18.199134 36240 net.cpp:144] Setting up pool5
I1107 01:50:18.199149 36240 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 01:50:18.199156 36240 net.cpp:159] Memory required for data: 2103537664
I1107 01:50:18.199162 36240 layer_factory.hpp:77] Creating layer fc6
I1107 01:50:18.199179 36240 net.cpp:94] Creating Layer fc6
I1107 01:50:18.199187 36240 net.cpp:435] fc6 <- pool5
I1107 01:50:18.199198 36240 net.cpp:409] fc6 -> fc6
I1107 01:50:18.531431 36240 net.cpp:144] Setting up fc6
I1107 01:50:18.531456 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.531460 36240 net.cpp:159] Memory required for data: 2107731968
I1107 01:50:18.531483 36240 layer_factory.hpp:77] Creating layer relu6
I1107 01:50:18.531499 36240 net.cpp:94] Creating Layer relu6
I1107 01:50:18.531503 36240 net.cpp:435] relu6 <- fc6
I1107 01:50:18.531530 36240 net.cpp:409] relu6 -> relu6
I1107 01:50:18.531548 36240 net.cpp:144] Setting up relu6
I1107 01:50:18.531551 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.531554 36240 net.cpp:159] Memory required for data: 2111926272
I1107 01:50:18.531556 36240 layer_factory.hpp:77] Creating layer drop6
I1107 01:50:18.531563 36240 net.cpp:94] Creating Layer drop6
I1107 01:50:18.531564 36240 net.cpp:435] drop6 <- relu6
I1107 01:50:18.531569 36240 net.cpp:409] drop6 -> drop6
I1107 01:50:18.531590 36240 net.cpp:144] Setting up drop6
I1107 01:50:18.531595 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.531597 36240 net.cpp:159] Memory required for data: 2116120576
I1107 01:50:18.531615 36240 layer_factory.hpp:77] Creating layer fc7
I1107 01:50:18.531620 36240 net.cpp:94] Creating Layer fc7
I1107 01:50:18.531623 36240 net.cpp:435] fc7 <- drop6
I1107 01:50:18.531627 36240 net.cpp:409] fc7 -> fc7
I1107 01:50:18.666527 36240 net.cpp:144] Setting up fc7
I1107 01:50:18.666550 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.666569 36240 net.cpp:159] Memory required for data: 2120314880
I1107 01:50:18.666576 36240 layer_factory.hpp:77] Creating layer bn7
I1107 01:50:18.666585 36240 net.cpp:94] Creating Layer bn7
I1107 01:50:18.666589 36240 net.cpp:435] bn7 <- fc7
I1107 01:50:18.666594 36240 net.cpp:409] bn7 -> scale7
I1107 01:50:18.667109 36240 net.cpp:144] Setting up bn7
I1107 01:50:18.667115 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.667117 36240 net.cpp:159] Memory required for data: 2124509184
I1107 01:50:18.667124 36240 layer_factory.hpp:77] Creating layer relu7
I1107 01:50:18.667129 36240 net.cpp:94] Creating Layer relu7
I1107 01:50:18.667131 36240 net.cpp:435] relu7 <- scale7
I1107 01:50:18.667135 36240 net.cpp:409] relu7 -> relu7
I1107 01:50:18.667152 36240 net.cpp:144] Setting up relu7
I1107 01:50:18.667157 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.667160 36240 net.cpp:159] Memory required for data: 2128703488
I1107 01:50:18.667161 36240 layer_factory.hpp:77] Creating layer drop7
I1107 01:50:18.667166 36240 net.cpp:94] Creating Layer drop7
I1107 01:50:18.667170 36240 net.cpp:435] drop7 <- relu7
I1107 01:50:18.667172 36240 net.cpp:409] drop7 -> drop7
I1107 01:50:18.667196 36240 net.cpp:144] Setting up drop7
I1107 01:50:18.667201 36240 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 01:50:18.667202 36240 net.cpp:159] Memory required for data: 2132897792
I1107 01:50:18.667204 36240 layer_factory.hpp:77] Creating layer fc8
I1107 01:50:18.667209 36240 net.cpp:94] Creating Layer fc8
I1107 01:50:18.667212 36240 net.cpp:435] fc8 <- drop7
I1107 01:50:18.667217 36240 net.cpp:409] fc8 -> fc8
I1107 01:50:18.668082 36240 net.cpp:144] Setting up fc8
I1107 01:50:18.668093 36240 net.cpp:151] Top shape: 256 2 (512)
I1107 01:50:18.668097 36240 net.cpp:159] Memory required for data: 2132899840
I1107 01:50:18.668102 36240 layer_factory.hpp:77] Creating layer loss
I1107 01:50:18.668108 36240 net.cpp:94] Creating Layer loss
I1107 01:50:18.668112 36240 net.cpp:435] loss <- fc8
I1107 01:50:18.668115 36240 net.cpp:435] loss <- label
I1107 01:50:18.668119 36240 net.cpp:409] loss -> loss
I1107 01:50:18.668126 36240 layer_factory.hpp:77] Creating layer loss
I1107 01:50:18.668186 36240 net.cpp:144] Setting up loss
I1107 01:50:18.668191 36240 net.cpp:151] Top shape: (1)
I1107 01:50:18.668193 36240 net.cpp:154]     with loss weight 1
I1107 01:50:18.668205 36240 net.cpp:159] Memory required for data: 2132899844
I1107 01:50:18.668206 36240 net.cpp:220] loss needs backward computation.
I1107 01:50:18.668220 36240 net.cpp:220] fc8 needs backward computation.
I1107 01:50:18.668222 36240 net.cpp:220] drop7 needs backward computation.
I1107 01:50:18.668226 36240 net.cpp:220] relu7 needs backward computation.
I1107 01:50:18.668227 36240 net.cpp:220] bn7 needs backward computation.
I1107 01:50:18.668231 36240 net.cpp:220] fc7 needs backward computation.
I1107 01:50:18.668233 36240 net.cpp:220] drop6 needs backward computation.
I1107 01:50:18.668236 36240 net.cpp:220] relu6 needs backward computation.
I1107 01:50:18.668251 36240 net.cpp:220] fc6 needs backward computation.
I1107 01:50:18.668253 36240 net.cpp:220] pool5 needs backward computation.
I1107 01:50:18.668256 36240 net.cpp:220] relu5 needs backward computation.
I1107 01:50:18.668258 36240 net.cpp:220] conv5 needs backward computation.
I1107 01:50:18.668262 36240 net.cpp:220] relu4 needs backward computation.
I1107 01:50:18.668264 36240 net.cpp:220] conv4 needs backward computation.
I1107 01:50:18.668267 36240 net.cpp:220] relu3 needs backward computation.
I1107 01:50:18.668269 36240 net.cpp:220] conv3 needs backward computation.
I1107 01:50:18.668272 36240 net.cpp:220] pool2 needs backward computation.
I1107 01:50:18.668275 36240 net.cpp:220] relu2 needs backward computation.
I1107 01:50:18.668277 36240 net.cpp:220] bn2 needs backward computation.
I1107 01:50:18.668280 36240 net.cpp:220] conv2 needs backward computation.
I1107 01:50:18.668282 36240 net.cpp:220] pool1 needs backward computation.
I1107 01:50:18.668285 36240 net.cpp:220] relu1 needs backward computation.
I1107 01:50:18.668287 36240 net.cpp:220] bn1 needs backward computation.
I1107 01:50:18.668290 36240 net.cpp:220] conv1 needs backward computation.
I1107 01:50:18.668294 36240 net.cpp:222] data does not need backward computation.
I1107 01:50:18.668298 36240 net.cpp:264] This network produces output loss
I1107 01:50:18.668313 36240 net.cpp:284] Network initialization done.
I1107 01:50:18.668586 36240 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/net_finetune.prototxt
I1107 01:50:18.668613 36240 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 01:50:18.668782 36240 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 01:50:18.668865 36240 layer_factory.hpp:77] Creating layer data
I1107 01:50:18.668901 36240 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:18.669870 36240 net.cpp:94] Creating Layer data
I1107 01:50:18.669880 36240 net.cpp:409] data -> data
I1107 01:50:18.669889 36240 net.cpp:409] data -> label
I1107 01:50:18.671061 36309 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 01:50:18.671097 36309 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 01:50:18.671366 36240 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 01:50:18.671444 36240 data_layer.cpp:83] output data size: 50,3,227,227
I1107 01:50:18.746668 36240 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 01:50:18.746738 36240 net.cpp:144] Setting up data
I1107 01:50:18.746747 36240 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 01:50:18.746749 36240 net.cpp:151] Top shape: 50 (50)
I1107 01:50:18.746752 36240 net.cpp:159] Memory required for data: 30917600
I1107 01:50:18.746755 36240 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 01:50:18.746764 36240 net.cpp:94] Creating Layer label_data_1_split
I1107 01:50:18.746783 36240 net.cpp:435] label_data_1_split <- label
I1107 01:50:18.746790 36240 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 01:50:18.746798 36240 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 01:50:18.746855 36240 net.cpp:144] Setting up label_data_1_split
I1107 01:50:18.746860 36240 net.cpp:151] Top shape: 50 (50)
I1107 01:50:18.746862 36240 net.cpp:151] Top shape: 50 (50)
I1107 01:50:18.746863 36240 net.cpp:159] Memory required for data: 30918000
I1107 01:50:18.746866 36240 layer_factory.hpp:77] Creating layer conv1
I1107 01:50:18.746876 36240 net.cpp:94] Creating Layer conv1
I1107 01:50:18.746888 36240 net.cpp:435] conv1 <- data
I1107 01:50:18.746893 36240 net.cpp:409] conv1 -> conv1
I1107 01:50:18.747486 36240 net.cpp:144] Setting up conv1
I1107 01:50:18.747493 36240 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:18.747495 36240 net.cpp:159] Memory required for data: 88998000
I1107 01:50:18.747504 36240 layer_factory.hpp:77] Creating layer bn1
I1107 01:50:18.747514 36240 net.cpp:94] Creating Layer bn1
I1107 01:50:18.747516 36240 net.cpp:435] bn1 <- conv1
I1107 01:50:18.747520 36240 net.cpp:409] bn1 -> scale1
I1107 01:50:18.748091 36240 net.cpp:144] Setting up bn1
I1107 01:50:18.748098 36240 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:18.748101 36240 net.cpp:159] Memory required for data: 147078000
I1107 01:50:18.748111 36240 layer_factory.hpp:77] Creating layer relu1
I1107 01:50:18.748116 36240 net.cpp:94] Creating Layer relu1
I1107 01:50:18.748121 36240 net.cpp:435] relu1 <- scale1
I1107 01:50:18.748126 36240 net.cpp:409] relu1 -> relu1
I1107 01:50:18.748142 36240 net.cpp:144] Setting up relu1
I1107 01:50:18.748147 36240 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 01:50:18.748150 36240 net.cpp:159] Memory required for data: 205158000
I1107 01:50:18.748152 36240 layer_factory.hpp:77] Creating layer pool1
I1107 01:50:18.748158 36240 net.cpp:94] Creating Layer pool1
I1107 01:50:18.748162 36240 net.cpp:435] pool1 <- relu1
I1107 01:50:18.748164 36240 net.cpp:409] pool1 -> pool1
I1107 01:50:18.748193 36240 net.cpp:144] Setting up pool1
I1107 01:50:18.748198 36240 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 01:50:18.748199 36240 net.cpp:159] Memory required for data: 219154800
I1107 01:50:18.748203 36240 layer_factory.hpp:77] Creating layer conv2
I1107 01:50:18.748209 36240 net.cpp:94] Creating Layer conv2
I1107 01:50:18.748219 36240 net.cpp:435] conv2 <- pool1
I1107 01:50:18.748222 36240 net.cpp:409] conv2 -> conv2
I1107 01:50:18.755542 36240 net.cpp:144] Setting up conv2
I1107 01:50:18.755563 36240 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:18.755581 36240 net.cpp:159] Memory required for data: 256479600
I1107 01:50:18.755594 36240 layer_factory.hpp:77] Creating layer bn2
I1107 01:50:18.755607 36240 net.cpp:94] Creating Layer bn2
I1107 01:50:18.755614 36240 net.cpp:435] bn2 <- conv2
I1107 01:50:18.755621 36240 net.cpp:409] bn2 -> scale2
I1107 01:50:18.756264 36240 net.cpp:144] Setting up bn2
I1107 01:50:18.756271 36240 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:18.756274 36240 net.cpp:159] Memory required for data: 293804400
I1107 01:50:18.756283 36240 layer_factory.hpp:77] Creating layer relu2
I1107 01:50:18.756291 36240 net.cpp:94] Creating Layer relu2
I1107 01:50:18.756296 36240 net.cpp:435] relu2 <- scale2
I1107 01:50:18.756302 36240 net.cpp:409] relu2 -> relu2
I1107 01:50:18.756321 36240 net.cpp:144] Setting up relu2
I1107 01:50:18.756326 36240 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 01:50:18.756330 36240 net.cpp:159] Memory required for data: 331129200
I1107 01:50:18.756332 36240 layer_factory.hpp:77] Creating layer pool2
I1107 01:50:18.756338 36240 net.cpp:94] Creating Layer pool2
I1107 01:50:18.756341 36240 net.cpp:435] pool2 <- relu2
I1107 01:50:18.756347 36240 net.cpp:409] pool2 -> pool2
I1107 01:50:18.756378 36240 net.cpp:144] Setting up pool2
I1107 01:50:18.756383 36240 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:18.756386 36240 net.cpp:159] Memory required for data: 339782000
I1107 01:50:18.756389 36240 layer_factory.hpp:77] Creating layer conv3
I1107 01:50:18.756399 36240 net.cpp:94] Creating Layer conv3
I1107 01:50:18.756403 36240 net.cpp:435] conv3 <- pool2
I1107 01:50:18.756408 36240 net.cpp:409] conv3 -> conv3
I1107 01:50:18.765950 36240 net.cpp:144] Setting up conv3
I1107 01:50:18.765974 36240 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:18.765977 36240 net.cpp:159] Memory required for data: 352761200
I1107 01:50:18.765986 36240 layer_factory.hpp:77] Creating layer relu3
I1107 01:50:18.765997 36240 net.cpp:94] Creating Layer relu3
I1107 01:50:18.766006 36240 net.cpp:435] relu3 <- conv3
I1107 01:50:18.766014 36240 net.cpp:409] relu3 -> relu3
I1107 01:50:18.766052 36240 net.cpp:144] Setting up relu3
I1107 01:50:18.766068 36240 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:18.766072 36240 net.cpp:159] Memory required for data: 365740400
I1107 01:50:18.766074 36240 layer_factory.hpp:77] Creating layer conv4
I1107 01:50:18.766088 36240 net.cpp:94] Creating Layer conv4
I1107 01:50:18.766093 36240 net.cpp:435] conv4 <- relu3
I1107 01:50:18.766104 36240 net.cpp:409] conv4 -> conv4
I1107 01:50:18.779240 36240 net.cpp:144] Setting up conv4
I1107 01:50:18.779270 36240 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:18.779274 36240 net.cpp:159] Memory required for data: 378719600
I1107 01:50:18.779290 36240 layer_factory.hpp:77] Creating layer relu4
I1107 01:50:18.779299 36240 net.cpp:94] Creating Layer relu4
I1107 01:50:18.779305 36240 net.cpp:435] relu4 <- conv4
I1107 01:50:18.779316 36240 net.cpp:409] relu4 -> relu4
I1107 01:50:18.779345 36240 net.cpp:144] Setting up relu4
I1107 01:50:18.779355 36240 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 01:50:18.779357 36240 net.cpp:159] Memory required for data: 391698800
I1107 01:50:18.779361 36240 layer_factory.hpp:77] Creating layer conv5
I1107 01:50:18.779369 36240 net.cpp:94] Creating Layer conv5
I1107 01:50:18.779372 36240 net.cpp:435] conv5 <- relu4
I1107 01:50:18.779377 36240 net.cpp:409] conv5 -> conv5
I1107 01:50:18.792448 36240 net.cpp:144] Setting up conv5
I1107 01:50:18.792488 36240 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:18.792491 36240 net.cpp:159] Memory required for data: 400351600
I1107 01:50:18.792500 36240 layer_factory.hpp:77] Creating layer relu5
I1107 01:50:18.792526 36240 net.cpp:94] Creating Layer relu5
I1107 01:50:18.792531 36240 net.cpp:435] relu5 <- conv5
I1107 01:50:18.792537 36240 net.cpp:409] relu5 -> relu5
I1107 01:50:18.792570 36240 net.cpp:144] Setting up relu5
I1107 01:50:18.792574 36240 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 01:50:18.792603 36240 net.cpp:159] Memory required for data: 409004400
I1107 01:50:18.792608 36240 layer_factory.hpp:77] Creating layer pool5
I1107 01:50:18.792618 36240 net.cpp:94] Creating Layer pool5
I1107 01:50:18.792623 36240 net.cpp:435] pool5 <- relu5
I1107 01:50:18.792630 36240 net.cpp:409] pool5 -> pool5
I1107 01:50:18.792672 36240 net.cpp:144] Setting up pool5
I1107 01:50:18.792680 36240 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 01:50:18.792683 36240 net.cpp:159] Memory required for data: 410847600
I1107 01:50:18.792687 36240 layer_factory.hpp:77] Creating layer fc6
I1107 01:50:18.792699 36240 net.cpp:94] Creating Layer fc6
I1107 01:50:18.792706 36240 net.cpp:435] fc6 <- pool5
I1107 01:50:18.792713 36240 net.cpp:409] fc6 -> fc6
I1107 01:50:19.116019 36240 net.cpp:144] Setting up fc6
I1107 01:50:19.116042 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.116045 36240 net.cpp:159] Memory required for data: 411666800
I1107 01:50:19.116055 36240 layer_factory.hpp:77] Creating layer relu6
I1107 01:50:19.116075 36240 net.cpp:94] Creating Layer relu6
I1107 01:50:19.116080 36240 net.cpp:435] relu6 <- fc6
I1107 01:50:19.116088 36240 net.cpp:409] relu6 -> relu6
I1107 01:50:19.116117 36240 net.cpp:144] Setting up relu6
I1107 01:50:19.116122 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.116125 36240 net.cpp:159] Memory required for data: 412486000
I1107 01:50:19.116128 36240 layer_factory.hpp:77] Creating layer drop6
I1107 01:50:19.116137 36240 net.cpp:94] Creating Layer drop6
I1107 01:50:19.116142 36240 net.cpp:435] drop6 <- relu6
I1107 01:50:19.116161 36240 net.cpp:409] drop6 -> drop6
I1107 01:50:19.116192 36240 net.cpp:144] Setting up drop6
I1107 01:50:19.116199 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.116201 36240 net.cpp:159] Memory required for data: 413305200
I1107 01:50:19.116204 36240 layer_factory.hpp:77] Creating layer fc7
I1107 01:50:19.116214 36240 net.cpp:94] Creating Layer fc7
I1107 01:50:19.116217 36240 net.cpp:435] fc7 <- drop6
I1107 01:50:19.116225 36240 net.cpp:409] fc7 -> fc7
I1107 01:50:19.255380 36240 net.cpp:144] Setting up fc7
I1107 01:50:19.255404 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.255409 36240 net.cpp:159] Memory required for data: 414124400
I1107 01:50:19.255417 36240 layer_factory.hpp:77] Creating layer bn7
I1107 01:50:19.255429 36240 net.cpp:94] Creating Layer bn7
I1107 01:50:19.255434 36240 net.cpp:435] bn7 <- fc7
I1107 01:50:19.255445 36240 net.cpp:409] bn7 -> scale7
I1107 01:50:19.256006 36240 net.cpp:144] Setting up bn7
I1107 01:50:19.256012 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.256016 36240 net.cpp:159] Memory required for data: 414943600
I1107 01:50:19.256026 36240 layer_factory.hpp:77] Creating layer relu7
I1107 01:50:19.256033 36240 net.cpp:94] Creating Layer relu7
I1107 01:50:19.256038 36240 net.cpp:435] relu7 <- scale7
I1107 01:50:19.256044 36240 net.cpp:409] relu7 -> relu7
I1107 01:50:19.256067 36240 net.cpp:144] Setting up relu7
I1107 01:50:19.256072 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.256075 36240 net.cpp:159] Memory required for data: 415762800
I1107 01:50:19.256078 36240 layer_factory.hpp:77] Creating layer drop7
I1107 01:50:19.256088 36240 net.cpp:94] Creating Layer drop7
I1107 01:50:19.256091 36240 net.cpp:435] drop7 <- relu7
I1107 01:50:19.256096 36240 net.cpp:409] drop7 -> drop7
I1107 01:50:19.256126 36240 net.cpp:144] Setting up drop7
I1107 01:50:19.256132 36240 net.cpp:151] Top shape: 50 4096 (204800)
I1107 01:50:19.256134 36240 net.cpp:159] Memory required for data: 416582000
I1107 01:50:19.256137 36240 layer_factory.hpp:77] Creating layer fc8
I1107 01:50:19.256146 36240 net.cpp:94] Creating Layer fc8
I1107 01:50:19.256150 36240 net.cpp:435] fc8 <- drop7
I1107 01:50:19.256155 36240 net.cpp:409] fc8 -> fc8
I1107 01:50:19.256335 36240 net.cpp:144] Setting up fc8
I1107 01:50:19.256340 36240 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:19.256343 36240 net.cpp:159] Memory required for data: 416582400
I1107 01:50:19.256361 36240 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 01:50:19.256367 36240 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 01:50:19.256371 36240 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 01:50:19.256381 36240 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 01:50:19.256388 36240 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 01:50:19.256417 36240 net.cpp:144] Setting up fc8_fc8_0_split
I1107 01:50:19.256422 36240 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:19.256426 36240 net.cpp:151] Top shape: 50 2 (100)
I1107 01:50:19.256429 36240 net.cpp:159] Memory required for data: 416583200
I1107 01:50:19.256433 36240 layer_factory.hpp:77] Creating layer loss
I1107 01:50:19.256441 36240 net.cpp:94] Creating Layer loss
I1107 01:50:19.256445 36240 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 01:50:19.256449 36240 net.cpp:435] loss <- label_data_1_split_0
I1107 01:50:19.256456 36240 net.cpp:409] loss -> loss
I1107 01:50:19.256466 36240 layer_factory.hpp:77] Creating layer loss
I1107 01:50:19.256546 36240 net.cpp:144] Setting up loss
I1107 01:50:19.256551 36240 net.cpp:151] Top shape: (1)
I1107 01:50:19.256553 36240 net.cpp:154]     with loss weight 1
I1107 01:50:19.256564 36240 net.cpp:159] Memory required for data: 416583204
I1107 01:50:19.256568 36240 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 01:50:19.256574 36240 net.cpp:94] Creating Layer accuracy-top1
I1107 01:50:19.256578 36240 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 01:50:19.256582 36240 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 01:50:19.256590 36240 net.cpp:409] accuracy-top1 -> top-1
I1107 01:50:19.256599 36240 net.cpp:144] Setting up accuracy-top1
I1107 01:50:19.256603 36240 net.cpp:151] Top shape: (1)
I1107 01:50:19.256606 36240 net.cpp:159] Memory required for data: 416583208
I1107 01:50:19.256610 36240 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 01:50:19.256614 36240 net.cpp:220] loss needs backward computation.
I1107 01:50:19.256619 36240 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 01:50:19.256623 36240 net.cpp:220] fc8 needs backward computation.
I1107 01:50:19.256628 36240 net.cpp:220] drop7 needs backward computation.
I1107 01:50:19.256631 36240 net.cpp:220] relu7 needs backward computation.
I1107 01:50:19.256635 36240 net.cpp:220] bn7 needs backward computation.
I1107 01:50:19.256639 36240 net.cpp:220] fc7 needs backward computation.
I1107 01:50:19.256644 36240 net.cpp:220] drop6 needs backward computation.
I1107 01:50:19.256649 36240 net.cpp:220] relu6 needs backward computation.
I1107 01:50:19.256652 36240 net.cpp:220] fc6 needs backward computation.
I1107 01:50:19.256656 36240 net.cpp:220] pool5 needs backward computation.
I1107 01:50:19.256661 36240 net.cpp:220] relu5 needs backward computation.
I1107 01:50:19.256665 36240 net.cpp:220] conv5 needs backward computation.
I1107 01:50:19.256669 36240 net.cpp:220] relu4 needs backward computation.
I1107 01:50:19.256676 36240 net.cpp:220] conv4 needs backward computation.
I1107 01:50:19.256681 36240 net.cpp:220] relu3 needs backward computation.
I1107 01:50:19.256685 36240 net.cpp:220] conv3 needs backward computation.
I1107 01:50:19.256690 36240 net.cpp:220] pool2 needs backward computation.
I1107 01:50:19.256692 36240 net.cpp:220] relu2 needs backward computation.
I1107 01:50:19.256697 36240 net.cpp:220] bn2 needs backward computation.
I1107 01:50:19.256701 36240 net.cpp:220] conv2 needs backward computation.
I1107 01:50:19.256705 36240 net.cpp:220] pool1 needs backward computation.
I1107 01:50:19.256708 36240 net.cpp:220] relu1 needs backward computation.
I1107 01:50:19.256713 36240 net.cpp:220] bn1 needs backward computation.
I1107 01:50:19.256717 36240 net.cpp:220] conv1 needs backward computation.
I1107 01:50:19.256722 36240 net.cpp:222] label_data_1_split does not need backward computation.
I1107 01:50:19.256726 36240 net.cpp:222] data does not need backward computation.
I1107 01:50:19.256731 36240 net.cpp:264] This network produces output loss
I1107 01:50:19.256734 36240 net.cpp:264] This network produces output top-1
I1107 01:50:19.256762 36240 net.cpp:284] Network initialization done.
I1107 01:50:19.256856 36240 solver.cpp:63] Solver scaffolding done.
I1107 01:50:19.258059 36240 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/sparse.caffemodel
W1107 01:50:19.834686 36240 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:19.838516 36240 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:19.890648 36240 net.cpp:860] Force copying param 4 weights from layer 'bn7'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:20.451489 36240 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:20.452585 36240 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W1107 01:50:20.490854 36240 net.cpp:860] Force copying param 4 weights from layer 'bn7'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I1107 01:50:20.495509 36240 caffe_interface.cpp:527] Starting Optimization
I1107 01:50:20.495532 36240 solver.cpp:335] Solving 
I1107 01:50:20.495533 36240 solver.cpp:336] Learning Rate Policy: step
I1107 01:50:20.497383 36240 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 01:50:22.010687 36240 solver.cpp:517]     Test net output #0: loss = 0.257515 (* 1 = 0.257515 loss)
I1107 01:50:22.010718 36240 solver.cpp:517]     Test net output #1: top-1 = 0.94675
I1107 01:50:22.269006 36240 solver.cpp:266] Iteration 0 (0 iter/s, 1.77335s/50 iter), loss = 0.0412688
I1107 01:50:22.269034 36240 solver.cpp:285]     Train net output #0: loss = 0.0412688 (* 1 = 0.0412688 loss)
I1107 01:50:22.269062 36240 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 01:50:34.938990 36240 solver.cpp:266] Iteration 50 (3.94651 iter/s, 12.6694s/50 iter), loss = 0.101308
I1107 01:50:34.939018 36240 solver.cpp:285]     Train net output #0: loss = 0.101308 (* 1 = 0.101308 loss)
I1107 01:50:34.939024 36240 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 01:50:47.602068 36240 solver.cpp:266] Iteration 100 (3.94867 iter/s, 12.6625s/50 iter), loss = 0.134318
I1107 01:50:47.602290 36240 solver.cpp:285]     Train net output #0: loss = 0.134318 (* 1 = 0.134318 loss)
I1107 01:50:47.602301 36240 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 01:51:00.222355 36240 solver.cpp:266] Iteration 150 (3.96213 iter/s, 12.6195s/50 iter), loss = 0.100682
I1107 01:51:00.222384 36240 solver.cpp:285]     Train net output #0: loss = 0.100682 (* 1 = 0.100682 loss)
I1107 01:51:00.222390 36240 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 01:51:12.905138 36240 solver.cpp:266] Iteration 200 (3.94255 iter/s, 12.6821s/50 iter), loss = 0.116737
I1107 01:51:12.905169 36240 solver.cpp:285]     Train net output #0: loss = 0.116737 (* 1 = 0.116737 loss)
I1107 01:51:12.905174 36240 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 01:51:25.529858 36240 solver.cpp:266] Iteration 250 (3.96068 iter/s, 12.6241s/50 iter), loss = 0.123426
I1107 01:51:25.529929 36240 solver.cpp:285]     Train net output #0: loss = 0.123426 (* 1 = 0.123426 loss)
I1107 01:51:25.529937 36240 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 01:51:38.281040 36240 solver.cpp:266] Iteration 300 (3.92142 iter/s, 12.7505s/50 iter), loss = 0.115279
I1107 01:51:38.281070 36240 solver.cpp:285]     Train net output #0: loss = 0.115279 (* 1 = 0.115279 loss)
I1107 01:51:38.281076 36240 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 01:51:51.018004 36240 solver.cpp:266] Iteration 350 (3.92578 iter/s, 12.7363s/50 iter), loss = 0.171351
I1107 01:51:51.018034 36240 solver.cpp:285]     Train net output #0: loss = 0.171351 (* 1 = 0.171351 loss)
I1107 01:51:51.018040 36240 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 01:52:03.768427 36240 solver.cpp:266] Iteration 400 (3.92164 iter/s, 12.7498s/50 iter), loss = 0.118395
I1107 01:52:03.768605 36240 solver.cpp:285]     Train net output #0: loss = 0.118395 (* 1 = 0.118395 loss)
I1107 01:52:03.768625 36240 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 01:52:16.579322 36240 solver.cpp:266] Iteration 450 (3.90317 iter/s, 12.8101s/50 iter), loss = 0.111328
I1107 01:52:16.579349 36240 solver.cpp:285]     Train net output #0: loss = 0.111328 (* 1 = 0.111328 loss)
I1107 01:52:16.579355 36240 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 01:52:29.349012 36240 solver.cpp:266] Iteration 500 (3.91572 iter/s, 12.7691s/50 iter), loss = 0.125101
I1107 01:52:29.349040 36240 solver.cpp:285]     Train net output #0: loss = 0.125101 (* 1 = 0.125101 loss)
I1107 01:52:29.349045 36240 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 01:52:42.150722 36240 solver.cpp:266] Iteration 550 (3.90592 iter/s, 12.8011s/50 iter), loss = 0.1192
I1107 01:52:42.150869 36240 solver.cpp:285]     Train net output #0: loss = 0.1192 (* 1 = 0.1192 loss)
I1107 01:52:42.150878 36240 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 01:52:54.955781 36240 solver.cpp:266] Iteration 600 (3.90493 iter/s, 12.8043s/50 iter), loss = 0.100403
I1107 01:52:54.955812 36240 solver.cpp:285]     Train net output #0: loss = 0.100403 (* 1 = 0.100403 loss)
I1107 01:52:54.955819 36240 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 01:53:07.794167 36240 solver.cpp:266] Iteration 650 (3.89475 iter/s, 12.8378s/50 iter), loss = 0.143513
I1107 01:53:07.794195 36240 solver.cpp:285]     Train net output #0: loss = 0.143513 (* 1 = 0.143513 loss)
I1107 01:53:07.794200 36240 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 01:53:20.692349 36240 solver.cpp:266] Iteration 700 (3.87668 iter/s, 12.8976s/50 iter), loss = 0.110498
I1107 01:53:20.692488 36240 solver.cpp:285]     Train net output #0: loss = 0.110498 (* 1 = 0.110498 loss)
I1107 01:53:20.692497 36240 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 01:53:33.813680 36240 solver.cpp:266] Iteration 750 (3.81078 iter/s, 13.1207s/50 iter), loss = 0.113398
I1107 01:53:33.813709 36240 solver.cpp:285]     Train net output #0: loss = 0.113398 (* 1 = 0.113398 loss)
I1107 01:53:33.813731 36240 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 01:53:46.661098 36240 solver.cpp:266] Iteration 800 (3.892 iter/s, 12.8469s/50 iter), loss = 0.131538
I1107 01:53:46.661126 36240 solver.cpp:285]     Train net output #0: loss = 0.131538 (* 1 = 0.131538 loss)
I1107 01:53:46.661132 36240 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 01:53:59.498061 36240 solver.cpp:266] Iteration 850 (3.89517 iter/s, 12.8364s/50 iter), loss = 0.140783
I1107 01:53:59.498205 36240 solver.cpp:285]     Train net output #0: loss = 0.140783 (* 1 = 0.140783 loss)
I1107 01:53:59.498214 36240 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 01:54:12.378484 36240 solver.cpp:266] Iteration 900 (3.88206 iter/s, 12.8798s/50 iter), loss = 0.146747
I1107 01:54:12.378512 36240 solver.cpp:285]     Train net output #0: loss = 0.146747 (* 1 = 0.146747 loss)
I1107 01:54:12.378520 36240 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 01:54:25.367626 36240 solver.cpp:266] Iteration 950 (3.84953 iter/s, 12.9886s/50 iter), loss = 0.0725768
I1107 01:54:25.367655 36240 solver.cpp:285]     Train net output #0: loss = 0.0725768 (* 1 = 0.0725768 loss)
I1107 01:54:25.367661 36240 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 01:54:38.142347 36240 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 01:54:39.652616 36240 solver.cpp:517]     Test net output #0: loss = 0.248559 (* 1 = 0.248559 loss)
I1107 01:54:39.652642 36240 solver.cpp:517]     Test net output #1: top-1 = 0.91175
I1107 01:54:39.904286 36240 solver.cpp:266] Iteration 1000 (3.43972 iter/s, 14.5361s/50 iter), loss = 0.102139
I1107 01:54:39.904310 36240 solver.cpp:285]     Train net output #0: loss = 0.102139 (* 1 = 0.102139 loss)
I1107 01:54:39.904316 36240 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 01:54:52.788947 36240 solver.cpp:266] Iteration 1050 (3.88075 iter/s, 12.8841s/50 iter), loss = 0.074938
I1107 01:54:52.788977 36240 solver.cpp:285]     Train net output #0: loss = 0.074938 (* 1 = 0.074938 loss)
I1107 01:54:52.788983 36240 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 01:55:05.771208 36240 solver.cpp:266] Iteration 1100 (3.85157 iter/s, 12.9817s/50 iter), loss = 0.0743082
I1107 01:55:05.771235 36240 solver.cpp:285]     Train net output #0: loss = 0.0743082 (* 1 = 0.0743082 loss)
I1107 01:55:05.771242 36240 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 01:55:18.668536 36240 solver.cpp:266] Iteration 1150 (3.87694 iter/s, 12.8968s/50 iter), loss = 0.124354
I1107 01:55:18.668706 36240 solver.cpp:285]     Train net output #0: loss = 0.124354 (* 1 = 0.124354 loss)
I1107 01:55:18.668715 36240 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 01:55:31.526033 36240 solver.cpp:266] Iteration 1200 (3.88899 iter/s, 12.8568s/50 iter), loss = 0.160539
I1107 01:55:31.526070 36240 solver.cpp:285]     Train net output #0: loss = 0.160539 (* 1 = 0.160539 loss)
I1107 01:55:31.526093 36240 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 01:55:44.424768 36240 solver.cpp:266] Iteration 1250 (3.87652 iter/s, 12.8982s/50 iter), loss = 0.0872985
I1107 01:55:44.424798 36240 solver.cpp:285]     Train net output #0: loss = 0.0872985 (* 1 = 0.0872985 loss)
I1107 01:55:44.424804 36240 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 01:55:57.338594 36240 solver.cpp:266] Iteration 1300 (3.87198 iter/s, 12.9133s/50 iter), loss = 0.112433
I1107 01:55:57.338737 36240 solver.cpp:285]     Train net output #0: loss = 0.112433 (* 1 = 0.112433 loss)
I1107 01:55:57.338745 36240 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 01:56:10.250986 36240 solver.cpp:266] Iteration 1350 (3.87245 iter/s, 12.9117s/50 iter), loss = 0.148503
I1107 01:56:10.251013 36240 solver.cpp:285]     Train net output #0: loss = 0.148503 (* 1 = 0.148503 loss)
I1107 01:56:10.251019 36240 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 01:56:23.179842 36240 solver.cpp:266] Iteration 1400 (3.86748 iter/s, 12.9283s/50 iter), loss = 0.097964
I1107 01:56:23.179870 36240 solver.cpp:285]     Train net output #0: loss = 0.097964 (* 1 = 0.097964 loss)
I1107 01:56:23.179877 36240 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 01:56:36.094668 36240 solver.cpp:266] Iteration 1450 (3.87168 iter/s, 12.9143s/50 iter), loss = 0.122159
I1107 01:56:36.094839 36240 solver.cpp:285]     Train net output #0: loss = 0.122159 (* 1 = 0.122159 loss)
I1107 01:56:36.094861 36240 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 01:56:49.024785 36240 solver.cpp:266] Iteration 1500 (3.86714 iter/s, 12.9294s/50 iter), loss = 0.0969487
I1107 01:56:49.024813 36240 solver.cpp:285]     Train net output #0: loss = 0.0969487 (* 1 = 0.0969487 loss)
I1107 01:56:49.024821 36240 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 01:57:01.885627 36240 solver.cpp:266] Iteration 1550 (3.88793 iter/s, 12.8603s/50 iter), loss = 0.109126
I1107 01:57:01.885656 36240 solver.cpp:285]     Train net output #0: loss = 0.109126 (* 1 = 0.109126 loss)
I1107 01:57:01.885663 36240 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 01:57:14.790459 36240 solver.cpp:266] Iteration 1600 (3.87468 iter/s, 12.9043s/50 iter), loss = 0.157363
I1107 01:57:14.790591 36240 solver.cpp:285]     Train net output #0: loss = 0.157363 (* 1 = 0.157363 loss)
I1107 01:57:14.790598 36240 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 01:57:27.687023 36240 solver.cpp:266] Iteration 1650 (3.8772 iter/s, 12.8959s/50 iter), loss = 0.0485972
I1107 01:57:27.687052 36240 solver.cpp:285]     Train net output #0: loss = 0.0485972 (* 1 = 0.0485972 loss)
I1107 01:57:27.687057 36240 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 01:57:40.635637 36240 solver.cpp:266] Iteration 1700 (3.86158 iter/s, 12.9481s/50 iter), loss = 0.105098
I1107 01:57:40.635665 36240 solver.cpp:285]     Train net output #0: loss = 0.105098 (* 1 = 0.105098 loss)
I1107 01:57:40.635671 36240 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 01:57:53.523267 36240 solver.cpp:266] Iteration 1750 (3.87985 iter/s, 12.8871s/50 iter), loss = 0.106363
I1107 01:57:53.523447 36240 solver.cpp:285]     Train net output #0: loss = 0.106363 (* 1 = 0.106363 loss)
I1107 01:57:53.523456 36240 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 01:58:06.400251 36240 solver.cpp:266] Iteration 1800 (3.8831 iter/s, 12.8763s/50 iter), loss = 0.0955538
I1107 01:58:06.400280 36240 solver.cpp:285]     Train net output #0: loss = 0.0955538 (* 1 = 0.0955538 loss)
I1107 01:58:06.400285 36240 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 01:58:19.310935 36240 solver.cpp:266] Iteration 1850 (3.87292 iter/s, 12.9101s/50 iter), loss = 0.0878794
I1107 01:58:19.310962 36240 solver.cpp:285]     Train net output #0: loss = 0.0878794 (* 1 = 0.0878794 loss)
I1107 01:58:19.310968 36240 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 01:58:32.187893 36240 solver.cpp:266] Iteration 1900 (3.88307 iter/s, 12.8764s/50 iter), loss = 0.0752906
I1107 01:58:32.188020 36240 solver.cpp:285]     Train net output #0: loss = 0.0752906 (* 1 = 0.0752906 loss)
I1107 01:58:32.188028 36240 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 01:58:45.100430 36240 solver.cpp:266] Iteration 1950 (3.8724 iter/s, 12.9119s/50 iter), loss = 0.112929
I1107 01:58:45.100458 36240 solver.cpp:285]     Train net output #0: loss = 0.112929 (* 1 = 0.112929 loss)
I1107 01:58:45.100464 36240 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 01:58:57.732142 36240 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 01:58:59.286164 36240 solver.cpp:517]     Test net output #0: loss = 0.198868 (* 1 = 0.198868 loss)
I1107 01:58:59.286182 36240 solver.cpp:517]     Test net output #1: top-1 = 0.9205
I1107 01:58:59.534276 36240 solver.cpp:266] Iteration 2000 (3.46422 iter/s, 14.4332s/50 iter), loss = 0.0627407
I1107 01:58:59.534301 36240 solver.cpp:285]     Train net output #0: loss = 0.0627407 (* 1 = 0.0627407 loss)
I1107 01:58:59.534307 36240 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 01:59:12.392190 36240 solver.cpp:266] Iteration 2050 (3.88882 iter/s, 12.8574s/50 iter), loss = 0.0992735
I1107 01:59:12.392334 36240 solver.cpp:285]     Train net output #0: loss = 0.0992735 (* 1 = 0.0992735 loss)
I1107 01:59:12.392343 36240 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 01:59:25.317162 36240 solver.cpp:266] Iteration 2100 (3.86868 iter/s, 12.9243s/50 iter), loss = 0.117956
I1107 01:59:25.317191 36240 solver.cpp:285]     Train net output #0: loss = 0.117956 (* 1 = 0.117956 loss)
I1107 01:59:25.317291 36240 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 01:59:38.236572 36240 solver.cpp:266] Iteration 2150 (3.87034 iter/s, 12.9188s/50 iter), loss = 0.113011
I1107 01:59:38.236601 36240 solver.cpp:285]     Train net output #0: loss = 0.113011 (* 1 = 0.113011 loss)
I1107 01:59:38.236608 36240 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 01:59:51.120332 36240 solver.cpp:266] Iteration 2200 (3.88102 iter/s, 12.8832s/50 iter), loss = 0.0885566
I1107 01:59:51.120466 36240 solver.cpp:285]     Train net output #0: loss = 0.0885566 (* 1 = 0.0885566 loss)
I1107 01:59:51.120474 36240 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 02:00:04.050338 36240 solver.cpp:266] Iteration 2250 (3.86717 iter/s, 12.9294s/50 iter), loss = 0.109167
I1107 02:00:04.050365 36240 solver.cpp:285]     Train net output #0: loss = 0.109167 (* 1 = 0.109167 loss)
I1107 02:00:04.050372 36240 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 02:00:16.950565 36240 solver.cpp:266] Iteration 2300 (3.87606 iter/s, 12.8997s/50 iter), loss = 0.100582
I1107 02:00:16.950593 36240 solver.cpp:285]     Train net output #0: loss = 0.100583 (* 1 = 0.100583 loss)
I1107 02:00:16.950615 36240 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 02:00:29.839444 36240 solver.cpp:266] Iteration 2350 (3.87947 iter/s, 12.8883s/50 iter), loss = 0.0879494
I1107 02:00:29.839607 36240 solver.cpp:285]     Train net output #0: loss = 0.0879495 (* 1 = 0.0879495 loss)
I1107 02:00:29.839614 36240 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 02:00:42.681375 36240 solver.cpp:266] Iteration 2400 (3.8937 iter/s, 12.8413s/50 iter), loss = 0.119624
I1107 02:00:42.681404 36240 solver.cpp:285]     Train net output #0: loss = 0.119624 (* 1 = 0.119624 loss)
I1107 02:00:42.681411 36240 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 02:00:55.493016 36240 solver.cpp:266] Iteration 2450 (3.90286 iter/s, 12.8111s/50 iter), loss = 0.138744
I1107 02:00:55.493044 36240 solver.cpp:285]     Train net output #0: loss = 0.138744 (* 1 = 0.138744 loss)
I1107 02:00:55.493067 36240 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 02:01:08.369900 36240 solver.cpp:266] Iteration 2500 (3.88309 iter/s, 12.8764s/50 iter), loss = 0.0770321
I1107 02:01:08.371107 36240 solver.cpp:285]     Train net output #0: loss = 0.0770321 (* 1 = 0.0770321 loss)
I1107 02:01:08.371114 36240 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 02:01:21.264189 36240 solver.cpp:266] Iteration 2550 (3.8782 iter/s, 12.8926s/50 iter), loss = 0.0587071
I1107 02:01:21.264216 36240 solver.cpp:285]     Train net output #0: loss = 0.0587071 (* 1 = 0.0587071 loss)
I1107 02:01:21.264222 36240 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 02:01:34.375267 36240 solver.cpp:266] Iteration 2600 (3.81373 iter/s, 13.1105s/50 iter), loss = 0.0709888
I1107 02:01:34.375295 36240 solver.cpp:285]     Train net output #0: loss = 0.0709888 (* 1 = 0.0709888 loss)
I1107 02:01:34.375301 36240 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 02:01:47.693410 36240 solver.cpp:266] Iteration 2650 (3.75443 iter/s, 13.3176s/50 iter), loss = 0.0428554
I1107 02:01:47.693578 36240 solver.cpp:285]     Train net output #0: loss = 0.0428555 (* 1 = 0.0428555 loss)
I1107 02:01:47.693586 36240 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 02:02:00.651749 36240 solver.cpp:266] Iteration 2700 (3.85872 iter/s, 12.9577s/50 iter), loss = 0.0423045
I1107 02:02:00.651778 36240 solver.cpp:285]     Train net output #0: loss = 0.0423045 (* 1 = 0.0423045 loss)
I1107 02:02:00.651799 36240 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 02:02:13.533339 36240 solver.cpp:266] Iteration 2750 (3.88167 iter/s, 12.8811s/50 iter), loss = 0.043153
I1107 02:02:13.533367 36240 solver.cpp:285]     Train net output #0: loss = 0.043153 (* 1 = 0.043153 loss)
I1107 02:02:13.533375 36240 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 02:02:26.506682 36240 solver.cpp:266] Iteration 2800 (3.85422 iter/s, 12.9728s/50 iter), loss = 0.0482867
I1107 02:02:26.506949 36240 solver.cpp:285]     Train net output #0: loss = 0.0482867 (* 1 = 0.0482867 loss)
I1107 02:02:26.507014 36240 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 02:02:39.428622 36240 solver.cpp:266] Iteration 2850 (3.86964 iter/s, 12.9211s/50 iter), loss = 0.0414545
I1107 02:02:39.428653 36240 solver.cpp:285]     Train net output #0: loss = 0.0414545 (* 1 = 0.0414545 loss)
I1107 02:02:39.428659 36240 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 02:02:52.293246 36240 solver.cpp:266] Iteration 2900 (3.88679 iter/s, 12.8641s/50 iter), loss = 0.0889018
I1107 02:02:52.293273 36240 solver.cpp:285]     Train net output #0: loss = 0.0889018 (* 1 = 0.0889018 loss)
I1107 02:02:52.293278 36240 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 02:03:05.167325 36240 solver.cpp:266] Iteration 2950 (3.88393 iter/s, 12.8735s/50 iter), loss = 0.0241138
I1107 02:03:05.167474 36240 solver.cpp:285]     Train net output #0: loss = 0.0241138 (* 1 = 0.0241138 loss)
I1107 02:03:05.167482 36240 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 02:03:17.805971 36240 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 02:03:19.376811 36240 solver.cpp:517]     Test net output #0: loss = 0.132812 (* 1 = 0.132812 loss)
I1107 02:03:19.376827 36240 solver.cpp:517]     Test net output #1: top-1 = 0.94725
I1107 02:03:19.631774 36240 solver.cpp:266] Iteration 3000 (3.45692 iter/s, 14.4637s/50 iter), loss = 0.0395882
I1107 02:03:19.631798 36240 solver.cpp:285]     Train net output #0: loss = 0.0395882 (* 1 = 0.0395882 loss)
I1107 02:03:19.631803 36240 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 02:03:32.561116 36240 solver.cpp:266] Iteration 3050 (3.86733 iter/s, 12.9288s/50 iter), loss = 0.0376095
I1107 02:03:32.561146 36240 solver.cpp:285]     Train net output #0: loss = 0.0376095 (* 1 = 0.0376095 loss)
I1107 02:03:32.561152 36240 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 02:03:45.503782 36240 solver.cpp:266] Iteration 3100 (3.86335 iter/s, 12.9421s/50 iter), loss = 0.034416
I1107 02:03:45.503985 36240 solver.cpp:285]     Train net output #0: loss = 0.034416 (* 1 = 0.034416 loss)
I1107 02:03:45.503994 36240 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 02:03:58.415024 36240 solver.cpp:266] Iteration 3150 (3.8728 iter/s, 12.9105s/50 iter), loss = 0.0306037
I1107 02:03:58.415052 36240 solver.cpp:285]     Train net output #0: loss = 0.0306037 (* 1 = 0.0306037 loss)
I1107 02:03:58.415058 36240 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 02:04:11.330291 36240 solver.cpp:266] Iteration 3200 (3.87155 iter/s, 12.9147s/50 iter), loss = 0.0642965
I1107 02:04:11.330320 36240 solver.cpp:285]     Train net output #0: loss = 0.0642965 (* 1 = 0.0642965 loss)
I1107 02:04:11.330327 36240 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 02:04:24.327870 36240 solver.cpp:266] Iteration 3250 (3.84703 iter/s, 12.997s/50 iter), loss = 0.0472481
I1107 02:04:24.328025 36240 solver.cpp:285]     Train net output #0: loss = 0.0472481 (* 1 = 0.0472481 loss)
I1107 02:04:24.328033 36240 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 02:04:37.312155 36240 solver.cpp:266] Iteration 3300 (3.851 iter/s, 12.9836s/50 iter), loss = 0.0449409
I1107 02:04:37.312196 36240 solver.cpp:285]     Train net output #0: loss = 0.0449409 (* 1 = 0.0449409 loss)
I1107 02:04:37.312202 36240 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 02:04:50.335466 36240 solver.cpp:266] Iteration 3350 (3.83943 iter/s, 13.0228s/50 iter), loss = 0.0430726
I1107 02:04:50.335494 36240 solver.cpp:285]     Train net output #0: loss = 0.0430726 (* 1 = 0.0430726 loss)
I1107 02:04:50.335500 36240 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I1107 02:05:03.284665 36240 solver.cpp:266] Iteration 3400 (3.8614 iter/s, 12.9487s/50 iter), loss = 0.0536323
I1107 02:05:03.284827 36240 solver.cpp:285]     Train net output #0: loss = 0.0536323 (* 1 = 0.0536323 loss)
I1107 02:05:03.284837 36240 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I1107 02:05:16.213892 36240 solver.cpp:266] Iteration 3450 (3.8674 iter/s, 12.9286s/50 iter), loss = 0.0329795
I1107 02:05:16.213920 36240 solver.cpp:285]     Train net output #0: loss = 0.0329795 (* 1 = 0.0329795 loss)
I1107 02:05:16.213927 36240 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I1107 02:05:29.103706 36240 solver.cpp:266] Iteration 3500 (3.87919 iter/s, 12.8893s/50 iter), loss = 0.0290658
I1107 02:05:29.103735 36240 solver.cpp:285]     Train net output #0: loss = 0.0290658 (* 1 = 0.0290658 loss)
I1107 02:05:29.103756 36240 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I1107 02:05:42.018251 36240 solver.cpp:266] Iteration 3550 (3.87176 iter/s, 12.914s/50 iter), loss = 0.0294198
I1107 02:05:42.018405 36240 solver.cpp:285]     Train net output #0: loss = 0.0294199 (* 1 = 0.0294199 loss)
I1107 02:05:42.018414 36240 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I1107 02:05:54.948766 36240 solver.cpp:266] Iteration 3600 (3.86702 iter/s, 12.9299s/50 iter), loss = 0.0330633
I1107 02:05:54.948794 36240 solver.cpp:285]     Train net output #0: loss = 0.0330633 (* 1 = 0.0330633 loss)
I1107 02:05:54.948817 36240 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I1107 02:06:07.859382 36240 solver.cpp:266] Iteration 3650 (3.87294 iter/s, 12.9101s/50 iter), loss = 0.0478699
I1107 02:06:07.859411 36240 solver.cpp:285]     Train net output #0: loss = 0.0478699 (* 1 = 0.0478699 loss)
I1107 02:06:07.859416 36240 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I1107 02:06:20.766219 36240 solver.cpp:266] Iteration 3700 (3.87407 iter/s, 12.9063s/50 iter), loss = 0.0465566
I1107 02:06:20.766410 36240 solver.cpp:285]     Train net output #0: loss = 0.0465566 (* 1 = 0.0465566 loss)
I1107 02:06:20.766419 36240 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I1107 02:06:33.656112 36240 solver.cpp:266] Iteration 3750 (3.87921 iter/s, 12.8892s/50 iter), loss = 0.0132519
I1107 02:06:33.656141 36240 solver.cpp:285]     Train net output #0: loss = 0.0132519 (* 1 = 0.0132519 loss)
I1107 02:06:33.656147 36240 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I1107 02:06:46.516474 36240 solver.cpp:266] Iteration 3800 (3.88807 iter/s, 12.8598s/50 iter), loss = 0.037566
I1107 02:06:46.516504 36240 solver.cpp:285]     Train net output #0: loss = 0.037566 (* 1 = 0.037566 loss)
I1107 02:06:46.516510 36240 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I1107 02:06:59.475962 36240 solver.cpp:266] Iteration 3850 (3.85833 iter/s, 12.959s/50 iter), loss = 0.0257344
I1107 02:06:59.476135 36240 solver.cpp:285]     Train net output #0: loss = 0.0257345 (* 1 = 0.0257345 loss)
I1107 02:06:59.476142 36240 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I1107 02:07:12.328001 36240 solver.cpp:266] Iteration 3900 (3.89063 iter/s, 12.8514s/50 iter), loss = 0.0381418
I1107 02:07:12.328045 36240 solver.cpp:285]     Train net output #0: loss = 0.0381419 (* 1 = 0.0381419 loss)
I1107 02:07:12.328052 36240 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I1107 02:07:25.199196 36240 solver.cpp:266] Iteration 3950 (3.88481 iter/s, 12.8707s/50 iter), loss = 0.0113888
I1107 02:07:25.199224 36240 solver.cpp:285]     Train net output #0: loss = 0.0113888 (* 1 = 0.0113888 loss)
I1107 02:07:25.199230 36240 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I1107 02:07:37.789713 36240 solver.cpp:418] Iteration 4000, Testing net (#0)
I1107 02:07:39.338485 36240 solver.cpp:517]     Test net output #0: loss = 0.136255 (* 1 = 0.136255 loss)
I1107 02:07:39.338510 36240 solver.cpp:517]     Test net output #1: top-1 = 0.947
I1107 02:07:39.591259 36240 solver.cpp:266] Iteration 4000 (3.47428 iter/s, 14.3915s/50 iter), loss = 0.0299674
I1107 02:07:39.591287 36240 solver.cpp:285]     Train net output #0: loss = 0.0299674 (* 1 = 0.0299674 loss)
I1107 02:07:39.591295 36240 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I1107 02:07:52.427047 36240 solver.cpp:266] Iteration 4050 (3.89552 iter/s, 12.8353s/50 iter), loss = 0.0169556
I1107 02:07:52.427076 36240 solver.cpp:285]     Train net output #0: loss = 0.0169556 (* 1 = 0.0169556 loss)
I1107 02:07:52.427081 36240 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I1107 02:08:05.253257 36240 solver.cpp:266] Iteration 4100 (3.89843 iter/s, 12.8257s/50 iter), loss = 0.0124217
I1107 02:08:05.253286 36240 solver.cpp:285]     Train net output #0: loss = 0.0124217 (* 1 = 0.0124217 loss)
I1107 02:08:05.253293 36240 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I1107 02:08:18.059649 36240 solver.cpp:266] Iteration 4150 (3.90446 iter/s, 12.8059s/50 iter), loss = 0.05711
I1107 02:08:18.059808 36240 solver.cpp:285]     Train net output #0: loss = 0.05711 (* 1 = 0.05711 loss)
I1107 02:08:18.059818 36240 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I1107 02:08:30.947335 36240 solver.cpp:266] Iteration 4200 (3.87987 iter/s, 12.887s/50 iter), loss = 0.0336998
I1107 02:08:30.947365 36240 solver.cpp:285]     Train net output #0: loss = 0.0336998 (* 1 = 0.0336998 loss)
I1107 02:08:30.947371 36240 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I1107 02:08:43.831022 36240 solver.cpp:266] Iteration 4250 (3.88103 iter/s, 12.8832s/50 iter), loss = 0.0268915
I1107 02:08:43.831050 36240 solver.cpp:285]     Train net output #0: loss = 0.0268915 (* 1 = 0.0268915 loss)
I1107 02:08:43.831055 36240 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I1107 02:08:56.647668 36240 solver.cpp:266] Iteration 4300 (3.90134 iter/s, 12.8161s/50 iter), loss = 0.0187404
I1107 02:08:56.647851 36240 solver.cpp:285]     Train net output #0: loss = 0.0187404 (* 1 = 0.0187404 loss)
I1107 02:08:56.647859 36240 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I1107 02:09:09.465761 36240 solver.cpp:266] Iteration 4350 (3.90094 iter/s, 12.8174s/50 iter), loss = 0.0419032
I1107 02:09:09.465790 36240 solver.cpp:285]     Train net output #0: loss = 0.0419032 (* 1 = 0.0419032 loss)
I1107 02:09:09.465796 36240 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I1107 02:09:22.317687 36240 solver.cpp:266] Iteration 4400 (3.89063 iter/s, 12.8514s/50 iter), loss = 0.0249274
I1107 02:09:22.317718 36240 solver.cpp:285]     Train net output #0: loss = 0.0249274 (* 1 = 0.0249274 loss)
I1107 02:09:22.317723 36240 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I1107 02:09:35.120838 36240 solver.cpp:266] Iteration 4450 (3.90545 iter/s, 12.8026s/50 iter), loss = 0.0463352
I1107 02:09:35.121039 36240 solver.cpp:285]     Train net output #0: loss = 0.0463352 (* 1 = 0.0463352 loss)
I1107 02:09:35.121047 36240 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I1107 02:09:47.943423 36240 solver.cpp:266] Iteration 4500 (3.89958 iter/s, 12.8219s/50 iter), loss = 0.0255546
I1107 02:09:47.943454 36240 solver.cpp:285]     Train net output #0: loss = 0.0255546 (* 1 = 0.0255546 loss)
I1107 02:09:47.943459 36240 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I1107 02:10:00.801338 36240 solver.cpp:266] Iteration 4550 (3.88881 iter/s, 12.8574s/50 iter), loss = 0.0274173
I1107 02:10:00.801367 36240 solver.cpp:285]     Train net output #0: loss = 0.0274173 (* 1 = 0.0274173 loss)
I1107 02:10:00.801373 36240 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I1107 02:10:13.629504 36240 solver.cpp:266] Iteration 4600 (3.89783 iter/s, 12.8276s/50 iter), loss = 0.0217118
I1107 02:10:13.629655 36240 solver.cpp:285]     Train net output #0: loss = 0.0217118 (* 1 = 0.0217118 loss)
I1107 02:10:13.629663 36240 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I1107 02:10:26.471822 36240 solver.cpp:266] Iteration 4650 (3.89357 iter/s, 12.8417s/50 iter), loss = 0.0335509
I1107 02:10:26.471851 36240 solver.cpp:285]     Train net output #0: loss = 0.0335509 (* 1 = 0.0335509 loss)
I1107 02:10:26.471856 36240 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I1107 02:10:39.295667 36240 solver.cpp:266] Iteration 4700 (3.89914 iter/s, 12.8233s/50 iter), loss = 0.0434665
I1107 02:10:39.295706 36240 solver.cpp:285]     Train net output #0: loss = 0.0434665 (* 1 = 0.0434665 loss)
I1107 02:10:39.295712 36240 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I1107 02:10:52.117113 36240 solver.cpp:266] Iteration 4750 (3.89988 iter/s, 12.8209s/50 iter), loss = 0.0232407
I1107 02:10:52.117287 36240 solver.cpp:285]     Train net output #0: loss = 0.0232407 (* 1 = 0.0232407 loss)
I1107 02:10:52.117297 36240 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I1107 02:11:04.926254 36240 solver.cpp:266] Iteration 4800 (3.90366 iter/s, 12.8085s/50 iter), loss = 0.0201862
I1107 02:11:04.926293 36240 solver.cpp:285]     Train net output #0: loss = 0.0201862 (* 1 = 0.0201862 loss)
I1107 02:11:04.926301 36240 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I1107 02:11:17.779803 36240 solver.cpp:266] Iteration 4850 (3.89014 iter/s, 12.853s/50 iter), loss = 0.00926664
I1107 02:11:17.779842 36240 solver.cpp:285]     Train net output #0: loss = 0.00926664 (* 1 = 0.00926664 loss)
I1107 02:11:17.779848 36240 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I1107 02:11:30.653781 36240 solver.cpp:266] Iteration 4900 (3.88396 iter/s, 12.8734s/50 iter), loss = 0.0407719
I1107 02:11:30.653937 36240 solver.cpp:285]     Train net output #0: loss = 0.0407719 (* 1 = 0.0407719 loss)
I1107 02:11:30.653947 36240 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I1107 02:11:43.489645 36240 solver.cpp:266] Iteration 4950 (3.89553 iter/s, 12.8352s/50 iter), loss = 0.0318765
I1107 02:11:43.489686 36240 solver.cpp:285]     Train net output #0: loss = 0.0318765 (* 1 = 0.0318765 loss)
I1107 02:11:43.489707 36240 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I1107 02:11:56.072703 36240 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_5000.caffemodel
I1107 02:11:58.475194 36240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_5000.solverstate
I1107 02:11:58.958240 36240 solver.cpp:418] Iteration 5000, Testing net (#0)
I1107 02:12:00.429106 36240 solver.cpp:517]     Test net output #0: loss = 0.158496 (* 1 = 0.158496 loss)
I1107 02:12:00.429121 36240 solver.cpp:517]     Test net output #1: top-1 = 0.94175
I1107 02:12:00.673935 36240 solver.cpp:266] Iteration 5000 (2.90975 iter/s, 17.1836s/50 iter), loss = 0.0315002
I1107 02:12:00.674110 36240 solver.cpp:285]     Train net output #0: loss = 0.0315002 (* 1 = 0.0315002 loss)
I1107 02:12:00.674120 36240 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I1107 02:12:13.297367 36240 solver.cpp:266] Iteration 5050 (3.96109 iter/s, 12.6228s/50 iter), loss = 0.0242117
I1107 02:12:13.297396 36240 solver.cpp:285]     Train net output #0: loss = 0.0242117 (* 1 = 0.0242117 loss)
I1107 02:12:13.297402 36240 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I1107 02:12:26.018522 36240 solver.cpp:266] Iteration 5100 (3.93062 iter/s, 12.7206s/50 iter), loss = 0.0482393
I1107 02:12:26.018551 36240 solver.cpp:285]     Train net output #0: loss = 0.0482393 (* 1 = 0.0482393 loss)
I1107 02:12:26.018573 36240 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I1107 02:12:38.955492 36240 solver.cpp:266] Iteration 5150 (3.86505 iter/s, 12.9364s/50 iter), loss = 0.00826293
I1107 02:12:38.955636 36240 solver.cpp:285]     Train net output #0: loss = 0.00826292 (* 1 = 0.00826292 loss)
I1107 02:12:38.955646 36240 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I1107 02:12:52.297909 36240 solver.cpp:266] Iteration 5200 (3.74763 iter/s, 13.3418s/50 iter), loss = 0.00706326
I1107 02:12:52.297938 36240 solver.cpp:285]     Train net output #0: loss = 0.00706326 (* 1 = 0.00706326 loss)
I1107 02:12:52.297960 36240 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I1107 02:13:05.131831 36240 solver.cpp:266] Iteration 5250 (3.89608 iter/s, 12.8334s/50 iter), loss = 0.00760496
I1107 02:13:05.131863 36240 solver.cpp:285]     Train net output #0: loss = 0.00760496 (* 1 = 0.00760496 loss)
I1107 02:13:05.131870 36240 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I1107 02:13:17.962636 36240 solver.cpp:266] Iteration 5300 (3.89703 iter/s, 12.8303s/50 iter), loss = 0.00481176
I1107 02:13:17.962772 36240 solver.cpp:285]     Train net output #0: loss = 0.00481176 (* 1 = 0.00481176 loss)
I1107 02:13:17.962797 36240 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I1107 02:13:30.799269 36240 solver.cpp:266] Iteration 5350 (3.89529 iter/s, 12.836s/50 iter), loss = 0.00794612
I1107 02:13:30.799299 36240 solver.cpp:285]     Train net output #0: loss = 0.00794612 (* 1 = 0.00794612 loss)
I1107 02:13:30.799321 36240 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I1107 02:13:43.686985 36240 solver.cpp:266] Iteration 5400 (3.87982 iter/s, 12.8872s/50 iter), loss = 0.031914
I1107 02:13:43.687014 36240 solver.cpp:285]     Train net output #0: loss = 0.031914 (* 1 = 0.031914 loss)
I1107 02:13:43.687021 36240 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I1107 02:13:56.548965 36240 solver.cpp:266] Iteration 5450 (3.88758 iter/s, 12.8615s/50 iter), loss = 0.0191412
I1107 02:13:56.549180 36240 solver.cpp:285]     Train net output #0: loss = 0.0191412 (* 1 = 0.0191412 loss)
I1107 02:13:56.549188 36240 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I1107 02:14:09.400641 36240 solver.cpp:266] Iteration 5500 (3.89076 iter/s, 12.851s/50 iter), loss = 0.00704006
I1107 02:14:09.400671 36240 solver.cpp:285]     Train net output #0: loss = 0.00704005 (* 1 = 0.00704005 loss)
I1107 02:14:09.400686 36240 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I1107 02:14:22.258376 36240 solver.cpp:266] Iteration 5550 (3.88887 iter/s, 12.8572s/50 iter), loss = 0.0108394
I1107 02:14:22.258406 36240 solver.cpp:285]     Train net output #0: loss = 0.0108394 (* 1 = 0.0108394 loss)
I1107 02:14:22.258412 36240 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I1107 02:14:35.106880 36240 solver.cpp:266] Iteration 5600 (3.89166 iter/s, 12.848s/50 iter), loss = 0.0180479
I1107 02:14:35.109061 36240 solver.cpp:285]     Train net output #0: loss = 0.0180479 (* 1 = 0.0180479 loss)
I1107 02:14:35.109069 36240 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I1107 02:14:47.958091 36240 solver.cpp:266] Iteration 5650 (3.89149 iter/s, 12.8485s/50 iter), loss = 0.011163
I1107 02:14:47.958122 36240 solver.cpp:285]     Train net output #0: loss = 0.011163 (* 1 = 0.011163 loss)
I1107 02:14:47.958128 36240 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I1107 02:15:00.819485 36240 solver.cpp:266] Iteration 5700 (3.88776 iter/s, 12.8609s/50 iter), loss = 0.022907
I1107 02:15:00.819515 36240 solver.cpp:285]     Train net output #0: loss = 0.022907 (* 1 = 0.022907 loss)
I1107 02:15:00.819521 36240 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I1107 02:15:13.682575 36240 solver.cpp:266] Iteration 5750 (3.88725 iter/s, 12.8626s/50 iter), loss = 0.0206089
I1107 02:15:13.682742 36240 solver.cpp:285]     Train net output #0: loss = 0.0206089 (* 1 = 0.0206089 loss)
I1107 02:15:13.682751 36240 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I1107 02:15:26.504657 36240 solver.cpp:266] Iteration 5800 (3.89972 iter/s, 12.8214s/50 iter), loss = 0.0114327
I1107 02:15:26.504686 36240 solver.cpp:285]     Train net output #0: loss = 0.0114327 (* 1 = 0.0114327 loss)
I1107 02:15:26.504693 36240 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I1107 02:15:39.334702 36240 solver.cpp:266] Iteration 5850 (3.89726 iter/s, 12.8295s/50 iter), loss = 0.0112562
I1107 02:15:39.334733 36240 solver.cpp:285]     Train net output #0: loss = 0.0112561 (* 1 = 0.0112561 loss)
I1107 02:15:39.334755 36240 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I1107 02:15:52.205550 36240 solver.cpp:266] Iteration 5900 (3.88491 iter/s, 12.8703s/50 iter), loss = 0.0336379
I1107 02:15:52.205701 36240 solver.cpp:285]     Train net output #0: loss = 0.0336379 (* 1 = 0.0336379 loss)
I1107 02:15:52.205709 36240 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I1107 02:16:05.061018 36240 solver.cpp:266] Iteration 5950 (3.88959 iter/s, 12.8548s/50 iter), loss = 0.0316092
I1107 02:16:05.061058 36240 solver.cpp:285]     Train net output #0: loss = 0.0316091 (* 1 = 0.0316091 loss)
I1107 02:16:05.061064 36240 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I1107 02:16:17.661829 36240 solver.cpp:418] Iteration 6000, Testing net (#0)
I1107 02:16:19.180820 36240 solver.cpp:517]     Test net output #0: loss = 0.146917 (* 1 = 0.146917 loss)
I1107 02:16:19.180837 36240 solver.cpp:517]     Test net output #1: top-1 = 0.951
I1107 02:16:19.434680 36240 solver.cpp:266] Iteration 6000 (3.47873 iter/s, 14.3731s/50 iter), loss = 0.00262612
I1107 02:16:19.434707 36240 solver.cpp:285]     Train net output #0: loss = 0.00262611 (* 1 = 0.00262611 loss)
I1107 02:16:19.434713 36240 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I1107 02:16:32.275003 36240 solver.cpp:266] Iteration 6050 (3.89414 iter/s, 12.8398s/50 iter), loss = 0.0199493
I1107 02:16:32.275167 36240 solver.cpp:285]     Train net output #0: loss = 0.0199492 (* 1 = 0.0199492 loss)
I1107 02:16:32.275176 36240 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I1107 02:16:45.122170 36240 solver.cpp:266] Iteration 6100 (3.89211 iter/s, 12.8465s/50 iter), loss = 0.0064772
I1107 02:16:45.122200 36240 solver.cpp:285]     Train net output #0: loss = 0.00647719 (* 1 = 0.00647719 loss)
I1107 02:16:45.122206 36240 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I1107 02:16:57.964459 36240 solver.cpp:266] Iteration 6150 (3.89354 iter/s, 12.8418s/50 iter), loss = 0.00438894
I1107 02:16:57.964489 36240 solver.cpp:285]     Train net output #0: loss = 0.00438893 (* 1 = 0.00438893 loss)
I1107 02:16:57.964511 36240 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I1107 02:17:10.815140 36240 solver.cpp:266] Iteration 6200 (3.891 iter/s, 12.8502s/50 iter), loss = 0.00524687
I1107 02:17:10.815292 36240 solver.cpp:285]     Train net output #0: loss = 0.00524687 (* 1 = 0.00524687 loss)
I1107 02:17:10.815301 36240 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I1107 02:17:23.670608 36240 solver.cpp:266] Iteration 6250 (3.88959 iter/s, 12.8548s/50 iter), loss = 0.0182484
I1107 02:17:23.670639 36240 solver.cpp:285]     Train net output #0: loss = 0.0182484 (* 1 = 0.0182484 loss)
I1107 02:17:23.670644 36240 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I1107 02:17:36.533835 36240 solver.cpp:266] Iteration 6300 (3.88721 iter/s, 12.8627s/50 iter), loss = 0.0118742
I1107 02:17:36.533866 36240 solver.cpp:285]     Train net output #0: loss = 0.0118742 (* 1 = 0.0118742 loss)
I1107 02:17:36.533874 36240 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I1107 02:17:49.396339 36240 solver.cpp:266] Iteration 6350 (3.88743 iter/s, 12.862s/50 iter), loss = 0.0161281
I1107 02:17:49.396524 36240 solver.cpp:285]     Train net output #0: loss = 0.0161281 (* 1 = 0.0161281 loss)
I1107 02:17:49.396534 36240 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I1107 02:18:02.280485 36240 solver.cpp:266] Iteration 6400 (3.88095 iter/s, 12.8835s/50 iter), loss = 0.0233032
I1107 02:18:02.280515 36240 solver.cpp:285]     Train net output #0: loss = 0.0233032 (* 1 = 0.0233032 loss)
I1107 02:18:02.280521 36240 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I1107 02:18:15.126978 36240 solver.cpp:266] Iteration 6450 (3.89227 iter/s, 12.846s/50 iter), loss = 0.0200362
I1107 02:18:15.127008 36240 solver.cpp:285]     Train net output #0: loss = 0.0200362 (* 1 = 0.0200362 loss)
I1107 02:18:15.127014 36240 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I1107 02:18:27.989243 36240 solver.cpp:266] Iteration 6500 (3.8875 iter/s, 12.8617s/50 iter), loss = 0.00799839
I1107 02:18:27.989403 36240 solver.cpp:285]     Train net output #0: loss = 0.00799839 (* 1 = 0.00799839 loss)
I1107 02:18:27.989413 36240 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I1107 02:18:40.817905 36240 solver.cpp:266] Iteration 6550 (3.89772 iter/s, 12.828s/50 iter), loss = 0.0460543
I1107 02:18:40.817936 36240 solver.cpp:285]     Train net output #0: loss = 0.0460543 (* 1 = 0.0460543 loss)
I1107 02:18:40.817942 36240 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I1107 02:18:53.674587 36240 solver.cpp:266] Iteration 6600 (3.88919 iter/s, 12.8562s/50 iter), loss = 0.0323216
I1107 02:18:53.674619 36240 solver.cpp:285]     Train net output #0: loss = 0.0323216 (* 1 = 0.0323216 loss)
I1107 02:18:53.674625 36240 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I1107 02:19:06.545390 36240 solver.cpp:266] Iteration 6650 (3.88492 iter/s, 12.8703s/50 iter), loss = 0.00616954
I1107 02:19:06.545541 36240 solver.cpp:285]     Train net output #0: loss = 0.00616954 (* 1 = 0.00616954 loss)
I1107 02:19:06.545549 36240 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I1107 02:19:19.357308 36240 solver.cpp:266] Iteration 6700 (3.90281 iter/s, 12.8113s/50 iter), loss = 0.0163869
I1107 02:19:19.357340 36240 solver.cpp:285]     Train net output #0: loss = 0.0163869 (* 1 = 0.0163869 loss)
I1107 02:19:19.357347 36240 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I1107 02:19:32.213052 36240 solver.cpp:266] Iteration 6750 (3.88947 iter/s, 12.8552s/50 iter), loss = 0.0119106
I1107 02:19:32.213081 36240 solver.cpp:285]     Train net output #0: loss = 0.0119106 (* 1 = 0.0119106 loss)
I1107 02:19:32.213088 36240 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I1107 02:19:45.084270 36240 solver.cpp:266] Iteration 6800 (3.88479 iter/s, 12.8707s/50 iter), loss = 0.00970744
I1107 02:19:45.084419 36240 solver.cpp:285]     Train net output #0: loss = 0.00970744 (* 1 = 0.00970744 loss)
I1107 02:19:45.084427 36240 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I1107 02:19:57.934254 36240 solver.cpp:266] Iteration 6850 (3.89125 iter/s, 12.8493s/50 iter), loss = 0.00510031
I1107 02:19:57.934284 36240 solver.cpp:285]     Train net output #0: loss = 0.00510031 (* 1 = 0.00510031 loss)
I1107 02:19:57.934306 36240 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I1107 02:20:10.788164 36240 solver.cpp:266] Iteration 6900 (3.89002 iter/s, 12.8534s/50 iter), loss = 0.00828714
I1107 02:20:10.788194 36240 solver.cpp:285]     Train net output #0: loss = 0.00828714 (* 1 = 0.00828714 loss)
I1107 02:20:10.788200 36240 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I1107 02:20:23.643177 36240 solver.cpp:266] Iteration 6950 (3.88969 iter/s, 12.8545s/50 iter), loss = 0.0344088
I1107 02:20:23.643378 36240 solver.cpp:285]     Train net output #0: loss = 0.0344088 (* 1 = 0.0344088 loss)
I1107 02:20:23.643388 36240 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I1107 02:20:36.220876 36240 solver.cpp:418] Iteration 7000, Testing net (#0)
I1107 02:20:37.774791 36240 solver.cpp:517]     Test net output #0: loss = 0.157359 (* 1 = 0.157359 loss)
I1107 02:20:37.774816 36240 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I1107 02:20:38.022420 36240 solver.cpp:266] Iteration 7000 (3.47741 iter/s, 14.3785s/50 iter), loss = 0.0179611
I1107 02:20:38.022444 36240 solver.cpp:285]     Train net output #0: loss = 0.0179611 (* 1 = 0.0179611 loss)
I1107 02:20:38.022449 36240 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I1107 02:20:50.851886 36240 solver.cpp:266] Iteration 7050 (3.89743 iter/s, 12.829s/50 iter), loss = 0.00302434
I1107 02:20:50.851928 36240 solver.cpp:285]     Train net output #0: loss = 0.00302434 (* 1 = 0.00302434 loss)
I1107 02:20:50.851935 36240 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I1107 02:21:03.709676 36240 solver.cpp:266] Iteration 7100 (3.88885 iter/s, 12.8573s/50 iter), loss = 0.0185086
I1107 02:21:03.709843 36240 solver.cpp:285]     Train net output #0: loss = 0.0185086 (* 1 = 0.0185086 loss)
I1107 02:21:03.709852 36240 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I1107 02:21:16.586210 36240 solver.cpp:266] Iteration 7150 (3.88323 iter/s, 12.8759s/50 iter), loss = 0.0107539
I1107 02:21:16.586251 36240 solver.cpp:285]     Train net output #0: loss = 0.0107539 (* 1 = 0.0107539 loss)
I1107 02:21:16.586256 36240 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I1107 02:21:29.415594 36240 solver.cpp:266] Iteration 7200 (3.89746 iter/s, 12.8289s/50 iter), loss = 0.00733462
I1107 02:21:29.415633 36240 solver.cpp:285]     Train net output #0: loss = 0.00733462 (* 1 = 0.00733462 loss)
I1107 02:21:29.415639 36240 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I1107 02:21:42.253949 36240 solver.cpp:266] Iteration 7250 (3.89474 iter/s, 12.8378s/50 iter), loss = 0.00815228
I1107 02:21:42.254118 36240 solver.cpp:285]     Train net output #0: loss = 0.00815228 (* 1 = 0.00815228 loss)
I1107 02:21:42.254127 36240 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I1107 02:21:55.086426 36240 solver.cpp:266] Iteration 7300 (3.89656 iter/s, 12.8318s/50 iter), loss = 0.01601
I1107 02:21:55.086464 36240 solver.cpp:285]     Train net output #0: loss = 0.01601 (* 1 = 0.01601 loss)
I1107 02:21:55.086470 36240 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I1107 02:22:07.934561 36240 solver.cpp:266] Iteration 7350 (3.89177 iter/s, 12.8476s/50 iter), loss = 0.00207145
I1107 02:22:07.934600 36240 solver.cpp:285]     Train net output #0: loss = 0.00207145 (* 1 = 0.00207145 loss)
I1107 02:22:07.934623 36240 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I1107 02:22:20.789276 36240 solver.cpp:266] Iteration 7400 (3.88978 iter/s, 12.8542s/50 iter), loss = 0.0158972
I1107 02:22:20.789438 36240 solver.cpp:285]     Train net output #0: loss = 0.0158972 (* 1 = 0.0158972 loss)
I1107 02:22:20.789446 36240 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I1107 02:22:33.676944 36240 solver.cpp:266] Iteration 7450 (3.87987 iter/s, 12.887s/50 iter), loss = 0.00559425
I1107 02:22:33.676983 36240 solver.cpp:285]     Train net output #0: loss = 0.00559425 (* 1 = 0.00559425 loss)
I1107 02:22:33.676990 36240 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I1107 02:22:46.551064 36240 solver.cpp:266] Iteration 7500 (3.88392 iter/s, 12.8736s/50 iter), loss = 0.00762677
I1107 02:22:46.551103 36240 solver.cpp:285]     Train net output #0: loss = 0.00762676 (* 1 = 0.00762676 loss)
I1107 02:22:46.551110 36240 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I1107 02:22:59.388133 36240 solver.cpp:266] Iteration 7550 (3.89513 iter/s, 12.8365s/50 iter), loss = 0.0140275
I1107 02:22:59.388324 36240 solver.cpp:285]     Train net output #0: loss = 0.0140275 (* 1 = 0.0140275 loss)
I1107 02:22:59.388334 36240 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I1107 02:23:12.228102 36240 solver.cpp:266] Iteration 7600 (3.89429 iter/s, 12.8393s/50 iter), loss = 0.0089458
I1107 02:23:12.228140 36240 solver.cpp:285]     Train net output #0: loss = 0.00894579 (* 1 = 0.00894579 loss)
I1107 02:23:12.228147 36240 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I1107 02:23:25.092833 36240 solver.cpp:266] Iteration 7650 (3.88675 iter/s, 12.8642s/50 iter), loss = 0.0193337
I1107 02:23:25.092871 36240 solver.cpp:285]     Train net output #0: loss = 0.0193337 (* 1 = 0.0193337 loss)
I1107 02:23:25.092878 36240 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I1107 02:23:37.982386 36240 solver.cpp:266] Iteration 7700 (3.87927 iter/s, 12.889s/50 iter), loss = 0.0119786
I1107 02:23:37.982569 36240 solver.cpp:285]     Train net output #0: loss = 0.0119786 (* 1 = 0.0119786 loss)
I1107 02:23:37.982579 36240 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I1107 02:23:50.819689 36240 solver.cpp:266] Iteration 7750 (3.8951 iter/s, 12.8366s/50 iter), loss = 0.008289
I1107 02:23:50.819728 36240 solver.cpp:285]     Train net output #0: loss = 0.00828899 (* 1 = 0.00828899 loss)
I1107 02:23:50.819734 36240 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I1107 02:24:03.687798 36240 solver.cpp:266] Iteration 7800 (3.88573 iter/s, 12.8676s/50 iter), loss = 0.00343416
I1107 02:24:03.687826 36240 solver.cpp:285]     Train net output #0: loss = 0.00343414 (* 1 = 0.00343414 loss)
I1107 02:24:03.687832 36240 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I1107 02:24:16.518080 36240 solver.cpp:266] Iteration 7850 (3.89719 iter/s, 12.8298s/50 iter), loss = 0.0072995
I1107 02:24:16.520539 36240 solver.cpp:285]     Train net output #0: loss = 0.00729948 (* 1 = 0.00729948 loss)
I1107 02:24:16.520561 36240 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I1107 02:24:29.388638 36240 solver.cpp:266] Iteration 7900 (3.88573 iter/s, 12.8676s/50 iter), loss = 0.00623079
I1107 02:24:29.388667 36240 solver.cpp:285]     Train net output #0: loss = 0.00623078 (* 1 = 0.00623078 loss)
I1107 02:24:29.388674 36240 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I1107 02:24:42.243464 36240 solver.cpp:266] Iteration 7950 (3.88975 iter/s, 12.8543s/50 iter), loss = 0.00871437
I1107 02:24:42.243494 36240 solver.cpp:285]     Train net output #0: loss = 0.00871436 (* 1 = 0.00871436 loss)
I1107 02:24:42.243499 36240 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I1107 02:24:54.841413 36240 solver.cpp:418] Iteration 8000, Testing net (#0)
I1107 02:24:56.409229 36240 solver.cpp:517]     Test net output #0: loss = 0.169915 (* 1 = 0.169915 loss)
I1107 02:24:56.409245 36240 solver.cpp:517]     Test net output #1: top-1 = 0.95025
I1107 02:24:56.660356 36240 solver.cpp:266] Iteration 8000 (3.46829 iter/s, 14.4163s/50 iter), loss = 0.0112535
I1107 02:24:56.660382 36240 solver.cpp:285]     Train net output #0: loss = 0.0112535 (* 1 = 0.0112535 loss)
I1107 02:24:56.660389 36240 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I1107 02:25:09.502193 36240 solver.cpp:266] Iteration 8050 (3.89368 iter/s, 12.8413s/50 iter), loss = 0.00681525
I1107 02:25:09.502223 36240 solver.cpp:285]     Train net output #0: loss = 0.00681524 (* 1 = 0.00681524 loss)
I1107 02:25:09.502230 36240 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I1107 02:25:22.352166 36240 solver.cpp:266] Iteration 8100 (3.89122 iter/s, 12.8495s/50 iter), loss = 0.00506858
I1107 02:25:22.352196 36240 solver.cpp:285]     Train net output #0: loss = 0.00506858 (* 1 = 0.00506858 loss)
I1107 02:25:22.352202 36240 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I1107 02:25:35.193284 36240 solver.cpp:266] Iteration 8150 (3.8939 iter/s, 12.8406s/50 iter), loss = 0.00420842
I1107 02:25:35.193431 36240 solver.cpp:285]     Train net output #0: loss = 0.00420842 (* 1 = 0.00420842 loss)
I1107 02:25:35.193440 36240 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I1107 02:25:48.049644 36240 solver.cpp:266] Iteration 8200 (3.88932 iter/s, 12.8557s/50 iter), loss = 0.0299886
I1107 02:25:48.049674 36240 solver.cpp:285]     Train net output #0: loss = 0.0299886 (* 1 = 0.0299886 loss)
I1107 02:25:48.049679 36240 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I1107 02:26:00.906939 36240 solver.cpp:266] Iteration 8250 (3.889 iter/s, 12.8568s/50 iter), loss = 0.0105536
I1107 02:26:00.906968 36240 solver.cpp:285]     Train net output #0: loss = 0.0105536 (* 1 = 0.0105536 loss)
I1107 02:26:00.906975 36240 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I1107 02:26:13.752187 36240 solver.cpp:266] Iteration 8300 (3.89265 iter/s, 12.8447s/50 iter), loss = 0.00613782
I1107 02:26:13.752382 36240 solver.cpp:285]     Train net output #0: loss = 0.00613782 (* 1 = 0.00613782 loss)
I1107 02:26:13.752393 36240 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I1107 02:26:26.618047 36240 solver.cpp:266] Iteration 8350 (3.88646 iter/s, 12.8652s/50 iter), loss = 0.00799826
I1107 02:26:26.618077 36240 solver.cpp:285]     Train net output #0: loss = 0.00799826 (* 1 = 0.00799826 loss)
I1107 02:26:26.618083 36240 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I1107 02:26:39.471767 36240 solver.cpp:266] Iteration 8400 (3.89008 iter/s, 12.8532s/50 iter), loss = 0.00774273
I1107 02:26:39.471797 36240 solver.cpp:285]     Train net output #0: loss = 0.00774273 (* 1 = 0.00774273 loss)
I1107 02:26:39.471818 36240 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I1107 02:26:52.355468 36240 solver.cpp:266] Iteration 8450 (3.88103 iter/s, 12.8832s/50 iter), loss = 0.00615089
I1107 02:26:52.355636 36240 solver.cpp:285]     Train net output #0: loss = 0.00615088 (* 1 = 0.00615088 loss)
I1107 02:26:52.355646 36240 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I1107 02:27:05.224401 36240 solver.cpp:266] Iteration 8500 (3.88552 iter/s, 12.8683s/50 iter), loss = 0.0039814
I1107 02:27:05.224432 36240 solver.cpp:285]     Train net output #0: loss = 0.0039814 (* 1 = 0.0039814 loss)
I1107 02:27:05.224453 36240 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I1107 02:27:18.075377 36240 solver.cpp:266] Iteration 8550 (3.89091 iter/s, 12.8505s/50 iter), loss = 0.016058
I1107 02:27:18.075407 36240 solver.cpp:285]     Train net output #0: loss = 0.016058 (* 1 = 0.016058 loss)
I1107 02:27:18.075413 36240 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I1107 02:27:30.938418 36240 solver.cpp:266] Iteration 8600 (3.88726 iter/s, 12.8625s/50 iter), loss = 0.00373979
I1107 02:27:30.938578 36240 solver.cpp:285]     Train net output #0: loss = 0.00373978 (* 1 = 0.00373978 loss)
I1107 02:27:30.938586 36240 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I1107 02:27:43.800252 36240 solver.cpp:266] Iteration 8650 (3.88767 iter/s, 12.8612s/50 iter), loss = 0.00756991
I1107 02:27:43.800282 36240 solver.cpp:285]     Train net output #0: loss = 0.00756991 (* 1 = 0.00756991 loss)
I1107 02:27:43.800289 36240 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I1107 02:27:56.687271 36240 solver.cpp:266] Iteration 8700 (3.88003 iter/s, 12.8865s/50 iter), loss = 0.0058925
I1107 02:27:56.687301 36240 solver.cpp:285]     Train net output #0: loss = 0.0058925 (* 1 = 0.0058925 loss)
I1107 02:27:56.687307 36240 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I1107 02:28:09.532516 36240 solver.cpp:266] Iteration 8750 (3.89265 iter/s, 12.8447s/50 iter), loss = 0.0114666
I1107 02:28:09.532696 36240 solver.cpp:285]     Train net output #0: loss = 0.0114666 (* 1 = 0.0114666 loss)
I1107 02:28:09.532706 36240 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I1107 02:28:22.385454 36240 solver.cpp:266] Iteration 8800 (3.89036 iter/s, 12.8523s/50 iter), loss = 0.00919102
I1107 02:28:22.385481 36240 solver.cpp:285]     Train net output #0: loss = 0.00919101 (* 1 = 0.00919101 loss)
I1107 02:28:22.385488 36240 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I1107 02:28:35.237856 36240 solver.cpp:266] Iteration 8850 (3.89048 iter/s, 12.8519s/50 iter), loss = 0.00260628
I1107 02:28:35.237885 36240 solver.cpp:285]     Train net output #0: loss = 0.00260626 (* 1 = 0.00260626 loss)
I1107 02:28:35.237892 36240 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I1107 02:28:48.110179 36240 solver.cpp:266] Iteration 8900 (3.88446 iter/s, 12.8718s/50 iter), loss = 0.011802
I1107 02:28:48.110353 36240 solver.cpp:285]     Train net output #0: loss = 0.011802 (* 1 = 0.011802 loss)
I1107 02:28:48.110363 36240 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I1107 02:29:01.011253 36240 solver.cpp:266] Iteration 8950 (3.87585 iter/s, 12.9004s/50 iter), loss = 0.00758049
I1107 02:29:01.011286 36240 solver.cpp:285]     Train net output #0: loss = 0.00758048 (* 1 = 0.00758048 loss)
I1107 02:29:01.011307 36240 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I1107 02:29:13.593614 36240 solver.cpp:418] Iteration 9000, Testing net (#0)
I1107 02:29:15.143805 36240 solver.cpp:517]     Test net output #0: loss = 0.178621 (* 1 = 0.178621 loss)
I1107 02:29:15.143831 36240 solver.cpp:517]     Test net output #1: top-1 = 0.951
I1107 02:29:15.392031 36240 solver.cpp:266] Iteration 9000 (3.477 iter/s, 14.3802s/50 iter), loss = 0.0138084
I1107 02:29:15.392055 36240 solver.cpp:285]     Train net output #0: loss = 0.0138084 (* 1 = 0.0138084 loss)
I1107 02:29:15.392078 36240 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I1107 02:29:28.222188 36240 solver.cpp:266] Iteration 9050 (3.89722 iter/s, 12.8296s/50 iter), loss = 0.00481237
I1107 02:29:28.222352 36240 solver.cpp:285]     Train net output #0: loss = 0.00481236 (* 1 = 0.00481236 loss)
I1107 02:29:28.222362 36240 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I1107 02:29:41.083535 36240 solver.cpp:266] Iteration 9100 (3.88781 iter/s, 12.8607s/50 iter), loss = 0.0281018
I1107 02:29:41.083564 36240 solver.cpp:285]     Train net output #0: loss = 0.0281018 (* 1 = 0.0281018 loss)
I1107 02:29:41.083587 36240 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I1107 02:29:53.929062 36240 solver.cpp:266] Iteration 9150 (3.89256 iter/s, 12.845s/50 iter), loss = 0.00321619
I1107 02:29:53.929093 36240 solver.cpp:285]     Train net output #0: loss = 0.00321619 (* 1 = 0.00321619 loss)
I1107 02:29:53.929100 36240 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I1107 02:30:06.743038 36240 solver.cpp:266] Iteration 9200 (3.90215 iter/s, 12.8135s/50 iter), loss = 0.00754629
I1107 02:30:06.743229 36240 solver.cpp:285]     Train net output #0: loss = 0.00754629 (* 1 = 0.00754629 loss)
I1107 02:30:06.743238 36240 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I1107 02:30:19.613090 36240 solver.cpp:266] Iteration 9250 (3.88519 iter/s, 12.8694s/50 iter), loss = 0.00242907
I1107 02:30:19.613118 36240 solver.cpp:285]     Train net output #0: loss = 0.00242906 (* 1 = 0.00242906 loss)
I1107 02:30:19.613126 36240 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I1107 02:30:32.467432 36240 solver.cpp:266] Iteration 9300 (3.88989 iter/s, 12.8538s/50 iter), loss = 0.00818507
I1107 02:30:32.467460 36240 solver.cpp:285]     Train net output #0: loss = 0.00818506 (* 1 = 0.00818506 loss)
I1107 02:30:32.467466 36240 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I1107 02:30:45.317418 36240 solver.cpp:266] Iteration 9350 (3.89121 iter/s, 12.8495s/50 iter), loss = 0.0030596
I1107 02:30:45.317596 36240 solver.cpp:285]     Train net output #0: loss = 0.00305959 (* 1 = 0.00305959 loss)
I1107 02:30:45.317605 36240 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I1107 02:30:58.170193 36240 solver.cpp:266] Iteration 9400 (3.89041 iter/s, 12.8521s/50 iter), loss = 0.0149431
I1107 02:30:58.170235 36240 solver.cpp:285]     Train net output #0: loss = 0.0149431 (* 1 = 0.0149431 loss)
I1107 02:30:58.170259 36240 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I1107 02:31:11.027676 36240 solver.cpp:266] Iteration 9450 (3.88895 iter/s, 12.857s/50 iter), loss = 0.0391044
I1107 02:31:11.027715 36240 solver.cpp:285]     Train net output #0: loss = 0.0391044 (* 1 = 0.0391044 loss)
I1107 02:31:11.027721 36240 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I1107 02:31:23.855607 36240 solver.cpp:266] Iteration 9500 (3.89791 iter/s, 12.8274s/50 iter), loss = 0.00863163
I1107 02:31:23.855780 36240 solver.cpp:285]     Train net output #0: loss = 0.00863163 (* 1 = 0.00863163 loss)
I1107 02:31:23.855790 36240 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I1107 02:31:36.722815 36240 solver.cpp:266] Iteration 9550 (3.88605 iter/s, 12.8665s/50 iter), loss = 0.0109059
I1107 02:31:36.722857 36240 solver.cpp:285]     Train net output #0: loss = 0.0109059 (* 1 = 0.0109059 loss)
I1107 02:31:36.722864 36240 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I1107 02:31:49.600987 36240 solver.cpp:266] Iteration 9600 (3.8827 iter/s, 12.8776s/50 iter), loss = 0.00626436
I1107 02:31:49.601016 36240 solver.cpp:285]     Train net output #0: loss = 0.00626435 (* 1 = 0.00626435 loss)
I1107 02:31:49.601022 36240 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I1107 02:32:02.458545 36240 solver.cpp:266] Iteration 9650 (3.88892 iter/s, 12.857s/50 iter), loss = 0.00821667
I1107 02:32:02.458739 36240 solver.cpp:285]     Train net output #0: loss = 0.00821666 (* 1 = 0.00821666 loss)
I1107 02:32:02.458748 36240 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I1107 02:32:15.311147 36240 solver.cpp:266] Iteration 9700 (3.89047 iter/s, 12.8519s/50 iter), loss = 0.0103627
I1107 02:32:15.311177 36240 solver.cpp:285]     Train net output #0: loss = 0.0103627 (* 1 = 0.0103627 loss)
I1107 02:32:15.311199 36240 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I1107 02:32:28.161890 36240 solver.cpp:266] Iteration 9750 (3.89098 iter/s, 12.8502s/50 iter), loss = 0.00978273
I1107 02:32:28.161919 36240 solver.cpp:285]     Train net output #0: loss = 0.00978272 (* 1 = 0.00978272 loss)
I1107 02:32:28.161927 36240 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I1107 02:32:41.004501 36240 solver.cpp:266] Iteration 9800 (3.89345 iter/s, 12.8421s/50 iter), loss = 0.00693188
I1107 02:32:41.004648 36240 solver.cpp:285]     Train net output #0: loss = 0.00693186 (* 1 = 0.00693186 loss)
I1107 02:32:41.004668 36240 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I1107 02:32:53.883790 36240 solver.cpp:266] Iteration 9850 (3.88239 iter/s, 12.8787s/50 iter), loss = 0.00906411
I1107 02:32:53.883821 36240 solver.cpp:285]     Train net output #0: loss = 0.00906408 (* 1 = 0.00906408 loss)
I1107 02:32:53.883827 36240 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I1107 02:33:06.753716 36240 solver.cpp:266] Iteration 9900 (3.88518 iter/s, 12.8694s/50 iter), loss = 0.00936045
I1107 02:33:06.753746 36240 solver.cpp:285]     Train net output #0: loss = 0.00936043 (* 1 = 0.00936043 loss)
I1107 02:33:06.753753 36240 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I1107 02:33:19.599623 36240 solver.cpp:266] Iteration 9950 (3.89245 iter/s, 12.8454s/50 iter), loss = 0.0181694
I1107 02:33:19.599788 36240 solver.cpp:285]     Train net output #0: loss = 0.0181694 (* 1 = 0.0181694 loss)
I1107 02:33:19.599798 36240 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I1107 02:33:32.181784 36240 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_10000.caffemodel
I1107 02:33:34.462846 36240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_10000.solverstate
I1107 02:33:34.941843 36240 solver.cpp:418] Iteration 10000, Testing net (#0)
I1107 02:33:36.406424 36240 solver.cpp:517]     Test net output #0: loss = 0.182615 (* 1 = 0.182615 loss)
I1107 02:33:36.406440 36240 solver.cpp:517]     Test net output #1: top-1 = 0.9515
I1107 02:33:36.651195 36240 solver.cpp:266] Iteration 10000 (2.93242 iter/s, 17.0508s/50 iter), loss = 0.0152211
I1107 02:33:36.651219 36240 solver.cpp:285]     Train net output #0: loss = 0.0152211 (* 1 = 0.0152211 loss)
I1107 02:33:36.651226 36240 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I1107 02:33:49.315078 36240 solver.cpp:266] Iteration 10050 (3.94839 iter/s, 12.6634s/50 iter), loss = 0.0113409
I1107 02:33:49.315119 36240 solver.cpp:285]     Train net output #0: loss = 0.0113409 (* 1 = 0.0113409 loss)
I1107 02:33:49.315125 36240 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I1107 02:34:02.094391 36240 solver.cpp:266] Iteration 10100 (3.91273 iter/s, 12.7788s/50 iter), loss = 0.00721258
I1107 02:34:02.094561 36240 solver.cpp:285]     Train net output #0: loss = 0.00721255 (* 1 = 0.00721255 loss)
I1107 02:34:02.094570 36240 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I1107 02:34:14.926167 36240 solver.cpp:266] Iteration 10150 (3.89678 iter/s, 12.8311s/50 iter), loss = 0.0189985
I1107 02:34:14.926203 36240 solver.cpp:285]     Train net output #0: loss = 0.0189985 (* 1 = 0.0189985 loss)
I1107 02:34:14.926226 36240 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I1107 02:34:27.791167 36240 solver.cpp:266] Iteration 10200 (3.88667 iter/s, 12.8645s/50 iter), loss = 0.00316922
I1107 02:34:27.791208 36240 solver.cpp:285]     Train net output #0: loss = 0.00316918 (* 1 = 0.00316918 loss)
I1107 02:34:27.791214 36240 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I1107 02:34:40.661329 36240 solver.cpp:266] Iteration 10250 (3.88512 iter/s, 12.8696s/50 iter), loss = 0.0175511
I1107 02:34:40.661530 36240 solver.cpp:285]     Train net output #0: loss = 0.0175511 (* 1 = 0.0175511 loss)
I1107 02:34:40.661542 36240 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I1107 02:34:53.514796 36240 solver.cpp:266] Iteration 10300 (3.89021 iter/s, 12.8528s/50 iter), loss = 0.00554248
I1107 02:34:53.514824 36240 solver.cpp:285]     Train net output #0: loss = 0.00554244 (* 1 = 0.00554244 loss)
I1107 02:34:53.514830 36240 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I1107 02:35:06.375326 36240 solver.cpp:266] Iteration 10350 (3.88802 iter/s, 12.86s/50 iter), loss = 0.0196406
I1107 02:35:06.375356 36240 solver.cpp:285]     Train net output #0: loss = 0.0196406 (* 1 = 0.0196406 loss)
I1107 02:35:06.375362 36240 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I1107 02:35:19.248847 36240 solver.cpp:266] Iteration 10400 (3.8841 iter/s, 12.873s/50 iter), loss = 0.00899514
I1107 02:35:19.249012 36240 solver.cpp:285]     Train net output #0: loss = 0.00899509 (* 1 = 0.00899509 loss)
I1107 02:35:19.249022 36240 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I1107 02:35:32.111757 36240 solver.cpp:266] Iteration 10450 (3.88734 iter/s, 12.8623s/50 iter), loss = 0.016564
I1107 02:35:32.111784 36240 solver.cpp:285]     Train net output #0: loss = 0.016564 (* 1 = 0.016564 loss)
I1107 02:35:32.111791 36240 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I1107 02:35:44.972193 36240 solver.cpp:266] Iteration 10500 (3.88805 iter/s, 12.8599s/50 iter), loss = 0.00730337
I1107 02:35:44.972221 36240 solver.cpp:285]     Train net output #0: loss = 0.00730333 (* 1 = 0.00730333 loss)
I1107 02:35:44.972227 36240 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I1107 02:35:57.834537 36240 solver.cpp:266] Iteration 10550 (3.88747 iter/s, 12.8618s/50 iter), loss = 0.00495657
I1107 02:35:57.836680 36240 solver.cpp:285]     Train net output #0: loss = 0.00495652 (* 1 = 0.00495652 loss)
I1107 02:35:57.836688 36240 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I1107 02:36:10.706115 36240 solver.cpp:266] Iteration 10600 (3.88532 iter/s, 12.8689s/50 iter), loss = 0.0166763
I1107 02:36:10.706143 36240 solver.cpp:285]     Train net output #0: loss = 0.0166763 (* 1 = 0.0166763 loss)
I1107 02:36:10.706149 36240 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I1107 02:36:23.559202 36240 solver.cpp:266] Iteration 10650 (3.89027 iter/s, 12.8526s/50 iter), loss = 0.0153885
I1107 02:36:23.559231 36240 solver.cpp:285]     Train net output #0: loss = 0.0153884 (* 1 = 0.0153884 loss)
I1107 02:36:23.559237 36240 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I1107 02:36:36.424895 36240 solver.cpp:266] Iteration 10700 (3.88646 iter/s, 12.8652s/50 iter), loss = 0.0141864
I1107 02:36:36.425704 36240 solver.cpp:285]     Train net output #0: loss = 0.0141863 (* 1 = 0.0141863 loss)
I1107 02:36:36.425714 36240 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I1107 02:36:49.310935 36240 solver.cpp:266] Iteration 10750 (3.88056 iter/s, 12.8847s/50 iter), loss = 0.00608987
I1107 02:36:49.310963 36240 solver.cpp:285]     Train net output #0: loss = 0.00608982 (* 1 = 0.00608982 loss)
I1107 02:36:49.310986 36240 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I1107 02:37:02.188320 36240 solver.cpp:266] Iteration 10800 (3.88293 iter/s, 12.8769s/50 iter), loss = 0.0127652
I1107 02:37:02.188349 36240 solver.cpp:285]     Train net output #0: loss = 0.0127651 (* 1 = 0.0127651 loss)
I1107 02:37:02.188366 36240 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I1107 02:37:15.043483 36240 solver.cpp:266] Iteration 10850 (3.88964 iter/s, 12.8546s/50 iter), loss = 0.01048
I1107 02:37:15.043670 36240 solver.cpp:285]     Train net output #0: loss = 0.01048 (* 1 = 0.01048 loss)
I1107 02:37:15.043680 36240 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I1107 02:37:27.879277 36240 solver.cpp:266] Iteration 10900 (3.89556 iter/s, 12.8351s/50 iter), loss = 0.00644587
I1107 02:37:27.879307 36240 solver.cpp:285]     Train net output #0: loss = 0.00644583 (* 1 = 0.00644583 loss)
I1107 02:37:27.879313 36240 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I1107 02:37:40.751497 36240 solver.cpp:266] Iteration 10950 (3.88449 iter/s, 12.8717s/50 iter), loss = 0.00700621
I1107 02:37:40.751528 36240 solver.cpp:285]     Train net output #0: loss = 0.00700617 (* 1 = 0.00700617 loss)
I1107 02:37:40.751533 36240 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I1107 02:37:53.369974 36240 solver.cpp:418] Iteration 11000, Testing net (#0)
I1107 02:37:54.924262 36240 solver.cpp:517]     Test net output #0: loss = 0.185062 (* 1 = 0.185062 loss)
I1107 02:37:54.924288 36240 solver.cpp:517]     Test net output #1: top-1 = 0.95075
I1107 02:37:55.178772 36240 solver.cpp:266] Iteration 11000 (3.4658 iter/s, 14.4267s/50 iter), loss = 0.0112515
I1107 02:37:55.178797 36240 solver.cpp:285]     Train net output #0: loss = 0.0112515 (* 1 = 0.0112515 loss)
I1107 02:37:55.178804 36240 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I1107 02:38:08.028421 36240 solver.cpp:266] Iteration 11050 (3.89131 iter/s, 12.8491s/50 iter), loss = 0.0105511
I1107 02:38:08.028452 36240 solver.cpp:285]     Train net output #0: loss = 0.0105511 (* 1 = 0.0105511 loss)
I1107 02:38:08.028458 36240 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I1107 02:38:20.887739 36240 solver.cpp:266] Iteration 11100 (3.88839 iter/s, 12.8588s/50 iter), loss = 0.00447595
I1107 02:38:20.887770 36240 solver.cpp:285]     Train net output #0: loss = 0.00447591 (* 1 = 0.00447591 loss)
I1107 02:38:20.887792 36240 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I1107 02:38:33.772687 36240 solver.cpp:266] Iteration 11150 (3.88065 iter/s, 12.8844s/50 iter), loss = 0.0094077
I1107 02:38:33.772856 36240 solver.cpp:285]     Train net output #0: loss = 0.00940766 (* 1 = 0.00940766 loss)
I1107 02:38:33.772866 36240 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I1107 02:38:46.601783 36240 solver.cpp:266] Iteration 11200 (3.89759 iter/s, 12.8284s/50 iter), loss = 0.0119714
I1107 02:38:46.601811 36240 solver.cpp:285]     Train net output #0: loss = 0.0119714 (* 1 = 0.0119714 loss)
I1107 02:38:46.601819 36240 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I1107 02:38:59.464720 36240 solver.cpp:266] Iteration 11250 (3.88729 iter/s, 12.8624s/50 iter), loss = 0.0106697
I1107 02:38:59.464746 36240 solver.cpp:285]     Train net output #0: loss = 0.0106697 (* 1 = 0.0106697 loss)
I1107 02:38:59.464752 36240 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I1107 02:39:12.339113 36240 solver.cpp:266] Iteration 11300 (3.88383 iter/s, 12.8739s/50 iter), loss = 0.0179468
I1107 02:39:12.339284 36240 solver.cpp:285]     Train net output #0: loss = 0.0179468 (* 1 = 0.0179468 loss)
I1107 02:39:12.339293 36240 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I1107 02:39:25.199079 36240 solver.cpp:266] Iteration 11350 (3.88823 iter/s, 12.8593s/50 iter), loss = 0.0296076
I1107 02:39:25.199110 36240 solver.cpp:285]     Train net output #0: loss = 0.0296076 (* 1 = 0.0296076 loss)
I1107 02:39:25.199116 36240 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I1107 02:39:38.068750 36240 solver.cpp:266] Iteration 11400 (3.88526 iter/s, 12.8692s/50 iter), loss = 0.0216317
I1107 02:39:38.068779 36240 solver.cpp:285]     Train net output #0: loss = 0.0216317 (* 1 = 0.0216317 loss)
I1107 02:39:38.068785 36240 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I1107 02:39:50.933799 36240 solver.cpp:266] Iteration 11450 (3.88666 iter/s, 12.8645s/50 iter), loss = 0.00779265
I1107 02:39:50.933979 36240 solver.cpp:285]     Train net output #0: loss = 0.00779261 (* 1 = 0.00779261 loss)
I1107 02:39:50.933986 36240 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I1107 02:40:03.810012 36240 solver.cpp:266] Iteration 11500 (3.88333 iter/s, 12.8755s/50 iter), loss = 0.0102179
I1107 02:40:03.810041 36240 solver.cpp:285]     Train net output #0: loss = 0.0102179 (* 1 = 0.0102179 loss)
I1107 02:40:03.810047 36240 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I1107 02:40:16.662196 36240 solver.cpp:266] Iteration 11550 (3.89055 iter/s, 12.8517s/50 iter), loss = 0.00426408
I1107 02:40:16.662226 36240 solver.cpp:285]     Train net output #0: loss = 0.00426404 (* 1 = 0.00426404 loss)
I1107 02:40:16.662235 36240 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I1107 02:40:29.540275 36240 solver.cpp:266] Iteration 11600 (3.88272 iter/s, 12.8776s/50 iter), loss = 0.00569187
I1107 02:40:29.540555 36240 solver.cpp:285]     Train net output #0: loss = 0.00569182 (* 1 = 0.00569182 loss)
I1107 02:40:29.540563 36240 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I1107 02:40:42.400581 36240 solver.cpp:266] Iteration 11650 (3.88816 iter/s, 12.8595s/50 iter), loss = 0.0103527
I1107 02:40:42.400614 36240 solver.cpp:285]     Train net output #0: loss = 0.0103527 (* 1 = 0.0103527 loss)
I1107 02:40:42.400619 36240 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I1107 02:40:55.261329 36240 solver.cpp:266] Iteration 11700 (3.88796 iter/s, 12.8602s/50 iter), loss = 0.00991939
I1107 02:40:55.261358 36240 solver.cpp:285]     Train net output #0: loss = 0.00991934 (* 1 = 0.00991934 loss)
I1107 02:40:55.261364 36240 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I1107 02:41:08.112658 36240 solver.cpp:266] Iteration 11750 (3.8908 iter/s, 12.8508s/50 iter), loss = 0.00300371
I1107 02:41:08.114859 36240 solver.cpp:285]     Train net output #0: loss = 0.00300366 (* 1 = 0.00300366 loss)
I1107 02:41:08.114867 36240 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I1107 02:41:20.978307 36240 solver.cpp:266] Iteration 11800 (3.88713 iter/s, 12.863s/50 iter), loss = 0.00910527
I1107 02:41:20.978338 36240 solver.cpp:285]     Train net output #0: loss = 0.00910522 (* 1 = 0.00910522 loss)
I1107 02:41:20.978343 36240 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I1107 02:41:33.855762 36240 solver.cpp:266] Iteration 11850 (3.88291 iter/s, 12.8769s/50 iter), loss = 0.00454358
I1107 02:41:33.855792 36240 solver.cpp:285]     Train net output #0: loss = 0.00454353 (* 1 = 0.00454353 loss)
I1107 02:41:33.855798 36240 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I1107 02:41:46.724381 36240 solver.cpp:266] Iteration 11900 (3.88558 iter/s, 12.8681s/50 iter), loss = 0.0288848
I1107 02:41:46.724541 36240 solver.cpp:285]     Train net output #0: loss = 0.0288848 (* 1 = 0.0288848 loss)
I1107 02:41:46.724550 36240 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I1107 02:41:59.573745 36240 solver.cpp:266] Iteration 11950 (3.89144 iter/s, 12.8487s/50 iter), loss = 0.00955064
I1107 02:41:59.573774 36240 solver.cpp:285]     Train net output #0: loss = 0.0095506 (* 1 = 0.0095506 loss)
I1107 02:41:59.573781 36240 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I1107 02:42:12.168893 36240 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_12000.caffemodel
I1107 02:42:14.435881 36240 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0/snapshots/_iter_12000.solverstate
I1107 02:42:15.016803 36240 solver.cpp:378] Iteration 12000, loss = 0.00254974
I1107 02:42:15.016836 36240 solver.cpp:418] Iteration 12000, Testing net (#0)
I1107 02:42:16.480978 36240 solver.cpp:517]     Test net output #0: loss = 0.186173 (* 1 = 0.186173 loss)
I1107 02:42:16.481004 36240 solver.cpp:517]     Test net output #1: top-1 = 0.951
I1107 02:42:16.481009 36240 solver.cpp:386] Optimization Done (3.87231 iter/s).
I1107 02:42:16.481011 36240 caffe_interface.cpp:530] Optimization Done.
I1107 02:42:17.457556 36681 pruning_runner.cpp:190] Sens info found, use it.
I1107 02:42:18.665087 36681 pruning_runner.cpp:217] Start compressing, please wait...
I1107 02:42:24.619588 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:42:30.661393 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:42:36.666296 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:42:42.688460 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:42:48.741168 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:42:55.005285 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:01.108692 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:07.325505 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:13.465047 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:19.753165 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:26.100478 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:32.291695 36681 pruning_runner.cpp:264] Compression complete 0%
I1107 02:43:38.376889 36681 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 02:43:38.377213 36681 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 02:43:38.377533 36681 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 02:43:38.377699 36681 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 02:43:38.377797 36681 layer_factory.hpp:77] Creating layer data
I1107 02:43:38.377833 36681 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:38.378232 36681 net.cpp:94] Creating Layer data
I1107 02:43:38.378244 36681 net.cpp:409] data -> data
I1107 02:43:38.378253 36681 net.cpp:409] data -> label
I1107 02:43:38.379730 37891 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 02:43:38.379760 37891 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 02:43:38.379931 36681 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 02:43:38.380020 36681 data_layer.cpp:83] output data size: 50,3,227,227
I1107 02:43:38.458815 36681 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:38.458883 36681 net.cpp:144] Setting up data
I1107 02:43:38.458890 36681 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 02:43:38.458895 36681 net.cpp:151] Top shape: 50 (50)
I1107 02:43:38.458897 36681 net.cpp:159] Memory required for data: 30917600
I1107 02:43:38.458904 36681 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 02:43:38.458926 36681 net.cpp:94] Creating Layer label_data_1_split
I1107 02:43:38.458933 36681 net.cpp:435] label_data_1_split <- label
I1107 02:43:38.458942 36681 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 02:43:38.458954 36681 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 02:43:38.459074 36681 net.cpp:144] Setting up label_data_1_split
I1107 02:43:38.459079 36681 net.cpp:151] Top shape: 50 (50)
I1107 02:43:38.459097 36681 net.cpp:151] Top shape: 50 (50)
I1107 02:43:38.459100 36681 net.cpp:159] Memory required for data: 30918000
I1107 02:43:38.459103 36681 layer_factory.hpp:77] Creating layer conv1
I1107 02:43:38.459116 36681 net.cpp:94] Creating Layer conv1
I1107 02:43:38.459121 36681 net.cpp:435] conv1 <- data
I1107 02:43:38.459128 36681 net.cpp:409] conv1 -> conv1
I1107 02:43:38.460794 36681 net.cpp:144] Setting up conv1
I1107 02:43:38.460806 36681 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:38.460808 36681 net.cpp:159] Memory required for data: 88998000
I1107 02:43:38.460819 36681 layer_factory.hpp:77] Creating layer bn1
I1107 02:43:38.460829 36681 net.cpp:94] Creating Layer bn1
I1107 02:43:38.460830 36681 net.cpp:435] bn1 <- conv1
I1107 02:43:38.460836 36681 net.cpp:409] bn1 -> scale1
I1107 02:43:38.461638 36681 net.cpp:144] Setting up bn1
I1107 02:43:38.461647 36681 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:38.461649 36681 net.cpp:159] Memory required for data: 147078000
I1107 02:43:38.461664 36681 layer_factory.hpp:77] Creating layer relu1
I1107 02:43:38.461675 36681 net.cpp:94] Creating Layer relu1
I1107 02:43:38.461679 36681 net.cpp:435] relu1 <- scale1
I1107 02:43:38.461686 36681 net.cpp:409] relu1 -> relu1
I1107 02:43:38.461717 36681 net.cpp:144] Setting up relu1
I1107 02:43:38.461724 36681 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:38.461726 36681 net.cpp:159] Memory required for data: 205158000
I1107 02:43:38.461730 36681 layer_factory.hpp:77] Creating layer pool1
I1107 02:43:38.461737 36681 net.cpp:94] Creating Layer pool1
I1107 02:43:38.461743 36681 net.cpp:435] pool1 <- relu1
I1107 02:43:38.461750 36681 net.cpp:409] pool1 -> pool1
I1107 02:43:38.461813 36681 net.cpp:144] Setting up pool1
I1107 02:43:38.461818 36681 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 02:43:38.461822 36681 net.cpp:159] Memory required for data: 219154800
I1107 02:43:38.461825 36681 layer_factory.hpp:77] Creating layer conv2
I1107 02:43:38.461836 36681 net.cpp:94] Creating Layer conv2
I1107 02:43:38.461843 36681 net.cpp:435] conv2 <- pool1
I1107 02:43:38.461850 36681 net.cpp:409] conv2 -> conv2
I1107 02:43:38.468936 36681 net.cpp:144] Setting up conv2
I1107 02:43:38.468955 36681 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:38.468957 36681 net.cpp:159] Memory required for data: 256479600
I1107 02:43:38.468972 36681 layer_factory.hpp:77] Creating layer bn2
I1107 02:43:38.468986 36681 net.cpp:94] Creating Layer bn2
I1107 02:43:38.468993 36681 net.cpp:435] bn2 <- conv2
I1107 02:43:38.469002 36681 net.cpp:409] bn2 -> scale2
I1107 02:43:38.471946 36681 net.cpp:144] Setting up bn2
I1107 02:43:38.471953 36681 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:38.471956 36681 net.cpp:159] Memory required for data: 293804400
I1107 02:43:38.471962 36681 layer_factory.hpp:77] Creating layer relu2
I1107 02:43:38.471968 36681 net.cpp:94] Creating Layer relu2
I1107 02:43:38.471971 36681 net.cpp:435] relu2 <- scale2
I1107 02:43:38.471974 36681 net.cpp:409] relu2 -> relu2
I1107 02:43:38.471992 36681 net.cpp:144] Setting up relu2
I1107 02:43:38.471995 36681 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:38.471997 36681 net.cpp:159] Memory required for data: 331129200
I1107 02:43:38.472000 36681 layer_factory.hpp:77] Creating layer pool2
I1107 02:43:38.472005 36681 net.cpp:94] Creating Layer pool2
I1107 02:43:38.472008 36681 net.cpp:435] pool2 <- relu2
I1107 02:43:38.472013 36681 net.cpp:409] pool2 -> pool2
I1107 02:43:38.474391 36681 net.cpp:144] Setting up pool2
I1107 02:43:38.474397 36681 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:38.474400 36681 net.cpp:159] Memory required for data: 339782000
I1107 02:43:38.474402 36681 layer_factory.hpp:77] Creating layer conv3
I1107 02:43:38.474409 36681 net.cpp:94] Creating Layer conv3
I1107 02:43:38.474412 36681 net.cpp:435] conv3 <- pool2
I1107 02:43:38.474417 36681 net.cpp:409] conv3 -> conv3
I1107 02:43:38.484552 36681 net.cpp:144] Setting up conv3
I1107 02:43:38.484575 36681 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:38.484609 36681 net.cpp:159] Memory required for data: 352761200
I1107 02:43:38.484629 36681 layer_factory.hpp:77] Creating layer relu3
I1107 02:43:38.484647 36681 net.cpp:94] Creating Layer relu3
I1107 02:43:38.484663 36681 net.cpp:435] relu3 <- conv3
I1107 02:43:38.484679 36681 net.cpp:409] relu3 -> relu3
I1107 02:43:38.484717 36681 net.cpp:144] Setting up relu3
I1107 02:43:38.484725 36681 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:38.484728 36681 net.cpp:159] Memory required for data: 365740400
I1107 02:43:38.484741 36681 layer_factory.hpp:77] Creating layer conv4
I1107 02:43:38.484757 36681 net.cpp:94] Creating Layer conv4
I1107 02:43:38.484764 36681 net.cpp:435] conv4 <- relu3
I1107 02:43:38.484771 36681 net.cpp:409] conv4 -> conv4
I1107 02:43:38.498456 36681 net.cpp:144] Setting up conv4
I1107 02:43:38.498478 36681 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:38.498481 36681 net.cpp:159] Memory required for data: 378719600
I1107 02:43:38.498493 36681 layer_factory.hpp:77] Creating layer relu4
I1107 02:43:38.498502 36681 net.cpp:94] Creating Layer relu4
I1107 02:43:38.498505 36681 net.cpp:435] relu4 <- conv4
I1107 02:43:38.498512 36681 net.cpp:409] relu4 -> relu4
I1107 02:43:38.498533 36681 net.cpp:144] Setting up relu4
I1107 02:43:38.498538 36681 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:38.498540 36681 net.cpp:159] Memory required for data: 391698800
I1107 02:43:38.498543 36681 layer_factory.hpp:77] Creating layer conv5
I1107 02:43:38.498550 36681 net.cpp:94] Creating Layer conv5
I1107 02:43:38.498555 36681 net.cpp:435] conv5 <- relu4
I1107 02:43:38.498560 36681 net.cpp:409] conv5 -> conv5
I1107 02:43:38.508360 36681 net.cpp:144] Setting up conv5
I1107 02:43:38.508383 36681 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:38.508386 36681 net.cpp:159] Memory required for data: 400351600
I1107 02:43:38.508394 36681 layer_factory.hpp:77] Creating layer relu5
I1107 02:43:38.508401 36681 net.cpp:94] Creating Layer relu5
I1107 02:43:38.508404 36681 net.cpp:435] relu5 <- conv5
I1107 02:43:38.508410 36681 net.cpp:409] relu5 -> relu5
I1107 02:43:38.508448 36681 net.cpp:144] Setting up relu5
I1107 02:43:38.508455 36681 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:38.508456 36681 net.cpp:159] Memory required for data: 409004400
I1107 02:43:38.508458 36681 layer_factory.hpp:77] Creating layer pool5
I1107 02:43:38.508466 36681 net.cpp:94] Creating Layer pool5
I1107 02:43:38.508467 36681 net.cpp:435] pool5 <- relu5
I1107 02:43:38.508471 36681 net.cpp:409] pool5 -> pool5
I1107 02:43:38.508496 36681 net.cpp:144] Setting up pool5
I1107 02:43:38.508502 36681 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 02:43:38.508503 36681 net.cpp:159] Memory required for data: 410847600
I1107 02:43:38.508505 36681 layer_factory.hpp:77] Creating layer fc6
I1107 02:43:38.508512 36681 net.cpp:94] Creating Layer fc6
I1107 02:43:38.508514 36681 net.cpp:435] fc6 <- pool5
I1107 02:43:38.508518 36681 net.cpp:409] fc6 -> fc6
I1107 02:43:38.839447 36681 net.cpp:144] Setting up fc6
I1107 02:43:38.839470 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.839473 36681 net.cpp:159] Memory required for data: 411666800
I1107 02:43:38.839498 36681 layer_factory.hpp:77] Creating layer relu6
I1107 02:43:38.839504 36681 net.cpp:94] Creating Layer relu6
I1107 02:43:38.839507 36681 net.cpp:435] relu6 <- fc6
I1107 02:43:38.839515 36681 net.cpp:409] relu6 -> relu6
I1107 02:43:38.839538 36681 net.cpp:144] Setting up relu6
I1107 02:43:38.839541 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.839543 36681 net.cpp:159] Memory required for data: 412486000
I1107 02:43:38.839545 36681 layer_factory.hpp:77] Creating layer drop6
I1107 02:43:38.839550 36681 net.cpp:94] Creating Layer drop6
I1107 02:43:38.839552 36681 net.cpp:435] drop6 <- relu6
I1107 02:43:38.839555 36681 net.cpp:409] drop6 -> drop6
I1107 02:43:38.839587 36681 net.cpp:144] Setting up drop6
I1107 02:43:38.839592 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.839594 36681 net.cpp:159] Memory required for data: 413305200
I1107 02:43:38.839617 36681 layer_factory.hpp:77] Creating layer fc7
I1107 02:43:38.839623 36681 net.cpp:94] Creating Layer fc7
I1107 02:43:38.839625 36681 net.cpp:435] fc7 <- drop6
I1107 02:43:38.839629 36681 net.cpp:409] fc7 -> fc7
I1107 02:43:38.978273 36681 net.cpp:144] Setting up fc7
I1107 02:43:38.978298 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.978301 36681 net.cpp:159] Memory required for data: 414124400
I1107 02:43:38.978308 36681 layer_factory.hpp:77] Creating layer bn7
I1107 02:43:38.978318 36681 net.cpp:94] Creating Layer bn7
I1107 02:43:38.978322 36681 net.cpp:435] bn7 <- fc7
I1107 02:43:38.978327 36681 net.cpp:409] bn7 -> scale7
I1107 02:43:38.978878 36681 net.cpp:144] Setting up bn7
I1107 02:43:38.978886 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.978889 36681 net.cpp:159] Memory required for data: 414943600
I1107 02:43:38.978896 36681 layer_factory.hpp:77] Creating layer relu7
I1107 02:43:38.978902 36681 net.cpp:94] Creating Layer relu7
I1107 02:43:38.978905 36681 net.cpp:435] relu7 <- scale7
I1107 02:43:38.978911 36681 net.cpp:409] relu7 -> relu7
I1107 02:43:38.978933 36681 net.cpp:144] Setting up relu7
I1107 02:43:38.978940 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.978945 36681 net.cpp:159] Memory required for data: 415762800
I1107 02:43:38.978948 36681 layer_factory.hpp:77] Creating layer drop7
I1107 02:43:38.978955 36681 net.cpp:94] Creating Layer drop7
I1107 02:43:38.978957 36681 net.cpp:435] drop7 <- relu7
I1107 02:43:38.978962 36681 net.cpp:409] drop7 -> drop7
I1107 02:43:38.978994 36681 net.cpp:144] Setting up drop7
I1107 02:43:38.978999 36681 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:38.979002 36681 net.cpp:159] Memory required for data: 416582000
I1107 02:43:38.979005 36681 layer_factory.hpp:77] Creating layer fc8
I1107 02:43:38.979012 36681 net.cpp:94] Creating Layer fc8
I1107 02:43:38.979013 36681 net.cpp:435] fc8 <- drop7
I1107 02:43:38.979018 36681 net.cpp:409] fc8 -> fc8
I1107 02:43:38.979897 36681 net.cpp:144] Setting up fc8
I1107 02:43:38.979908 36681 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:38.979912 36681 net.cpp:159] Memory required for data: 416582400
I1107 02:43:38.979917 36681 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 02:43:38.979923 36681 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 02:43:38.979926 36681 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 02:43:38.979933 36681 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 02:43:38.979939 36681 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 02:43:38.979974 36681 net.cpp:144] Setting up fc8_fc8_0_split
I1107 02:43:38.979982 36681 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:38.979986 36681 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:38.979988 36681 net.cpp:159] Memory required for data: 416583200
I1107 02:43:38.979991 36681 layer_factory.hpp:77] Creating layer loss
I1107 02:43:38.979996 36681 net.cpp:94] Creating Layer loss
I1107 02:43:38.980000 36681 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 02:43:38.980003 36681 net.cpp:435] loss <- label_data_1_split_0
I1107 02:43:38.980010 36681 net.cpp:409] loss -> loss
I1107 02:43:38.980020 36681 layer_factory.hpp:77] Creating layer loss
I1107 02:43:38.980094 36681 net.cpp:144] Setting up loss
I1107 02:43:38.980099 36681 net.cpp:151] Top shape: (1)
I1107 02:43:38.980103 36681 net.cpp:154]     with loss weight 1
I1107 02:43:38.980113 36681 net.cpp:159] Memory required for data: 416583204
I1107 02:43:38.980114 36681 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 02:43:38.980120 36681 net.cpp:94] Creating Layer accuracy-top1
I1107 02:43:38.980124 36681 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 02:43:38.980129 36681 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 02:43:38.980135 36681 net.cpp:409] accuracy-top1 -> top-1
I1107 02:43:38.980146 36681 net.cpp:144] Setting up accuracy-top1
I1107 02:43:38.980152 36681 net.cpp:151] Top shape: (1)
I1107 02:43:38.980156 36681 net.cpp:159] Memory required for data: 416583208
I1107 02:43:38.980175 36681 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 02:43:38.980180 36681 net.cpp:220] loss needs backward computation.
I1107 02:43:38.980185 36681 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 02:43:38.980190 36681 net.cpp:220] fc8 needs backward computation.
I1107 02:43:38.980193 36681 net.cpp:220] drop7 needs backward computation.
I1107 02:43:38.980196 36681 net.cpp:220] relu7 needs backward computation.
I1107 02:43:38.980201 36681 net.cpp:220] bn7 needs backward computation.
I1107 02:43:38.980206 36681 net.cpp:220] fc7 needs backward computation.
I1107 02:43:38.980209 36681 net.cpp:220] drop6 needs backward computation.
I1107 02:43:38.980212 36681 net.cpp:220] relu6 needs backward computation.
I1107 02:43:38.980216 36681 net.cpp:220] fc6 needs backward computation.
I1107 02:43:38.980221 36681 net.cpp:220] pool5 needs backward computation.
I1107 02:43:38.980226 36681 net.cpp:220] relu5 needs backward computation.
I1107 02:43:38.980229 36681 net.cpp:220] conv5 needs backward computation.
I1107 02:43:38.980233 36681 net.cpp:220] relu4 needs backward computation.
I1107 02:43:38.980237 36681 net.cpp:220] conv4 needs backward computation.
I1107 02:43:38.980240 36681 net.cpp:220] relu3 needs backward computation.
I1107 02:43:38.980244 36681 net.cpp:220] conv3 needs backward computation.
I1107 02:43:38.980247 36681 net.cpp:220] pool2 needs backward computation.
I1107 02:43:38.980252 36681 net.cpp:220] relu2 needs backward computation.
I1107 02:43:38.980254 36681 net.cpp:220] bn2 needs backward computation.
I1107 02:43:38.980258 36681 net.cpp:220] conv2 needs backward computation.
I1107 02:43:38.980262 36681 net.cpp:220] pool1 needs backward computation.
I1107 02:43:38.980267 36681 net.cpp:220] relu1 needs backward computation.
I1107 02:43:38.980269 36681 net.cpp:220] bn1 needs backward computation.
I1107 02:43:38.980273 36681 net.cpp:220] conv1 needs backward computation.
I1107 02:43:38.980278 36681 net.cpp:222] label_data_1_split does not need backward computation.
I1107 02:43:38.980281 36681 net.cpp:222] data does not need backward computation.
I1107 02:43:38.980284 36681 net.cpp:264] This network produces output loss
I1107 02:43:38.980288 36681 net.cpp:264] This network produces output top-1
I1107 02:43:38.980312 36681 net.cpp:284] Network initialization done.
I1107 02:43:39.054612 36681 caffe_interface.cpp:363] Running for 80 iterations.
I1107 02:43:39.098079 36681 caffe_interface.cpp:125] Batch 0, loss = 0.130165
I1107 02:43:39.098103 36681 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I1107 02:43:39.118480 36681 caffe_interface.cpp:125] Batch 1, loss = 0.064811
I1107 02:43:39.118496 36681 caffe_interface.cpp:125] Batch 1, top-1 = 0.94
I1107 02:43:39.138226 36681 caffe_interface.cpp:125] Batch 2, loss = 0.409306
I1107 02:43:39.138236 36681 caffe_interface.cpp:125] Batch 2, top-1 = 0.92
I1107 02:43:39.157474 36681 caffe_interface.cpp:125] Batch 3, loss = 0.660754
I1107 02:43:39.157481 36681 caffe_interface.cpp:125] Batch 3, top-1 = 0.88
I1107 02:43:39.176903 36681 caffe_interface.cpp:125] Batch 4, loss = 0.352614
I1107 02:43:39.176908 36681 caffe_interface.cpp:125] Batch 4, top-1 = 0.92
I1107 02:43:39.197217 36681 caffe_interface.cpp:125] Batch 5, loss = 0.0118616
I1107 02:43:39.197226 36681 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 02:43:39.217330 36681 caffe_interface.cpp:125] Batch 6, loss = 0.23968
I1107 02:43:39.217337 36681 caffe_interface.cpp:125] Batch 6, top-1 = 0.96
I1107 02:43:39.236794 36681 caffe_interface.cpp:125] Batch 7, loss = 0.0960491
I1107 02:43:39.236802 36681 caffe_interface.cpp:125] Batch 7, top-1 = 0.98
I1107 02:43:39.256913 36681 caffe_interface.cpp:125] Batch 8, loss = 0.0194443
I1107 02:43:39.256922 36681 caffe_interface.cpp:125] Batch 8, top-1 = 1
I1107 02:43:39.274801 36681 caffe_interface.cpp:125] Batch 9, loss = 0.379004
I1107 02:43:39.274809 36681 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I1107 02:43:39.292793 36681 caffe_interface.cpp:125] Batch 10, loss = 0.0749314
I1107 02:43:39.292798 36681 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I1107 02:43:39.310894 36681 caffe_interface.cpp:125] Batch 11, loss = 0.423768
I1107 02:43:39.310902 36681 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I1107 02:43:39.329990 36681 caffe_interface.cpp:125] Batch 12, loss = 0.403304
I1107 02:43:39.329998 36681 caffe_interface.cpp:125] Batch 12, top-1 = 0.9
I1107 02:43:39.348366 36681 caffe_interface.cpp:125] Batch 13, loss = 0.136489
I1107 02:43:39.348374 36681 caffe_interface.cpp:125] Batch 13, top-1 = 0.96
I1107 02:43:39.367326 36681 caffe_interface.cpp:125] Batch 14, loss = 0.208619
I1107 02:43:39.367336 36681 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I1107 02:43:39.385380 36681 caffe_interface.cpp:125] Batch 15, loss = 0.0949489
I1107 02:43:39.385390 36681 caffe_interface.cpp:125] Batch 15, top-1 = 0.94
I1107 02:43:39.403865 36681 caffe_interface.cpp:125] Batch 16, loss = 0.0371586
I1107 02:43:39.403873 36681 caffe_interface.cpp:125] Batch 16, top-1 = 0.98
I1107 02:43:39.422024 36681 caffe_interface.cpp:125] Batch 17, loss = 0.0608946
I1107 02:43:39.422032 36681 caffe_interface.cpp:125] Batch 17, top-1 = 0.98
I1107 02:43:39.440879 36681 caffe_interface.cpp:125] Batch 18, loss = 0.157082
I1107 02:43:39.440886 36681 caffe_interface.cpp:125] Batch 18, top-1 = 0.96
I1107 02:43:39.459090 36681 caffe_interface.cpp:125] Batch 19, loss = 0.0971258
I1107 02:43:39.459098 36681 caffe_interface.cpp:125] Batch 19, top-1 = 0.96
I1107 02:43:39.478334 36681 caffe_interface.cpp:125] Batch 20, loss = 0.0914682
I1107 02:43:39.478343 36681 caffe_interface.cpp:125] Batch 20, top-1 = 0.94
I1107 02:43:39.497750 36681 caffe_interface.cpp:125] Batch 21, loss = 0.167237
I1107 02:43:39.497766 36681 caffe_interface.cpp:125] Batch 21, top-1 = 0.96
I1107 02:43:39.516213 36681 caffe_interface.cpp:125] Batch 22, loss = 0.178254
I1107 02:43:39.516222 36681 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I1107 02:43:39.534862 36681 caffe_interface.cpp:125] Batch 23, loss = 0.165698
I1107 02:43:39.534868 36681 caffe_interface.cpp:125] Batch 23, top-1 = 0.98
I1107 02:43:39.552886 36681 caffe_interface.cpp:125] Batch 24, loss = 0.214403
I1107 02:43:39.552893 36681 caffe_interface.cpp:125] Batch 24, top-1 = 0.9
I1107 02:43:39.571632 36681 caffe_interface.cpp:125] Batch 25, loss = 0.144658
I1107 02:43:39.571640 36681 caffe_interface.cpp:125] Batch 25, top-1 = 0.96
I1107 02:43:39.589576 36681 caffe_interface.cpp:125] Batch 26, loss = 0.109018
I1107 02:43:39.589583 36681 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I1107 02:43:39.607578 36681 caffe_interface.cpp:125] Batch 27, loss = 0.114305
I1107 02:43:39.607585 36681 caffe_interface.cpp:125] Batch 27, top-1 = 0.98
I1107 02:43:39.625583 36681 caffe_interface.cpp:125] Batch 28, loss = 0.237567
I1107 02:43:39.625592 36681 caffe_interface.cpp:125] Batch 28, top-1 = 0.9
I1107 02:43:39.644018 36681 caffe_interface.cpp:125] Batch 29, loss = 0.0135332
I1107 02:43:39.644029 36681 caffe_interface.cpp:125] Batch 29, top-1 = 1
I1107 02:43:39.662204 36681 caffe_interface.cpp:125] Batch 30, loss = 0.0196078
I1107 02:43:39.662214 36681 caffe_interface.cpp:125] Batch 30, top-1 = 0.98
I1107 02:43:39.681313 36681 caffe_interface.cpp:125] Batch 31, loss = 0.0172651
I1107 02:43:39.681320 36681 caffe_interface.cpp:125] Batch 31, top-1 = 1
I1107 02:43:39.699287 36681 caffe_interface.cpp:125] Batch 32, loss = 0.0867615
I1107 02:43:39.699295 36681 caffe_interface.cpp:125] Batch 32, top-1 = 0.98
I1107 02:43:39.718515 36681 caffe_interface.cpp:125] Batch 33, loss = 0.157893
I1107 02:43:39.718523 36681 caffe_interface.cpp:125] Batch 33, top-1 = 0.96
I1107 02:43:39.736730 36681 caffe_interface.cpp:125] Batch 34, loss = 0.259408
I1107 02:43:39.736738 36681 caffe_interface.cpp:125] Batch 34, top-1 = 0.92
I1107 02:43:39.755710 36681 caffe_interface.cpp:125] Batch 35, loss = 0.0199272
I1107 02:43:39.755718 36681 caffe_interface.cpp:125] Batch 35, top-1 = 1
I1107 02:43:39.773887 36681 caffe_interface.cpp:125] Batch 36, loss = 0.231628
I1107 02:43:39.773895 36681 caffe_interface.cpp:125] Batch 36, top-1 = 0.92
I1107 02:43:39.792850 36681 caffe_interface.cpp:125] Batch 37, loss = 0.125434
I1107 02:43:39.792872 36681 caffe_interface.cpp:125] Batch 37, top-1 = 0.96
I1107 02:43:39.811292 36681 caffe_interface.cpp:125] Batch 38, loss = 0.302331
I1107 02:43:39.811300 36681 caffe_interface.cpp:125] Batch 38, top-1 = 0.92
I1107 02:43:39.830094 36681 caffe_interface.cpp:125] Batch 39, loss = 0.0758601
I1107 02:43:39.830101 36681 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I1107 02:43:39.847904 36681 caffe_interface.cpp:125] Batch 40, loss = 0.110498
I1107 02:43:39.847910 36681 caffe_interface.cpp:125] Batch 40, top-1 = 0.98
I1107 02:43:39.866880 36681 caffe_interface.cpp:125] Batch 41, loss = 0.0786084
I1107 02:43:39.866888 36681 caffe_interface.cpp:125] Batch 41, top-1 = 0.98
I1107 02:43:39.884675 36681 caffe_interface.cpp:125] Batch 42, loss = 0.00673806
I1107 02:43:39.884683 36681 caffe_interface.cpp:125] Batch 42, top-1 = 1
I1107 02:43:39.903805 36681 caffe_interface.cpp:125] Batch 43, loss = 0.366113
I1107 02:43:39.903816 36681 caffe_interface.cpp:125] Batch 43, top-1 = 0.92
I1107 02:43:39.921866 36681 caffe_interface.cpp:125] Batch 44, loss = 0.306635
I1107 02:43:39.921876 36681 caffe_interface.cpp:125] Batch 44, top-1 = 0.94
I1107 02:43:39.940800 36681 caffe_interface.cpp:125] Batch 45, loss = 0.517978
I1107 02:43:39.940809 36681 caffe_interface.cpp:125] Batch 45, top-1 = 0.9
I1107 02:43:39.959996 36681 caffe_interface.cpp:125] Batch 46, loss = 0.0420171
I1107 02:43:39.960003 36681 caffe_interface.cpp:125] Batch 46, top-1 = 0.98
I1107 02:43:39.978202 36681 caffe_interface.cpp:125] Batch 47, loss = 0.213479
I1107 02:43:39.978210 36681 caffe_interface.cpp:125] Batch 47, top-1 = 0.94
I1107 02:43:39.997454 36681 caffe_interface.cpp:125] Batch 48, loss = 0.215365
I1107 02:43:39.997462 36681 caffe_interface.cpp:125] Batch 48, top-1 = 0.94
I1107 02:43:40.015286 36681 caffe_interface.cpp:125] Batch 49, loss = 0.175168
I1107 02:43:40.015295 36681 caffe_interface.cpp:125] Batch 49, top-1 = 0.94
I1107 02:43:40.033881 36681 caffe_interface.cpp:125] Batch 50, loss = 0.224275
I1107 02:43:40.033888 36681 caffe_interface.cpp:125] Batch 50, top-1 = 0.94
I1107 02:43:40.051844 36681 caffe_interface.cpp:125] Batch 51, loss = 0.430807
I1107 02:43:40.051852 36681 caffe_interface.cpp:125] Batch 51, top-1 = 0.86
I1107 02:43:40.069713 36681 caffe_interface.cpp:125] Batch 52, loss = 0.120554
I1107 02:43:40.069721 36681 caffe_interface.cpp:125] Batch 52, top-1 = 0.98
I1107 02:43:40.087527 36681 caffe_interface.cpp:125] Batch 53, loss = 0.10239
I1107 02:43:40.087535 36681 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I1107 02:43:40.106097 36681 caffe_interface.cpp:125] Batch 54, loss = 0.00863703
I1107 02:43:40.106106 36681 caffe_interface.cpp:125] Batch 54, top-1 = 1
I1107 02:43:40.124269 36681 caffe_interface.cpp:125] Batch 55, loss = 0.390642
I1107 02:43:40.124277 36681 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I1107 02:43:40.143133 36681 caffe_interface.cpp:125] Batch 56, loss = 0.0842315
I1107 02:43:40.143141 36681 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 02:43:40.161090 36681 caffe_interface.cpp:125] Batch 57, loss = 0.0256587
I1107 02:43:40.161099 36681 caffe_interface.cpp:125] Batch 57, top-1 = 0.98
I1107 02:43:40.180465 36681 caffe_interface.cpp:125] Batch 58, loss = 0.187883
I1107 02:43:40.180474 36681 caffe_interface.cpp:125] Batch 58, top-1 = 0.9
I1107 02:43:40.198426 36681 caffe_interface.cpp:125] Batch 59, loss = 0.323547
I1107 02:43:40.198437 36681 caffe_interface.cpp:125] Batch 59, top-1 = 0.92
I1107 02:43:40.217372 36681 caffe_interface.cpp:125] Batch 60, loss = 0.307618
I1107 02:43:40.217381 36681 caffe_interface.cpp:125] Batch 60, top-1 = 0.94
I1107 02:43:40.235549 36681 caffe_interface.cpp:125] Batch 61, loss = 0.150091
I1107 02:43:40.235558 36681 caffe_interface.cpp:125] Batch 61, top-1 = 0.96
I1107 02:43:40.254755 36681 caffe_interface.cpp:125] Batch 62, loss = 0.157509
I1107 02:43:40.254763 36681 caffe_interface.cpp:125] Batch 62, top-1 = 0.96
I1107 02:43:40.272711 36681 caffe_interface.cpp:125] Batch 63, loss = 0.278436
I1107 02:43:40.272719 36681 caffe_interface.cpp:125] Batch 63, top-1 = 0.96
I1107 02:43:40.292906 36681 caffe_interface.cpp:125] Batch 64, loss = 0.404674
I1107 02:43:40.292920 36681 caffe_interface.cpp:125] Batch 64, top-1 = 0.88
I1107 02:43:40.311794 36681 caffe_interface.cpp:125] Batch 65, loss = 0.197027
I1107 02:43:40.311806 36681 caffe_interface.cpp:125] Batch 65, top-1 = 0.96
I1107 02:43:40.329435 36681 caffe_interface.cpp:125] Batch 66, loss = 0.517516
I1107 02:43:40.329442 36681 caffe_interface.cpp:125] Batch 66, top-1 = 0.92
I1107 02:43:40.348306 36681 caffe_interface.cpp:125] Batch 67, loss = 0.239617
I1107 02:43:40.348315 36681 caffe_interface.cpp:125] Batch 67, top-1 = 0.92
I1107 02:43:40.366271 36681 caffe_interface.cpp:125] Batch 68, loss = 0.285258
I1107 02:43:40.366277 36681 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I1107 02:43:40.385007 36681 caffe_interface.cpp:125] Batch 69, loss = 0.161983
I1107 02:43:40.385015 36681 caffe_interface.cpp:125] Batch 69, top-1 = 0.94
I1107 02:43:40.403167 36681 caffe_interface.cpp:125] Batch 70, loss = 0.00595577
I1107 02:43:40.403172 36681 caffe_interface.cpp:125] Batch 70, top-1 = 1
I1107 02:43:40.421018 36681 caffe_interface.cpp:125] Batch 71, loss = 0.218233
I1107 02:43:40.421025 36681 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I1107 02:43:40.438956 36681 caffe_interface.cpp:125] Batch 72, loss = 0.164423
I1107 02:43:40.438963 36681 caffe_interface.cpp:125] Batch 72, top-1 = 0.98
I1107 02:43:40.457098 36681 caffe_interface.cpp:125] Batch 73, loss = 0.0816387
I1107 02:43:40.457106 36681 caffe_interface.cpp:125] Batch 73, top-1 = 0.96
I1107 02:43:40.475294 36681 caffe_interface.cpp:125] Batch 74, loss = 0.0618442
I1107 02:43:40.475304 36681 caffe_interface.cpp:125] Batch 74, top-1 = 0.96
I1107 02:43:40.493789 36681 caffe_interface.cpp:125] Batch 75, loss = 0.286663
I1107 02:43:40.493798 36681 caffe_interface.cpp:125] Batch 75, top-1 = 0.92
I1107 02:43:40.511734 36681 caffe_interface.cpp:125] Batch 76, loss = 0.240671
I1107 02:43:40.511742 36681 caffe_interface.cpp:125] Batch 76, top-1 = 0.94
I1107 02:43:40.530452 36681 caffe_interface.cpp:125] Batch 77, loss = 0.257717
I1107 02:43:40.530458 36681 caffe_interface.cpp:125] Batch 77, top-1 = 0.94
I1107 02:43:40.548238 36681 caffe_interface.cpp:125] Batch 78, loss = 0.154695
I1107 02:43:40.548244 36681 caffe_interface.cpp:125] Batch 78, top-1 = 0.94
I1107 02:43:40.566928 36681 caffe_interface.cpp:125] Batch 79, loss = 0.00306388
I1107 02:43:40.566936 36681 caffe_interface.cpp:125] Batch 79, top-1 = 1
I1107 02:43:40.566937 36681 caffe_interface.cpp:130] Loss: 0.186202
I1107 02:43:40.566942 36681 caffe_interface.cpp:142] loss = 0.186202 (* 1 = 0.186202 loss)
I1107 02:43:40.566946 36681 caffe_interface.cpp:142] top-1 = 0.951
I1107 02:43:40.803973 36681 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/sparse.caffemodel
I1107 02:43:40.803999 36681 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.1:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.950999737    | 0.00424987078  |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 1526191        | -59.4636688%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 1397275624     | -35.1286659%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config1.prototxt
I1107 02:43:41.063823 37922 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/net_finetune.prototxt
I1107 02:43:41.234535 37922 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 02:43:41.235008 37922 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24848105472, dev_info[0]: total=25620447232 free=24848105472
I1107 02:43:41.235018 37922 caffe_interface.cpp:493] Using GPUs 0
I1107 02:43:41.235260 37922 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 02:43:41.818444 37922 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/net_finetune.prototxt"
type: "Adam"
I1107 02:43:41.818562 37922 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/net_finetune.prototxt
I1107 02:43:41.818787 37922 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 02:43:41.818800 37922 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 02:43:41.818945 37922 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 02:43:41.819012 37922 layer_factory.hpp:77] Creating layer data
I1107 02:43:41.819137 37922 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:41.819494 37922 net.cpp:94] Creating Layer data
I1107 02:43:41.819501 37922 net.cpp:409] data -> data
I1107 02:43:41.819510 37922 net.cpp:409] data -> label
I1107 02:43:41.821007 37961 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 02:43:41.821051 37961 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 02:43:41.821303 37922 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 02:43:41.821368 37922 data_layer.cpp:83] output data size: 256,3,227,227
I1107 02:43:42.203014 37922 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:42.203065 37922 net.cpp:144] Setting up data
I1107 02:43:42.203073 37922 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 02:43:42.203076 37922 net.cpp:151] Top shape: 256 (256)
I1107 02:43:42.203095 37922 net.cpp:159] Memory required for data: 158298112
I1107 02:43:42.203100 37922 layer_factory.hpp:77] Creating layer conv1
I1107 02:43:42.203115 37922 net.cpp:94] Creating Layer conv1
I1107 02:43:42.203119 37922 net.cpp:435] conv1 <- data
I1107 02:43:42.203135 37922 net.cpp:409] conv1 -> conv1
I1107 02:43:42.205029 37922 net.cpp:144] Setting up conv1
I1107 02:43:42.205040 37922 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 02:43:42.205044 37922 net.cpp:159] Memory required for data: 455667712
I1107 02:43:42.205058 37922 layer_factory.hpp:77] Creating layer bn1
I1107 02:43:42.205067 37922 net.cpp:94] Creating Layer bn1
I1107 02:43:42.205070 37922 net.cpp:435] bn1 <- conv1
I1107 02:43:42.205075 37922 net.cpp:409] bn1 -> scale1
I1107 02:43:42.206305 37922 net.cpp:144] Setting up bn1
I1107 02:43:42.206311 37922 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 02:43:42.206315 37922 net.cpp:159] Memory required for data: 753037312
I1107 02:43:42.206323 37922 layer_factory.hpp:77] Creating layer relu1
I1107 02:43:42.206328 37922 net.cpp:94] Creating Layer relu1
I1107 02:43:42.206332 37922 net.cpp:435] relu1 <- scale1
I1107 02:43:42.206336 37922 net.cpp:409] relu1 -> relu1
I1107 02:43:42.206365 37922 net.cpp:144] Setting up relu1
I1107 02:43:42.206370 37922 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 02:43:42.206374 37922 net.cpp:159] Memory required for data: 1050406912
I1107 02:43:42.206377 37922 layer_factory.hpp:77] Creating layer pool1
I1107 02:43:42.206382 37922 net.cpp:94] Creating Layer pool1
I1107 02:43:42.206384 37922 net.cpp:435] pool1 <- relu1
I1107 02:43:42.206387 37922 net.cpp:409] pool1 -> pool1
I1107 02:43:42.206442 37922 net.cpp:144] Setting up pool1
I1107 02:43:42.206447 37922 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 02:43:42.206449 37922 net.cpp:159] Memory required for data: 1122070528
I1107 02:43:42.206452 37922 layer_factory.hpp:77] Creating layer conv2
I1107 02:43:42.206459 37922 net.cpp:94] Creating Layer conv2
I1107 02:43:42.206461 37922 net.cpp:435] conv2 <- pool1
I1107 02:43:42.206465 37922 net.cpp:409] conv2 -> conv2
I1107 02:43:42.221294 37922 net.cpp:144] Setting up conv2
I1107 02:43:42.221310 37922 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 02:43:42.221312 37922 net.cpp:159] Memory required for data: 1313173504
I1107 02:43:42.221323 37922 layer_factory.hpp:77] Creating layer bn2
I1107 02:43:42.221333 37922 net.cpp:94] Creating Layer bn2
I1107 02:43:42.221335 37922 net.cpp:435] bn2 <- conv2
I1107 02:43:42.221341 37922 net.cpp:409] bn2 -> scale2
I1107 02:43:42.221895 37922 net.cpp:144] Setting up bn2
I1107 02:43:42.221902 37922 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 02:43:42.221905 37922 net.cpp:159] Memory required for data: 1504276480
I1107 02:43:42.221915 37922 layer_factory.hpp:77] Creating layer relu2
I1107 02:43:42.221918 37922 net.cpp:94] Creating Layer relu2
I1107 02:43:42.221922 37922 net.cpp:435] relu2 <- scale2
I1107 02:43:42.221926 37922 net.cpp:409] relu2 -> relu2
I1107 02:43:42.221945 37922 net.cpp:144] Setting up relu2
I1107 02:43:42.221951 37922 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 02:43:42.221954 37922 net.cpp:159] Memory required for data: 1695379456
I1107 02:43:42.221956 37922 layer_factory.hpp:77] Creating layer pool2
I1107 02:43:42.221962 37922 net.cpp:94] Creating Layer pool2
I1107 02:43:42.221966 37922 net.cpp:435] pool2 <- relu2
I1107 02:43:42.221985 37922 net.cpp:409] pool2 -> pool2
I1107 02:43:42.222012 37922 net.cpp:144] Setting up pool2
I1107 02:43:42.222018 37922 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 02:43:42.222020 37922 net.cpp:159] Memory required for data: 1739681792
I1107 02:43:42.222024 37922 layer_factory.hpp:77] Creating layer conv3
I1107 02:43:42.222031 37922 net.cpp:94] Creating Layer conv3
I1107 02:43:42.222034 37922 net.cpp:435] conv3 <- pool2
I1107 02:43:42.222039 37922 net.cpp:409] conv3 -> conv3
I1107 02:43:42.235200 37922 net.cpp:144] Setting up conv3
I1107 02:43:42.235244 37922 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 02:43:42.235249 37922 net.cpp:159] Memory required for data: 1806135296
I1107 02:43:42.235265 37922 layer_factory.hpp:77] Creating layer relu3
I1107 02:43:42.235280 37922 net.cpp:94] Creating Layer relu3
I1107 02:43:42.235285 37922 net.cpp:435] relu3 <- conv3
I1107 02:43:42.235294 37922 net.cpp:409] relu3 -> relu3
I1107 02:43:42.235358 37922 net.cpp:144] Setting up relu3
I1107 02:43:42.235383 37922 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 02:43:42.235406 37922 net.cpp:159] Memory required for data: 1872588800
I1107 02:43:42.235426 37922 layer_factory.hpp:77] Creating layer conv4
I1107 02:43:42.235457 37922 net.cpp:94] Creating Layer conv4
I1107 02:43:42.235479 37922 net.cpp:435] conv4 <- relu3
I1107 02:43:42.235510 37922 net.cpp:409] conv4 -> conv4
I1107 02:43:42.262706 37922 net.cpp:144] Setting up conv4
I1107 02:43:42.262796 37922 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 02:43:42.262821 37922 net.cpp:159] Memory required for data: 1939042304
I1107 02:43:42.262856 37922 layer_factory.hpp:77] Creating layer relu4
I1107 02:43:42.262886 37922 net.cpp:94] Creating Layer relu4
I1107 02:43:42.262907 37922 net.cpp:435] relu4 <- conv4
I1107 02:43:42.262935 37922 net.cpp:409] relu4 -> relu4
I1107 02:43:42.262998 37922 net.cpp:144] Setting up relu4
I1107 02:43:42.263021 37922 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 02:43:42.263042 37922 net.cpp:159] Memory required for data: 2005495808
I1107 02:43:42.263063 37922 layer_factory.hpp:77] Creating layer conv5
I1107 02:43:42.263092 37922 net.cpp:94] Creating Layer conv5
I1107 02:43:42.263123 37922 net.cpp:435] conv5 <- relu4
I1107 02:43:42.263146 37922 net.cpp:409] conv5 -> conv5
I1107 02:43:42.301062 37922 net.cpp:144] Setting up conv5
I1107 02:43:42.301105 37922 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 02:43:42.301113 37922 net.cpp:159] Memory required for data: 2049798144
I1107 02:43:42.301131 37922 layer_factory.hpp:77] Creating layer relu5
I1107 02:43:42.301151 37922 net.cpp:94] Creating Layer relu5
I1107 02:43:42.301159 37922 net.cpp:435] relu5 <- conv5
I1107 02:43:42.301172 37922 net.cpp:409] relu5 -> relu5
I1107 02:43:42.301223 37922 net.cpp:144] Setting up relu5
I1107 02:43:42.301239 37922 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 02:43:42.301247 37922 net.cpp:159] Memory required for data: 2094100480
I1107 02:43:42.301254 37922 layer_factory.hpp:77] Creating layer pool5
I1107 02:43:42.301265 37922 net.cpp:94] Creating Layer pool5
I1107 02:43:42.301275 37922 net.cpp:435] pool5 <- relu5
I1107 02:43:42.301285 37922 net.cpp:409] pool5 -> pool5
I1107 02:43:42.301349 37922 net.cpp:144] Setting up pool5
I1107 02:43:42.301362 37922 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 02:43:42.301370 37922 net.cpp:159] Memory required for data: 2103537664
I1107 02:43:42.301376 37922 layer_factory.hpp:77] Creating layer fc6
I1107 02:43:42.301393 37922 net.cpp:94] Creating Layer fc6
I1107 02:43:42.301401 37922 net.cpp:435] fc6 <- pool5
I1107 02:43:42.301412 37922 net.cpp:409] fc6 -> fc6
I1107 02:43:42.656560 37922 net.cpp:144] Setting up fc6
I1107 02:43:42.656584 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.656586 37922 net.cpp:159] Memory required for data: 2107731968
I1107 02:43:42.656610 37922 layer_factory.hpp:77] Creating layer relu6
I1107 02:43:42.656617 37922 net.cpp:94] Creating Layer relu6
I1107 02:43:42.656620 37922 net.cpp:435] relu6 <- fc6
I1107 02:43:42.656646 37922 net.cpp:409] relu6 -> relu6
I1107 02:43:42.656661 37922 net.cpp:144] Setting up relu6
I1107 02:43:42.656664 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.656667 37922 net.cpp:159] Memory required for data: 2111926272
I1107 02:43:42.656669 37922 layer_factory.hpp:77] Creating layer drop6
I1107 02:43:42.656674 37922 net.cpp:94] Creating Layer drop6
I1107 02:43:42.656677 37922 net.cpp:435] drop6 <- relu6
I1107 02:43:42.656679 37922 net.cpp:409] drop6 -> drop6
I1107 02:43:42.656708 37922 net.cpp:144] Setting up drop6
I1107 02:43:42.656713 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.656714 37922 net.cpp:159] Memory required for data: 2116120576
I1107 02:43:42.656718 37922 layer_factory.hpp:77] Creating layer fc7
I1107 02:43:42.656738 37922 net.cpp:94] Creating Layer fc7
I1107 02:43:42.656744 37922 net.cpp:435] fc7 <- drop6
I1107 02:43:42.656747 37922 net.cpp:409] fc7 -> fc7
I1107 02:43:42.795116 37922 net.cpp:144] Setting up fc7
I1107 02:43:42.795138 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.795141 37922 net.cpp:159] Memory required for data: 2120314880
I1107 02:43:42.795148 37922 layer_factory.hpp:77] Creating layer bn7
I1107 02:43:42.795157 37922 net.cpp:94] Creating Layer bn7
I1107 02:43:42.795161 37922 net.cpp:435] bn7 <- fc7
I1107 02:43:42.795166 37922 net.cpp:409] bn7 -> scale7
I1107 02:43:42.795711 37922 net.cpp:144] Setting up bn7
I1107 02:43:42.795717 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.795719 37922 net.cpp:159] Memory required for data: 2124509184
I1107 02:43:42.795727 37922 layer_factory.hpp:77] Creating layer relu7
I1107 02:43:42.795732 37922 net.cpp:94] Creating Layer relu7
I1107 02:43:42.795733 37922 net.cpp:435] relu7 <- scale7
I1107 02:43:42.795737 37922 net.cpp:409] relu7 -> relu7
I1107 02:43:42.795754 37922 net.cpp:144] Setting up relu7
I1107 02:43:42.795759 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.795761 37922 net.cpp:159] Memory required for data: 2128703488
I1107 02:43:42.795763 37922 layer_factory.hpp:77] Creating layer drop7
I1107 02:43:42.795768 37922 net.cpp:94] Creating Layer drop7
I1107 02:43:42.795770 37922 net.cpp:435] drop7 <- relu7
I1107 02:43:42.795774 37922 net.cpp:409] drop7 -> drop7
I1107 02:43:42.795796 37922 net.cpp:144] Setting up drop7
I1107 02:43:42.795801 37922 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 02:43:42.795804 37922 net.cpp:159] Memory required for data: 2132897792
I1107 02:43:42.795805 37922 layer_factory.hpp:77] Creating layer fc8
I1107 02:43:42.795810 37922 net.cpp:94] Creating Layer fc8
I1107 02:43:42.795814 37922 net.cpp:435] fc8 <- drop7
I1107 02:43:42.795816 37922 net.cpp:409] fc8 -> fc8
I1107 02:43:42.796682 37922 net.cpp:144] Setting up fc8
I1107 02:43:42.796694 37922 net.cpp:151] Top shape: 256 2 (512)
I1107 02:43:42.796696 37922 net.cpp:159] Memory required for data: 2132899840
I1107 02:43:42.796702 37922 layer_factory.hpp:77] Creating layer loss
I1107 02:43:42.796708 37922 net.cpp:94] Creating Layer loss
I1107 02:43:42.796715 37922 net.cpp:435] loss <- fc8
I1107 02:43:42.796718 37922 net.cpp:435] loss <- label
I1107 02:43:42.796722 37922 net.cpp:409] loss -> loss
I1107 02:43:42.796730 37922 layer_factory.hpp:77] Creating layer loss
I1107 02:43:42.796790 37922 net.cpp:144] Setting up loss
I1107 02:43:42.796795 37922 net.cpp:151] Top shape: (1)
I1107 02:43:42.796797 37922 net.cpp:154]     with loss weight 1
I1107 02:43:42.796808 37922 net.cpp:159] Memory required for data: 2132899844
I1107 02:43:42.796811 37922 net.cpp:220] loss needs backward computation.
I1107 02:43:42.796824 37922 net.cpp:220] fc8 needs backward computation.
I1107 02:43:42.796828 37922 net.cpp:220] drop7 needs backward computation.
I1107 02:43:42.796829 37922 net.cpp:220] relu7 needs backward computation.
I1107 02:43:42.796833 37922 net.cpp:220] bn7 needs backward computation.
I1107 02:43:42.796835 37922 net.cpp:220] fc7 needs backward computation.
I1107 02:43:42.796838 37922 net.cpp:220] drop6 needs backward computation.
I1107 02:43:42.796840 37922 net.cpp:220] relu6 needs backward computation.
I1107 02:43:42.796854 37922 net.cpp:220] fc6 needs backward computation.
I1107 02:43:42.796857 37922 net.cpp:220] pool5 needs backward computation.
I1107 02:43:42.796860 37922 net.cpp:220] relu5 needs backward computation.
I1107 02:43:42.796864 37922 net.cpp:220] conv5 needs backward computation.
I1107 02:43:42.796865 37922 net.cpp:220] relu4 needs backward computation.
I1107 02:43:42.796869 37922 net.cpp:220] conv4 needs backward computation.
I1107 02:43:42.796871 37922 net.cpp:220] relu3 needs backward computation.
I1107 02:43:42.796875 37922 net.cpp:220] conv3 needs backward computation.
I1107 02:43:42.796876 37922 net.cpp:220] pool2 needs backward computation.
I1107 02:43:42.796880 37922 net.cpp:220] relu2 needs backward computation.
I1107 02:43:42.796882 37922 net.cpp:220] bn2 needs backward computation.
I1107 02:43:42.796885 37922 net.cpp:220] conv2 needs backward computation.
I1107 02:43:42.796887 37922 net.cpp:220] pool1 needs backward computation.
I1107 02:43:42.796890 37922 net.cpp:220] relu1 needs backward computation.
I1107 02:43:42.796893 37922 net.cpp:220] bn1 needs backward computation.
I1107 02:43:42.796896 37922 net.cpp:220] conv1 needs backward computation.
I1107 02:43:42.796900 37922 net.cpp:222] data does not need backward computation.
I1107 02:43:42.796902 37922 net.cpp:264] This network produces output loss
I1107 02:43:42.796918 37922 net.cpp:284] Network initialization done.
I1107 02:43:42.797194 37922 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/net_finetune.prototxt
I1107 02:43:42.797221 37922 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 02:43:42.797384 37922 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 02:43:42.797468 37922 layer_factory.hpp:77] Creating layer data
I1107 02:43:42.797504 37922 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:42.798441 37922 net.cpp:94] Creating Layer data
I1107 02:43:42.798454 37922 net.cpp:409] data -> data
I1107 02:43:42.798462 37922 net.cpp:409] data -> label
I1107 02:43:42.799602 37991 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 02:43:42.799636 37991 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 02:43:42.799885 37922 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 02:43:42.799948 37922 data_layer.cpp:83] output data size: 50,3,227,227
I1107 02:43:42.875766 37922 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 02:43:42.875814 37922 net.cpp:144] Setting up data
I1107 02:43:42.875821 37922 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 02:43:42.875825 37922 net.cpp:151] Top shape: 50 (50)
I1107 02:43:42.875826 37922 net.cpp:159] Memory required for data: 30917600
I1107 02:43:42.875831 37922 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 02:43:42.875840 37922 net.cpp:94] Creating Layer label_data_1_split
I1107 02:43:42.875844 37922 net.cpp:435] label_data_1_split <- label
I1107 02:43:42.875864 37922 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 02:43:42.875871 37922 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 02:43:42.875932 37922 net.cpp:144] Setting up label_data_1_split
I1107 02:43:42.875936 37922 net.cpp:151] Top shape: 50 (50)
I1107 02:43:42.875938 37922 net.cpp:151] Top shape: 50 (50)
I1107 02:43:42.875941 37922 net.cpp:159] Memory required for data: 30918000
I1107 02:43:42.875942 37922 layer_factory.hpp:77] Creating layer conv1
I1107 02:43:42.875962 37922 net.cpp:94] Creating Layer conv1
I1107 02:43:42.875965 37922 net.cpp:435] conv1 <- data
I1107 02:43:42.875969 37922 net.cpp:409] conv1 -> conv1
I1107 02:43:42.876564 37922 net.cpp:144] Setting up conv1
I1107 02:43:42.876571 37922 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:42.876574 37922 net.cpp:159] Memory required for data: 88998000
I1107 02:43:42.876583 37922 layer_factory.hpp:77] Creating layer bn1
I1107 02:43:42.876590 37922 net.cpp:94] Creating Layer bn1
I1107 02:43:42.876595 37922 net.cpp:435] bn1 <- conv1
I1107 02:43:42.876600 37922 net.cpp:409] bn1 -> scale1
I1107 02:43:42.877151 37922 net.cpp:144] Setting up bn1
I1107 02:43:42.877157 37922 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:42.877161 37922 net.cpp:159] Memory required for data: 147078000
I1107 02:43:42.877171 37922 layer_factory.hpp:77] Creating layer relu1
I1107 02:43:42.877176 37922 net.cpp:94] Creating Layer relu1
I1107 02:43:42.877182 37922 net.cpp:435] relu1 <- scale1
I1107 02:43:42.877185 37922 net.cpp:409] relu1 -> relu1
I1107 02:43:42.877200 37922 net.cpp:144] Setting up relu1
I1107 02:43:42.877205 37922 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 02:43:42.877209 37922 net.cpp:159] Memory required for data: 205158000
I1107 02:43:42.877212 37922 layer_factory.hpp:77] Creating layer pool1
I1107 02:43:42.877216 37922 net.cpp:94] Creating Layer pool1
I1107 02:43:42.877218 37922 net.cpp:435] pool1 <- relu1
I1107 02:43:42.877223 37922 net.cpp:409] pool1 -> pool1
I1107 02:43:42.877246 37922 net.cpp:144] Setting up pool1
I1107 02:43:42.877250 37922 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 02:43:42.877252 37922 net.cpp:159] Memory required for data: 219154800
I1107 02:43:42.877255 37922 layer_factory.hpp:77] Creating layer conv2
I1107 02:43:42.877261 37922 net.cpp:94] Creating Layer conv2
I1107 02:43:42.877264 37922 net.cpp:435] conv2 <- pool1
I1107 02:43:42.877267 37922 net.cpp:409] conv2 -> conv2
I1107 02:43:42.883782 37922 net.cpp:144] Setting up conv2
I1107 02:43:42.883816 37922 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:42.883817 37922 net.cpp:159] Memory required for data: 256479600
I1107 02:43:42.883828 37922 layer_factory.hpp:77] Creating layer bn2
I1107 02:43:42.883847 37922 net.cpp:94] Creating Layer bn2
I1107 02:43:42.883852 37922 net.cpp:435] bn2 <- conv2
I1107 02:43:42.883857 37922 net.cpp:409] bn2 -> scale2
I1107 02:43:42.884461 37922 net.cpp:144] Setting up bn2
I1107 02:43:42.884470 37922 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:42.884474 37922 net.cpp:159] Memory required for data: 293804400
I1107 02:43:42.884486 37922 layer_factory.hpp:77] Creating layer relu2
I1107 02:43:42.884495 37922 net.cpp:94] Creating Layer relu2
I1107 02:43:42.884501 37922 net.cpp:435] relu2 <- scale2
I1107 02:43:42.884507 37922 net.cpp:409] relu2 -> relu2
I1107 02:43:42.884534 37922 net.cpp:144] Setting up relu2
I1107 02:43:42.884541 37922 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 02:43:42.884546 37922 net.cpp:159] Memory required for data: 331129200
I1107 02:43:42.884551 37922 layer_factory.hpp:77] Creating layer pool2
I1107 02:43:42.884558 37922 net.cpp:94] Creating Layer pool2
I1107 02:43:42.884563 37922 net.cpp:435] pool2 <- relu2
I1107 02:43:42.884570 37922 net.cpp:409] pool2 -> pool2
I1107 02:43:42.884614 37922 net.cpp:144] Setting up pool2
I1107 02:43:42.884621 37922 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:42.884626 37922 net.cpp:159] Memory required for data: 339782000
I1107 02:43:42.884631 37922 layer_factory.hpp:77] Creating layer conv3
I1107 02:43:42.884642 37922 net.cpp:94] Creating Layer conv3
I1107 02:43:42.884649 37922 net.cpp:435] conv3 <- pool2
I1107 02:43:42.884655 37922 net.cpp:409] conv3 -> conv3
I1107 02:43:42.894305 37922 net.cpp:144] Setting up conv3
I1107 02:43:42.894330 37922 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:42.894333 37922 net.cpp:159] Memory required for data: 352761200
I1107 02:43:42.894345 37922 layer_factory.hpp:77] Creating layer relu3
I1107 02:43:42.894356 37922 net.cpp:94] Creating Layer relu3
I1107 02:43:42.894363 37922 net.cpp:435] relu3 <- conv3
I1107 02:43:42.894371 37922 net.cpp:409] relu3 -> relu3
I1107 02:43:42.894407 37922 net.cpp:144] Setting up relu3
I1107 02:43:42.894415 37922 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:42.894419 37922 net.cpp:159] Memory required for data: 365740400
I1107 02:43:42.894425 37922 layer_factory.hpp:77] Creating layer conv4
I1107 02:43:42.894438 37922 net.cpp:94] Creating Layer conv4
I1107 02:43:42.894444 37922 net.cpp:435] conv4 <- relu3
I1107 02:43:42.894451 37922 net.cpp:409] conv4 -> conv4
I1107 02:43:42.910780 37922 net.cpp:144] Setting up conv4
I1107 02:43:42.910859 37922 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:42.910876 37922 net.cpp:159] Memory required for data: 378719600
I1107 02:43:42.910904 37922 layer_factory.hpp:77] Creating layer relu4
I1107 02:43:42.910926 37922 net.cpp:94] Creating Layer relu4
I1107 02:43:42.910943 37922 net.cpp:435] relu4 <- conv4
I1107 02:43:42.910962 37922 net.cpp:409] relu4 -> relu4
I1107 02:43:42.911018 37922 net.cpp:144] Setting up relu4
I1107 02:43:42.911034 37922 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 02:43:42.911048 37922 net.cpp:159] Memory required for data: 391698800
I1107 02:43:42.911063 37922 layer_factory.hpp:77] Creating layer conv5
I1107 02:43:42.911085 37922 net.cpp:94] Creating Layer conv5
I1107 02:43:42.911098 37922 net.cpp:435] conv5 <- relu4
I1107 02:43:42.911113 37922 net.cpp:409] conv5 -> conv5
I1107 02:43:42.920936 37922 net.cpp:144] Setting up conv5
I1107 02:43:42.921000 37922 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:42.921013 37922 net.cpp:159] Memory required for data: 400351600
I1107 02:43:42.921034 37922 layer_factory.hpp:77] Creating layer relu5
I1107 02:43:42.921054 37922 net.cpp:94] Creating Layer relu5
I1107 02:43:42.921068 37922 net.cpp:435] relu5 <- conv5
I1107 02:43:42.921095 37922 net.cpp:409] relu5 -> relu5
I1107 02:43:42.921146 37922 net.cpp:144] Setting up relu5
I1107 02:43:42.921161 37922 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 02:43:42.921185 37922 net.cpp:159] Memory required for data: 409004400
I1107 02:43:42.921197 37922 layer_factory.hpp:77] Creating layer pool5
I1107 02:43:42.921216 37922 net.cpp:94] Creating Layer pool5
I1107 02:43:42.921229 37922 net.cpp:435] pool5 <- relu5
I1107 02:43:42.921244 37922 net.cpp:409] pool5 -> pool5
I1107 02:43:42.921309 37922 net.cpp:144] Setting up pool5
I1107 02:43:42.921325 37922 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 02:43:42.921339 37922 net.cpp:159] Memory required for data: 410847600
I1107 02:43:42.921352 37922 layer_factory.hpp:77] Creating layer fc6
I1107 02:43:42.921370 37922 net.cpp:94] Creating Layer fc6
I1107 02:43:42.921384 37922 net.cpp:435] fc6 <- pool5
I1107 02:43:42.921401 37922 net.cpp:409] fc6 -> fc6
I1107 02:43:43.227697 37922 net.cpp:144] Setting up fc6
I1107 02:43:43.227725 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.227726 37922 net.cpp:159] Memory required for data: 411666800
I1107 02:43:43.227749 37922 layer_factory.hpp:77] Creating layer relu6
I1107 02:43:43.227757 37922 net.cpp:94] Creating Layer relu6
I1107 02:43:43.227761 37922 net.cpp:435] relu6 <- fc6
I1107 02:43:43.227766 37922 net.cpp:409] relu6 -> relu6
I1107 02:43:43.227802 37922 net.cpp:144] Setting up relu6
I1107 02:43:43.227807 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.227810 37922 net.cpp:159] Memory required for data: 412486000
I1107 02:43:43.227813 37922 layer_factory.hpp:77] Creating layer drop6
I1107 02:43:43.227823 37922 net.cpp:94] Creating Layer drop6
I1107 02:43:43.227825 37922 net.cpp:435] drop6 <- relu6
I1107 02:43:43.227830 37922 net.cpp:409] drop6 -> drop6
I1107 02:43:43.227859 37922 net.cpp:144] Setting up drop6
I1107 02:43:43.227864 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.227865 37922 net.cpp:159] Memory required for data: 413305200
I1107 02:43:43.227869 37922 layer_factory.hpp:77] Creating layer fc7
I1107 02:43:43.227890 37922 net.cpp:94] Creating Layer fc7
I1107 02:43:43.227895 37922 net.cpp:435] fc7 <- drop6
I1107 02:43:43.227900 37922 net.cpp:409] fc7 -> fc7
I1107 02:43:43.360863 37922 net.cpp:144] Setting up fc7
I1107 02:43:43.360890 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.360893 37922 net.cpp:159] Memory required for data: 414124400
I1107 02:43:43.360900 37922 layer_factory.hpp:77] Creating layer bn7
I1107 02:43:43.360909 37922 net.cpp:94] Creating Layer bn7
I1107 02:43:43.360922 37922 net.cpp:435] bn7 <- fc7
I1107 02:43:43.360929 37922 net.cpp:409] bn7 -> scale7
I1107 02:43:43.361455 37922 net.cpp:144] Setting up bn7
I1107 02:43:43.361462 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.361464 37922 net.cpp:159] Memory required for data: 414943600
I1107 02:43:43.361472 37922 layer_factory.hpp:77] Creating layer relu7
I1107 02:43:43.361479 37922 net.cpp:94] Creating Layer relu7
I1107 02:43:43.361481 37922 net.cpp:435] relu7 <- scale7
I1107 02:43:43.361486 37922 net.cpp:409] relu7 -> relu7
I1107 02:43:43.361507 37922 net.cpp:144] Setting up relu7
I1107 02:43:43.361512 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.361515 37922 net.cpp:159] Memory required for data: 415762800
I1107 02:43:43.361516 37922 layer_factory.hpp:77] Creating layer drop7
I1107 02:43:43.361521 37922 net.cpp:94] Creating Layer drop7
I1107 02:43:43.361526 37922 net.cpp:435] drop7 <- relu7
I1107 02:43:43.361531 37922 net.cpp:409] drop7 -> drop7
I1107 02:43:43.361574 37922 net.cpp:144] Setting up drop7
I1107 02:43:43.361580 37922 net.cpp:151] Top shape: 50 4096 (204800)
I1107 02:43:43.361582 37922 net.cpp:159] Memory required for data: 416582000
I1107 02:43:43.361584 37922 layer_factory.hpp:77] Creating layer fc8
I1107 02:43:43.361591 37922 net.cpp:94] Creating Layer fc8
I1107 02:43:43.361595 37922 net.cpp:435] fc8 <- drop7
I1107 02:43:43.361601 37922 net.cpp:409] fc8 -> fc8
I1107 02:43:43.361768 37922 net.cpp:144] Setting up fc8
I1107 02:43:43.361773 37922 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:43.361775 37922 net.cpp:159] Memory required for data: 416582400
I1107 02:43:43.361794 37922 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 02:43:43.361799 37922 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 02:43:43.361804 37922 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 02:43:43.361809 37922 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 02:43:43.361832 37922 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 02:43:43.361869 37922 net.cpp:144] Setting up fc8_fc8_0_split
I1107 02:43:43.361874 37922 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:43.361878 37922 net.cpp:151] Top shape: 50 2 (100)
I1107 02:43:43.361881 37922 net.cpp:159] Memory required for data: 416583200
I1107 02:43:43.361884 37922 layer_factory.hpp:77] Creating layer loss
I1107 02:43:43.361891 37922 net.cpp:94] Creating Layer loss
I1107 02:43:43.361898 37922 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 02:43:43.361903 37922 net.cpp:435] loss <- label_data_1_split_0
I1107 02:43:43.361910 37922 net.cpp:409] loss -> loss
I1107 02:43:43.361920 37922 layer_factory.hpp:77] Creating layer loss
I1107 02:43:43.362010 37922 net.cpp:144] Setting up loss
I1107 02:43:43.362016 37922 net.cpp:151] Top shape: (1)
I1107 02:43:43.362020 37922 net.cpp:154]     with loss weight 1
I1107 02:43:43.362033 37922 net.cpp:159] Memory required for data: 416583204
I1107 02:43:43.362038 37922 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 02:43:43.362046 37922 net.cpp:94] Creating Layer accuracy-top1
I1107 02:43:43.362051 37922 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 02:43:43.362056 37922 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 02:43:43.362063 37922 net.cpp:409] accuracy-top1 -> top-1
I1107 02:43:43.362072 37922 net.cpp:144] Setting up accuracy-top1
I1107 02:43:43.362078 37922 net.cpp:151] Top shape: (1)
I1107 02:43:43.362082 37922 net.cpp:159] Memory required for data: 416583208
I1107 02:43:43.362087 37922 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 02:43:43.362092 37922 net.cpp:220] loss needs backward computation.
I1107 02:43:43.362097 37922 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 02:43:43.362100 37922 net.cpp:220] fc8 needs backward computation.
I1107 02:43:43.362104 37922 net.cpp:220] drop7 needs backward computation.
I1107 02:43:43.362108 37922 net.cpp:220] relu7 needs backward computation.
I1107 02:43:43.362112 37922 net.cpp:220] bn7 needs backward computation.
I1107 02:43:43.362118 37922 net.cpp:220] fc7 needs backward computation.
I1107 02:43:43.362123 37922 net.cpp:220] drop6 needs backward computation.
I1107 02:43:43.362126 37922 net.cpp:220] relu6 needs backward computation.
I1107 02:43:43.362130 37922 net.cpp:220] fc6 needs backward computation.
I1107 02:43:43.362135 37922 net.cpp:220] pool5 needs backward computation.
I1107 02:43:43.362140 37922 net.cpp:220] relu5 needs backward computation.
I1107 02:43:43.362144 37922 net.cpp:220] conv5 needs backward computation.
I1107 02:43:43.362149 37922 net.cpp:220] relu4 needs backward computation.
I1107 02:43:43.362154 37922 net.cpp:220] conv4 needs backward computation.
I1107 02:43:43.362160 37922 net.cpp:220] relu3 needs backward computation.
I1107 02:43:43.362165 37922 net.cpp:220] conv3 needs backward computation.
I1107 02:43:43.362169 37922 net.cpp:220] pool2 needs backward computation.
I1107 02:43:43.362174 37922 net.cpp:220] relu2 needs backward computation.
I1107 02:43:43.362179 37922 net.cpp:220] bn2 needs backward computation.
I1107 02:43:43.362182 37922 net.cpp:220] conv2 needs backward computation.
I1107 02:43:43.362186 37922 net.cpp:220] pool1 needs backward computation.
I1107 02:43:43.362191 37922 net.cpp:220] relu1 needs backward computation.
I1107 02:43:43.362196 37922 net.cpp:220] bn1 needs backward computation.
I1107 02:43:43.362200 37922 net.cpp:220] conv1 needs backward computation.
I1107 02:43:43.362205 37922 net.cpp:222] label_data_1_split does not need backward computation.
I1107 02:43:43.362210 37922 net.cpp:222] data does not need backward computation.
I1107 02:43:43.362215 37922 net.cpp:264] This network produces output loss
I1107 02:43:43.362218 37922 net.cpp:264] This network produces output top-1
I1107 02:43:43.362257 37922 net.cpp:284] Network initialization done.
I1107 02:43:43.362362 37922 solver.cpp:63] Solver scaffolding done.
I1107 02:43:43.363565 37922 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/sparse.caffemodel
I1107 02:43:44.908418 37922 caffe_interface.cpp:527] Starting Optimization
I1107 02:43:44.908437 37922 solver.cpp:335] Solving 
I1107 02:43:44.908439 37922 solver.cpp:336] Learning Rate Policy: step
I1107 02:43:44.910333 37922 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 02:43:46.426113 37922 solver.cpp:517]     Test net output #0: loss = 0.186202 (* 1 = 0.186202 loss)
I1107 02:43:46.426134 37922 solver.cpp:517]     Test net output #1: top-1 = 0.951
I1107 02:43:46.681890 37922 solver.cpp:266] Iteration 0 (0 iter/s, 1.77334s/50 iter), loss = 0.016375
I1107 02:43:46.681929 37922 solver.cpp:285]     Train net output #0: loss = 0.016375 (* 1 = 0.016375 loss)
I1107 02:43:46.681946 37922 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 02:43:59.166451 37922 solver.cpp:266] Iteration 50 (4.00513 iter/s, 12.484s/50 iter), loss = 0.082817
I1107 02:43:59.166481 37922 solver.cpp:285]     Train net output #0: loss = 0.082817 (* 1 = 0.082817 loss)
I1107 02:43:59.166487 37922 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 02:44:11.713574 37922 solver.cpp:266] Iteration 100 (3.98516 iter/s, 12.5465s/50 iter), loss = 0.0655839
I1107 02:44:11.713799 37922 solver.cpp:285]     Train net output #0: loss = 0.0655839 (* 1 = 0.0655839 loss)
I1107 02:44:11.713809 37922 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 02:44:24.306622 37922 solver.cpp:266] Iteration 150 (3.97069 iter/s, 12.5923s/50 iter), loss = 0.0812319
I1107 02:44:24.306650 37922 solver.cpp:285]     Train net output #0: loss = 0.0812319 (* 1 = 0.0812319 loss)
I1107 02:44:24.306656 37922 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 02:44:36.905243 37922 solver.cpp:266] Iteration 200 (3.96887 iter/s, 12.598s/50 iter), loss = 0.0756438
I1107 02:44:36.905284 37922 solver.cpp:285]     Train net output #0: loss = 0.0756438 (* 1 = 0.0756438 loss)
I1107 02:44:36.905306 37922 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 02:44:49.537268 37922 solver.cpp:266] Iteration 250 (3.95838 iter/s, 12.6314s/50 iter), loss = 0.0623727
I1107 02:44:49.537324 37922 solver.cpp:285]     Train net output #0: loss = 0.0623727 (* 1 = 0.0623727 loss)
I1107 02:44:49.537331 37922 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 02:45:02.249459 37922 solver.cpp:266] Iteration 300 (3.93342 iter/s, 12.7116s/50 iter), loss = 0.0647512
I1107 02:45:02.249490 37922 solver.cpp:285]     Train net output #0: loss = 0.0647512 (* 1 = 0.0647512 loss)
I1107 02:45:02.249495 37922 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 02:45:15.061590 37922 solver.cpp:266] Iteration 350 (3.90272 iter/s, 12.8116s/50 iter), loss = 0.134321
I1107 02:45:15.061620 37922 solver.cpp:285]     Train net output #0: loss = 0.134321 (* 1 = 0.134321 loss)
I1107 02:45:15.061626 37922 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 02:45:27.914052 37922 solver.cpp:266] Iteration 400 (3.89046 iter/s, 12.8519s/50 iter), loss = 0.0582451
I1107 02:45:27.914212 37922 solver.cpp:285]     Train net output #0: loss = 0.0582451 (* 1 = 0.0582451 loss)
I1107 02:45:27.914222 37922 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 02:45:40.735116 37922 solver.cpp:266] Iteration 450 (3.90002 iter/s, 12.8204s/50 iter), loss = 0.105244
I1107 02:45:40.735146 37922 solver.cpp:285]     Train net output #0: loss = 0.105244 (* 1 = 0.105244 loss)
I1107 02:45:40.735152 37922 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 02:45:53.582659 37922 solver.cpp:266] Iteration 500 (3.89195 iter/s, 12.847s/50 iter), loss = 0.0891345
I1107 02:45:53.582689 37922 solver.cpp:285]     Train net output #0: loss = 0.0891345 (* 1 = 0.0891345 loss)
I1107 02:45:53.582695 37922 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 02:46:06.436471 37922 solver.cpp:266] Iteration 550 (3.89005 iter/s, 12.8533s/50 iter), loss = 0.0950925
I1107 02:46:06.437387 37922 solver.cpp:285]     Train net output #0: loss = 0.0950925 (* 1 = 0.0950925 loss)
I1107 02:46:06.437395 37922 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 02:46:19.289714 37922 solver.cpp:266] Iteration 600 (3.89049 iter/s, 12.8519s/50 iter), loss = 0.0735981
I1107 02:46:19.289744 37922 solver.cpp:285]     Train net output #0: loss = 0.0735981 (* 1 = 0.0735981 loss)
I1107 02:46:19.289752 37922 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 02:46:32.132388 37922 solver.cpp:266] Iteration 650 (3.89342 iter/s, 12.8422s/50 iter), loss = 0.0738648
I1107 02:46:32.132429 37922 solver.cpp:285]     Train net output #0: loss = 0.0738648 (* 1 = 0.0738648 loss)
I1107 02:46:32.132436 37922 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 02:46:44.981488 37922 solver.cpp:266] Iteration 700 (3.89148 iter/s, 12.8486s/50 iter), loss = 0.079846
I1107 02:46:44.981648 37922 solver.cpp:285]     Train net output #0: loss = 0.079846 (* 1 = 0.079846 loss)
I1107 02:46:44.981657 37922 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 02:46:57.828081 37922 solver.cpp:266] Iteration 750 (3.89228 iter/s, 12.846s/50 iter), loss = 0.0766274
I1107 02:46:57.828110 37922 solver.cpp:285]     Train net output #0: loss = 0.0766274 (* 1 = 0.0766274 loss)
I1107 02:46:57.828116 37922 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 02:47:10.645638 37922 solver.cpp:266] Iteration 800 (3.90105 iter/s, 12.817s/50 iter), loss = 0.07711
I1107 02:47:10.645669 37922 solver.cpp:285]     Train net output #0: loss = 0.07711 (* 1 = 0.07711 loss)
I1107 02:47:10.645675 37922 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 02:47:23.507640 37922 solver.cpp:266] Iteration 850 (3.88757 iter/s, 12.8615s/50 iter), loss = 0.0979169
I1107 02:47:23.507802 37922 solver.cpp:285]     Train net output #0: loss = 0.0979169 (* 1 = 0.0979169 loss)
I1107 02:47:23.507809 37922 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 02:47:36.327787 37922 solver.cpp:266] Iteration 900 (3.90031 iter/s, 12.8195s/50 iter), loss = 0.0971033
I1107 02:47:36.327817 37922 solver.cpp:285]     Train net output #0: loss = 0.0971033 (* 1 = 0.0971033 loss)
I1107 02:47:36.327822 37922 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 02:47:49.175776 37922 solver.cpp:266] Iteration 950 (3.89181 iter/s, 12.8475s/50 iter), loss = 0.0703295
I1107 02:47:49.175804 37922 solver.cpp:285]     Train net output #0: loss = 0.0703295 (* 1 = 0.0703295 loss)
I1107 02:47:49.175810 37922 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 02:48:01.778249 37922 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 02:48:03.321007 37922 solver.cpp:517]     Test net output #0: loss = 0.458389 (* 1 = 0.458389 loss)
I1107 02:48:03.321022 37922 solver.cpp:517]     Test net output #1: top-1 = 0.867
I1107 02:48:03.570269 37922 solver.cpp:266] Iteration 1000 (3.47369 iter/s, 14.3939s/50 iter), loss = 0.0752222
I1107 02:48:03.570296 37922 solver.cpp:285]     Train net output #0: loss = 0.0752222 (* 1 = 0.0752222 loss)
I1107 02:48:03.570318 37922 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 02:48:16.415256 37922 solver.cpp:266] Iteration 1050 (3.89272 iter/s, 12.8445s/50 iter), loss = 0.0587669
I1107 02:48:16.415284 37922 solver.cpp:285]     Train net output #0: loss = 0.0587669 (* 1 = 0.0587669 loss)
I1107 02:48:16.415289 37922 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 02:48:29.273903 37922 solver.cpp:266] Iteration 1100 (3.88859 iter/s, 12.8581s/50 iter), loss = 0.0966492
I1107 02:48:29.273931 37922 solver.cpp:285]     Train net output #0: loss = 0.0966492 (* 1 = 0.0966492 loss)
I1107 02:48:29.273937 37922 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 02:48:42.129392 37922 solver.cpp:266] Iteration 1150 (3.88954 iter/s, 12.855s/50 iter), loss = 0.122399
I1107 02:48:42.129561 37922 solver.cpp:285]     Train net output #0: loss = 0.122399 (* 1 = 0.122399 loss)
I1107 02:48:42.129570 37922 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 02:48:54.963845 37922 solver.cpp:266] Iteration 1200 (3.89596 iter/s, 12.8338s/50 iter), loss = 0.0962712
I1107 02:48:54.963874 37922 solver.cpp:285]     Train net output #0: loss = 0.0962712 (* 1 = 0.0962712 loss)
I1107 02:48:54.963879 37922 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 02:49:07.800674 37922 solver.cpp:266] Iteration 1250 (3.8952 iter/s, 12.8363s/50 iter), loss = 0.0697145
I1107 02:49:07.800705 37922 solver.cpp:285]     Train net output #0: loss = 0.0697145 (* 1 = 0.0697145 loss)
I1107 02:49:07.800710 37922 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 02:49:20.642452 37922 solver.cpp:266] Iteration 1300 (3.8937 iter/s, 12.8413s/50 iter), loss = 0.0827327
I1107 02:49:20.642616 37922 solver.cpp:285]     Train net output #0: loss = 0.0827327 (* 1 = 0.0827327 loss)
I1107 02:49:20.642623 37922 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 02:49:33.476717 37922 solver.cpp:266] Iteration 1350 (3.89602 iter/s, 12.8336s/50 iter), loss = 0.0561906
I1107 02:49:33.476747 37922 solver.cpp:285]     Train net output #0: loss = 0.0561906 (* 1 = 0.0561906 loss)
I1107 02:49:33.476752 37922 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 02:49:46.327095 37922 solver.cpp:266] Iteration 1400 (3.89109 iter/s, 12.8499s/50 iter), loss = 0.0737373
I1107 02:49:46.327124 37922 solver.cpp:285]     Train net output #0: loss = 0.0737373 (* 1 = 0.0737373 loss)
I1107 02:49:46.327131 37922 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 02:49:59.125560 37922 solver.cpp:266] Iteration 1450 (3.90687 iter/s, 12.798s/50 iter), loss = 0.0703857
I1107 02:49:59.125716 37922 solver.cpp:285]     Train net output #0: loss = 0.0703857 (* 1 = 0.0703857 loss)
I1107 02:49:59.125725 37922 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 02:50:11.957849 37922 solver.cpp:266] Iteration 1500 (3.89662 iter/s, 12.8316s/50 iter), loss = 0.0597672
I1107 02:50:11.957877 37922 solver.cpp:285]     Train net output #0: loss = 0.0597672 (* 1 = 0.0597672 loss)
I1107 02:50:11.957883 37922 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 02:50:24.786227 37922 solver.cpp:266] Iteration 1550 (3.89776 iter/s, 12.8279s/50 iter), loss = 0.0580746
I1107 02:50:24.786257 37922 solver.cpp:285]     Train net output #0: loss = 0.0580746 (* 1 = 0.0580746 loss)
I1107 02:50:24.786263 37922 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 02:50:37.588999 37922 solver.cpp:266] Iteration 1600 (3.90556 iter/s, 12.8023s/50 iter), loss = 0.146938
I1107 02:50:37.589169 37922 solver.cpp:285]     Train net output #0: loss = 0.146938 (* 1 = 0.146938 loss)
I1107 02:50:37.589177 37922 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 02:50:50.435307 37922 solver.cpp:266] Iteration 1650 (3.89237 iter/s, 12.8457s/50 iter), loss = 0.0735678
I1107 02:50:50.435335 37922 solver.cpp:285]     Train net output #0: loss = 0.0735678 (* 1 = 0.0735678 loss)
I1107 02:50:50.435341 37922 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 02:51:03.223361 37922 solver.cpp:266] Iteration 1700 (3.91005 iter/s, 12.7875s/50 iter), loss = 0.0920579
I1107 02:51:03.223389 37922 solver.cpp:285]     Train net output #0: loss = 0.0920579 (* 1 = 0.0920579 loss)
I1107 02:51:03.223395 37922 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 02:51:16.058146 37922 solver.cpp:266] Iteration 1750 (3.89582 iter/s, 12.8343s/50 iter), loss = 0.0884159
I1107 02:51:16.058298 37922 solver.cpp:285]     Train net output #0: loss = 0.0884159 (* 1 = 0.0884159 loss)
I1107 02:51:16.058306 37922 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 02:51:28.833112 37922 solver.cpp:266] Iteration 1800 (3.9141 iter/s, 12.7743s/50 iter), loss = 0.057023
I1107 02:51:28.833140 37922 solver.cpp:285]     Train net output #0: loss = 0.057023 (* 1 = 0.057023 loss)
I1107 02:51:28.833145 37922 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 02:51:41.644809 37922 solver.cpp:266] Iteration 1850 (3.90284 iter/s, 12.8112s/50 iter), loss = 0.0708617
I1107 02:51:41.644839 37922 solver.cpp:285]     Train net output #0: loss = 0.0708617 (* 1 = 0.0708617 loss)
I1107 02:51:41.644843 37922 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 02:51:54.460479 37922 solver.cpp:266] Iteration 1900 (3.90163 iter/s, 12.8152s/50 iter), loss = 0.0456047
I1107 02:51:54.460660 37922 solver.cpp:285]     Train net output #0: loss = 0.0456047 (* 1 = 0.0456047 loss)
I1107 02:51:54.460669 37922 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 02:52:07.286155 37922 solver.cpp:266] Iteration 1950 (3.89863 iter/s, 12.825s/50 iter), loss = 0.108764
I1107 02:52:07.286185 37922 solver.cpp:285]     Train net output #0: loss = 0.108764 (* 1 = 0.108764 loss)
I1107 02:52:07.286191 37922 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 02:52:19.837313 37922 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 02:52:21.351399 37922 solver.cpp:517]     Test net output #0: loss = 0.267107 (* 1 = 0.267107 loss)
I1107 02:52:21.351414 37922 solver.cpp:517]     Test net output #1: top-1 = 0.9115
I1107 02:52:21.602867 37922 solver.cpp:266] Iteration 2000 (3.49256 iter/s, 14.3161s/50 iter), loss = 0.0630738
I1107 02:52:21.602896 37922 solver.cpp:285]     Train net output #0: loss = 0.0630738 (* 1 = 0.0630738 loss)
I1107 02:52:21.602902 37922 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 02:52:34.403702 37922 solver.cpp:266] Iteration 2050 (3.90615 iter/s, 12.8003s/50 iter), loss = 0.0611172
I1107 02:52:34.403878 37922 solver.cpp:285]     Train net output #0: loss = 0.0611172 (* 1 = 0.0611172 loss)
I1107 02:52:34.403887 37922 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 02:52:47.147805 37922 solver.cpp:266] Iteration 2100 (3.92359 iter/s, 12.7434s/50 iter), loss = 0.0835692
I1107 02:52:47.147836 37922 solver.cpp:285]     Train net output #0: loss = 0.0835693 (* 1 = 0.0835693 loss)
I1107 02:52:47.147842 37922 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 02:52:59.964339 37922 solver.cpp:266] Iteration 2150 (3.90137 iter/s, 12.816s/50 iter), loss = 0.0508113
I1107 02:52:59.964368 37922 solver.cpp:285]     Train net output #0: loss = 0.0508113 (* 1 = 0.0508113 loss)
I1107 02:52:59.964375 37922 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 02:53:12.780560 37922 solver.cpp:266] Iteration 2200 (3.90146 iter/s, 12.8157s/50 iter), loss = 0.0794533
I1107 02:53:12.780727 37922 solver.cpp:285]     Train net output #0: loss = 0.0794534 (* 1 = 0.0794534 loss)
I1107 02:53:12.780735 37922 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 02:53:25.602272 37922 solver.cpp:266] Iteration 2250 (3.89983 iter/s, 12.8211s/50 iter), loss = 0.0827115
I1107 02:53:25.602303 37922 solver.cpp:285]     Train net output #0: loss = 0.0827115 (* 1 = 0.0827115 loss)
I1107 02:53:25.602308 37922 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 02:53:38.406994 37922 solver.cpp:266] Iteration 2300 (3.90497 iter/s, 12.8042s/50 iter), loss = 0.0647799
I1107 02:53:38.407024 37922 solver.cpp:285]     Train net output #0: loss = 0.0647799 (* 1 = 0.0647799 loss)
I1107 02:53:38.407029 37922 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 02:53:51.207351 37922 solver.cpp:266] Iteration 2350 (3.9063 iter/s, 12.7998s/50 iter), loss = 0.0534892
I1107 02:53:51.207500 37922 solver.cpp:285]     Train net output #0: loss = 0.0534892 (* 1 = 0.0534892 loss)
I1107 02:53:51.207509 37922 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 02:54:03.993414 37922 solver.cpp:266] Iteration 2400 (3.9107 iter/s, 12.7854s/50 iter), loss = 0.127093
I1107 02:54:03.993445 37922 solver.cpp:285]     Train net output #0: loss = 0.127093 (* 1 = 0.127093 loss)
I1107 02:54:03.993450 37922 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 02:54:16.779923 37922 solver.cpp:266] Iteration 2450 (3.91053 iter/s, 12.786s/50 iter), loss = 0.0879046
I1107 02:54:16.779954 37922 solver.cpp:285]     Train net output #0: loss = 0.0879046 (* 1 = 0.0879046 loss)
I1107 02:54:16.779960 37922 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 02:54:29.564879 37922 solver.cpp:266] Iteration 2500 (3.911 iter/s, 12.7844s/50 iter), loss = 0.0644912
I1107 02:54:29.565068 37922 solver.cpp:285]     Train net output #0: loss = 0.0644912 (* 1 = 0.0644912 loss)
I1107 02:54:29.565075 37922 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 02:54:42.373999 37922 solver.cpp:266] Iteration 2550 (3.90367 iter/s, 12.8085s/50 iter), loss = 0.0402222
I1107 02:54:42.374032 37922 solver.cpp:285]     Train net output #0: loss = 0.0402223 (* 1 = 0.0402223 loss)
I1107 02:54:42.374053 37922 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 02:54:55.148998 37922 solver.cpp:266] Iteration 2600 (3.91405 iter/s, 12.7745s/50 iter), loss = 0.075227
I1107 02:54:55.149029 37922 solver.cpp:285]     Train net output #0: loss = 0.0752271 (* 1 = 0.0752271 loss)
I1107 02:54:55.149044 37922 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 02:55:07.993034 37922 solver.cpp:266] Iteration 2650 (3.89301 iter/s, 12.8435s/50 iter), loss = 0.00928182
I1107 02:55:07.993191 37922 solver.cpp:285]     Train net output #0: loss = 0.00928183 (* 1 = 0.00928183 loss)
I1107 02:55:07.993201 37922 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 02:55:20.745829 37922 solver.cpp:266] Iteration 2700 (3.92091 iter/s, 12.7522s/50 iter), loss = 0.0312315
I1107 02:55:20.745862 37922 solver.cpp:285]     Train net output #0: loss = 0.0312315 (* 1 = 0.0312315 loss)
I1107 02:55:20.745867 37922 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 02:55:33.581775 37922 solver.cpp:266] Iteration 2750 (3.89547 iter/s, 12.8354s/50 iter), loss = 0.0263157
I1107 02:55:33.581805 37922 solver.cpp:285]     Train net output #0: loss = 0.0263157 (* 1 = 0.0263157 loss)
I1107 02:55:33.581811 37922 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 02:55:46.357223 37922 solver.cpp:266] Iteration 2800 (3.91391 iter/s, 12.7749s/50 iter), loss = 0.0275533
I1107 02:55:46.358443 37922 solver.cpp:285]     Train net output #0: loss = 0.0275533 (* 1 = 0.0275533 loss)
I1107 02:55:46.358464 37922 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 02:55:59.159185 37922 solver.cpp:266] Iteration 2850 (3.90617 iter/s, 12.8003s/50 iter), loss = 0.0233686
I1107 02:55:59.159215 37922 solver.cpp:285]     Train net output #0: loss = 0.0233686 (* 1 = 0.0233686 loss)
I1107 02:55:59.159221 37922 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 02:56:11.932960 37922 solver.cpp:266] Iteration 2900 (3.91443 iter/s, 12.7733s/50 iter), loss = 0.0737184
I1107 02:56:11.932989 37922 solver.cpp:285]     Train net output #0: loss = 0.0737184 (* 1 = 0.0737184 loss)
I1107 02:56:11.932994 37922 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 02:56:24.739820 37922 solver.cpp:266] Iteration 2950 (3.90431 iter/s, 12.8063s/50 iter), loss = 0.0243708
I1107 02:56:24.739985 37922 solver.cpp:285]     Train net output #0: loss = 0.0243708 (* 1 = 0.0243708 loss)
I1107 02:56:24.739995 37922 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 02:56:37.261142 37922 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 02:56:38.781517 37922 solver.cpp:517]     Test net output #0: loss = 0.129316 (* 1 = 0.129316 loss)
I1107 02:56:38.781532 37922 solver.cpp:517]     Test net output #1: top-1 = 0.949
I1107 02:56:39.031143 37922 solver.cpp:266] Iteration 3000 (3.4988 iter/s, 14.2906s/50 iter), loss = 0.0162651
I1107 02:56:39.031172 37922 solver.cpp:285]     Train net output #0: loss = 0.0162651 (* 1 = 0.0162651 loss)
I1107 02:56:39.031177 37922 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 02:56:51.804344 37922 solver.cpp:266] Iteration 3050 (3.9146 iter/s, 12.7727s/50 iter), loss = 0.0271623
I1107 02:56:51.804374 37922 solver.cpp:285]     Train net output #0: loss = 0.0271623 (* 1 = 0.0271623 loss)
I1107 02:56:51.804380 37922 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 02:57:04.618948 37922 solver.cpp:266] Iteration 3100 (3.90196 iter/s, 12.8141s/50 iter), loss = 0.0252941
I1107 02:57:04.619109 37922 solver.cpp:285]     Train net output #0: loss = 0.0252941 (* 1 = 0.0252941 loss)
I1107 02:57:04.619118 37922 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 02:57:17.371995 37922 solver.cpp:266] Iteration 3150 (3.92083 iter/s, 12.7524s/50 iter), loss = 0.0177114
I1107 02:57:17.372026 37922 solver.cpp:285]     Train net output #0: loss = 0.0177114 (* 1 = 0.0177114 loss)
I1107 02:57:17.372032 37922 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 02:57:30.123442 37922 solver.cpp:266] Iteration 3200 (3.92128 iter/s, 12.7509s/50 iter), loss = 0.0579832
I1107 02:57:30.123472 37922 solver.cpp:285]     Train net output #0: loss = 0.0579832 (* 1 = 0.0579832 loss)
I1107 02:57:30.123478 37922 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 02:57:42.941301 37922 solver.cpp:266] Iteration 3250 (3.90096 iter/s, 12.8173s/50 iter), loss = 0.0294735
I1107 02:57:42.942091 37922 solver.cpp:285]     Train net output #0: loss = 0.0294735 (* 1 = 0.0294735 loss)
I1107 02:57:42.942101 37922 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 02:57:55.759886 37922 solver.cpp:266] Iteration 3300 (3.90097 iter/s, 12.8173s/50 iter), loss = 0.0283121
I1107 02:57:55.759914 37922 solver.cpp:285]     Train net output #0: loss = 0.0283121 (* 1 = 0.0283121 loss)
I1107 02:57:55.759937 37922 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 02:58:08.536068 37922 solver.cpp:266] Iteration 3350 (3.91369 iter/s, 12.7757s/50 iter), loss = 0.0242103
I1107 02:58:08.536098 37922 solver.cpp:285]     Train net output #0: loss = 0.0242103 (* 1 = 0.0242103 loss)
I1107 02:58:08.536103 37922 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I1107 02:58:21.332134 37922 solver.cpp:266] Iteration 3400 (3.90761 iter/s, 12.7956s/50 iter), loss = 0.0173886
I1107 02:58:21.332279 37922 solver.cpp:285]     Train net output #0: loss = 0.0173886 (* 1 = 0.0173886 loss)
I1107 02:58:21.332288 37922 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I1107 02:58:34.076449 37922 solver.cpp:266] Iteration 3450 (3.92351 iter/s, 12.7437s/50 iter), loss = 0.0157312
I1107 02:58:34.076480 37922 solver.cpp:285]     Train net output #0: loss = 0.0157312 (* 1 = 0.0157312 loss)
I1107 02:58:34.076485 37922 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I1107 02:58:46.896142 37922 solver.cpp:266] Iteration 3500 (3.90041 iter/s, 12.8192s/50 iter), loss = 0.0318733
I1107 02:58:46.896172 37922 solver.cpp:285]     Train net output #0: loss = 0.0318734 (* 1 = 0.0318734 loss)
I1107 02:58:46.896178 37922 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I1107 02:58:59.668141 37922 solver.cpp:266] Iteration 3550 (3.91497 iter/s, 12.7715s/50 iter), loss = 0.0387125
I1107 02:58:59.669212 37922 solver.cpp:285]     Train net output #0: loss = 0.0387125 (* 1 = 0.0387125 loss)
I1107 02:58:59.669220 37922 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I1107 02:59:12.462103 37922 solver.cpp:266] Iteration 3600 (3.90858 iter/s, 12.7924s/50 iter), loss = 0.0067
I1107 02:59:12.462136 37922 solver.cpp:285]     Train net output #0: loss = 0.00670002 (* 1 = 0.00670002 loss)
I1107 02:59:12.462142 37922 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I1107 02:59:25.194804 37922 solver.cpp:266] Iteration 3650 (3.92707 iter/s, 12.7321s/50 iter), loss = 0.0360534
I1107 02:59:25.194835 37922 solver.cpp:285]     Train net output #0: loss = 0.0360535 (* 1 = 0.0360535 loss)
I1107 02:59:25.194841 37922 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I1107 02:59:37.969861 37922 solver.cpp:266] Iteration 3700 (3.91405 iter/s, 12.7745s/50 iter), loss = 0.0340844
I1107 02:59:37.970037 37922 solver.cpp:285]     Train net output #0: loss = 0.0340844 (* 1 = 0.0340844 loss)
I1107 02:59:37.970046 37922 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I1107 02:59:50.710952 37922 solver.cpp:266] Iteration 3750 (3.92453 iter/s, 12.7404s/50 iter), loss = 0.0144397
I1107 02:59:50.710980 37922 solver.cpp:285]     Train net output #0: loss = 0.0144397 (* 1 = 0.0144397 loss)
I1107 02:59:50.710985 37922 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I1107 03:00:03.483309 37922 solver.cpp:266] Iteration 3800 (3.91488 iter/s, 12.7718s/50 iter), loss = 0.0359376
I1107 03:00:03.483338 37922 solver.cpp:285]     Train net output #0: loss = 0.0359376 (* 1 = 0.0359376 loss)
I1107 03:00:03.483345 37922 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I1107 03:00:16.236930 37922 solver.cpp:266] Iteration 3850 (3.92063 iter/s, 12.7531s/50 iter), loss = 0.00989147
I1107 03:00:16.237440 37922 solver.cpp:285]     Train net output #0: loss = 0.00989148 (* 1 = 0.00989148 loss)
I1107 03:00:16.237449 37922 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I1107 03:00:29.049017 37922 solver.cpp:266] Iteration 3900 (3.90288 iter/s, 12.811s/50 iter), loss = 0.0232979
I1107 03:00:29.049044 37922 solver.cpp:285]     Train net output #0: loss = 0.0232979 (* 1 = 0.0232979 loss)
I1107 03:00:29.049051 37922 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I1107 03:00:41.812269 37922 solver.cpp:266] Iteration 3950 (3.91767 iter/s, 12.7627s/50 iter), loss = 0.0164831
I1107 03:00:41.812299 37922 solver.cpp:285]     Train net output #0: loss = 0.0164831 (* 1 = 0.0164831 loss)
I1107 03:00:41.812305 37922 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I1107 03:00:54.303112 37922 solver.cpp:418] Iteration 4000, Testing net (#0)
I1107 03:00:55.819002 37922 solver.cpp:517]     Test net output #0: loss = 0.13777 (* 1 = 0.13777 loss)
I1107 03:00:55.819017 37922 solver.cpp:517]     Test net output #1: top-1 = 0.94375
I1107 03:00:56.068094 37922 solver.cpp:266] Iteration 4000 (3.50749 iter/s, 14.2552s/50 iter), loss = 0.0110414
I1107 03:00:56.068119 37922 solver.cpp:285]     Train net output #0: loss = 0.0110414 (* 1 = 0.0110414 loss)
I1107 03:00:56.068140 37922 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I1107 03:01:08.824513 37922 solver.cpp:266] Iteration 4050 (3.91977 iter/s, 12.7559s/50 iter), loss = 0.00545481
I1107 03:01:08.824542 37922 solver.cpp:285]     Train net output #0: loss = 0.00545482 (* 1 = 0.00545482 loss)
I1107 03:01:08.824548 37922 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I1107 03:01:21.591483 37922 solver.cpp:266] Iteration 4100 (3.91653 iter/s, 12.7664s/50 iter), loss = 0.00349819
I1107 03:01:21.591513 37922 solver.cpp:285]     Train net output #0: loss = 0.00349819 (* 1 = 0.00349819 loss)
I1107 03:01:21.591519 37922 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I1107 03:01:34.349267 37922 solver.cpp:266] Iteration 4150 (3.91935 iter/s, 12.7572s/50 iter), loss = 0.00582678
I1107 03:01:34.350453 37922 solver.cpp:285]     Train net output #0: loss = 0.00582679 (* 1 = 0.00582679 loss)
I1107 03:01:34.350461 37922 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I1107 03:01:47.114954 37922 solver.cpp:266] Iteration 4200 (3.91727 iter/s, 12.764s/50 iter), loss = 0.0177064
I1107 03:01:47.114984 37922 solver.cpp:285]     Train net output #0: loss = 0.0177064 (* 1 = 0.0177064 loss)
I1107 03:01:47.114989 37922 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I1107 03:01:59.893174 37922 solver.cpp:266] Iteration 4250 (3.91308 iter/s, 12.7777s/50 iter), loss = 0.0152862
I1107 03:01:59.893204 37922 solver.cpp:285]     Train net output #0: loss = 0.0152862 (* 1 = 0.0152862 loss)
I1107 03:01:59.893210 37922 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I1107 03:02:12.636176 37922 solver.cpp:266] Iteration 4300 (3.92389 iter/s, 12.7425s/50 iter), loss = 0.00766078
I1107 03:02:12.636382 37922 solver.cpp:285]     Train net output #0: loss = 0.00766079 (* 1 = 0.00766079 loss)
I1107 03:02:12.636390 37922 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I1107 03:02:25.413456 37922 solver.cpp:266] Iteration 4350 (3.91342 iter/s, 12.7766s/50 iter), loss = 0.00968781
I1107 03:02:25.413486 37922 solver.cpp:285]     Train net output #0: loss = 0.00968782 (* 1 = 0.00968782 loss)
I1107 03:02:25.413491 37922 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I1107 03:02:38.186556 37922 solver.cpp:266] Iteration 4400 (3.91464 iter/s, 12.7726s/50 iter), loss = 0.0164019
I1107 03:02:38.186586 37922 solver.cpp:285]     Train net output #0: loss = 0.0164019 (* 1 = 0.0164019 loss)
I1107 03:02:38.186592 37922 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I1107 03:02:50.880195 37922 solver.cpp:266] Iteration 4450 (3.93915 iter/s, 12.6931s/50 iter), loss = 0.0178764
I1107 03:02:50.880368 37922 solver.cpp:285]     Train net output #0: loss = 0.0178764 (* 1 = 0.0178764 loss)
I1107 03:02:50.880376 37922 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I1107 03:03:03.656055 37922 solver.cpp:266] Iteration 4500 (3.91384 iter/s, 12.7752s/50 iter), loss = 0.0145654
I1107 03:03:03.656086 37922 solver.cpp:285]     Train net output #0: loss = 0.0145654 (* 1 = 0.0145654 loss)
I1107 03:03:03.656091 37922 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I1107 03:03:16.451778 37922 solver.cpp:266] Iteration 4550 (3.90772 iter/s, 12.7952s/50 iter), loss = 0.0141698
I1107 03:03:16.451808 37922 solver.cpp:285]     Train net output #0: loss = 0.0141698 (* 1 = 0.0141698 loss)
I1107 03:03:16.451814 37922 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I1107 03:03:29.192898 37922 solver.cpp:266] Iteration 4600 (3.92447 iter/s, 12.7406s/50 iter), loss = 0.0155883
I1107 03:03:29.193068 37922 solver.cpp:285]     Train net output #0: loss = 0.0155883 (* 1 = 0.0155883 loss)
I1107 03:03:29.193078 37922 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I1107 03:03:41.960142 37922 solver.cpp:266] Iteration 4650 (3.91648 iter/s, 12.7666s/50 iter), loss = 0.0226567
I1107 03:03:41.960171 37922 solver.cpp:285]     Train net output #0: loss = 0.0226567 (* 1 = 0.0226567 loss)
I1107 03:03:41.960177 37922 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I1107 03:03:54.734850 37922 solver.cpp:266] Iteration 4700 (3.91415 iter/s, 12.7742s/50 iter), loss = 0.0148294
I1107 03:03:54.734879 37922 solver.cpp:285]     Train net output #0: loss = 0.0148294 (* 1 = 0.0148294 loss)
I1107 03:03:54.734886 37922 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I1107 03:04:07.478420 37922 solver.cpp:266] Iteration 4750 (3.92371 iter/s, 12.743s/50 iter), loss = 0.0141659
I1107 03:04:07.478574 37922 solver.cpp:285]     Train net output #0: loss = 0.0141659 (* 1 = 0.0141659 loss)
I1107 03:04:07.478581 37922 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I1107 03:04:20.221693 37922 solver.cpp:266] Iteration 4800 (3.92384 iter/s, 12.7426s/50 iter), loss = 0.0107883
I1107 03:04:20.221719 37922 solver.cpp:285]     Train net output #0: loss = 0.0107883 (* 1 = 0.0107883 loss)
I1107 03:04:20.221725 37922 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I1107 03:04:32.964809 37922 solver.cpp:266] Iteration 4850 (3.92385 iter/s, 12.7426s/50 iter), loss = 0.00220607
I1107 03:04:32.964838 37922 solver.cpp:285]     Train net output #0: loss = 0.00220607 (* 1 = 0.00220607 loss)
I1107 03:04:32.964844 37922 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I1107 03:04:45.733721 37922 solver.cpp:266] Iteration 4900 (3.91593 iter/s, 12.7684s/50 iter), loss = 0.0167383
I1107 03:04:45.733880 37922 solver.cpp:285]     Train net output #0: loss = 0.0167383 (* 1 = 0.0167383 loss)
I1107 03:04:45.733888 37922 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I1107 03:04:58.503988 37922 solver.cpp:266] Iteration 4950 (3.91555 iter/s, 12.7696s/50 iter), loss = 0.0374205
I1107 03:04:58.504017 37922 solver.cpp:285]     Train net output #0: loss = 0.0374205 (* 1 = 0.0374205 loss)
I1107 03:04:58.504024 37922 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I1107 03:05:11.011071 37922 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_5000.caffemodel
I1107 03:05:13.368379 37922 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_5000.solverstate
I1107 03:05:13.834120 37922 solver.cpp:418] Iteration 5000, Testing net (#0)
I1107 03:05:15.299095 37922 solver.cpp:517]     Test net output #0: loss = 0.153531 (* 1 = 0.153531 loss)
I1107 03:05:15.299110 37922 solver.cpp:517]     Test net output #1: top-1 = 0.949
I1107 03:05:15.542775 37922 solver.cpp:266] Iteration 5000 (2.9346 iter/s, 17.0381s/50 iter), loss = 0.0122028
I1107 03:05:15.542801 37922 solver.cpp:285]     Train net output #0: loss = 0.0122028 (* 1 = 0.0122028 loss)
I1107 03:05:15.542807 37922 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I1107 03:05:28.104773 37922 solver.cpp:266] Iteration 5050 (3.98043 iter/s, 12.5615s/50 iter), loss = 0.0116633
I1107 03:05:28.104957 37922 solver.cpp:285]     Train net output #0: loss = 0.0116633 (* 1 = 0.0116633 loss)
I1107 03:05:28.104967 37922 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I1107 03:05:40.748080 37922 solver.cpp:266] Iteration 5100 (3.95488 iter/s, 12.6426s/50 iter), loss = 0.0146739
I1107 03:05:40.748109 37922 solver.cpp:285]     Train net output #0: loss = 0.0146739 (* 1 = 0.0146739 loss)
I1107 03:05:40.748131 37922 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I1107 03:05:53.504200 37922 solver.cpp:266] Iteration 5150 (3.91985 iter/s, 12.7556s/50 iter), loss = 0.00321126
I1107 03:05:53.504230 37922 solver.cpp:285]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1107 03:05:53.504236 37922 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I1107 03:06:06.256224 37922 solver.cpp:266] Iteration 5200 (3.92111 iter/s, 12.7515s/50 iter), loss = 0.00538847
I1107 03:06:06.256371 37922 solver.cpp:285]     Train net output #0: loss = 0.00538848 (* 1 = 0.00538848 loss)
I1107 03:06:06.256378 37922 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I1107 03:06:19.006940 37922 solver.cpp:266] Iteration 5250 (3.92155 iter/s, 12.7501s/50 iter), loss = 0.00990796
I1107 03:06:19.006968 37922 solver.cpp:285]     Train net output #0: loss = 0.00990796 (* 1 = 0.00990796 loss)
I1107 03:06:19.006974 37922 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I1107 03:06:31.764349 37922 solver.cpp:266] Iteration 5300 (3.91946 iter/s, 12.7569s/50 iter), loss = 0.00835179
I1107 03:06:31.764379 37922 solver.cpp:285]     Train net output #0: loss = 0.00835179 (* 1 = 0.00835179 loss)
I1107 03:06:31.764384 37922 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I1107 03:06:44.559098 37922 solver.cpp:266] Iteration 5350 (3.90802 iter/s, 12.7942s/50 iter), loss = 0.00328456
I1107 03:06:44.559257 37922 solver.cpp:285]     Train net output #0: loss = 0.00328456 (* 1 = 0.00328456 loss)
I1107 03:06:44.559264 37922 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I1107 03:06:57.316745 37922 solver.cpp:266] Iteration 5400 (3.91942 iter/s, 12.757s/50 iter), loss = 0.0177857
I1107 03:06:57.316776 37922 solver.cpp:285]     Train net output #0: loss = 0.0177857 (* 1 = 0.0177857 loss)
I1107 03:06:57.316782 37922 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I1107 03:07:10.092180 37922 solver.cpp:266] Iteration 5450 (3.91393 iter/s, 12.7749s/50 iter), loss = 0.009064
I1107 03:07:10.092208 37922 solver.cpp:285]     Train net output #0: loss = 0.009064 (* 1 = 0.009064 loss)
I1107 03:07:10.092216 37922 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I1107 03:07:22.840276 37922 solver.cpp:266] Iteration 5500 (3.92232 iter/s, 12.7476s/50 iter), loss = 0.00176432
I1107 03:07:22.840427 37922 solver.cpp:285]     Train net output #0: loss = 0.00176431 (* 1 = 0.00176431 loss)
I1107 03:07:22.840436 37922 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I1107 03:07:35.597149 37922 solver.cpp:266] Iteration 5550 (3.91966 iter/s, 12.7562s/50 iter), loss = 0.00340657
I1107 03:07:35.597177 37922 solver.cpp:285]     Train net output #0: loss = 0.00340657 (* 1 = 0.00340657 loss)
I1107 03:07:35.597183 37922 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I1107 03:07:48.322993 37922 solver.cpp:266] Iteration 5600 (3.92918 iter/s, 12.7253s/50 iter), loss = 0.0119235
I1107 03:07:48.323022 37922 solver.cpp:285]     Train net output #0: loss = 0.0119235 (* 1 = 0.0119235 loss)
I1107 03:07:48.323029 37922 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I1107 03:08:01.096580 37922 solver.cpp:266] Iteration 5650 (3.91449 iter/s, 12.773s/50 iter), loss = 0.0110709
I1107 03:08:01.096724 37922 solver.cpp:285]     Train net output #0: loss = 0.0110709 (* 1 = 0.0110709 loss)
I1107 03:08:01.096732 37922 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I1107 03:08:13.851850 37922 solver.cpp:266] Iteration 5700 (3.92015 iter/s, 12.7546s/50 iter), loss = 0.024521
I1107 03:08:13.851879 37922 solver.cpp:285]     Train net output #0: loss = 0.024521 (* 1 = 0.024521 loss)
I1107 03:08:13.851886 37922 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I1107 03:08:26.643507 37922 solver.cpp:266] Iteration 5750 (3.90896 iter/s, 12.7911s/50 iter), loss = 0.00455629
I1107 03:08:26.643538 37922 solver.cpp:285]     Train net output #0: loss = 0.0045563 (* 1 = 0.0045563 loss)
I1107 03:08:26.643544 37922 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I1107 03:08:39.375123 37922 solver.cpp:266] Iteration 5800 (3.9274 iter/s, 12.7311s/50 iter), loss = 0.00772138
I1107 03:08:39.375290 37922 solver.cpp:285]     Train net output #0: loss = 0.00772139 (* 1 = 0.00772139 loss)
I1107 03:08:39.375299 37922 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I1107 03:08:52.133776 37922 solver.cpp:266] Iteration 5850 (3.91911 iter/s, 12.758s/50 iter), loss = 0.00524977
I1107 03:08:52.133806 37922 solver.cpp:285]     Train net output #0: loss = 0.00524978 (* 1 = 0.00524978 loss)
I1107 03:08:52.133812 37922 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I1107 03:09:04.879871 37922 solver.cpp:266] Iteration 5900 (3.92293 iter/s, 12.7456s/50 iter), loss = 0.0112246
I1107 03:09:04.879901 37922 solver.cpp:285]     Train net output #0: loss = 0.0112246 (* 1 = 0.0112246 loss)
I1107 03:09:04.879925 37922 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I1107 03:09:17.604138 37922 solver.cpp:266] Iteration 5950 (3.92966 iter/s, 12.7237s/50 iter), loss = 0.0276794
I1107 03:09:17.604296 37922 solver.cpp:285]     Train net output #0: loss = 0.0276794 (* 1 = 0.0276794 loss)
I1107 03:09:17.604305 37922 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I1107 03:09:30.136993 37922 solver.cpp:418] Iteration 6000, Testing net (#0)
I1107 03:09:31.634917 37922 solver.cpp:517]     Test net output #0: loss = 0.159342 (* 1 = 0.159342 loss)
I1107 03:09:31.634932 37922 solver.cpp:517]     Test net output #1: top-1 = 0.9525
I1107 03:09:31.883955 37922 solver.cpp:266] Iteration 6000 (3.50162 iter/s, 14.2791s/50 iter), loss = 0.00311996
I1107 03:09:31.883983 37922 solver.cpp:285]     Train net output #0: loss = 0.00311997 (* 1 = 0.00311997 loss)
I1107 03:09:31.883990 37922 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I1107 03:09:44.642889 37922 solver.cpp:266] Iteration 6050 (3.91899 iter/s, 12.7584s/50 iter), loss = 0.0152876
I1107 03:09:44.642917 37922 solver.cpp:285]     Train net output #0: loss = 0.0152876 (* 1 = 0.0152876 loss)
I1107 03:09:44.642925 37922 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I1107 03:09:57.398852 37922 solver.cpp:266] Iteration 6100 (3.9199 iter/s, 12.7554s/50 iter), loss = 0.00198256
I1107 03:09:57.399019 37922 solver.cpp:285]     Train net output #0: loss = 0.00198256 (* 1 = 0.00198256 loss)
I1107 03:09:57.399029 37922 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I1107 03:10:10.194213 37922 solver.cpp:266] Iteration 6150 (3.90787 iter/s, 12.7947s/50 iter), loss = 0.00186725
I1107 03:10:10.194245 37922 solver.cpp:285]     Train net output #0: loss = 0.00186725 (* 1 = 0.00186725 loss)
I1107 03:10:10.194267 37922 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I1107 03:10:22.930232 37922 solver.cpp:266] Iteration 6200 (3.92604 iter/s, 12.7355s/50 iter), loss = 0.00180593
I1107 03:10:22.930263 37922 solver.cpp:285]     Train net output #0: loss = 0.00180593 (* 1 = 0.00180593 loss)
I1107 03:10:22.930269 37922 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I1107 03:10:35.678915 37922 solver.cpp:266] Iteration 6250 (3.92214 iter/s, 12.7482s/50 iter), loss = 0.0107991
I1107 03:10:35.679051 37922 solver.cpp:285]     Train net output #0: loss = 0.0107991 (* 1 = 0.0107991 loss)
I1107 03:10:35.679059 37922 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I1107 03:10:48.444772 37922 solver.cpp:266] Iteration 6300 (3.91689 iter/s, 12.7652s/50 iter), loss = 0.00805205
I1107 03:10:48.444802 37922 solver.cpp:285]     Train net output #0: loss = 0.00805205 (* 1 = 0.00805205 loss)
I1107 03:10:48.444808 37922 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I1107 03:11:01.209476 37922 solver.cpp:266] Iteration 6350 (3.91721 iter/s, 12.7642s/50 iter), loss = 0.00629607
I1107 03:11:01.209506 37922 solver.cpp:285]     Train net output #0: loss = 0.00629608 (* 1 = 0.00629608 loss)
I1107 03:11:01.209527 37922 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I1107 03:11:13.991863 37922 solver.cpp:266] Iteration 6400 (3.9118 iter/s, 12.7819s/50 iter), loss = 0.0158274
I1107 03:11:13.992024 37922 solver.cpp:285]     Train net output #0: loss = 0.0158274 (* 1 = 0.0158274 loss)
I1107 03:11:13.992033 37922 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I1107 03:11:26.748122 37922 solver.cpp:266] Iteration 6450 (3.91985 iter/s, 12.7556s/50 iter), loss = 0.0134745
I1107 03:11:26.748152 37922 solver.cpp:285]     Train net output #0: loss = 0.0134745 (* 1 = 0.0134745 loss)
I1107 03:11:26.748158 37922 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I1107 03:11:39.490990 37922 solver.cpp:266] Iteration 6500 (3.92393 iter/s, 12.7423s/50 iter), loss = 0.00325712
I1107 03:11:39.491019 37922 solver.cpp:285]     Train net output #0: loss = 0.00325713 (* 1 = 0.00325713 loss)
I1107 03:11:39.491025 37922 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I1107 03:11:52.229851 37922 solver.cpp:266] Iteration 6550 (3.92516 iter/s, 12.7383s/50 iter), loss = 0.0178395
I1107 03:11:52.230016 37922 solver.cpp:285]     Train net output #0: loss = 0.0178395 (* 1 = 0.0178395 loss)
I1107 03:11:52.230026 37922 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I1107 03:12:05.021574 37922 solver.cpp:266] Iteration 6600 (3.90898 iter/s, 12.7911s/50 iter), loss = 0.00865384
I1107 03:12:05.021605 37922 solver.cpp:285]     Train net output #0: loss = 0.00865385 (* 1 = 0.00865385 loss)
I1107 03:12:05.021627 37922 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I1107 03:12:17.754173 37922 solver.cpp:266] Iteration 6650 (3.92709 iter/s, 12.7321s/50 iter), loss = 0.00152155
I1107 03:12:17.754201 37922 solver.cpp:285]     Train net output #0: loss = 0.00152156 (* 1 = 0.00152156 loss)
I1107 03:12:17.754207 37922 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I1107 03:12:30.526748 37922 solver.cpp:266] Iteration 6700 (3.9148 iter/s, 12.772s/50 iter), loss = 0.00901314
I1107 03:12:30.526890 37922 solver.cpp:285]     Train net output #0: loss = 0.00901315 (* 1 = 0.00901315 loss)
I1107 03:12:30.526898 37922 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I1107 03:12:43.293838 37922 solver.cpp:266] Iteration 6750 (3.91652 iter/s, 12.7665s/50 iter), loss = 0.0128457
I1107 03:12:43.293879 37922 solver.cpp:285]     Train net output #0: loss = 0.0128457 (* 1 = 0.0128457 loss)
I1107 03:12:43.293885 37922 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I1107 03:12:56.071040 37922 solver.cpp:266] Iteration 6800 (3.91338 iter/s, 12.7767s/50 iter), loss = 0.0148047
I1107 03:12:56.071070 37922 solver.cpp:285]     Train net output #0: loss = 0.0148048 (* 1 = 0.0148048 loss)
I1107 03:12:56.071076 37922 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I1107 03:13:08.826934 37922 solver.cpp:266] Iteration 6850 (3.91992 iter/s, 12.7554s/50 iter), loss = 0.00488815
I1107 03:13:08.827091 37922 solver.cpp:285]     Train net output #0: loss = 0.00488816 (* 1 = 0.00488816 loss)
I1107 03:13:08.827100 37922 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I1107 03:13:21.586200 37922 solver.cpp:266] Iteration 6900 (3.91892 iter/s, 12.7586s/50 iter), loss = 0.00320175
I1107 03:13:21.586230 37922 solver.cpp:285]     Train net output #0: loss = 0.00320176 (* 1 = 0.00320176 loss)
I1107 03:13:21.586253 37922 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I1107 03:13:34.351927 37922 solver.cpp:266] Iteration 6950 (3.9169 iter/s, 12.7652s/50 iter), loss = 0.0193492
I1107 03:13:34.351955 37922 solver.cpp:285]     Train net output #0: loss = 0.0193492 (* 1 = 0.0193492 loss)
I1107 03:13:34.351961 37922 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I1107 03:13:46.836261 37922 solver.cpp:418] Iteration 7000, Testing net (#0)
I1107 03:13:48.361304 37922 solver.cpp:517]     Test net output #0: loss = 0.173874 (* 1 = 0.173874 loss)
I1107 03:13:48.361320 37922 solver.cpp:517]     Test net output #1: top-1 = 0.95025
I1107 03:13:48.609534 37922 solver.cpp:266] Iteration 7000 (3.50704 iter/s, 14.257s/50 iter), loss = 0.00439922
I1107 03:13:48.609563 37922 solver.cpp:285]     Train net output #0: loss = 0.00439923 (* 1 = 0.00439923 loss)
I1107 03:13:48.609570 37922 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I1107 03:14:01.355300 37922 solver.cpp:266] Iteration 7050 (3.92303 iter/s, 12.7452s/50 iter), loss = 0.00209725
I1107 03:14:01.355329 37922 solver.cpp:285]     Train net output #0: loss = 0.00209726 (* 1 = 0.00209726 loss)
I1107 03:14:01.355335 37922 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I1107 03:14:14.135190 37922 solver.cpp:266] Iteration 7100 (3.91256 iter/s, 12.7794s/50 iter), loss = 0.00946135
I1107 03:14:14.135221 37922 solver.cpp:285]     Train net output #0: loss = 0.00946137 (* 1 = 0.00946137 loss)
I1107 03:14:14.135227 37922 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I1107 03:14:26.875736 37922 solver.cpp:266] Iteration 7150 (3.92464 iter/s, 12.74s/50 iter), loss = 0.00863581
I1107 03:14:26.875932 37922 solver.cpp:285]     Train net output #0: loss = 0.00863583 (* 1 = 0.00863583 loss)
I1107 03:14:26.875941 37922 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I1107 03:14:39.631850 37922 solver.cpp:266] Iteration 7200 (3.9199 iter/s, 12.7554s/50 iter), loss = 0.00337039
I1107 03:14:39.631881 37922 solver.cpp:285]     Train net output #0: loss = 0.00337041 (* 1 = 0.00337041 loss)
I1107 03:14:39.631889 37922 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I1107 03:14:52.405773 37922 solver.cpp:266] Iteration 7250 (3.91439 iter/s, 12.7734s/50 iter), loss = 0.00316591
I1107 03:14:52.405805 37922 solver.cpp:285]     Train net output #0: loss = 0.00316592 (* 1 = 0.00316592 loss)
I1107 03:14:52.405813 37922 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I1107 03:15:05.163609 37922 solver.cpp:266] Iteration 7300 (3.91932 iter/s, 12.7573s/50 iter), loss = 0.022828
I1107 03:15:05.163765 37922 solver.cpp:285]     Train net output #0: loss = 0.022828 (* 1 = 0.022828 loss)
I1107 03:15:05.163774 37922 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I1107 03:15:17.948856 37922 solver.cpp:266] Iteration 7350 (3.91096 iter/s, 12.7846s/50 iter), loss = 0.00124438
I1107 03:15:17.948887 37922 solver.cpp:285]     Train net output #0: loss = 0.0012444 (* 1 = 0.0012444 loss)
I1107 03:15:17.948894 37922 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I1107 03:15:30.718778 37922 solver.cpp:266] Iteration 7400 (3.91561 iter/s, 12.7694s/50 iter), loss = 0.00283902
I1107 03:15:30.718811 37922 solver.cpp:285]     Train net output #0: loss = 0.00283903 (* 1 = 0.00283903 loss)
I1107 03:15:30.718818 37922 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I1107 03:15:43.446811 37922 solver.cpp:266] Iteration 7450 (3.9285 iter/s, 12.7275s/50 iter), loss = 0.00208792
I1107 03:15:43.446970 37922 solver.cpp:285]     Train net output #0: loss = 0.00208794 (* 1 = 0.00208794 loss)
I1107 03:15:43.446980 37922 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I1107 03:15:56.208859 37922 solver.cpp:266] Iteration 7500 (3.91807 iter/s, 12.7614s/50 iter), loss = 0.00386525
I1107 03:15:56.208890 37922 solver.cpp:285]     Train net output #0: loss = 0.00386526 (* 1 = 0.00386526 loss)
I1107 03:15:56.208899 37922 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I1107 03:16:08.991189 37922 solver.cpp:266] Iteration 7550 (3.91181 iter/s, 12.7818s/50 iter), loss = 0.00866982
I1107 03:16:08.991220 37922 solver.cpp:285]     Train net output #0: loss = 0.00866984 (* 1 = 0.00866984 loss)
I1107 03:16:08.991228 37922 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I1107 03:16:21.787173 37922 solver.cpp:266] Iteration 7600 (3.90764 iter/s, 12.7955s/50 iter), loss = 0.00236692
I1107 03:16:21.787348 37922 solver.cpp:285]     Train net output #0: loss = 0.00236694 (* 1 = 0.00236694 loss)
I1107 03:16:21.787361 37922 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I1107 03:16:34.510459 37922 solver.cpp:266] Iteration 7650 (3.93001 iter/s, 12.7226s/50 iter), loss = 0.0191166
I1107 03:16:34.510491 37922 solver.cpp:285]     Train net output #0: loss = 0.0191166 (* 1 = 0.0191166 loss)
I1107 03:16:34.510499 37922 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I1107 03:16:47.260735 37922 solver.cpp:266] Iteration 7700 (3.92165 iter/s, 12.7497s/50 iter), loss = 0.0043512
I1107 03:16:47.260764 37922 solver.cpp:285]     Train net output #0: loss = 0.00435121 (* 1 = 0.00435121 loss)
I1107 03:16:47.260771 37922 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I1107 03:17:00.029875 37922 solver.cpp:266] Iteration 7750 (3.91585 iter/s, 12.7686s/50 iter), loss = 0.00255062
I1107 03:17:00.030068 37922 solver.cpp:285]     Train net output #0: loss = 0.00255063 (* 1 = 0.00255063 loss)
I1107 03:17:00.030081 37922 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I1107 03:17:12.795686 37922 solver.cpp:266] Iteration 7800 (3.91692 iter/s, 12.7651s/50 iter), loss = 0.0038525
I1107 03:17:12.795717 37922 solver.cpp:285]     Train net output #0: loss = 0.00385251 (* 1 = 0.00385251 loss)
I1107 03:17:12.795725 37922 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I1107 03:17:25.552134 37922 solver.cpp:266] Iteration 7850 (3.91975 iter/s, 12.7559s/50 iter), loss = 0.0113391
I1107 03:17:25.552166 37922 solver.cpp:285]     Train net output #0: loss = 0.0113391 (* 1 = 0.0113391 loss)
I1107 03:17:25.552173 37922 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I1107 03:17:38.343240 37922 solver.cpp:266] Iteration 7900 (3.90913 iter/s, 12.7906s/50 iter), loss = 0.00141908
I1107 03:17:38.343410 37922 solver.cpp:285]     Train net output #0: loss = 0.0014191 (* 1 = 0.0014191 loss)
I1107 03:17:38.343417 37922 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I1107 03:17:51.062379 37922 solver.cpp:266] Iteration 7950 (3.93129 iter/s, 12.7185s/50 iter), loss = 0.00418438
I1107 03:17:51.062409 37922 solver.cpp:285]     Train net output #0: loss = 0.0041844 (* 1 = 0.0041844 loss)
I1107 03:17:51.062425 37922 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I1107 03:18:03.616869 37922 solver.cpp:418] Iteration 8000, Testing net (#0)
I1107 03:18:05.134366 37922 solver.cpp:517]     Test net output #0: loss = 0.187737 (* 1 = 0.187737 loss)
I1107 03:18:05.134382 37922 solver.cpp:517]     Test net output #1: top-1 = 0.95
I1107 03:18:05.382158 37922 solver.cpp:266] Iteration 8000 (3.49182 iter/s, 14.3192s/50 iter), loss = 0.00149817
I1107 03:18:05.382184 37922 solver.cpp:285]     Train net output #0: loss = 0.00149819 (* 1 = 0.00149819 loss)
I1107 03:18:05.382205 37922 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I1107 03:18:18.155288 37922 solver.cpp:266] Iteration 8050 (3.91463 iter/s, 12.7726s/50 iter), loss = 0.00710534
I1107 03:18:18.155438 37922 solver.cpp:285]     Train net output #0: loss = 0.00710536 (* 1 = 0.00710536 loss)
I1107 03:18:18.155447 37922 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I1107 03:18:30.955704 37922 solver.cpp:266] Iteration 8100 (3.90632 iter/s, 12.7998s/50 iter), loss = 0.00980938
I1107 03:18:30.955734 37922 solver.cpp:285]     Train net output #0: loss = 0.00980941 (* 1 = 0.00980941 loss)
I1107 03:18:30.955739 37922 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I1107 03:18:43.705046 37922 solver.cpp:266] Iteration 8150 (3.92193 iter/s, 12.7488s/50 iter), loss = 0.00222386
I1107 03:18:43.705075 37922 solver.cpp:285]     Train net output #0: loss = 0.00222389 (* 1 = 0.00222389 loss)
I1107 03:18:43.705081 37922 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I1107 03:18:56.468798 37922 solver.cpp:266] Iteration 8200 (3.9175 iter/s, 12.7632s/50 iter), loss = 0.0224995
I1107 03:18:56.468945 37922 solver.cpp:285]     Train net output #0: loss = 0.0224996 (* 1 = 0.0224996 loss)
I1107 03:18:56.468955 37922 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I1107 03:19:09.199216 37922 solver.cpp:266] Iteration 8250 (3.9278 iter/s, 12.7298s/50 iter), loss = 0.00227402
I1107 03:19:09.199246 37922 solver.cpp:285]     Train net output #0: loss = 0.00227405 (* 1 = 0.00227405 loss)
I1107 03:19:09.199252 37922 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I1107 03:19:21.918488 37922 solver.cpp:266] Iteration 8300 (3.9312 iter/s, 12.7188s/50 iter), loss = 0.00585985
I1107 03:19:21.918516 37922 solver.cpp:285]     Train net output #0: loss = 0.00585988 (* 1 = 0.00585988 loss)
I1107 03:19:21.918522 37922 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I1107 03:19:34.713168 37922 solver.cpp:266] Iteration 8350 (3.90803 iter/s, 12.7942s/50 iter), loss = 0.00173681
I1107 03:19:34.713325 37922 solver.cpp:285]     Train net output #0: loss = 0.00173684 (* 1 = 0.00173684 loss)
I1107 03:19:34.713335 37922 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I1107 03:19:47.492606 37922 solver.cpp:266] Iteration 8400 (3.91273 iter/s, 12.7788s/50 iter), loss = 0.00305165
I1107 03:19:47.492635 37922 solver.cpp:285]     Train net output #0: loss = 0.00305168 (* 1 = 0.00305168 loss)
I1107 03:19:47.492657 37922 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I1107 03:20:00.233415 37922 solver.cpp:266] Iteration 8450 (3.92456 iter/s, 12.7403s/50 iter), loss = 0.0032706
I1107 03:20:00.233444 37922 solver.cpp:285]     Train net output #0: loss = 0.00327063 (* 1 = 0.00327063 loss)
I1107 03:20:00.233451 37922 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I1107 03:20:12.996032 37922 solver.cpp:266] Iteration 8500 (3.91785 iter/s, 12.7621s/50 iter), loss = 0.00265643
I1107 03:20:12.996208 37922 solver.cpp:285]     Train net output #0: loss = 0.00265645 (* 1 = 0.00265645 loss)
I1107 03:20:12.996217 37922 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I1107 03:20:25.770710 37922 solver.cpp:266] Iteration 8550 (3.9142 iter/s, 12.774s/50 iter), loss = 0.00771843
I1107 03:20:25.770752 37922 solver.cpp:285]     Train net output #0: loss = 0.00771846 (* 1 = 0.00771846 loss)
I1107 03:20:25.770759 37922 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I1107 03:20:38.542771 37922 solver.cpp:266] Iteration 8600 (3.91496 iter/s, 12.7715s/50 iter), loss = 0.00447382
I1107 03:20:38.542803 37922 solver.cpp:285]     Train net output #0: loss = 0.00447384 (* 1 = 0.00447384 loss)
I1107 03:20:38.542809 37922 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I1107 03:20:51.315830 37922 solver.cpp:266] Iteration 8650 (3.91465 iter/s, 12.7725s/50 iter), loss = 0.0027451
I1107 03:20:51.315992 37922 solver.cpp:285]     Train net output #0: loss = 0.00274512 (* 1 = 0.00274512 loss)
I1107 03:20:51.316001 37922 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I1107 03:21:04.106982 37922 solver.cpp:266] Iteration 8700 (3.90915 iter/s, 12.7905s/50 iter), loss = 0.00364001
I1107 03:21:04.107015 37922 solver.cpp:285]     Train net output #0: loss = 0.00364004 (* 1 = 0.00364004 loss)
I1107 03:21:04.107022 37922 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I1107 03:21:16.901479 37922 solver.cpp:266] Iteration 8750 (3.90809 iter/s, 12.794s/50 iter), loss = 0.00978941
I1107 03:21:16.901510 37922 solver.cpp:285]     Train net output #0: loss = 0.00978944 (* 1 = 0.00978944 loss)
I1107 03:21:16.901535 37922 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I1107 03:21:29.637984 37922 solver.cpp:266] Iteration 8800 (3.92589 iter/s, 12.736s/50 iter), loss = 0.0035821
I1107 03:21:29.638134 37922 solver.cpp:285]     Train net output #0: loss = 0.00358213 (* 1 = 0.00358213 loss)
I1107 03:21:29.638144 37922 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I1107 03:21:42.387157 37922 solver.cpp:266] Iteration 8850 (3.92202 iter/s, 12.7485s/50 iter), loss = 0.00200016
I1107 03:21:42.387188 37922 solver.cpp:285]     Train net output #0: loss = 0.00200018 (* 1 = 0.00200018 loss)
I1107 03:21:42.387210 37922 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I1107 03:21:55.180094 37922 solver.cpp:266] Iteration 8900 (3.90857 iter/s, 12.7924s/50 iter), loss = 0.00725565
I1107 03:21:55.180124 37922 solver.cpp:285]     Train net output #0: loss = 0.00725568 (* 1 = 0.00725568 loss)
I1107 03:21:55.180130 37922 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I1107 03:22:07.913255 37922 solver.cpp:266] Iteration 8950 (3.92692 iter/s, 12.7326s/50 iter), loss = 0.0128339
I1107 03:22:07.913413 37922 solver.cpp:285]     Train net output #0: loss = 0.0128339 (* 1 = 0.0128339 loss)
I1107 03:22:07.913424 37922 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I1107 03:22:20.434090 37922 solver.cpp:418] Iteration 9000, Testing net (#0)
I1107 03:22:21.955924 37922 solver.cpp:517]     Test net output #0: loss = 0.194033 (* 1 = 0.194033 loss)
I1107 03:22:21.955940 37922 solver.cpp:517]     Test net output #1: top-1 = 0.95125
I1107 03:22:22.207612 37922 solver.cpp:266] Iteration 9000 (3.49806 iter/s, 14.2937s/50 iter), loss = 0.00786196
I1107 03:22:22.207643 37922 solver.cpp:285]     Train net output #0: loss = 0.007862 (* 1 = 0.007862 loss)
I1107 03:22:22.207665 37922 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I1107 03:22:34.981874 37922 solver.cpp:266] Iteration 9050 (3.91428 iter/s, 12.7737s/50 iter), loss = 0.00201214
I1107 03:22:34.981905 37922 solver.cpp:285]     Train net output #0: loss = 0.00201217 (* 1 = 0.00201217 loss)
I1107 03:22:34.981912 37922 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I1107 03:22:47.714763 37922 solver.cpp:266] Iteration 9100 (3.927 iter/s, 12.7324s/50 iter), loss = 0.00634554
I1107 03:22:47.714964 37922 solver.cpp:285]     Train net output #0: loss = 0.00634557 (* 1 = 0.00634557 loss)
I1107 03:22:47.714975 37922 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I1107 03:23:00.487213 37922 solver.cpp:266] Iteration 9150 (3.91489 iter/s, 12.7718s/50 iter), loss = 0.0032975
I1107 03:23:00.487243 37922 solver.cpp:285]     Train net output #0: loss = 0.00329753 (* 1 = 0.00329753 loss)
I1107 03:23:00.487267 37922 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I1107 03:23:13.259819 37922 solver.cpp:266] Iteration 9200 (3.91479 iter/s, 12.7721s/50 iter), loss = 0.00169323
I1107 03:23:13.259848 37922 solver.cpp:285]     Train net output #0: loss = 0.00169326 (* 1 = 0.00169326 loss)
I1107 03:23:13.259855 37922 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I1107 03:23:26.005466 37922 solver.cpp:266] Iteration 9250 (3.92307 iter/s, 12.7451s/50 iter), loss = 0.00258229
I1107 03:23:26.005620 37922 solver.cpp:285]     Train net output #0: loss = 0.00258232 (* 1 = 0.00258232 loss)
I1107 03:23:26.005630 37922 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I1107 03:23:38.756057 37922 solver.cpp:266] Iteration 9300 (3.92158 iter/s, 12.7499s/50 iter), loss = 0.00320356
I1107 03:23:38.756088 37922 solver.cpp:285]     Train net output #0: loss = 0.00320359 (* 1 = 0.00320359 loss)
I1107 03:23:38.756094 37922 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I1107 03:23:51.509264 37922 solver.cpp:266] Iteration 9350 (3.92074 iter/s, 12.7527s/50 iter), loss = 0.00116603
I1107 03:23:51.509294 37922 solver.cpp:285]     Train net output #0: loss = 0.00116606 (* 1 = 0.00116606 loss)
I1107 03:23:51.509301 37922 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I1107 03:24:04.323093 37922 solver.cpp:266] Iteration 9400 (3.90219 iter/s, 12.8133s/50 iter), loss = 0.0036357
I1107 03:24:04.323297 37922 solver.cpp:285]     Train net output #0: loss = 0.00363573 (* 1 = 0.00363573 loss)
I1107 03:24:04.323305 37922 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I1107 03:24:17.066005 37922 solver.cpp:266] Iteration 9450 (3.92396 iter/s, 12.7422s/50 iter), loss = 0.0347596
I1107 03:24:17.066045 37922 solver.cpp:285]     Train net output #0: loss = 0.0347596 (* 1 = 0.0347596 loss)
I1107 03:24:17.066053 37922 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I1107 03:24:29.807811 37922 solver.cpp:266] Iteration 9500 (3.92425 iter/s, 12.7413s/50 iter), loss = 0.0157205
I1107 03:24:29.807843 37922 solver.cpp:285]     Train net output #0: loss = 0.0157206 (* 1 = 0.0157206 loss)
I1107 03:24:29.807866 37922 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I1107 03:24:42.587321 37922 solver.cpp:266] Iteration 9550 (3.91267 iter/s, 12.779s/50 iter), loss = 0.00400464
I1107 03:24:42.587496 37922 solver.cpp:285]     Train net output #0: loss = 0.00400467 (* 1 = 0.00400467 loss)
I1107 03:24:42.587507 37922 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I1107 03:24:55.339454 37922 solver.cpp:266] Iteration 9600 (3.92112 iter/s, 12.7515s/50 iter), loss = 0.00327811
I1107 03:24:55.339483 37922 solver.cpp:285]     Train net output #0: loss = 0.00327814 (* 1 = 0.00327814 loss)
I1107 03:24:55.339489 37922 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I1107 03:25:08.115231 37922 solver.cpp:266] Iteration 9650 (3.91382 iter/s, 12.7753s/50 iter), loss = 0.00634741
I1107 03:25:08.115263 37922 solver.cpp:285]     Train net output #0: loss = 0.00634744 (* 1 = 0.00634744 loss)
I1107 03:25:08.115269 37922 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I1107 03:25:20.887133 37922 solver.cpp:266] Iteration 9700 (3.915 iter/s, 12.7714s/50 iter), loss = 0.00426078
I1107 03:25:20.887315 37922 solver.cpp:285]     Train net output #0: loss = 0.00426081 (* 1 = 0.00426081 loss)
I1107 03:25:20.887326 37922 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I1107 03:25:33.621548 37922 solver.cpp:266] Iteration 9750 (3.92657 iter/s, 12.7337s/50 iter), loss = 0.0112689
I1107 03:25:33.621580 37922 solver.cpp:285]     Train net output #0: loss = 0.0112689 (* 1 = 0.0112689 loss)
I1107 03:25:33.621587 37922 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I1107 03:25:46.410670 37922 solver.cpp:266] Iteration 9800 (3.90973 iter/s, 12.7886s/50 iter), loss = 0.00348207
I1107 03:25:46.410701 37922 solver.cpp:285]     Train net output #0: loss = 0.0034821 (* 1 = 0.0034821 loss)
I1107 03:25:46.410707 37922 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I1107 03:25:59.200457 37922 solver.cpp:266] Iteration 9850 (3.90953 iter/s, 12.7893s/50 iter), loss = 0.00282251
I1107 03:25:59.200614 37922 solver.cpp:285]     Train net output #0: loss = 0.00282254 (* 1 = 0.00282254 loss)
I1107 03:25:59.200623 37922 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I1107 03:26:11.928179 37922 solver.cpp:266] Iteration 9900 (3.92863 iter/s, 12.7271s/50 iter), loss = 0.00550132
I1107 03:26:11.928210 37922 solver.cpp:285]     Train net output #0: loss = 0.00550134 (* 1 = 0.00550134 loss)
I1107 03:26:11.928216 37922 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I1107 03:26:24.690780 37922 solver.cpp:266] Iteration 9950 (3.91786 iter/s, 12.7621s/50 iter), loss = 0.0160917
I1107 03:26:24.690810 37922 solver.cpp:285]     Train net output #0: loss = 0.0160918 (* 1 = 0.0160918 loss)
I1107 03:26:24.690834 37922 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I1107 03:26:37.273236 37922 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_10000.caffemodel
I1107 03:26:39.554360 37922 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_10000.solverstate
I1107 03:26:40.030097 37922 solver.cpp:418] Iteration 10000, Testing net (#0)
I1107 03:26:41.504529 37922 solver.cpp:517]     Test net output #0: loss = 0.197784 (* 1 = 0.197784 loss)
I1107 03:26:41.504546 37922 solver.cpp:517]     Test net output #1: top-1 = 0.951
I1107 03:26:41.745628 37922 solver.cpp:266] Iteration 10000 (2.93183 iter/s, 17.0542s/50 iter), loss = 0.00433054
I1107 03:26:41.745656 37922 solver.cpp:285]     Train net output #0: loss = 0.00433058 (* 1 = 0.00433058 loss)
I1107 03:26:41.745662 37922 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I1107 03:26:54.328171 37922 solver.cpp:266] Iteration 10050 (3.97392 iter/s, 12.582s/50 iter), loss = 0.0241488
I1107 03:26:54.328202 37922 solver.cpp:285]     Train net output #0: loss = 0.0241488 (* 1 = 0.0241488 loss)
I1107 03:26:54.328207 37922 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I1107 03:27:06.944937 37922 solver.cpp:266] Iteration 10100 (3.96314 iter/s, 12.6163s/50 iter), loss = 0.0018072
I1107 03:27:06.944967 37922 solver.cpp:285]     Train net output #0: loss = 0.00180723 (* 1 = 0.00180723 loss)
I1107 03:27:06.944972 37922 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I1107 03:27:19.700354 37922 solver.cpp:266] Iteration 10150 (3.92006 iter/s, 12.7549s/50 iter), loss = 0.00393123
I1107 03:27:19.700500 37922 solver.cpp:285]     Train net output #0: loss = 0.00393127 (* 1 = 0.00393127 loss)
I1107 03:27:19.700508 37922 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I1107 03:27:32.474668 37922 solver.cpp:266] Iteration 10200 (3.9143 iter/s, 12.7737s/50 iter), loss = 0.00860362
I1107 03:27:32.474699 37922 solver.cpp:285]     Train net output #0: loss = 0.00860365 (* 1 = 0.00860365 loss)
I1107 03:27:32.474705 37922 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I1107 03:27:45.248293 37922 solver.cpp:266] Iteration 10250 (3.91448 iter/s, 12.7731s/50 iter), loss = 0.0286688
I1107 03:27:45.248324 37922 solver.cpp:285]     Train net output #0: loss = 0.0286688 (* 1 = 0.0286688 loss)
I1107 03:27:45.248332 37922 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I1107 03:27:58.023169 37922 solver.cpp:266] Iteration 10300 (3.91409 iter/s, 12.7744s/50 iter), loss = 0.00131776
I1107 03:27:58.023346 37922 solver.cpp:285]     Train net output #0: loss = 0.00131778 (* 1 = 0.00131778 loss)
I1107 03:27:58.023356 37922 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I1107 03:28:10.836189 37922 solver.cpp:266] Iteration 10350 (3.90248 iter/s, 12.8124s/50 iter), loss = 0.00546809
I1107 03:28:10.836218 37922 solver.cpp:285]     Train net output #0: loss = 0.00546811 (* 1 = 0.00546811 loss)
I1107 03:28:10.836225 37922 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I1107 03:28:23.625941 37922 solver.cpp:266] Iteration 10400 (3.90954 iter/s, 12.7892s/50 iter), loss = 0.0081848
I1107 03:28:23.625970 37922 solver.cpp:285]     Train net output #0: loss = 0.00818482 (* 1 = 0.00818482 loss)
I1107 03:28:23.625977 37922 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I1107 03:28:36.360249 37922 solver.cpp:266] Iteration 10450 (3.92656 iter/s, 12.7338s/50 iter), loss = 0.00676665
I1107 03:28:36.360422 37922 solver.cpp:285]     Train net output #0: loss = 0.00676667 (* 1 = 0.00676667 loss)
I1107 03:28:36.360431 37922 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I1107 03:28:49.169138 37922 solver.cpp:266] Iteration 10500 (3.90374 iter/s, 12.8082s/50 iter), loss = 0.002238
I1107 03:28:49.169168 37922 solver.cpp:285]     Train net output #0: loss = 0.00223802 (* 1 = 0.00223802 loss)
I1107 03:28:49.169174 37922 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I1107 03:29:01.927251 37922 solver.cpp:266] Iteration 10550 (3.91923 iter/s, 12.7576s/50 iter), loss = 0.00242713
I1107 03:29:01.927281 37922 solver.cpp:285]     Train net output #0: loss = 0.00242715 (* 1 = 0.00242715 loss)
I1107 03:29:01.927289 37922 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I1107 03:29:14.699348 37922 solver.cpp:266] Iteration 10600 (3.91494 iter/s, 12.7716s/50 iter), loss = 0.0100947
I1107 03:29:14.699510 37922 solver.cpp:285]     Train net output #0: loss = 0.0100947 (* 1 = 0.0100947 loss)
I1107 03:29:14.699519 37922 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I1107 03:29:27.477741 37922 solver.cpp:266] Iteration 10650 (3.91305 iter/s, 12.7777s/50 iter), loss = 0.0122386
I1107 03:29:27.477771 37922 solver.cpp:285]     Train net output #0: loss = 0.0122386 (* 1 = 0.0122386 loss)
I1107 03:29:27.477777 37922 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I1107 03:29:40.258265 37922 solver.cpp:266] Iteration 10700 (3.91236 iter/s, 12.78s/50 iter), loss = 0.0160517
I1107 03:29:40.258292 37922 solver.cpp:285]     Train net output #0: loss = 0.0160518 (* 1 = 0.0160518 loss)
I1107 03:29:40.258298 37922 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I1107 03:29:53.046669 37922 solver.cpp:266] Iteration 10750 (3.90995 iter/s, 12.7879s/50 iter), loss = 0.00782695
I1107 03:29:53.046815 37922 solver.cpp:285]     Train net output #0: loss = 0.00782698 (* 1 = 0.00782698 loss)
I1107 03:29:53.046824 37922 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I1107 03:30:05.810644 37922 solver.cpp:266] Iteration 10800 (3.91747 iter/s, 12.7633s/50 iter), loss = 0.00906244
I1107 03:30:05.810675 37922 solver.cpp:285]     Train net output #0: loss = 0.00906247 (* 1 = 0.00906247 loss)
I1107 03:30:05.810681 37922 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I1107 03:30:18.577128 37922 solver.cpp:266] Iteration 10850 (3.91666 iter/s, 12.766s/50 iter), loss = 0.00833428
I1107 03:30:18.577157 37922 solver.cpp:285]     Train net output #0: loss = 0.00833431 (* 1 = 0.00833431 loss)
I1107 03:30:18.577163 37922 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I1107 03:30:31.343396 37922 solver.cpp:266] Iteration 10900 (3.91673 iter/s, 12.7657s/50 iter), loss = 0.00409174
I1107 03:30:31.343569 37922 solver.cpp:285]     Train net output #0: loss = 0.00409177 (* 1 = 0.00409177 loss)
I1107 03:30:31.343577 37922 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I1107 03:30:44.109910 37922 solver.cpp:266] Iteration 10950 (3.9167 iter/s, 12.7659s/50 iter), loss = 0.00519223
I1107 03:30:44.109941 37922 solver.cpp:285]     Train net output #0: loss = 0.00519226 (* 1 = 0.00519226 loss)
I1107 03:30:44.109947 37922 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I1107 03:30:56.614356 37922 solver.cpp:418] Iteration 11000, Testing net (#0)
I1107 03:30:58.145786 37922 solver.cpp:517]     Test net output #0: loss = 0.20005 (* 1 = 0.20005 loss)
I1107 03:30:58.145802 37922 solver.cpp:517]     Test net output #1: top-1 = 0.95175
I1107 03:30:58.391309 37922 solver.cpp:266] Iteration 11000 (3.5012 iter/s, 14.2808s/50 iter), loss = 0.00352719
I1107 03:30:58.391336 37922 solver.cpp:285]     Train net output #0: loss = 0.00352721 (* 1 = 0.00352721 loss)
I1107 03:30:58.391358 37922 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I1107 03:31:11.188449 37922 solver.cpp:266] Iteration 11050 (3.90728 iter/s, 12.7966s/50 iter), loss = 0.00430508
I1107 03:31:11.188638 37922 solver.cpp:285]     Train net output #0: loss = 0.0043051 (* 1 = 0.0043051 loss)
I1107 03:31:11.188647 37922 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I1107 03:31:23.937660 37922 solver.cpp:266] Iteration 11100 (3.92202 iter/s, 12.7485s/50 iter), loss = 0.0107298
I1107 03:31:23.937690 37922 solver.cpp:285]     Train net output #0: loss = 0.0107298 (* 1 = 0.0107298 loss)
I1107 03:31:23.937695 37922 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I1107 03:31:36.705977 37922 solver.cpp:266] Iteration 11150 (3.9161 iter/s, 12.7678s/50 iter), loss = 0.00429737
I1107 03:31:36.706005 37922 solver.cpp:285]     Train net output #0: loss = 0.00429739 (* 1 = 0.00429739 loss)
I1107 03:31:36.706012 37922 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I1107 03:31:49.493993 37922 solver.cpp:266] Iteration 11200 (3.91007 iter/s, 12.7875s/50 iter), loss = 0.00647012
I1107 03:31:49.494159 37922 solver.cpp:285]     Train net output #0: loss = 0.00647014 (* 1 = 0.00647014 loss)
I1107 03:31:49.494168 37922 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I1107 03:32:02.226156 37922 solver.cpp:266] Iteration 11250 (3.92726 iter/s, 12.7315s/50 iter), loss = 0.00657569
I1107 03:32:02.226186 37922 solver.cpp:285]     Train net output #0: loss = 0.00657571 (* 1 = 0.00657571 loss)
I1107 03:32:02.226192 37922 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I1107 03:32:15.030483 37922 solver.cpp:266] Iteration 11300 (3.90509 iter/s, 12.8038s/50 iter), loss = 0.00319323
I1107 03:32:15.030513 37922 solver.cpp:285]     Train net output #0: loss = 0.00319325 (* 1 = 0.00319325 loss)
I1107 03:32:15.030519 37922 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I1107 03:32:27.801034 37922 solver.cpp:266] Iteration 11350 (3.91542 iter/s, 12.77s/50 iter), loss = 0.0106292
I1107 03:32:27.801225 37922 solver.cpp:285]     Train net output #0: loss = 0.0106292 (* 1 = 0.0106292 loss)
I1107 03:32:27.801235 37922 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I1107 03:32:40.599613 37922 solver.cpp:266] Iteration 11400 (3.90689 iter/s, 12.7979s/50 iter), loss = 0.0169266
I1107 03:32:40.599644 37922 solver.cpp:285]     Train net output #0: loss = 0.0169267 (* 1 = 0.0169267 loss)
I1107 03:32:40.599651 37922 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I1107 03:32:53.334753 37922 solver.cpp:266] Iteration 11450 (3.9263 iter/s, 12.7346s/50 iter), loss = 0.00539732
I1107 03:32:53.334785 37922 solver.cpp:285]     Train net output #0: loss = 0.00539735 (* 1 = 0.00539735 loss)
I1107 03:32:53.334792 37922 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I1107 03:33:06.110623 37922 solver.cpp:266] Iteration 11500 (3.91379 iter/s, 12.7753s/50 iter), loss = 0.0031081
I1107 03:33:06.110780 37922 solver.cpp:285]     Train net output #0: loss = 0.00310812 (* 1 = 0.00310812 loss)
I1107 03:33:06.110790 37922 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I1107 03:33:18.893877 37922 solver.cpp:266] Iteration 11550 (3.91158 iter/s, 12.7826s/50 iter), loss = 0.00318288
I1107 03:33:18.893906 37922 solver.cpp:285]     Train net output #0: loss = 0.0031829 (* 1 = 0.0031829 loss)
I1107 03:33:18.893913 37922 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I1107 03:33:31.663957 37922 solver.cpp:266] Iteration 11600 (3.91558 iter/s, 12.7695s/50 iter), loss = 0.00339685
I1107 03:33:31.663988 37922 solver.cpp:285]     Train net output #0: loss = 0.00339687 (* 1 = 0.00339687 loss)
I1107 03:33:31.663995 37922 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I1107 03:33:44.434279 37922 solver.cpp:266] Iteration 11650 (3.9155 iter/s, 12.7698s/50 iter), loss = 0.00623702
I1107 03:33:44.434453 37922 solver.cpp:285]     Train net output #0: loss = 0.00623704 (* 1 = 0.00623704 loss)
I1107 03:33:44.434463 37922 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I1107 03:33:57.235270 37922 solver.cpp:266] Iteration 11700 (3.90616 iter/s, 12.8003s/50 iter), loss = 0.00979932
I1107 03:33:57.235301 37922 solver.cpp:285]     Train net output #0: loss = 0.00979935 (* 1 = 0.00979935 loss)
I1107 03:33:57.235323 37922 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I1107 03:34:09.978008 37922 solver.cpp:266] Iteration 11750 (3.92398 iter/s, 12.7422s/50 iter), loss = 0.00304438
I1107 03:34:09.978039 37922 solver.cpp:285]     Train net output #0: loss = 0.00304441 (* 1 = 0.00304441 loss)
I1107 03:34:09.978061 37922 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I1107 03:34:22.758337 37922 solver.cpp:266] Iteration 11800 (3.91244 iter/s, 12.7798s/50 iter), loss = 0.00532569
I1107 03:34:22.758487 37922 solver.cpp:285]     Train net output #0: loss = 0.00532572 (* 1 = 0.00532572 loss)
I1107 03:34:22.758496 37922 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I1107 03:34:35.536742 37922 solver.cpp:266] Iteration 11850 (3.91306 iter/s, 12.7777s/50 iter), loss = 0.00778259
I1107 03:34:35.536773 37922 solver.cpp:285]     Train net output #0: loss = 0.00778262 (* 1 = 0.00778262 loss)
I1107 03:34:35.536795 37922 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I1107 03:34:48.323951 37922 solver.cpp:266] Iteration 11900 (3.91033 iter/s, 12.7866s/50 iter), loss = 0.00828429
I1107 03:34:48.323982 37922 solver.cpp:285]     Train net output #0: loss = 0.00828433 (* 1 = 0.00828433 loss)
I1107 03:34:48.323988 37922 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I1107 03:35:01.098235 37922 solver.cpp:266] Iteration 11950 (3.91429 iter/s, 12.7737s/50 iter), loss = 0.00650488
I1107 03:35:01.098394 37922 solver.cpp:285]     Train net output #0: loss = 0.00650492 (* 1 = 0.00650492 loss)
I1107 03:35:01.098402 37922 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I1107 03:35:13.621441 37922 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_12000.caffemodel
I1107 03:35:15.891832 37922 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.1/snapshots/_iter_12000.solverstate
I1107 03:35:16.467339 37922 solver.cpp:378] Iteration 12000, loss = 0.00242711
I1107 03:35:16.467362 37922 solver.cpp:418] Iteration 12000, Testing net (#0)
I1107 03:35:17.933348 37922 solver.cpp:517]     Test net output #0: loss = 0.201062 (* 1 = 0.201062 loss)
I1107 03:35:17.933364 37922 solver.cpp:517]     Test net output #1: top-1 = 0.952
I1107 03:35:17.933368 37922 solver.cpp:386] Optimization Done (3.90111 iter/s).
I1107 03:35:17.933372 37922 caffe_interface.cpp:530] Optimization Done.
I1107 03:35:18.881366 38230 pruning_runner.cpp:190] Sens info found, use it.
I1107 03:35:20.091589 38230 pruning_runner.cpp:217] Start compressing, please wait...
I1107 03:35:26.027740 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:35:32.033013 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:35:38.064025 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:35:44.393148 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:35:50.470127 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:35:56.628505 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:03.080981 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:09.332830 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:15.627777 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:21.793886 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:27.818557 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:33.709087 38230 pruning_runner.cpp:264] Compression complete 0%
I1107 03:36:39.645428 38230 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 03:36:39.645704 38230 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 03:36:39.646049 38230 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 03:36:39.646216 38230 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 03:36:39.646322 38230 layer_factory.hpp:77] Creating layer data
I1107 03:36:39.646356 38230 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:39.646765 38230 net.cpp:94] Creating Layer data
I1107 03:36:39.646773 38230 net.cpp:409] data -> data
I1107 03:36:39.646782 38230 net.cpp:409] data -> label
I1107 03:36:39.647788 39444 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 03:36:39.647824 39444 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 03:36:39.648041 38230 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 03:36:39.648105 38230 data_layer.cpp:83] output data size: 50,3,227,227
I1107 03:36:39.724473 38230 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:39.724527 38230 net.cpp:144] Setting up data
I1107 03:36:39.724536 38230 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 03:36:39.724539 38230 net.cpp:151] Top shape: 50 (50)
I1107 03:36:39.724541 38230 net.cpp:159] Memory required for data: 30917600
I1107 03:36:39.724563 38230 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 03:36:39.724573 38230 net.cpp:94] Creating Layer label_data_1_split
I1107 03:36:39.724578 38230 net.cpp:435] label_data_1_split <- label
I1107 03:36:39.724584 38230 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 03:36:39.724596 38230 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 03:36:39.724650 38230 net.cpp:144] Setting up label_data_1_split
I1107 03:36:39.724655 38230 net.cpp:151] Top shape: 50 (50)
I1107 03:36:39.724673 38230 net.cpp:151] Top shape: 50 (50)
I1107 03:36:39.724676 38230 net.cpp:159] Memory required for data: 30918000
I1107 03:36:39.724679 38230 layer_factory.hpp:77] Creating layer conv1
I1107 03:36:39.724689 38230 net.cpp:94] Creating Layer conv1
I1107 03:36:39.724694 38230 net.cpp:435] conv1 <- data
I1107 03:36:39.724699 38230 net.cpp:409] conv1 -> conv1
I1107 03:36:39.726379 38230 net.cpp:144] Setting up conv1
I1107 03:36:39.726392 38230 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:39.726394 38230 net.cpp:159] Memory required for data: 88998000
I1107 03:36:39.726405 38230 layer_factory.hpp:77] Creating layer bn1
I1107 03:36:39.726431 38230 net.cpp:94] Creating Layer bn1
I1107 03:36:39.726435 38230 net.cpp:435] bn1 <- conv1
I1107 03:36:39.726440 38230 net.cpp:409] bn1 -> scale1
I1107 03:36:39.727073 38230 net.cpp:144] Setting up bn1
I1107 03:36:39.727079 38230 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:39.727082 38230 net.cpp:159] Memory required for data: 147078000
I1107 03:36:39.727092 38230 layer_factory.hpp:77] Creating layer relu1
I1107 03:36:39.727099 38230 net.cpp:94] Creating Layer relu1
I1107 03:36:39.727102 38230 net.cpp:435] relu1 <- scale1
I1107 03:36:39.727105 38230 net.cpp:409] relu1 -> relu1
I1107 03:36:39.727133 38230 net.cpp:144] Setting up relu1
I1107 03:36:39.727138 38230 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:39.727140 38230 net.cpp:159] Memory required for data: 205158000
I1107 03:36:39.727143 38230 layer_factory.hpp:77] Creating layer pool1
I1107 03:36:39.727147 38230 net.cpp:94] Creating Layer pool1
I1107 03:36:39.727150 38230 net.cpp:435] pool1 <- relu1
I1107 03:36:39.727154 38230 net.cpp:409] pool1 -> pool1
I1107 03:36:39.727216 38230 net.cpp:144] Setting up pool1
I1107 03:36:39.727221 38230 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 03:36:39.727223 38230 net.cpp:159] Memory required for data: 219154800
I1107 03:36:39.727226 38230 layer_factory.hpp:77] Creating layer conv2
I1107 03:36:39.727232 38230 net.cpp:94] Creating Layer conv2
I1107 03:36:39.727236 38230 net.cpp:435] conv2 <- pool1
I1107 03:36:39.727241 38230 net.cpp:409] conv2 -> conv2
I1107 03:36:39.734695 38230 net.cpp:144] Setting up conv2
I1107 03:36:39.734714 38230 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:39.734716 38230 net.cpp:159] Memory required for data: 256479600
I1107 03:36:39.734728 38230 layer_factory.hpp:77] Creating layer bn2
I1107 03:36:39.734737 38230 net.cpp:94] Creating Layer bn2
I1107 03:36:39.734743 38230 net.cpp:435] bn2 <- conv2
I1107 03:36:39.734750 38230 net.cpp:409] bn2 -> scale2
I1107 03:36:39.735358 38230 net.cpp:144] Setting up bn2
I1107 03:36:39.735366 38230 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:39.735368 38230 net.cpp:159] Memory required for data: 293804400
I1107 03:36:39.735376 38230 layer_factory.hpp:77] Creating layer relu2
I1107 03:36:39.735383 38230 net.cpp:94] Creating Layer relu2
I1107 03:36:39.735386 38230 net.cpp:435] relu2 <- scale2
I1107 03:36:39.735391 38230 net.cpp:409] relu2 -> relu2
I1107 03:36:39.735424 38230 net.cpp:144] Setting up relu2
I1107 03:36:39.735430 38230 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:39.735432 38230 net.cpp:159] Memory required for data: 331129200
I1107 03:36:39.735435 38230 layer_factory.hpp:77] Creating layer pool2
I1107 03:36:39.735442 38230 net.cpp:94] Creating Layer pool2
I1107 03:36:39.735445 38230 net.cpp:435] pool2 <- relu2
I1107 03:36:39.735450 38230 net.cpp:409] pool2 -> pool2
I1107 03:36:39.735477 38230 net.cpp:144] Setting up pool2
I1107 03:36:39.735482 38230 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:39.735486 38230 net.cpp:159] Memory required for data: 339782000
I1107 03:36:39.735488 38230 layer_factory.hpp:77] Creating layer conv3
I1107 03:36:39.735496 38230 net.cpp:94] Creating Layer conv3
I1107 03:36:39.735500 38230 net.cpp:435] conv3 <- pool2
I1107 03:36:39.735505 38230 net.cpp:409] conv3 -> conv3
I1107 03:36:39.748962 38230 net.cpp:144] Setting up conv3
I1107 03:36:39.749002 38230 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:39.749025 38230 net.cpp:159] Memory required for data: 352761200
I1107 03:36:39.749037 38230 layer_factory.hpp:77] Creating layer relu3
I1107 03:36:39.749044 38230 net.cpp:94] Creating Layer relu3
I1107 03:36:39.749063 38230 net.cpp:435] relu3 <- conv3
I1107 03:36:39.749069 38230 net.cpp:409] relu3 -> relu3
I1107 03:36:39.749096 38230 net.cpp:144] Setting up relu3
I1107 03:36:39.749104 38230 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:39.749109 38230 net.cpp:159] Memory required for data: 365740400
I1107 03:36:39.749112 38230 layer_factory.hpp:77] Creating layer conv4
I1107 03:36:39.749126 38230 net.cpp:94] Creating Layer conv4
I1107 03:36:39.749133 38230 net.cpp:435] conv4 <- relu3
I1107 03:36:39.749140 38230 net.cpp:409] conv4 -> conv4
I1107 03:36:39.762387 38230 net.cpp:144] Setting up conv4
I1107 03:36:39.762411 38230 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:39.762414 38230 net.cpp:159] Memory required for data: 378719600
I1107 03:36:39.762428 38230 layer_factory.hpp:77] Creating layer relu4
I1107 03:36:39.762439 38230 net.cpp:94] Creating Layer relu4
I1107 03:36:39.762445 38230 net.cpp:435] relu4 <- conv4
I1107 03:36:39.762452 38230 net.cpp:409] relu4 -> relu4
I1107 03:36:39.762478 38230 net.cpp:144] Setting up relu4
I1107 03:36:39.762483 38230 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:39.762487 38230 net.cpp:159] Memory required for data: 391698800
I1107 03:36:39.762491 38230 layer_factory.hpp:77] Creating layer conv5
I1107 03:36:39.762501 38230 net.cpp:94] Creating Layer conv5
I1107 03:36:39.762506 38230 net.cpp:435] conv5 <- relu4
I1107 03:36:39.762511 38230 net.cpp:409] conv5 -> conv5
I1107 03:36:39.775817 38230 net.cpp:144] Setting up conv5
I1107 03:36:39.775843 38230 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:39.775846 38230 net.cpp:159] Memory required for data: 400351600
I1107 03:36:39.775857 38230 layer_factory.hpp:77] Creating layer relu5
I1107 03:36:39.775867 38230 net.cpp:94] Creating Layer relu5
I1107 03:36:39.775873 38230 net.cpp:435] relu5 <- conv5
I1107 03:36:39.775883 38230 net.cpp:409] relu5 -> relu5
I1107 03:36:39.775913 38230 net.cpp:144] Setting up relu5
I1107 03:36:39.775919 38230 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:39.775923 38230 net.cpp:159] Memory required for data: 409004400
I1107 03:36:39.775930 38230 layer_factory.hpp:77] Creating layer pool5
I1107 03:36:39.775945 38230 net.cpp:94] Creating Layer pool5
I1107 03:36:39.775949 38230 net.cpp:435] pool5 <- relu5
I1107 03:36:39.775955 38230 net.cpp:409] pool5 -> pool5
I1107 03:36:39.775997 38230 net.cpp:144] Setting up pool5
I1107 03:36:39.776003 38230 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 03:36:39.776007 38230 net.cpp:159] Memory required for data: 410847600
I1107 03:36:39.776011 38230 layer_factory.hpp:77] Creating layer fc6
I1107 03:36:39.776021 38230 net.cpp:94] Creating Layer fc6
I1107 03:36:39.776026 38230 net.cpp:435] fc6 <- pool5
I1107 03:36:39.776033 38230 net.cpp:409] fc6 -> fc6
I1107 03:36:40.078670 38230 net.cpp:144] Setting up fc6
I1107 03:36:40.078694 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.078698 38230 net.cpp:159] Memory required for data: 411666800
I1107 03:36:40.078707 38230 layer_factory.hpp:77] Creating layer relu6
I1107 03:36:40.078727 38230 net.cpp:94] Creating Layer relu6
I1107 03:36:40.078732 38230 net.cpp:435] relu6 <- fc6
I1107 03:36:40.078738 38230 net.cpp:409] relu6 -> relu6
I1107 03:36:40.078781 38230 net.cpp:144] Setting up relu6
I1107 03:36:40.078786 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.078788 38230 net.cpp:159] Memory required for data: 412486000
I1107 03:36:40.078793 38230 layer_factory.hpp:77] Creating layer drop6
I1107 03:36:40.078799 38230 net.cpp:94] Creating Layer drop6
I1107 03:36:40.078804 38230 net.cpp:435] drop6 <- relu6
I1107 03:36:40.078807 38230 net.cpp:409] drop6 -> drop6
I1107 03:36:40.078832 38230 net.cpp:144] Setting up drop6
I1107 03:36:40.078836 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.078838 38230 net.cpp:159] Memory required for data: 413305200
I1107 03:36:40.078853 38230 layer_factory.hpp:77] Creating layer fc7
I1107 03:36:40.078861 38230 net.cpp:94] Creating Layer fc7
I1107 03:36:40.078866 38230 net.cpp:435] fc7 <- drop6
I1107 03:36:40.078871 38230 net.cpp:409] fc7 -> fc7
I1107 03:36:40.212857 38230 net.cpp:144] Setting up fc7
I1107 03:36:40.212883 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.212885 38230 net.cpp:159] Memory required for data: 414124400
I1107 03:36:40.212895 38230 layer_factory.hpp:77] Creating layer bn7
I1107 03:36:40.212922 38230 net.cpp:94] Creating Layer bn7
I1107 03:36:40.212927 38230 net.cpp:435] bn7 <- fc7
I1107 03:36:40.212936 38230 net.cpp:409] bn7 -> scale7
I1107 03:36:40.213464 38230 net.cpp:144] Setting up bn7
I1107 03:36:40.213471 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.213475 38230 net.cpp:159] Memory required for data: 414943600
I1107 03:36:40.213485 38230 layer_factory.hpp:77] Creating layer relu7
I1107 03:36:40.213491 38230 net.cpp:94] Creating Layer relu7
I1107 03:36:40.213495 38230 net.cpp:435] relu7 <- scale7
I1107 03:36:40.213502 38230 net.cpp:409] relu7 -> relu7
I1107 03:36:40.213522 38230 net.cpp:144] Setting up relu7
I1107 03:36:40.213526 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.213531 38230 net.cpp:159] Memory required for data: 415762800
I1107 03:36:40.213534 38230 layer_factory.hpp:77] Creating layer drop7
I1107 03:36:40.213541 38230 net.cpp:94] Creating Layer drop7
I1107 03:36:40.213546 38230 net.cpp:435] drop7 <- relu7
I1107 03:36:40.213551 38230 net.cpp:409] drop7 -> drop7
I1107 03:36:40.213578 38230 net.cpp:144] Setting up drop7
I1107 03:36:40.213582 38230 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:40.213585 38230 net.cpp:159] Memory required for data: 416582000
I1107 03:36:40.213589 38230 layer_factory.hpp:77] Creating layer fc8
I1107 03:36:40.213598 38230 net.cpp:94] Creating Layer fc8
I1107 03:36:40.213601 38230 net.cpp:435] fc8 <- drop7
I1107 03:36:40.213608 38230 net.cpp:409] fc8 -> fc8
I1107 03:36:40.214490 38230 net.cpp:144] Setting up fc8
I1107 03:36:40.214504 38230 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:40.214507 38230 net.cpp:159] Memory required for data: 416582400
I1107 03:36:40.214514 38230 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 03:36:40.214521 38230 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 03:36:40.214527 38230 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 03:36:40.214534 38230 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 03:36:40.214542 38230 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 03:36:40.214573 38230 net.cpp:144] Setting up fc8_fc8_0_split
I1107 03:36:40.214578 38230 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:40.214582 38230 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:40.214586 38230 net.cpp:159] Memory required for data: 416583200
I1107 03:36:40.214588 38230 layer_factory.hpp:77] Creating layer loss
I1107 03:36:40.214596 38230 net.cpp:94] Creating Layer loss
I1107 03:36:40.214601 38230 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 03:36:40.214606 38230 net.cpp:435] loss <- label_data_1_split_0
I1107 03:36:40.214612 38230 net.cpp:409] loss -> loss
I1107 03:36:40.214622 38230 layer_factory.hpp:77] Creating layer loss
I1107 03:36:40.214690 38230 net.cpp:144] Setting up loss
I1107 03:36:40.214695 38230 net.cpp:151] Top shape: (1)
I1107 03:36:40.214699 38230 net.cpp:154]     with loss weight 1
I1107 03:36:40.214711 38230 net.cpp:159] Memory required for data: 416583204
I1107 03:36:40.214715 38230 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 03:36:40.214721 38230 net.cpp:94] Creating Layer accuracy-top1
I1107 03:36:40.214725 38230 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 03:36:40.214730 38230 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 03:36:40.214736 38230 net.cpp:409] accuracy-top1 -> top-1
I1107 03:36:40.214746 38230 net.cpp:144] Setting up accuracy-top1
I1107 03:36:40.214751 38230 net.cpp:151] Top shape: (1)
I1107 03:36:40.214754 38230 net.cpp:159] Memory required for data: 416583208
I1107 03:36:40.214766 38230 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 03:36:40.214771 38230 net.cpp:220] loss needs backward computation.
I1107 03:36:40.214776 38230 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 03:36:40.214781 38230 net.cpp:220] fc8 needs backward computation.
I1107 03:36:40.214784 38230 net.cpp:220] drop7 needs backward computation.
I1107 03:36:40.214789 38230 net.cpp:220] relu7 needs backward computation.
I1107 03:36:40.214792 38230 net.cpp:220] bn7 needs backward computation.
I1107 03:36:40.214797 38230 net.cpp:220] fc7 needs backward computation.
I1107 03:36:40.214802 38230 net.cpp:220] drop6 needs backward computation.
I1107 03:36:40.214807 38230 net.cpp:220] relu6 needs backward computation.
I1107 03:36:40.214809 38230 net.cpp:220] fc6 needs backward computation.
I1107 03:36:40.214814 38230 net.cpp:220] pool5 needs backward computation.
I1107 03:36:40.214818 38230 net.cpp:220] relu5 needs backward computation.
I1107 03:36:40.214823 38230 net.cpp:220] conv5 needs backward computation.
I1107 03:36:40.214828 38230 net.cpp:220] relu4 needs backward computation.
I1107 03:36:40.214833 38230 net.cpp:220] conv4 needs backward computation.
I1107 03:36:40.214836 38230 net.cpp:220] relu3 needs backward computation.
I1107 03:36:40.214840 38230 net.cpp:220] conv3 needs backward computation.
I1107 03:36:40.214845 38230 net.cpp:220] pool2 needs backward computation.
I1107 03:36:40.214849 38230 net.cpp:220] relu2 needs backward computation.
I1107 03:36:40.214854 38230 net.cpp:220] bn2 needs backward computation.
I1107 03:36:40.214857 38230 net.cpp:220] conv2 needs backward computation.
I1107 03:36:40.214861 38230 net.cpp:220] pool1 needs backward computation.
I1107 03:36:40.214866 38230 net.cpp:220] relu1 needs backward computation.
I1107 03:36:40.214870 38230 net.cpp:220] bn1 needs backward computation.
I1107 03:36:40.214874 38230 net.cpp:220] conv1 needs backward computation.
I1107 03:36:40.214879 38230 net.cpp:222] label_data_1_split does not need backward computation.
I1107 03:36:40.214884 38230 net.cpp:222] data does not need backward computation.
I1107 03:36:40.214887 38230 net.cpp:264] This network produces output loss
I1107 03:36:40.214892 38230 net.cpp:264] This network produces output top-1
I1107 03:36:40.214911 38230 net.cpp:284] Network initialization done.
I1107 03:36:40.289253 38230 caffe_interface.cpp:363] Running for 80 iterations.
I1107 03:36:40.332146 38230 caffe_interface.cpp:125] Batch 0, loss = 0.140694
I1107 03:36:40.332168 38230 caffe_interface.cpp:125] Batch 0, top-1 = 0.98
I1107 03:36:40.352551 38230 caffe_interface.cpp:125] Batch 1, loss = 0.0939683
I1107 03:36:40.352563 38230 caffe_interface.cpp:125] Batch 1, top-1 = 0.96
I1107 03:36:40.371938 38230 caffe_interface.cpp:125] Batch 2, loss = 0.519815
I1107 03:36:40.371948 38230 caffe_interface.cpp:125] Batch 2, top-1 = 0.9
I1107 03:36:40.391228 38230 caffe_interface.cpp:125] Batch 3, loss = 0.595973
I1107 03:36:40.391250 38230 caffe_interface.cpp:125] Batch 3, top-1 = 0.86
I1107 03:36:40.409207 38230 caffe_interface.cpp:125] Batch 4, loss = 0.279878
I1107 03:36:40.409225 38230 caffe_interface.cpp:125] Batch 4, top-1 = 0.96
I1107 03:36:40.427628 38230 caffe_interface.cpp:125] Batch 5, loss = 0.00204909
I1107 03:36:40.427649 38230 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 03:36:40.445721 38230 caffe_interface.cpp:125] Batch 6, loss = 0.228604
I1107 03:36:40.445739 38230 caffe_interface.cpp:125] Batch 6, top-1 = 0.94
I1107 03:36:40.464460 38230 caffe_interface.cpp:125] Batch 7, loss = 0.14502
I1107 03:36:40.464478 38230 caffe_interface.cpp:125] Batch 7, top-1 = 0.98
I1107 03:36:40.482794 38230 caffe_interface.cpp:125] Batch 8, loss = 0.0795415
I1107 03:36:40.482810 38230 caffe_interface.cpp:125] Batch 8, top-1 = 0.98
I1107 03:36:40.502180 38230 caffe_interface.cpp:125] Batch 9, loss = 0.354888
I1107 03:36:40.502197 38230 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I1107 03:36:40.519940 38230 caffe_interface.cpp:125] Batch 10, loss = 0.155524
I1107 03:36:40.519956 38230 caffe_interface.cpp:125] Batch 10, top-1 = 0.94
I1107 03:36:40.539198 38230 caffe_interface.cpp:125] Batch 11, loss = 0.351147
I1107 03:36:40.539214 38230 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I1107 03:36:40.558599 38230 caffe_interface.cpp:125] Batch 12, loss = 0.401992
I1107 03:36:40.558609 38230 caffe_interface.cpp:125] Batch 12, top-1 = 0.92
I1107 03:36:40.576625 38230 caffe_interface.cpp:125] Batch 13, loss = 0.149836
I1107 03:36:40.576633 38230 caffe_interface.cpp:125] Batch 13, top-1 = 0.96
I1107 03:36:40.595451 38230 caffe_interface.cpp:125] Batch 14, loss = 0.23905
I1107 03:36:40.595458 38230 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I1107 03:36:40.613327 38230 caffe_interface.cpp:125] Batch 15, loss = 0.214023
I1107 03:36:40.613335 38230 caffe_interface.cpp:125] Batch 15, top-1 = 0.92
I1107 03:36:40.631399 38230 caffe_interface.cpp:125] Batch 16, loss = 0.0899401
I1107 03:36:40.631407 38230 caffe_interface.cpp:125] Batch 16, top-1 = 0.98
I1107 03:36:40.649653 38230 caffe_interface.cpp:125] Batch 17, loss = 0.00402994
I1107 03:36:40.649663 38230 caffe_interface.cpp:125] Batch 17, top-1 = 1
I1107 03:36:40.668352 38230 caffe_interface.cpp:125] Batch 18, loss = 0.149139
I1107 03:36:40.668359 38230 caffe_interface.cpp:125] Batch 18, top-1 = 0.94
I1107 03:36:40.686381 38230 caffe_interface.cpp:125] Batch 19, loss = 0.228365
I1107 03:36:40.686391 38230 caffe_interface.cpp:125] Batch 19, top-1 = 0.94
I1107 03:36:40.705456 38230 caffe_interface.cpp:125] Batch 20, loss = 0.124061
I1107 03:36:40.705466 38230 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I1107 03:36:40.723429 38230 caffe_interface.cpp:125] Batch 21, loss = 0.0966903
I1107 03:36:40.723438 38230 caffe_interface.cpp:125] Batch 21, top-1 = 0.96
I1107 03:36:40.742730 38230 caffe_interface.cpp:125] Batch 22, loss = 0.183919
I1107 03:36:40.742738 38230 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I1107 03:36:40.761006 38230 caffe_interface.cpp:125] Batch 23, loss = 0.167591
I1107 03:36:40.761016 38230 caffe_interface.cpp:125] Batch 23, top-1 = 0.96
I1107 03:36:40.780375 38230 caffe_interface.cpp:125] Batch 24, loss = 0.411121
I1107 03:36:40.780385 38230 caffe_interface.cpp:125] Batch 24, top-1 = 0.92
I1107 03:36:40.799321 38230 caffe_interface.cpp:125] Batch 25, loss = 0.192548
I1107 03:36:40.799331 38230 caffe_interface.cpp:125] Batch 25, top-1 = 0.96
I1107 03:36:40.817162 38230 caffe_interface.cpp:125] Batch 26, loss = 0.208144
I1107 03:36:40.817167 38230 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I1107 03:36:40.836395 38230 caffe_interface.cpp:125] Batch 27, loss = 0.0880221
I1107 03:36:40.836403 38230 caffe_interface.cpp:125] Batch 27, top-1 = 0.98
I1107 03:36:40.854553 38230 caffe_interface.cpp:125] Batch 28, loss = 0.23947
I1107 03:36:40.854562 38230 caffe_interface.cpp:125] Batch 28, top-1 = 0.92
I1107 03:36:40.872470 38230 caffe_interface.cpp:125] Batch 29, loss = 0.0462963
I1107 03:36:40.872478 38230 caffe_interface.cpp:125] Batch 29, top-1 = 0.98
I1107 03:36:40.890460 38230 caffe_interface.cpp:125] Batch 30, loss = 0.0095506
I1107 03:36:40.890466 38230 caffe_interface.cpp:125] Batch 30, top-1 = 1
I1107 03:36:40.909135 38230 caffe_interface.cpp:125] Batch 31, loss = 0.0054056
I1107 03:36:40.909143 38230 caffe_interface.cpp:125] Batch 31, top-1 = 1
I1107 03:36:40.927167 38230 caffe_interface.cpp:125] Batch 32, loss = 0.191063
I1107 03:36:40.927175 38230 caffe_interface.cpp:125] Batch 32, top-1 = 0.96
I1107 03:36:40.946087 38230 caffe_interface.cpp:125] Batch 33, loss = 0.0992586
I1107 03:36:40.946095 38230 caffe_interface.cpp:125] Batch 33, top-1 = 0.96
I1107 03:36:40.964344 38230 caffe_interface.cpp:125] Batch 34, loss = 0.329555
I1107 03:36:40.964354 38230 caffe_interface.cpp:125] Batch 34, top-1 = 0.9
I1107 03:36:40.983456 38230 caffe_interface.cpp:125] Batch 35, loss = 0.0104738
I1107 03:36:40.983467 38230 caffe_interface.cpp:125] Batch 35, top-1 = 1
I1107 03:36:41.002524 38230 caffe_interface.cpp:125] Batch 36, loss = 0.372884
I1107 03:36:41.002533 38230 caffe_interface.cpp:125] Batch 36, top-1 = 0.92
I1107 03:36:41.020515 38230 caffe_interface.cpp:125] Batch 37, loss = 0.143545
I1107 03:36:41.020540 38230 caffe_interface.cpp:125] Batch 37, top-1 = 0.98
I1107 03:36:41.039628 38230 caffe_interface.cpp:125] Batch 38, loss = 0.294614
I1107 03:36:41.039635 38230 caffe_interface.cpp:125] Batch 38, top-1 = 0.94
I1107 03:36:41.057591 38230 caffe_interface.cpp:125] Batch 39, loss = 0.174273
I1107 03:36:41.057597 38230 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I1107 03:36:41.076407 38230 caffe_interface.cpp:125] Batch 40, loss = 0.177219
I1107 03:36:41.076413 38230 caffe_interface.cpp:125] Batch 40, top-1 = 0.98
I1107 03:36:41.094656 38230 caffe_interface.cpp:125] Batch 41, loss = 0.0404937
I1107 03:36:41.094663 38230 caffe_interface.cpp:125] Batch 41, top-1 = 0.96
I1107 03:36:41.112606 38230 caffe_interface.cpp:125] Batch 42, loss = 0.0379089
I1107 03:36:41.112612 38230 caffe_interface.cpp:125] Batch 42, top-1 = 0.98
I1107 03:36:41.130445 38230 caffe_interface.cpp:125] Batch 43, loss = 0.157753
I1107 03:36:41.130452 38230 caffe_interface.cpp:125] Batch 43, top-1 = 0.94
I1107 03:36:41.149096 38230 caffe_interface.cpp:125] Batch 44, loss = 0.452811
I1107 03:36:41.149102 38230 caffe_interface.cpp:125] Batch 44, top-1 = 0.9
I1107 03:36:41.167091 38230 caffe_interface.cpp:125] Batch 45, loss = 0.482349
I1107 03:36:41.167099 38230 caffe_interface.cpp:125] Batch 45, top-1 = 0.9
I1107 03:36:41.186131 38230 caffe_interface.cpp:125] Batch 46, loss = 0.0910215
I1107 03:36:41.186136 38230 caffe_interface.cpp:125] Batch 46, top-1 = 0.98
I1107 03:36:41.204161 38230 caffe_interface.cpp:125] Batch 47, loss = 0.293564
I1107 03:36:41.204171 38230 caffe_interface.cpp:125] Batch 47, top-1 = 0.94
I1107 03:36:41.223594 38230 caffe_interface.cpp:125] Batch 48, loss = 0.174741
I1107 03:36:41.223603 38230 caffe_interface.cpp:125] Batch 48, top-1 = 0.96
I1107 03:36:41.243487 38230 caffe_interface.cpp:125] Batch 49, loss = 0.196739
I1107 03:36:41.243499 38230 caffe_interface.cpp:125] Batch 49, top-1 = 0.94
I1107 03:36:41.261999 38230 caffe_interface.cpp:125] Batch 50, loss = 0.208053
I1107 03:36:41.262009 38230 caffe_interface.cpp:125] Batch 50, top-1 = 0.94
I1107 03:36:41.281119 38230 caffe_interface.cpp:125] Batch 51, loss = 0.453962
I1107 03:36:41.281128 38230 caffe_interface.cpp:125] Batch 51, top-1 = 0.88
I1107 03:36:41.299145 38230 caffe_interface.cpp:125] Batch 52, loss = 0.0909647
I1107 03:36:41.299152 38230 caffe_interface.cpp:125] Batch 52, top-1 = 0.96
I1107 03:36:41.317960 38230 caffe_interface.cpp:125] Batch 53, loss = 0.0518436
I1107 03:36:41.317968 38230 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I1107 03:36:41.335940 38230 caffe_interface.cpp:125] Batch 54, loss = 0.00768762
I1107 03:36:41.335947 38230 caffe_interface.cpp:125] Batch 54, top-1 = 1
I1107 03:36:41.354255 38230 caffe_interface.cpp:125] Batch 55, loss = 0.395882
I1107 03:36:41.354262 38230 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I1107 03:36:41.372267 38230 caffe_interface.cpp:125] Batch 56, loss = 0.121863
I1107 03:36:41.372274 38230 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 03:36:41.390899 38230 caffe_interface.cpp:125] Batch 57, loss = 0.0210786
I1107 03:36:41.390907 38230 caffe_interface.cpp:125] Batch 57, top-1 = 1
I1107 03:36:41.408962 38230 caffe_interface.cpp:125] Batch 58, loss = 0.274531
I1107 03:36:41.408967 38230 caffe_interface.cpp:125] Batch 58, top-1 = 0.94
I1107 03:36:41.427871 38230 caffe_interface.cpp:125] Batch 59, loss = 0.254226
I1107 03:36:41.427880 38230 caffe_interface.cpp:125] Batch 59, top-1 = 0.92
I1107 03:36:41.445750 38230 caffe_interface.cpp:125] Batch 60, loss = 0.151239
I1107 03:36:41.445762 38230 caffe_interface.cpp:125] Batch 60, top-1 = 0.98
I1107 03:36:41.465167 38230 caffe_interface.cpp:125] Batch 61, loss = 0.167069
I1107 03:36:41.465174 38230 caffe_interface.cpp:125] Batch 61, top-1 = 0.98
I1107 03:36:41.483160 38230 caffe_interface.cpp:125] Batch 62, loss = 0.133496
I1107 03:36:41.483170 38230 caffe_interface.cpp:125] Batch 62, top-1 = 0.96
I1107 03:36:41.502341 38230 caffe_interface.cpp:125] Batch 63, loss = 0.347793
I1107 03:36:41.502349 38230 caffe_interface.cpp:125] Batch 63, top-1 = 0.92
I1107 03:36:41.520397 38230 caffe_interface.cpp:125] Batch 64, loss = 0.367528
I1107 03:36:41.520406 38230 caffe_interface.cpp:125] Batch 64, top-1 = 0.88
I1107 03:36:41.539597 38230 caffe_interface.cpp:125] Batch 65, loss = 0.269613
I1107 03:36:41.539609 38230 caffe_interface.cpp:125] Batch 65, top-1 = 0.94
I1107 03:36:41.558410 38230 caffe_interface.cpp:125] Batch 66, loss = 0.499896
I1107 03:36:41.558419 38230 caffe_interface.cpp:125] Batch 66, top-1 = 0.94
I1107 03:36:41.576243 38230 caffe_interface.cpp:125] Batch 67, loss = 0.212498
I1107 03:36:41.576251 38230 caffe_interface.cpp:125] Batch 67, top-1 = 0.94
I1107 03:36:41.595353 38230 caffe_interface.cpp:125] Batch 68, loss = 0.340253
I1107 03:36:41.595360 38230 caffe_interface.cpp:125] Batch 68, top-1 = 0.94
I1107 03:36:41.613394 38230 caffe_interface.cpp:125] Batch 69, loss = 0.172695
I1107 03:36:41.613404 38230 caffe_interface.cpp:125] Batch 69, top-1 = 0.92
I1107 03:36:41.632230 38230 caffe_interface.cpp:125] Batch 70, loss = 0.00425294
I1107 03:36:41.632236 38230 caffe_interface.cpp:125] Batch 70, top-1 = 1
I1107 03:36:41.650223 38230 caffe_interface.cpp:125] Batch 71, loss = 0.283422
I1107 03:36:41.650229 38230 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I1107 03:36:41.668146 38230 caffe_interface.cpp:125] Batch 72, loss = 0.222219
I1107 03:36:41.668153 38230 caffe_interface.cpp:125] Batch 72, top-1 = 0.96
I1107 03:36:41.686131 38230 caffe_interface.cpp:125] Batch 73, loss = 0.069521
I1107 03:36:41.686137 38230 caffe_interface.cpp:125] Batch 73, top-1 = 0.96
I1107 03:36:41.704787 38230 caffe_interface.cpp:125] Batch 74, loss = 0.0102566
I1107 03:36:41.704794 38230 caffe_interface.cpp:125] Batch 74, top-1 = 1
I1107 03:36:41.723035 38230 caffe_interface.cpp:125] Batch 75, loss = 0.13948
I1107 03:36:41.723043 38230 caffe_interface.cpp:125] Batch 75, top-1 = 0.96
I1107 03:36:41.742076 38230 caffe_interface.cpp:125] Batch 76, loss = 0.293234
I1107 03:36:41.742084 38230 caffe_interface.cpp:125] Batch 76, top-1 = 0.96
I1107 03:36:41.760073 38230 caffe_interface.cpp:125] Batch 77, loss = 0.404053
I1107 03:36:41.760084 38230 caffe_interface.cpp:125] Batch 77, top-1 = 0.92
I1107 03:36:41.779731 38230 caffe_interface.cpp:125] Batch 78, loss = 0.171134
I1107 03:36:41.779738 38230 caffe_interface.cpp:125] Batch 78, top-1 = 0.92
I1107 03:36:41.797765 38230 caffe_interface.cpp:125] Batch 79, loss = 0.0286866
I1107 03:36:41.797776 38230 caffe_interface.cpp:125] Batch 79, top-1 = 0.98
I1107 03:36:41.797780 38230 caffe_interface.cpp:130] Loss: 0.201062
I1107 03:36:41.797785 38230 caffe_interface.cpp:142] loss = 0.201062 (* 1 = 0.201062 loss)
I1107 03:36:41.797792 38230 caffe_interface.cpp:142] top-1 = 0.952
I1107 03:36:42.044893 38230 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/sparse.caffemodel
I1107 03:36:42.044919 38230 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.2:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.951999664    | 0.0052497983   |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 1526191        | -59.4636688%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 1397275624     | -35.1286659%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config2.prototxt
I1107 03:36:42.304477 39477 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/net_finetune.prototxt
I1107 03:36:42.477180 39477 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 03:36:42.477689 39477 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24848105472, dev_info[0]: total=25620447232 free=24848105472
I1107 03:36:42.477700 39477 caffe_interface.cpp:493] Using GPUs 0
I1107 03:36:42.477952 39477 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 03:36:43.066042 39477 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/net_finetune.prototxt"
type: "Adam"
I1107 03:36:43.066155 39477 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/net_finetune.prototxt
I1107 03:36:43.066395 39477 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 03:36:43.066408 39477 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 03:36:43.066553 39477 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 03:36:43.066617 39477 layer_factory.hpp:77] Creating layer data
I1107 03:36:43.066740 39477 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:43.067153 39477 net.cpp:94] Creating Layer data
I1107 03:36:43.067162 39477 net.cpp:409] data -> data
I1107 03:36:43.067171 39477 net.cpp:409] data -> label
I1107 03:36:43.068585 39516 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 03:36:43.068629 39516 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 03:36:43.068887 39477 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 03:36:43.068966 39477 data_layer.cpp:83] output data size: 256,3,227,227
I1107 03:36:43.444634 39477 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:43.444702 39477 net.cpp:144] Setting up data
I1107 03:36:43.444710 39477 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 03:36:43.444715 39477 net.cpp:151] Top shape: 256 (256)
I1107 03:36:43.444716 39477 net.cpp:159] Memory required for data: 158298112
I1107 03:36:43.444721 39477 layer_factory.hpp:77] Creating layer conv1
I1107 03:36:43.444736 39477 net.cpp:94] Creating Layer conv1
I1107 03:36:43.444739 39477 net.cpp:435] conv1 <- data
I1107 03:36:43.444770 39477 net.cpp:409] conv1 -> conv1
I1107 03:36:43.446807 39477 net.cpp:144] Setting up conv1
I1107 03:36:43.446820 39477 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 03:36:43.446822 39477 net.cpp:159] Memory required for data: 455667712
I1107 03:36:43.446836 39477 layer_factory.hpp:77] Creating layer bn1
I1107 03:36:43.446846 39477 net.cpp:94] Creating Layer bn1
I1107 03:36:43.446849 39477 net.cpp:435] bn1 <- conv1
I1107 03:36:43.446853 39477 net.cpp:409] bn1 -> scale1
I1107 03:36:43.448083 39477 net.cpp:144] Setting up bn1
I1107 03:36:43.448089 39477 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 03:36:43.448092 39477 net.cpp:159] Memory required for data: 753037312
I1107 03:36:43.448102 39477 layer_factory.hpp:77] Creating layer relu1
I1107 03:36:43.448107 39477 net.cpp:94] Creating Layer relu1
I1107 03:36:43.448110 39477 net.cpp:435] relu1 <- scale1
I1107 03:36:43.448114 39477 net.cpp:409] relu1 -> relu1
I1107 03:36:43.448138 39477 net.cpp:144] Setting up relu1
I1107 03:36:43.448143 39477 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 03:36:43.448145 39477 net.cpp:159] Memory required for data: 1050406912
I1107 03:36:43.448148 39477 layer_factory.hpp:77] Creating layer pool1
I1107 03:36:43.448153 39477 net.cpp:94] Creating Layer pool1
I1107 03:36:43.448155 39477 net.cpp:435] pool1 <- relu1
I1107 03:36:43.448159 39477 net.cpp:409] pool1 -> pool1
I1107 03:36:43.448220 39477 net.cpp:144] Setting up pool1
I1107 03:36:43.448225 39477 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 03:36:43.448226 39477 net.cpp:159] Memory required for data: 1122070528
I1107 03:36:43.448228 39477 layer_factory.hpp:77] Creating layer conv2
I1107 03:36:43.448235 39477 net.cpp:94] Creating Layer conv2
I1107 03:36:43.448237 39477 net.cpp:435] conv2 <- pool1
I1107 03:36:43.448241 39477 net.cpp:409] conv2 -> conv2
I1107 03:36:43.463222 39477 net.cpp:144] Setting up conv2
I1107 03:36:43.463240 39477 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 03:36:43.463243 39477 net.cpp:159] Memory required for data: 1313173504
I1107 03:36:43.463254 39477 layer_factory.hpp:77] Creating layer bn2
I1107 03:36:43.463264 39477 net.cpp:94] Creating Layer bn2
I1107 03:36:43.463268 39477 net.cpp:435] bn2 <- conv2
I1107 03:36:43.463274 39477 net.cpp:409] bn2 -> scale2
I1107 03:36:43.463866 39477 net.cpp:144] Setting up bn2
I1107 03:36:43.463874 39477 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 03:36:43.463876 39477 net.cpp:159] Memory required for data: 1504276480
I1107 03:36:43.463886 39477 layer_factory.hpp:77] Creating layer relu2
I1107 03:36:43.463891 39477 net.cpp:94] Creating Layer relu2
I1107 03:36:43.463896 39477 net.cpp:435] relu2 <- scale2
I1107 03:36:43.463901 39477 net.cpp:409] relu2 -> relu2
I1107 03:36:43.463919 39477 net.cpp:144] Setting up relu2
I1107 03:36:43.463925 39477 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 03:36:43.463929 39477 net.cpp:159] Memory required for data: 1695379456
I1107 03:36:43.463932 39477 layer_factory.hpp:77] Creating layer pool2
I1107 03:36:43.463938 39477 net.cpp:94] Creating Layer pool2
I1107 03:36:43.463941 39477 net.cpp:435] pool2 <- relu2
I1107 03:36:43.463960 39477 net.cpp:409] pool2 -> pool2
I1107 03:36:43.463989 39477 net.cpp:144] Setting up pool2
I1107 03:36:43.463994 39477 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 03:36:43.463997 39477 net.cpp:159] Memory required for data: 1739681792
I1107 03:36:43.464000 39477 layer_factory.hpp:77] Creating layer conv3
I1107 03:36:43.464009 39477 net.cpp:94] Creating Layer conv3
I1107 03:36:43.464020 39477 net.cpp:435] conv3 <- pool2
I1107 03:36:43.464030 39477 net.cpp:409] conv3 -> conv3
I1107 03:36:43.474143 39477 net.cpp:144] Setting up conv3
I1107 03:36:43.474180 39477 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 03:36:43.474184 39477 net.cpp:159] Memory required for data: 1806135296
I1107 03:36:43.474192 39477 layer_factory.hpp:77] Creating layer relu3
I1107 03:36:43.474200 39477 net.cpp:94] Creating Layer relu3
I1107 03:36:43.474205 39477 net.cpp:435] relu3 <- conv3
I1107 03:36:43.474210 39477 net.cpp:409] relu3 -> relu3
I1107 03:36:43.474246 39477 net.cpp:144] Setting up relu3
I1107 03:36:43.474256 39477 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 03:36:43.474259 39477 net.cpp:159] Memory required for data: 1872588800
I1107 03:36:43.474262 39477 layer_factory.hpp:77] Creating layer conv4
I1107 03:36:43.474274 39477 net.cpp:94] Creating Layer conv4
I1107 03:36:43.474282 39477 net.cpp:435] conv4 <- relu3
I1107 03:36:43.474288 39477 net.cpp:409] conv4 -> conv4
I1107 03:36:43.490617 39477 net.cpp:144] Setting up conv4
I1107 03:36:43.490639 39477 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 03:36:43.490644 39477 net.cpp:159] Memory required for data: 1939042304
I1107 03:36:43.490660 39477 layer_factory.hpp:77] Creating layer relu4
I1107 03:36:43.490670 39477 net.cpp:94] Creating Layer relu4
I1107 03:36:43.490676 39477 net.cpp:435] relu4 <- conv4
I1107 03:36:43.490686 39477 net.cpp:409] relu4 -> relu4
I1107 03:36:43.490713 39477 net.cpp:144] Setting up relu4
I1107 03:36:43.490720 39477 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 03:36:43.490723 39477 net.cpp:159] Memory required for data: 2005495808
I1107 03:36:43.490727 39477 layer_factory.hpp:77] Creating layer conv5
I1107 03:36:43.490738 39477 net.cpp:94] Creating Layer conv5
I1107 03:36:43.490742 39477 net.cpp:435] conv5 <- relu4
I1107 03:36:43.490749 39477 net.cpp:409] conv5 -> conv5
I1107 03:36:43.507014 39477 net.cpp:144] Setting up conv5
I1107 03:36:43.507043 39477 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 03:36:43.507047 39477 net.cpp:159] Memory required for data: 2049798144
I1107 03:36:43.507064 39477 layer_factory.hpp:77] Creating layer relu5
I1107 03:36:43.507086 39477 net.cpp:94] Creating Layer relu5
I1107 03:36:43.507108 39477 net.cpp:435] relu5 <- conv5
I1107 03:36:43.507127 39477 net.cpp:409] relu5 -> relu5
I1107 03:36:43.507174 39477 net.cpp:144] Setting up relu5
I1107 03:36:43.507181 39477 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 03:36:43.507185 39477 net.cpp:159] Memory required for data: 2094100480
I1107 03:36:43.507190 39477 layer_factory.hpp:77] Creating layer pool5
I1107 03:36:43.507208 39477 net.cpp:94] Creating Layer pool5
I1107 03:36:43.507222 39477 net.cpp:435] pool5 <- relu5
I1107 03:36:43.507242 39477 net.cpp:409] pool5 -> pool5
I1107 03:36:43.507287 39477 net.cpp:144] Setting up pool5
I1107 03:36:43.507294 39477 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 03:36:43.507299 39477 net.cpp:159] Memory required for data: 2103537664
I1107 03:36:43.507313 39477 layer_factory.hpp:77] Creating layer fc6
I1107 03:36:43.507333 39477 net.cpp:94] Creating Layer fc6
I1107 03:36:43.507339 39477 net.cpp:435] fc6 <- pool5
I1107 03:36:43.507346 39477 net.cpp:409] fc6 -> fc6
I1107 03:36:43.838199 39477 net.cpp:144] Setting up fc6
I1107 03:36:43.838222 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.838224 39477 net.cpp:159] Memory required for data: 2107731968
I1107 03:36:43.838254 39477 layer_factory.hpp:77] Creating layer relu6
I1107 03:36:43.838261 39477 net.cpp:94] Creating Layer relu6
I1107 03:36:43.838264 39477 net.cpp:435] relu6 <- fc6
I1107 03:36:43.838300 39477 net.cpp:409] relu6 -> relu6
I1107 03:36:43.838323 39477 net.cpp:144] Setting up relu6
I1107 03:36:43.838328 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.838330 39477 net.cpp:159] Memory required for data: 2111926272
I1107 03:36:43.838332 39477 layer_factory.hpp:77] Creating layer drop6
I1107 03:36:43.838338 39477 net.cpp:94] Creating Layer drop6
I1107 03:36:43.838341 39477 net.cpp:435] drop6 <- relu6
I1107 03:36:43.838346 39477 net.cpp:409] drop6 -> drop6
I1107 03:36:43.838389 39477 net.cpp:144] Setting up drop6
I1107 03:36:43.838394 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.838397 39477 net.cpp:159] Memory required for data: 2116120576
I1107 03:36:43.838399 39477 layer_factory.hpp:77] Creating layer fc7
I1107 03:36:43.838405 39477 net.cpp:94] Creating Layer fc7
I1107 03:36:43.838410 39477 net.cpp:435] fc7 <- drop6
I1107 03:36:43.838416 39477 net.cpp:409] fc7 -> fc7
I1107 03:36:43.970739 39477 net.cpp:144] Setting up fc7
I1107 03:36:43.970767 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.970768 39477 net.cpp:159] Memory required for data: 2120314880
I1107 03:36:43.970791 39477 layer_factory.hpp:77] Creating layer bn7
I1107 03:36:43.970799 39477 net.cpp:94] Creating Layer bn7
I1107 03:36:43.970803 39477 net.cpp:435] bn7 <- fc7
I1107 03:36:43.970808 39477 net.cpp:409] bn7 -> scale7
I1107 03:36:43.971289 39477 net.cpp:144] Setting up bn7
I1107 03:36:43.971297 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.971298 39477 net.cpp:159] Memory required for data: 2124509184
I1107 03:36:43.971305 39477 layer_factory.hpp:77] Creating layer relu7
I1107 03:36:43.971310 39477 net.cpp:94] Creating Layer relu7
I1107 03:36:43.971313 39477 net.cpp:435] relu7 <- scale7
I1107 03:36:43.971319 39477 net.cpp:409] relu7 -> relu7
I1107 03:36:43.971339 39477 net.cpp:144] Setting up relu7
I1107 03:36:43.971344 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.971345 39477 net.cpp:159] Memory required for data: 2128703488
I1107 03:36:43.971349 39477 layer_factory.hpp:77] Creating layer drop7
I1107 03:36:43.971369 39477 net.cpp:94] Creating Layer drop7
I1107 03:36:43.971374 39477 net.cpp:435] drop7 <- relu7
I1107 03:36:43.971379 39477 net.cpp:409] drop7 -> drop7
I1107 03:36:43.971405 39477 net.cpp:144] Setting up drop7
I1107 03:36:43.971411 39477 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 03:36:43.971413 39477 net.cpp:159] Memory required for data: 2132897792
I1107 03:36:43.971416 39477 layer_factory.hpp:77] Creating layer fc8
I1107 03:36:43.971422 39477 net.cpp:94] Creating Layer fc8
I1107 03:36:43.971426 39477 net.cpp:435] fc8 <- drop7
I1107 03:36:43.971432 39477 net.cpp:409] fc8 -> fc8
I1107 03:36:43.972285 39477 net.cpp:144] Setting up fc8
I1107 03:36:43.972297 39477 net.cpp:151] Top shape: 256 2 (512)
I1107 03:36:43.972301 39477 net.cpp:159] Memory required for data: 2132899840
I1107 03:36:43.972307 39477 layer_factory.hpp:77] Creating layer loss
I1107 03:36:43.972316 39477 net.cpp:94] Creating Layer loss
I1107 03:36:43.972322 39477 net.cpp:435] loss <- fc8
I1107 03:36:43.972328 39477 net.cpp:435] loss <- label
I1107 03:36:43.972333 39477 net.cpp:409] loss -> loss
I1107 03:36:43.972347 39477 layer_factory.hpp:77] Creating layer loss
I1107 03:36:43.972427 39477 net.cpp:144] Setting up loss
I1107 03:36:43.972432 39477 net.cpp:151] Top shape: (1)
I1107 03:36:43.972434 39477 net.cpp:154]     with loss weight 1
I1107 03:36:43.972447 39477 net.cpp:159] Memory required for data: 2132899844
I1107 03:36:43.972450 39477 net.cpp:220] loss needs backward computation.
I1107 03:36:43.972468 39477 net.cpp:220] fc8 needs backward computation.
I1107 03:36:43.972471 39477 net.cpp:220] drop7 needs backward computation.
I1107 03:36:43.972473 39477 net.cpp:220] relu7 needs backward computation.
I1107 03:36:43.972476 39477 net.cpp:220] bn7 needs backward computation.
I1107 03:36:43.972481 39477 net.cpp:220] fc7 needs backward computation.
I1107 03:36:43.972484 39477 net.cpp:220] drop6 needs backward computation.
I1107 03:36:43.972488 39477 net.cpp:220] relu6 needs backward computation.
I1107 03:36:43.972519 39477 net.cpp:220] fc6 needs backward computation.
I1107 03:36:43.972524 39477 net.cpp:220] pool5 needs backward computation.
I1107 03:36:43.972528 39477 net.cpp:220] relu5 needs backward computation.
I1107 03:36:43.972532 39477 net.cpp:220] conv5 needs backward computation.
I1107 03:36:43.972535 39477 net.cpp:220] relu4 needs backward computation.
I1107 03:36:43.972540 39477 net.cpp:220] conv4 needs backward computation.
I1107 03:36:43.972543 39477 net.cpp:220] relu3 needs backward computation.
I1107 03:36:43.972548 39477 net.cpp:220] conv3 needs backward computation.
I1107 03:36:43.972550 39477 net.cpp:220] pool2 needs backward computation.
I1107 03:36:43.972555 39477 net.cpp:220] relu2 needs backward computation.
I1107 03:36:43.972558 39477 net.cpp:220] bn2 needs backward computation.
I1107 03:36:43.972563 39477 net.cpp:220] conv2 needs backward computation.
I1107 03:36:43.972565 39477 net.cpp:220] pool1 needs backward computation.
I1107 03:36:43.972570 39477 net.cpp:220] relu1 needs backward computation.
I1107 03:36:43.972573 39477 net.cpp:220] bn1 needs backward computation.
I1107 03:36:43.972576 39477 net.cpp:220] conv1 needs backward computation.
I1107 03:36:43.972581 39477 net.cpp:222] data does not need backward computation.
I1107 03:36:43.972585 39477 net.cpp:264] This network produces output loss
I1107 03:36:43.972605 39477 net.cpp:284] Network initialization done.
I1107 03:36:43.972882 39477 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/net_finetune.prototxt
I1107 03:36:43.972916 39477 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 03:36:43.973084 39477 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 03:36:43.973188 39477 layer_factory.hpp:77] Creating layer data
I1107 03:36:43.973233 39477 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:43.974040 39477 net.cpp:94] Creating Layer data
I1107 03:36:43.974047 39477 net.cpp:409] data -> data
I1107 03:36:43.974056 39477 net.cpp:409] data -> label
I1107 03:36:43.975335 39546 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 03:36:43.975373 39546 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 03:36:43.975615 39477 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 03:36:43.975682 39477 data_layer.cpp:83] output data size: 50,3,227,227
I1107 03:36:44.050436 39477 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 03:36:44.050503 39477 net.cpp:144] Setting up data
I1107 03:36:44.050513 39477 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 03:36:44.050516 39477 net.cpp:151] Top shape: 50 (50)
I1107 03:36:44.050518 39477 net.cpp:159] Memory required for data: 30917600
I1107 03:36:44.050539 39477 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 03:36:44.050549 39477 net.cpp:94] Creating Layer label_data_1_split
I1107 03:36:44.050552 39477 net.cpp:435] label_data_1_split <- label
I1107 03:36:44.050559 39477 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 03:36:44.050567 39477 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 03:36:44.050629 39477 net.cpp:144] Setting up label_data_1_split
I1107 03:36:44.050634 39477 net.cpp:151] Top shape: 50 (50)
I1107 03:36:44.050637 39477 net.cpp:151] Top shape: 50 (50)
I1107 03:36:44.050639 39477 net.cpp:159] Memory required for data: 30918000
I1107 03:36:44.050642 39477 layer_factory.hpp:77] Creating layer conv1
I1107 03:36:44.050655 39477 net.cpp:94] Creating Layer conv1
I1107 03:36:44.050660 39477 net.cpp:435] conv1 <- data
I1107 03:36:44.050667 39477 net.cpp:409] conv1 -> conv1
I1107 03:36:44.051252 39477 net.cpp:144] Setting up conv1
I1107 03:36:44.051259 39477 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:44.051261 39477 net.cpp:159] Memory required for data: 88998000
I1107 03:36:44.051273 39477 layer_factory.hpp:77] Creating layer bn1
I1107 03:36:44.051285 39477 net.cpp:94] Creating Layer bn1
I1107 03:36:44.051288 39477 net.cpp:435] bn1 <- conv1
I1107 03:36:44.051295 39477 net.cpp:409] bn1 -> scale1
I1107 03:36:44.051872 39477 net.cpp:144] Setting up bn1
I1107 03:36:44.051878 39477 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:44.051880 39477 net.cpp:159] Memory required for data: 147078000
I1107 03:36:44.051893 39477 layer_factory.hpp:77] Creating layer relu1
I1107 03:36:44.051903 39477 net.cpp:94] Creating Layer relu1
I1107 03:36:44.051906 39477 net.cpp:435] relu1 <- scale1
I1107 03:36:44.051910 39477 net.cpp:409] relu1 -> relu1
I1107 03:36:44.051935 39477 net.cpp:144] Setting up relu1
I1107 03:36:44.051940 39477 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 03:36:44.051944 39477 net.cpp:159] Memory required for data: 205158000
I1107 03:36:44.051945 39477 layer_factory.hpp:77] Creating layer pool1
I1107 03:36:44.051952 39477 net.cpp:94] Creating Layer pool1
I1107 03:36:44.051956 39477 net.cpp:435] pool1 <- relu1
I1107 03:36:44.051962 39477 net.cpp:409] pool1 -> pool1
I1107 03:36:44.052237 39477 net.cpp:144] Setting up pool1
I1107 03:36:44.052242 39477 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 03:36:44.052245 39477 net.cpp:159] Memory required for data: 219154800
I1107 03:36:44.052248 39477 layer_factory.hpp:77] Creating layer conv2
I1107 03:36:44.052258 39477 net.cpp:94] Creating Layer conv2
I1107 03:36:44.052263 39477 net.cpp:435] conv2 <- pool1
I1107 03:36:44.052268 39477 net.cpp:409] conv2 -> conv2
I1107 03:36:44.058544 39477 net.cpp:144] Setting up conv2
I1107 03:36:44.058579 39477 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:44.058583 39477 net.cpp:159] Memory required for data: 256479600
I1107 03:36:44.058593 39477 layer_factory.hpp:77] Creating layer bn2
I1107 03:36:44.058605 39477 net.cpp:94] Creating Layer bn2
I1107 03:36:44.058609 39477 net.cpp:435] bn2 <- conv2
I1107 03:36:44.058617 39477 net.cpp:409] bn2 -> scale2
I1107 03:36:44.059267 39477 net.cpp:144] Setting up bn2
I1107 03:36:44.059278 39477 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:44.059280 39477 net.cpp:159] Memory required for data: 293804400
I1107 03:36:44.059289 39477 layer_factory.hpp:77] Creating layer relu2
I1107 03:36:44.059295 39477 net.cpp:94] Creating Layer relu2
I1107 03:36:44.059299 39477 net.cpp:435] relu2 <- scale2
I1107 03:36:44.059303 39477 net.cpp:409] relu2 -> relu2
I1107 03:36:44.059322 39477 net.cpp:144] Setting up relu2
I1107 03:36:44.059326 39477 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 03:36:44.059329 39477 net.cpp:159] Memory required for data: 331129200
I1107 03:36:44.059332 39477 layer_factory.hpp:77] Creating layer pool2
I1107 03:36:44.059337 39477 net.cpp:94] Creating Layer pool2
I1107 03:36:44.059340 39477 net.cpp:435] pool2 <- relu2
I1107 03:36:44.059345 39477 net.cpp:409] pool2 -> pool2
I1107 03:36:44.059375 39477 net.cpp:144] Setting up pool2
I1107 03:36:44.059377 39477 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:44.059381 39477 net.cpp:159] Memory required for data: 339782000
I1107 03:36:44.059383 39477 layer_factory.hpp:77] Creating layer conv3
I1107 03:36:44.059391 39477 net.cpp:94] Creating Layer conv3
I1107 03:36:44.059393 39477 net.cpp:435] conv3 <- pool2
I1107 03:36:44.059398 39477 net.cpp:409] conv3 -> conv3
I1107 03:36:44.069149 39477 net.cpp:144] Setting up conv3
I1107 03:36:44.069177 39477 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:44.069182 39477 net.cpp:159] Memory required for data: 352761200
I1107 03:36:44.069190 39477 layer_factory.hpp:77] Creating layer relu3
I1107 03:36:44.069200 39477 net.cpp:94] Creating Layer relu3
I1107 03:36:44.069205 39477 net.cpp:435] relu3 <- conv3
I1107 03:36:44.069211 39477 net.cpp:409] relu3 -> relu3
I1107 03:36:44.069245 39477 net.cpp:144] Setting up relu3
I1107 03:36:44.069254 39477 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:44.069258 39477 net.cpp:159] Memory required for data: 365740400
I1107 03:36:44.069262 39477 layer_factory.hpp:77] Creating layer conv4
I1107 03:36:44.069274 39477 net.cpp:94] Creating Layer conv4
I1107 03:36:44.069281 39477 net.cpp:435] conv4 <- relu3
I1107 03:36:44.069298 39477 net.cpp:409] conv4 -> conv4
I1107 03:36:44.086153 39477 net.cpp:144] Setting up conv4
I1107 03:36:44.086180 39477 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:44.086184 39477 net.cpp:159] Memory required for data: 378719600
I1107 03:36:44.086199 39477 layer_factory.hpp:77] Creating layer relu4
I1107 03:36:44.086207 39477 net.cpp:94] Creating Layer relu4
I1107 03:36:44.086211 39477 net.cpp:435] relu4 <- conv4
I1107 03:36:44.086220 39477 net.cpp:409] relu4 -> relu4
I1107 03:36:44.086271 39477 net.cpp:144] Setting up relu4
I1107 03:36:44.086277 39477 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 03:36:44.086280 39477 net.cpp:159] Memory required for data: 391698800
I1107 03:36:44.086292 39477 layer_factory.hpp:77] Creating layer conv5
I1107 03:36:44.086318 39477 net.cpp:94] Creating Layer conv5
I1107 03:36:44.086339 39477 net.cpp:435] conv5 <- relu4
I1107 03:36:44.086356 39477 net.cpp:409] conv5 -> conv5
I1107 03:36:44.106247 39477 net.cpp:144] Setting up conv5
I1107 03:36:44.106276 39477 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:44.106279 39477 net.cpp:159] Memory required for data: 400351600
I1107 03:36:44.106290 39477 layer_factory.hpp:77] Creating layer relu5
I1107 03:36:44.106302 39477 net.cpp:94] Creating Layer relu5
I1107 03:36:44.106309 39477 net.cpp:435] relu5 <- conv5
I1107 03:36:44.106321 39477 net.cpp:409] relu5 -> relu5
I1107 03:36:44.106361 39477 net.cpp:144] Setting up relu5
I1107 03:36:44.106369 39477 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 03:36:44.106392 39477 net.cpp:159] Memory required for data: 409004400
I1107 03:36:44.106397 39477 layer_factory.hpp:77] Creating layer pool5
I1107 03:36:44.106418 39477 net.cpp:94] Creating Layer pool5
I1107 03:36:44.106425 39477 net.cpp:435] pool5 <- relu5
I1107 03:36:44.106432 39477 net.cpp:409] pool5 -> pool5
I1107 03:36:44.106478 39477 net.cpp:144] Setting up pool5
I1107 03:36:44.106485 39477 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 03:36:44.106489 39477 net.cpp:159] Memory required for data: 410847600
I1107 03:36:44.106493 39477 layer_factory.hpp:77] Creating layer fc6
I1107 03:36:44.106503 39477 net.cpp:94] Creating Layer fc6
I1107 03:36:44.106509 39477 net.cpp:435] fc6 <- pool5
I1107 03:36:44.106516 39477 net.cpp:409] fc6 -> fc6
I1107 03:36:44.437146 39477 net.cpp:144] Setting up fc6
I1107 03:36:44.437167 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.437170 39477 net.cpp:159] Memory required for data: 411666800
I1107 03:36:44.437194 39477 layer_factory.hpp:77] Creating layer relu6
I1107 03:36:44.437201 39477 net.cpp:94] Creating Layer relu6
I1107 03:36:44.437214 39477 net.cpp:435] relu6 <- fc6
I1107 03:36:44.437222 39477 net.cpp:409] relu6 -> relu6
I1107 03:36:44.437252 39477 net.cpp:144] Setting up relu6
I1107 03:36:44.437258 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.437260 39477 net.cpp:159] Memory required for data: 412486000
I1107 03:36:44.437263 39477 layer_factory.hpp:77] Creating layer drop6
I1107 03:36:44.437270 39477 net.cpp:94] Creating Layer drop6
I1107 03:36:44.437273 39477 net.cpp:435] drop6 <- relu6
I1107 03:36:44.437278 39477 net.cpp:409] drop6 -> drop6
I1107 03:36:44.437307 39477 net.cpp:144] Setting up drop6
I1107 03:36:44.437312 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.437315 39477 net.cpp:159] Memory required for data: 413305200
I1107 03:36:44.437317 39477 layer_factory.hpp:77] Creating layer fc7
I1107 03:36:44.437325 39477 net.cpp:94] Creating Layer fc7
I1107 03:36:44.437330 39477 net.cpp:435] fc7 <- drop6
I1107 03:36:44.437336 39477 net.cpp:409] fc7 -> fc7
I1107 03:36:44.578343 39477 net.cpp:144] Setting up fc7
I1107 03:36:44.578366 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.578369 39477 net.cpp:159] Memory required for data: 414124400
I1107 03:36:44.578387 39477 layer_factory.hpp:77] Creating layer bn7
I1107 03:36:44.578400 39477 net.cpp:94] Creating Layer bn7
I1107 03:36:44.578407 39477 net.cpp:435] bn7 <- fc7
I1107 03:36:44.578415 39477 net.cpp:409] bn7 -> scale7
I1107 03:36:44.579061 39477 net.cpp:144] Setting up bn7
I1107 03:36:44.579068 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.579071 39477 net.cpp:159] Memory required for data: 414943600
I1107 03:36:44.579079 39477 layer_factory.hpp:77] Creating layer relu7
I1107 03:36:44.579084 39477 net.cpp:94] Creating Layer relu7
I1107 03:36:44.579087 39477 net.cpp:435] relu7 <- scale7
I1107 03:36:44.579092 39477 net.cpp:409] relu7 -> relu7
I1107 03:36:44.579113 39477 net.cpp:144] Setting up relu7
I1107 03:36:44.579118 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.579120 39477 net.cpp:159] Memory required for data: 415762800
I1107 03:36:44.579123 39477 layer_factory.hpp:77] Creating layer drop7
I1107 03:36:44.579129 39477 net.cpp:94] Creating Layer drop7
I1107 03:36:44.579133 39477 net.cpp:435] drop7 <- relu7
I1107 03:36:44.579136 39477 net.cpp:409] drop7 -> drop7
I1107 03:36:44.579166 39477 net.cpp:144] Setting up drop7
I1107 03:36:44.579172 39477 net.cpp:151] Top shape: 50 4096 (204800)
I1107 03:36:44.579174 39477 net.cpp:159] Memory required for data: 416582000
I1107 03:36:44.579177 39477 layer_factory.hpp:77] Creating layer fc8
I1107 03:36:44.579182 39477 net.cpp:94] Creating Layer fc8
I1107 03:36:44.579186 39477 net.cpp:435] fc8 <- drop7
I1107 03:36:44.579191 39477 net.cpp:409] fc8 -> fc8
I1107 03:36:44.579385 39477 net.cpp:144] Setting up fc8
I1107 03:36:44.579391 39477 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:44.579393 39477 net.cpp:159] Memory required for data: 416582400
I1107 03:36:44.579412 39477 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 03:36:44.579418 39477 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 03:36:44.579421 39477 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 03:36:44.579427 39477 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 03:36:44.579432 39477 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 03:36:44.579463 39477 net.cpp:144] Setting up fc8_fc8_0_split
I1107 03:36:44.579468 39477 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:44.579473 39477 net.cpp:151] Top shape: 50 2 (100)
I1107 03:36:44.579474 39477 net.cpp:159] Memory required for data: 416583200
I1107 03:36:44.579488 39477 layer_factory.hpp:77] Creating layer loss
I1107 03:36:44.579493 39477 net.cpp:94] Creating Layer loss
I1107 03:36:44.579495 39477 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 03:36:44.579499 39477 net.cpp:435] loss <- label_data_1_split_0
I1107 03:36:44.579504 39477 net.cpp:409] loss -> loss
I1107 03:36:44.579510 39477 layer_factory.hpp:77] Creating layer loss
I1107 03:36:44.579593 39477 net.cpp:144] Setting up loss
I1107 03:36:44.579597 39477 net.cpp:151] Top shape: (1)
I1107 03:36:44.579599 39477 net.cpp:154]     with loss weight 1
I1107 03:36:44.579610 39477 net.cpp:159] Memory required for data: 416583204
I1107 03:36:44.579613 39477 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 03:36:44.579618 39477 net.cpp:94] Creating Layer accuracy-top1
I1107 03:36:44.579620 39477 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 03:36:44.579623 39477 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 03:36:44.579628 39477 net.cpp:409] accuracy-top1 -> top-1
I1107 03:36:44.579634 39477 net.cpp:144] Setting up accuracy-top1
I1107 03:36:44.579638 39477 net.cpp:151] Top shape: (1)
I1107 03:36:44.579639 39477 net.cpp:159] Memory required for data: 416583208
I1107 03:36:44.579643 39477 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 03:36:44.579645 39477 net.cpp:220] loss needs backward computation.
I1107 03:36:44.579648 39477 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 03:36:44.579651 39477 net.cpp:220] fc8 needs backward computation.
I1107 03:36:44.579654 39477 net.cpp:220] drop7 needs backward computation.
I1107 03:36:44.579656 39477 net.cpp:220] relu7 needs backward computation.
I1107 03:36:44.579659 39477 net.cpp:220] bn7 needs backward computation.
I1107 03:36:44.579661 39477 net.cpp:220] fc7 needs backward computation.
I1107 03:36:44.579664 39477 net.cpp:220] drop6 needs backward computation.
I1107 03:36:44.579668 39477 net.cpp:220] relu6 needs backward computation.
I1107 03:36:44.579670 39477 net.cpp:220] fc6 needs backward computation.
I1107 03:36:44.579672 39477 net.cpp:220] pool5 needs backward computation.
I1107 03:36:44.579675 39477 net.cpp:220] relu5 needs backward computation.
I1107 03:36:44.579679 39477 net.cpp:220] conv5 needs backward computation.
I1107 03:36:44.579680 39477 net.cpp:220] relu4 needs backward computation.
I1107 03:36:44.579684 39477 net.cpp:220] conv4 needs backward computation.
I1107 03:36:44.579686 39477 net.cpp:220] relu3 needs backward computation.
I1107 03:36:44.579689 39477 net.cpp:220] conv3 needs backward computation.
I1107 03:36:44.579691 39477 net.cpp:220] pool2 needs backward computation.
I1107 03:36:44.579694 39477 net.cpp:220] relu2 needs backward computation.
I1107 03:36:44.579696 39477 net.cpp:220] bn2 needs backward computation.
I1107 03:36:44.579699 39477 net.cpp:220] conv2 needs backward computation.
I1107 03:36:44.579704 39477 net.cpp:220] pool1 needs backward computation.
I1107 03:36:44.579706 39477 net.cpp:220] relu1 needs backward computation.
I1107 03:36:44.579710 39477 net.cpp:220] bn1 needs backward computation.
I1107 03:36:44.579712 39477 net.cpp:220] conv1 needs backward computation.
I1107 03:36:44.579715 39477 net.cpp:222] label_data_1_split does not need backward computation.
I1107 03:36:44.579718 39477 net.cpp:222] data does not need backward computation.
I1107 03:36:44.579720 39477 net.cpp:264] This network produces output loss
I1107 03:36:44.579723 39477 net.cpp:264] This network produces output top-1
I1107 03:36:44.579747 39477 net.cpp:284] Network initialization done.
I1107 03:36:44.579839 39477 solver.cpp:63] Solver scaffolding done.
I1107 03:36:44.581034 39477 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/sparse.caffemodel
I1107 03:36:46.171059 39477 caffe_interface.cpp:527] Starting Optimization
I1107 03:36:46.171080 39477 solver.cpp:335] Solving 
I1107 03:36:46.171083 39477 solver.cpp:336] Learning Rate Policy: step
I1107 03:36:46.173000 39477 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 03:36:47.687790 39477 solver.cpp:517]     Test net output #0: loss = 0.201062 (* 1 = 0.201062 loss)
I1107 03:36:47.687811 39477 solver.cpp:517]     Test net output #1: top-1 = 0.952
I1107 03:36:47.943411 39477 solver.cpp:266] Iteration 0 (0 iter/s, 1.7722s/50 iter), loss = 0.00708158
I1107 03:36:47.943450 39477 solver.cpp:285]     Train net output #0: loss = 0.00708158 (* 1 = 0.00708158 loss)
I1107 03:36:47.943477 39477 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 03:37:00.352419 39477 solver.cpp:266] Iteration 50 (4.02953 iter/s, 12.4084s/50 iter), loss = 0.0761457
I1107 03:37:00.352450 39477 solver.cpp:285]     Train net output #0: loss = 0.0761457 (* 1 = 0.0761457 loss)
I1107 03:37:00.352458 39477 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 03:37:12.810025 39477 solver.cpp:266] Iteration 100 (4.01381 iter/s, 12.457s/50 iter), loss = 0.0358214
I1107 03:37:12.810271 39477 solver.cpp:285]     Train net output #0: loss = 0.0358214 (* 1 = 0.0358214 loss)
I1107 03:37:12.810281 39477 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 03:37:25.298519 39477 solver.cpp:266] Iteration 150 (4.00395 iter/s, 12.4877s/50 iter), loss = 0.0567347
I1107 03:37:25.298560 39477 solver.cpp:285]     Train net output #0: loss = 0.0567347 (* 1 = 0.0567347 loss)
I1107 03:37:25.298583 39477 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 03:37:37.819075 39477 solver.cpp:266] Iteration 200 (3.99363 iter/s, 12.5199s/50 iter), loss = 0.0445556
I1107 03:37:37.819108 39477 solver.cpp:285]     Train net output #0: loss = 0.0445556 (* 1 = 0.0445556 loss)
I1107 03:37:37.819113 39477 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 03:37:50.398893 39477 solver.cpp:266] Iteration 250 (3.97482 iter/s, 12.5792s/50 iter), loss = 0.0709874
I1107 03:37:50.398952 39477 solver.cpp:285]     Train net output #0: loss = 0.0709875 (* 1 = 0.0709875 loss)
I1107 03:37:50.398959 39477 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 03:38:03.042243 39477 solver.cpp:266] Iteration 300 (3.95485 iter/s, 12.6427s/50 iter), loss = 0.0651485
I1107 03:38:03.042284 39477 solver.cpp:285]     Train net output #0: loss = 0.0651485 (* 1 = 0.0651485 loss)
I1107 03:38:03.042289 39477 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 03:38:15.783829 39477 solver.cpp:266] Iteration 350 (3.92435 iter/s, 12.741s/50 iter), loss = 0.0786765
I1107 03:38:15.783860 39477 solver.cpp:285]     Train net output #0: loss = 0.0786766 (* 1 = 0.0786766 loss)
I1107 03:38:15.783881 39477 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 03:38:28.541851 39477 solver.cpp:266] Iteration 400 (3.91928 iter/s, 12.7575s/50 iter), loss = 0.0757035
I1107 03:38:28.542006 39477 solver.cpp:285]     Train net output #0: loss = 0.0757035 (* 1 = 0.0757035 loss)
I1107 03:38:28.542023 39477 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 03:38:41.305218 39477 solver.cpp:266] Iteration 450 (3.91766 iter/s, 12.7627s/50 iter), loss = 0.0497341
I1107 03:38:41.305248 39477 solver.cpp:285]     Train net output #0: loss = 0.0497341 (* 1 = 0.0497341 loss)
I1107 03:38:41.305254 39477 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 03:38:54.116257 39477 solver.cpp:266] Iteration 500 (3.90305 iter/s, 12.8105s/50 iter), loss = 0.0548487
I1107 03:38:54.116288 39477 solver.cpp:285]     Train net output #0: loss = 0.0548487 (* 1 = 0.0548487 loss)
I1107 03:38:54.116294 39477 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 03:39:06.884214 39477 solver.cpp:266] Iteration 550 (3.91622 iter/s, 12.7674s/50 iter), loss = 0.0475327
I1107 03:39:06.884383 39477 solver.cpp:285]     Train net output #0: loss = 0.0475327 (* 1 = 0.0475327 loss)
I1107 03:39:06.884392 39477 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 03:39:19.601085 39477 solver.cpp:266] Iteration 600 (3.93199 iter/s, 12.7162s/50 iter), loss = 0.0815808
I1107 03:39:19.601116 39477 solver.cpp:285]     Train net output #0: loss = 0.0815808 (* 1 = 0.0815808 loss)
I1107 03:39:19.601121 39477 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 03:39:32.409603 39477 solver.cpp:266] Iteration 650 (3.90382 iter/s, 12.808s/50 iter), loss = 0.0739278
I1107 03:39:32.409632 39477 solver.cpp:285]     Train net output #0: loss = 0.0739278 (* 1 = 0.0739278 loss)
I1107 03:39:32.409656 39477 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 03:39:45.205641 39477 solver.cpp:266] Iteration 700 (3.90762 iter/s, 12.7955s/50 iter), loss = 0.0498688
I1107 03:39:45.205807 39477 solver.cpp:285]     Train net output #0: loss = 0.0498688 (* 1 = 0.0498688 loss)
I1107 03:39:45.205816 39477 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 03:39:57.997741 39477 solver.cpp:266] Iteration 750 (3.90887 iter/s, 12.7914s/50 iter), loss = 0.0521854
I1107 03:39:57.997771 39477 solver.cpp:285]     Train net output #0: loss = 0.0521854 (* 1 = 0.0521854 loss)
I1107 03:39:57.997776 39477 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 03:40:10.763379 39477 solver.cpp:266] Iteration 800 (3.91693 iter/s, 12.7651s/50 iter), loss = 0.0889909
I1107 03:40:10.763422 39477 solver.cpp:285]     Train net output #0: loss = 0.0889909 (* 1 = 0.0889909 loss)
I1107 03:40:10.763427 39477 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 03:40:23.516103 39477 solver.cpp:266] Iteration 850 (3.9209 iter/s, 12.7522s/50 iter), loss = 0.0676827
I1107 03:40:23.517436 39477 solver.cpp:285]     Train net output #0: loss = 0.0676827 (* 1 = 0.0676827 loss)
I1107 03:40:23.517459 39477 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 03:40:36.310174 39477 solver.cpp:266] Iteration 900 (3.90862 iter/s, 12.7922s/50 iter), loss = 0.0789126
I1107 03:40:36.310204 39477 solver.cpp:285]     Train net output #0: loss = 0.0789126 (* 1 = 0.0789126 loss)
I1107 03:40:36.310209 39477 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 03:40:49.116349 39477 solver.cpp:266] Iteration 950 (3.90453 iter/s, 12.8056s/50 iter), loss = 0.0496611
I1107 03:40:49.116379 39477 solver.cpp:285]     Train net output #0: loss = 0.0496611 (* 1 = 0.0496611 loss)
I1107 03:40:49.116400 39477 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 03:41:01.664345 39477 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 03:41:03.195531 39477 solver.cpp:517]     Test net output #0: loss = 0.397341 (* 1 = 0.397341 loss)
I1107 03:41:03.195549 39477 solver.cpp:517]     Test net output #1: top-1 = 0.89325
I1107 03:41:03.445132 39477 solver.cpp:266] Iteration 1000 (3.48963 iter/s, 14.3282s/50 iter), loss = 0.0613177
I1107 03:41:03.445160 39477 solver.cpp:285]     Train net output #0: loss = 0.0613178 (* 1 = 0.0613178 loss)
I1107 03:41:03.445168 39477 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 03:41:16.265321 39477 solver.cpp:266] Iteration 1050 (3.90026 iter/s, 12.8197s/50 iter), loss = 0.0441235
I1107 03:41:16.265362 39477 solver.cpp:285]     Train net output #0: loss = 0.0441235 (* 1 = 0.0441235 loss)
I1107 03:41:16.265368 39477 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 03:41:29.038872 39477 solver.cpp:266] Iteration 1100 (3.91451 iter/s, 12.773s/50 iter), loss = 0.0788996
I1107 03:41:29.038902 39477 solver.cpp:285]     Train net output #0: loss = 0.0788996 (* 1 = 0.0788996 loss)
I1107 03:41:29.038919 39477 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 03:41:41.863289 39477 solver.cpp:266] Iteration 1150 (3.89898 iter/s, 12.8239s/50 iter), loss = 0.121697
I1107 03:41:41.863513 39477 solver.cpp:285]     Train net output #0: loss = 0.121698 (* 1 = 0.121698 loss)
I1107 03:41:41.863523 39477 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 03:41:54.677462 39477 solver.cpp:266] Iteration 1200 (3.90215 iter/s, 12.8134s/50 iter), loss = 0.0794641
I1107 03:41:54.677490 39477 solver.cpp:285]     Train net output #0: loss = 0.0794642 (* 1 = 0.0794642 loss)
I1107 03:41:54.677497 39477 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 03:42:07.458041 39477 solver.cpp:266] Iteration 1250 (3.91235 iter/s, 12.78s/50 iter), loss = 0.0442459
I1107 03:42:07.458070 39477 solver.cpp:285]     Train net output #0: loss = 0.044246 (* 1 = 0.044246 loss)
I1107 03:42:07.458076 39477 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 03:42:20.208837 39477 solver.cpp:266] Iteration 1300 (3.92149 iter/s, 12.7503s/50 iter), loss = 0.0683856
I1107 03:42:20.208986 39477 solver.cpp:285]     Train net output #0: loss = 0.0683857 (* 1 = 0.0683857 loss)
I1107 03:42:20.208994 39477 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 03:42:32.991360 39477 solver.cpp:266] Iteration 1350 (3.91179 iter/s, 12.7819s/50 iter), loss = 0.0545205
I1107 03:42:32.991400 39477 solver.cpp:285]     Train net output #0: loss = 0.0545205 (* 1 = 0.0545205 loss)
I1107 03:42:32.991407 39477 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 03:42:45.853129 39477 solver.cpp:266] Iteration 1400 (3.88766 iter/s, 12.8612s/50 iter), loss = 0.0843809
I1107 03:42:45.853159 39477 solver.cpp:285]     Train net output #0: loss = 0.0843809 (* 1 = 0.0843809 loss)
I1107 03:42:45.853180 39477 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 03:42:58.612613 39477 solver.cpp:266] Iteration 1450 (3.91882 iter/s, 12.7589s/50 iter), loss = 0.0818656
I1107 03:42:58.612782 39477 solver.cpp:285]     Train net output #0: loss = 0.0818657 (* 1 = 0.0818657 loss)
I1107 03:42:58.612790 39477 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 03:43:11.446511 39477 solver.cpp:266] Iteration 1500 (3.89614 iter/s, 12.8332s/50 iter), loss = 0.0456597
I1107 03:43:11.446540 39477 solver.cpp:285]     Train net output #0: loss = 0.0456597 (* 1 = 0.0456597 loss)
I1107 03:43:11.446547 39477 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 03:43:24.246666 39477 solver.cpp:266] Iteration 1550 (3.90637 iter/s, 12.7996s/50 iter), loss = 0.0838375
I1107 03:43:24.246697 39477 solver.cpp:285]     Train net output #0: loss = 0.0838375 (* 1 = 0.0838375 loss)
I1107 03:43:24.246702 39477 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 03:43:37.034479 39477 solver.cpp:266] Iteration 1600 (3.91014 iter/s, 12.7873s/50 iter), loss = 0.125312
I1107 03:43:37.034644 39477 solver.cpp:285]     Train net output #0: loss = 0.125312 (* 1 = 0.125312 loss)
I1107 03:43:37.034653 39477 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 03:43:49.826529 39477 solver.cpp:266] Iteration 1650 (3.90888 iter/s, 12.7914s/50 iter), loss = 0.0592119
I1107 03:43:49.826558 39477 solver.cpp:285]     Train net output #0: loss = 0.0592119 (* 1 = 0.0592119 loss)
I1107 03:43:49.826565 39477 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 03:44:02.605032 39477 solver.cpp:266] Iteration 1700 (3.91299 iter/s, 12.778s/50 iter), loss = 0.0935453
I1107 03:44:02.605063 39477 solver.cpp:285]     Train net output #0: loss = 0.0935453 (* 1 = 0.0935453 loss)
I1107 03:44:02.605068 39477 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 03:44:15.396709 39477 solver.cpp:266] Iteration 1750 (3.90896 iter/s, 12.7911s/50 iter), loss = 0.056415
I1107 03:44:15.396860 39477 solver.cpp:285]     Train net output #0: loss = 0.056415 (* 1 = 0.056415 loss)
I1107 03:44:15.396868 39477 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 03:44:28.208534 39477 solver.cpp:266] Iteration 1800 (3.90285 iter/s, 12.8112s/50 iter), loss = 0.0392589
I1107 03:44:28.208562 39477 solver.cpp:285]     Train net output #0: loss = 0.0392589 (* 1 = 0.0392589 loss)
I1107 03:44:28.208568 39477 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 03:44:40.999229 39477 solver.cpp:266] Iteration 1850 (3.90926 iter/s, 12.7902s/50 iter), loss = 0.0773082
I1107 03:44:40.999258 39477 solver.cpp:285]     Train net output #0: loss = 0.0773082 (* 1 = 0.0773082 loss)
I1107 03:44:40.999264 39477 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 03:44:53.771512 39477 solver.cpp:266] Iteration 1900 (3.91489 iter/s, 12.7717s/50 iter), loss = 0.0718782
I1107 03:44:53.771699 39477 solver.cpp:285]     Train net output #0: loss = 0.0718783 (* 1 = 0.0718783 loss)
I1107 03:44:53.771708 39477 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 03:45:06.522780 39477 solver.cpp:266] Iteration 1950 (3.92139 iter/s, 12.7506s/50 iter), loss = 0.180288
I1107 03:45:06.522809 39477 solver.cpp:285]     Train net output #0: loss = 0.180288 (* 1 = 0.180288 loss)
I1107 03:45:06.522814 39477 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 03:45:19.041776 39477 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 03:45:20.556452 39477 solver.cpp:517]     Test net output #0: loss = 0.210696 (* 1 = 0.210696 loss)
I1107 03:45:20.556468 39477 solver.cpp:517]     Test net output #1: top-1 = 0.9295
I1107 03:45:20.802307 39477 solver.cpp:266] Iteration 2000 (3.50166 iter/s, 14.2789s/50 iter), loss = 0.051588
I1107 03:45:20.802333 39477 solver.cpp:285]     Train net output #0: loss = 0.0515881 (* 1 = 0.0515881 loss)
I1107 03:45:20.802340 39477 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 03:45:33.592355 39477 solver.cpp:266] Iteration 2050 (3.90945 iter/s, 12.7895s/50 iter), loss = 0.0635237
I1107 03:45:33.592525 39477 solver.cpp:285]     Train net output #0: loss = 0.0635237 (* 1 = 0.0635237 loss)
I1107 03:45:33.592532 39477 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 03:45:46.398469 39477 solver.cpp:266] Iteration 2100 (3.90459 iter/s, 12.8054s/50 iter), loss = 0.0385952
I1107 03:45:46.398499 39477 solver.cpp:285]     Train net output #0: loss = 0.0385952 (* 1 = 0.0385952 loss)
I1107 03:45:46.398505 39477 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 03:45:59.138211 39477 solver.cpp:266] Iteration 2150 (3.92489 iter/s, 12.7392s/50 iter), loss = 0.0661851
I1107 03:45:59.138242 39477 solver.cpp:285]     Train net output #0: loss = 0.0661851 (* 1 = 0.0661851 loss)
I1107 03:45:59.138263 39477 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 03:46:11.895661 39477 solver.cpp:266] Iteration 2200 (3.91944 iter/s, 12.7569s/50 iter), loss = 0.0581043
I1107 03:46:11.895802 39477 solver.cpp:285]     Train net output #0: loss = 0.0581043 (* 1 = 0.0581043 loss)
I1107 03:46:11.895810 39477 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 03:46:24.648625 39477 solver.cpp:266] Iteration 2250 (3.92085 iter/s, 12.7523s/50 iter), loss = 0.0815158
I1107 03:46:24.648654 39477 solver.cpp:285]     Train net output #0: loss = 0.0815158 (* 1 = 0.0815158 loss)
I1107 03:46:24.648676 39477 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 03:46:37.386272 39477 solver.cpp:266] Iteration 2300 (3.92554 iter/s, 12.7371s/50 iter), loss = 0.067624
I1107 03:46:37.386302 39477 solver.cpp:285]     Train net output #0: loss = 0.0676241 (* 1 = 0.0676241 loss)
I1107 03:46:37.386308 39477 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 03:46:50.179767 39477 solver.cpp:266] Iteration 2350 (3.9084 iter/s, 12.793s/50 iter), loss = 0.0556197
I1107 03:46:50.179935 39477 solver.cpp:285]     Train net output #0: loss = 0.0556197 (* 1 = 0.0556197 loss)
I1107 03:46:50.179944 39477 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 03:47:02.975157 39477 solver.cpp:266] Iteration 2400 (3.90786 iter/s, 12.7947s/50 iter), loss = 0.0998774
I1107 03:47:02.975185 39477 solver.cpp:285]     Train net output #0: loss = 0.0998774 (* 1 = 0.0998774 loss)
I1107 03:47:02.975191 39477 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 03:47:15.750136 39477 solver.cpp:266] Iteration 2450 (3.91406 iter/s, 12.7744s/50 iter), loss = 0.105219
I1107 03:47:15.750165 39477 solver.cpp:285]     Train net output #0: loss = 0.105219 (* 1 = 0.105219 loss)
I1107 03:47:15.750171 39477 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 03:47:28.478483 39477 solver.cpp:266] Iteration 2500 (3.9284 iter/s, 12.7278s/50 iter), loss = 0.0476635
I1107 03:47:28.478655 39477 solver.cpp:285]     Train net output #0: loss = 0.0476636 (* 1 = 0.0476636 loss)
I1107 03:47:28.478663 39477 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 03:47:41.280589 39477 solver.cpp:266] Iteration 2550 (3.90581 iter/s, 12.8014s/50 iter), loss = 0.0442101
I1107 03:47:41.280617 39477 solver.cpp:285]     Train net output #0: loss = 0.0442101 (* 1 = 0.0442101 loss)
I1107 03:47:41.280624 39477 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 03:47:54.038759 39477 solver.cpp:266] Iteration 2600 (3.91922 iter/s, 12.7576s/50 iter), loss = 0.0351289
I1107 03:47:54.038789 39477 solver.cpp:285]     Train net output #0: loss = 0.035129 (* 1 = 0.035129 loss)
I1107 03:47:54.038794 39477 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 03:48:06.783483 39477 solver.cpp:266] Iteration 2650 (3.92336 iter/s, 12.7442s/50 iter), loss = 0.00736074
I1107 03:48:06.783638 39477 solver.cpp:285]     Train net output #0: loss = 0.00736079 (* 1 = 0.00736079 loss)
I1107 03:48:06.783645 39477 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 03:48:19.528832 39477 solver.cpp:266] Iteration 2700 (3.9232 iter/s, 12.7447s/50 iter), loss = 0.0244525
I1107 03:48:19.528865 39477 solver.cpp:285]     Train net output #0: loss = 0.0244526 (* 1 = 0.0244526 loss)
I1107 03:48:19.528872 39477 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 03:48:32.328703 39477 solver.cpp:266] Iteration 2750 (3.90645 iter/s, 12.7993s/50 iter), loss = 0.0205514
I1107 03:48:32.328733 39477 solver.cpp:285]     Train net output #0: loss = 0.0205514 (* 1 = 0.0205514 loss)
I1107 03:48:32.328740 39477 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 03:48:45.115118 39477 solver.cpp:266] Iteration 2800 (3.91056 iter/s, 12.7859s/50 iter), loss = 0.0195338
I1107 03:48:45.115279 39477 solver.cpp:285]     Train net output #0: loss = 0.0195339 (* 1 = 0.0195339 loss)
I1107 03:48:45.115289 39477 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 03:48:57.841223 39477 solver.cpp:266] Iteration 2850 (3.92914 iter/s, 12.7254s/50 iter), loss = 0.0177987
I1107 03:48:57.841253 39477 solver.cpp:285]     Train net output #0: loss = 0.0177987 (* 1 = 0.0177987 loss)
I1107 03:48:57.841259 39477 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 03:49:10.600076 39477 solver.cpp:266] Iteration 2900 (3.91901 iter/s, 12.7583s/50 iter), loss = 0.0603706
I1107 03:49:10.600108 39477 solver.cpp:285]     Train net output #0: loss = 0.0603707 (* 1 = 0.0603707 loss)
I1107 03:49:10.600114 39477 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 03:49:23.359653 39477 solver.cpp:266] Iteration 2950 (3.91879 iter/s, 12.759s/50 iter), loss = 0.0161452
I1107 03:49:23.359819 39477 solver.cpp:285]     Train net output #0: loss = 0.0161453 (* 1 = 0.0161453 loss)
I1107 03:49:23.359828 39477 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 03:49:35.848803 39477 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 03:49:37.361621 39477 solver.cpp:517]     Test net output #0: loss = 0.124832 (* 1 = 0.124832 loss)
I1107 03:49:37.361637 39477 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I1107 03:49:37.612679 39477 solver.cpp:266] Iteration 3000 (3.5082 iter/s, 14.2523s/50 iter), loss = 0.0160052
I1107 03:49:37.612704 39477 solver.cpp:285]     Train net output #0: loss = 0.0160052 (* 1 = 0.0160052 loss)
I1107 03:49:37.612710 39477 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 03:49:50.381399 39477 solver.cpp:266] Iteration 3050 (3.91598 iter/s, 12.7682s/50 iter), loss = 0.0278514
I1107 03:49:50.381438 39477 solver.cpp:285]     Train net output #0: loss = 0.0278515 (* 1 = 0.0278515 loss)
I1107 03:49:50.381444 39477 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 03:50:03.118643 39477 solver.cpp:266] Iteration 3100 (3.92566 iter/s, 12.7367s/50 iter), loss = 0.0296274
I1107 03:50:03.118778 39477 solver.cpp:285]     Train net output #0: loss = 0.0296275 (* 1 = 0.0296275 loss)
I1107 03:50:03.118786 39477 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 03:50:15.877236 39477 solver.cpp:266] Iteration 3150 (3.91912 iter/s, 12.758s/50 iter), loss = 0.00870236
I1107 03:50:15.877266 39477 solver.cpp:285]     Train net output #0: loss = 0.0087024 (* 1 = 0.0087024 loss)
I1107 03:50:15.877274 39477 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 03:50:28.684222 39477 solver.cpp:266] Iteration 3200 (3.90428 iter/s, 12.8065s/50 iter), loss = 0.0257872
I1107 03:50:28.684252 39477 solver.cpp:285]     Train net output #0: loss = 0.0257872 (* 1 = 0.0257872 loss)
I1107 03:50:28.684257 39477 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 03:50:41.417860 39477 solver.cpp:266] Iteration 3250 (3.92677 iter/s, 12.7331s/50 iter), loss = 0.0178181
I1107 03:50:41.418043 39477 solver.cpp:285]     Train net output #0: loss = 0.0178181 (* 1 = 0.0178181 loss)
I1107 03:50:41.418053 39477 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 03:50:54.169827 39477 solver.cpp:266] Iteration 3300 (3.92117 iter/s, 12.7513s/50 iter), loss = 0.027611
I1107 03:50:54.169855 39477 solver.cpp:285]     Train net output #0: loss = 0.0276111 (* 1 = 0.0276111 loss)
I1107 03:50:54.169862 39477 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 03:51:06.917465 39477 solver.cpp:266] Iteration 3350 (3.92246 iter/s, 12.7471s/50 iter), loss = 0.0345974
I1107 03:51:06.917496 39477 solver.cpp:285]     Train net output #0: loss = 0.0345974 (* 1 = 0.0345974 loss)
I1107 03:51:06.917518 39477 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I1107 03:51:19.684317 39477 solver.cpp:266] Iteration 3400 (3.91655 iter/s, 12.7663s/50 iter), loss = 0.0142175
I1107 03:51:19.684476 39477 solver.cpp:285]     Train net output #0: loss = 0.0142175 (* 1 = 0.0142175 loss)
I1107 03:51:19.684485 39477 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I1107 03:51:32.403456 39477 solver.cpp:266] Iteration 3450 (3.93129 iter/s, 12.7185s/50 iter), loss = 0.0288034
I1107 03:51:32.403499 39477 solver.cpp:285]     Train net output #0: loss = 0.0288034 (* 1 = 0.0288034 loss)
I1107 03:51:32.403506 39477 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I1107 03:51:45.168694 39477 solver.cpp:266] Iteration 3500 (3.91705 iter/s, 12.7647s/50 iter), loss = 0.0118989
I1107 03:51:45.168725 39477 solver.cpp:285]     Train net output #0: loss = 0.011899 (* 1 = 0.011899 loss)
I1107 03:51:45.168730 39477 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I1107 03:51:57.864265 39477 solver.cpp:266] Iteration 3550 (3.93854 iter/s, 12.695s/50 iter), loss = 0.0254578
I1107 03:51:57.864415 39477 solver.cpp:285]     Train net output #0: loss = 0.0254579 (* 1 = 0.0254579 loss)
I1107 03:51:57.864423 39477 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I1107 03:52:10.614375 39477 solver.cpp:266] Iteration 3600 (3.92173 iter/s, 12.7495s/50 iter), loss = 0.00531607
I1107 03:52:10.614406 39477 solver.cpp:285]     Train net output #0: loss = 0.00531611 (* 1 = 0.00531611 loss)
I1107 03:52:10.614413 39477 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I1107 03:52:23.366986 39477 solver.cpp:266] Iteration 3650 (3.92093 iter/s, 12.7521s/50 iter), loss = 0.028308
I1107 03:52:23.367013 39477 solver.cpp:285]     Train net output #0: loss = 0.028308 (* 1 = 0.028308 loss)
I1107 03:52:23.367019 39477 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I1107 03:52:36.102813 39477 solver.cpp:266] Iteration 3700 (3.92609 iter/s, 12.7353s/50 iter), loss = 0.027854
I1107 03:52:36.102960 39477 solver.cpp:285]     Train net output #0: loss = 0.027854 (* 1 = 0.027854 loss)
I1107 03:52:36.102968 39477 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I1107 03:52:48.832631 39477 solver.cpp:266] Iteration 3750 (3.92798 iter/s, 12.7292s/50 iter), loss = 0.0086795
I1107 03:52:48.832660 39477 solver.cpp:285]     Train net output #0: loss = 0.00867954 (* 1 = 0.00867954 loss)
I1107 03:52:48.832666 39477 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I1107 03:53:01.535532 39477 solver.cpp:266] Iteration 3800 (3.93627 iter/s, 12.7024s/50 iter), loss = 0.0214533
I1107 03:53:01.535562 39477 solver.cpp:285]     Train net output #0: loss = 0.0214533 (* 1 = 0.0214533 loss)
I1107 03:53:01.535568 39477 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I1107 03:53:14.277650 39477 solver.cpp:266] Iteration 3850 (3.92416 iter/s, 12.7416s/50 iter), loss = 0.00783587
I1107 03:53:14.277833 39477 solver.cpp:285]     Train net output #0: loss = 0.0078359 (* 1 = 0.0078359 loss)
I1107 03:53:14.277842 39477 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I1107 03:53:27.024561 39477 solver.cpp:266] Iteration 3900 (3.92273 iter/s, 12.7462s/50 iter), loss = 0.00288076
I1107 03:53:27.024591 39477 solver.cpp:285]     Train net output #0: loss = 0.00288079 (* 1 = 0.00288079 loss)
I1107 03:53:27.024597 39477 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I1107 03:53:39.751042 39477 solver.cpp:266] Iteration 3950 (3.92898 iter/s, 12.726s/50 iter), loss = 0.0080778
I1107 03:53:39.751071 39477 solver.cpp:285]     Train net output #0: loss = 0.00807782 (* 1 = 0.00807782 loss)
I1107 03:53:39.751077 39477 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I1107 03:53:52.205884 39477 solver.cpp:418] Iteration 4000, Testing net (#0)
I1107 03:53:53.711529 39477 solver.cpp:517]     Test net output #0: loss = 0.132563 (* 1 = 0.132563 loss)
I1107 03:53:53.711545 39477 solver.cpp:517]     Test net output #1: top-1 = 0.95325
I1107 03:53:53.956703 39477 solver.cpp:266] Iteration 4000 (3.51987 iter/s, 14.2051s/50 iter), loss = 0.00956721
I1107 03:53:53.956733 39477 solver.cpp:285]     Train net output #0: loss = 0.00956724 (* 1 = 0.00956724 loss)
I1107 03:53:53.956739 39477 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I1107 03:54:06.673112 39477 solver.cpp:266] Iteration 4050 (3.93209 iter/s, 12.7159s/50 iter), loss = 0.00677719
I1107 03:54:06.673141 39477 solver.cpp:285]     Train net output #0: loss = 0.00677721 (* 1 = 0.00677721 loss)
I1107 03:54:06.673148 39477 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I1107 03:54:19.373769 39477 solver.cpp:266] Iteration 4100 (3.93697 iter/s, 12.7001s/50 iter), loss = 0.00855801
I1107 03:54:19.373795 39477 solver.cpp:285]     Train net output #0: loss = 0.00855804 (* 1 = 0.00855804 loss)
I1107 03:54:19.373801 39477 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I1107 03:54:32.128012 39477 solver.cpp:266] Iteration 4150 (3.92042 iter/s, 12.7537s/50 iter), loss = 0.0289772
I1107 03:54:32.128160 39477 solver.cpp:285]     Train net output #0: loss = 0.0289772 (* 1 = 0.0289772 loss)
I1107 03:54:32.128168 39477 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I1107 03:54:44.826583 39477 solver.cpp:266] Iteration 4200 (3.93765 iter/s, 12.6979s/50 iter), loss = 0.00569366
I1107 03:54:44.826624 39477 solver.cpp:285]     Train net output #0: loss = 0.0056937 (* 1 = 0.0056937 loss)
I1107 03:54:44.826630 39477 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I1107 03:54:57.527516 39477 solver.cpp:266] Iteration 4250 (3.93688 iter/s, 12.7004s/50 iter), loss = 0.0119025
I1107 03:54:57.527556 39477 solver.cpp:285]     Train net output #0: loss = 0.0119025 (* 1 = 0.0119025 loss)
I1107 03:54:57.527562 39477 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I1107 03:55:10.269450 39477 solver.cpp:266] Iteration 4300 (3.92422 iter/s, 12.7414s/50 iter), loss = 0.00577749
I1107 03:55:10.269614 39477 solver.cpp:285]     Train net output #0: loss = 0.00577752 (* 1 = 0.00577752 loss)
I1107 03:55:10.269623 39477 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I1107 03:55:22.983093 39477 solver.cpp:266] Iteration 4350 (3.93299 iter/s, 12.713s/50 iter), loss = 0.0023275
I1107 03:55:22.983122 39477 solver.cpp:285]     Train net output #0: loss = 0.00232753 (* 1 = 0.00232753 loss)
I1107 03:55:22.983129 39477 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I1107 03:55:35.731889 39477 solver.cpp:266] Iteration 4400 (3.9221 iter/s, 12.7483s/50 iter), loss = 0.0163205
I1107 03:55:35.731917 39477 solver.cpp:285]     Train net output #0: loss = 0.0163205 (* 1 = 0.0163205 loss)
I1107 03:55:35.731925 39477 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I1107 03:55:48.426275 39477 solver.cpp:266] Iteration 4450 (3.93891 iter/s, 12.6939s/50 iter), loss = 0.0143074
I1107 03:55:48.426441 39477 solver.cpp:285]     Train net output #0: loss = 0.0143074 (* 1 = 0.0143074 loss)
I1107 03:55:48.426450 39477 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I1107 03:56:01.130705 39477 solver.cpp:266] Iteration 4500 (3.93584 iter/s, 12.7038s/50 iter), loss = 0.00773151
I1107 03:56:01.130733 39477 solver.cpp:285]     Train net output #0: loss = 0.00773154 (* 1 = 0.00773154 loss)
I1107 03:56:01.130739 39477 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I1107 03:56:13.868012 39477 solver.cpp:266] Iteration 4550 (3.92564 iter/s, 12.7368s/50 iter), loss = 0.00481087
I1107 03:56:13.868041 39477 solver.cpp:285]     Train net output #0: loss = 0.0048109 (* 1 = 0.0048109 loss)
I1107 03:56:13.868046 39477 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I1107 03:56:26.568608 39477 solver.cpp:266] Iteration 4600 (3.93698 iter/s, 12.7001s/50 iter), loss = 0.00168958
I1107 03:56:26.568812 39477 solver.cpp:285]     Train net output #0: loss = 0.00168961 (* 1 = 0.00168961 loss)
I1107 03:56:26.568821 39477 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I1107 03:56:39.304810 39477 solver.cpp:266] Iteration 4650 (3.92603 iter/s, 12.7355s/50 iter), loss = 0.0132385
I1107 03:56:39.304836 39477 solver.cpp:285]     Train net output #0: loss = 0.0132385 (* 1 = 0.0132385 loss)
I1107 03:56:39.304843 39477 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I1107 03:56:52.036952 39477 solver.cpp:266] Iteration 4700 (3.92723 iter/s, 12.7316s/50 iter), loss = 0.00631535
I1107 03:56:52.036979 39477 solver.cpp:285]     Train net output #0: loss = 0.00631537 (* 1 = 0.00631537 loss)
I1107 03:56:52.036984 39477 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I1107 03:57:04.770421 39477 solver.cpp:266] Iteration 4750 (3.92682 iter/s, 12.7329s/50 iter), loss = 0.00982195
I1107 03:57:04.770587 39477 solver.cpp:285]     Train net output #0: loss = 0.00982197 (* 1 = 0.00982197 loss)
I1107 03:57:04.770596 39477 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I1107 03:57:17.503137 39477 solver.cpp:266] Iteration 4800 (3.9271 iter/s, 12.7321s/50 iter), loss = 0.00328872
I1107 03:57:17.503167 39477 solver.cpp:285]     Train net output #0: loss = 0.00328874 (* 1 = 0.00328874 loss)
I1107 03:57:17.503173 39477 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I1107 03:57:30.198613 39477 solver.cpp:266] Iteration 4850 (3.93857 iter/s, 12.695s/50 iter), loss = 0.00171568
I1107 03:57:30.198644 39477 solver.cpp:285]     Train net output #0: loss = 0.0017157 (* 1 = 0.0017157 loss)
I1107 03:57:30.198652 39477 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I1107 03:57:42.920162 39477 solver.cpp:266] Iteration 4900 (3.9305 iter/s, 12.721s/50 iter), loss = 0.0177127
I1107 03:57:42.920318 39477 solver.cpp:285]     Train net output #0: loss = 0.0177127 (* 1 = 0.0177127 loss)
I1107 03:57:42.920326 39477 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I1107 03:57:55.678694 39477 solver.cpp:266] Iteration 4950 (3.91915 iter/s, 12.7579s/50 iter), loss = 0.00617038
I1107 03:57:55.678722 39477 solver.cpp:285]     Train net output #0: loss = 0.00617038 (* 1 = 0.00617038 loss)
I1107 03:57:55.678727 39477 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I1107 03:58:08.133760 39477 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_5000.caffemodel
I1107 03:58:10.495795 39477 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_5000.solverstate
I1107 03:58:10.973896 39477 solver.cpp:418] Iteration 5000, Testing net (#0)
I1107 03:58:12.441485 39477 solver.cpp:517]     Test net output #0: loss = 0.150904 (* 1 = 0.150904 loss)
I1107 03:58:12.441503 39477 solver.cpp:517]     Test net output #1: top-1 = 0.94925
I1107 03:58:12.682811 39477 solver.cpp:266] Iteration 5000 (2.94058 iter/s, 17.0034s/50 iter), loss = 0.0301682
I1107 03:58:12.682837 39477 solver.cpp:285]     Train net output #0: loss = 0.0301682 (* 1 = 0.0301682 loss)
I1107 03:58:12.682843 39477 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I1107 03:58:25.231029 39477 solver.cpp:266] Iteration 5050 (3.98479 iter/s, 12.5477s/50 iter), loss = 0.018732
I1107 03:58:25.231220 39477 solver.cpp:285]     Train net output #0: loss = 0.018732 (* 1 = 0.018732 loss)
I1107 03:58:25.231230 39477 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I1107 03:58:37.821763 39477 solver.cpp:266] Iteration 5100 (3.97139 iter/s, 12.5901s/50 iter), loss = 0.0128493
I1107 03:58:37.821791 39477 solver.cpp:285]     Train net output #0: loss = 0.0128493 (* 1 = 0.0128493 loss)
I1107 03:58:37.821799 39477 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I1107 03:58:50.453989 39477 solver.cpp:266] Iteration 5150 (3.95829 iter/s, 12.6317s/50 iter), loss = 0.0027232
I1107 03:58:50.454020 39477 solver.cpp:285]     Train net output #0: loss = 0.00272322 (* 1 = 0.00272322 loss)
I1107 03:58:50.454025 39477 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I1107 03:59:03.165956 39477 solver.cpp:266] Iteration 5200 (3.93346 iter/s, 12.7114s/50 iter), loss = 0.00438961
I1107 03:59:03.166129 39477 solver.cpp:285]     Train net output #0: loss = 0.00438962 (* 1 = 0.00438962 loss)
I1107 03:59:03.166137 39477 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I1107 03:59:15.898205 39477 solver.cpp:266] Iteration 5250 (3.92724 iter/s, 12.7316s/50 iter), loss = 0.00619272
I1107 03:59:15.898236 39477 solver.cpp:285]     Train net output #0: loss = 0.00619273 (* 1 = 0.00619273 loss)
I1107 03:59:15.898258 39477 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I1107 03:59:28.573052 39477 solver.cpp:266] Iteration 5300 (3.94498 iter/s, 12.6743s/50 iter), loss = 0.00586846
I1107 03:59:28.573081 39477 solver.cpp:285]     Train net output #0: loss = 0.00586848 (* 1 = 0.00586848 loss)
I1107 03:59:28.573088 39477 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I1107 03:59:41.260390 39477 solver.cpp:266] Iteration 5350 (3.9411 iter/s, 12.6868s/50 iter), loss = 0.0100102
I1107 03:59:41.260550 39477 solver.cpp:285]     Train net output #0: loss = 0.0100102 (* 1 = 0.0100102 loss)
I1107 03:59:41.260556 39477 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I1107 03:59:53.982043 39477 solver.cpp:266] Iteration 5400 (3.93051 iter/s, 12.721s/50 iter), loss = 0.0189204
I1107 03:59:53.982071 39477 solver.cpp:285]     Train net output #0: loss = 0.0189204 (* 1 = 0.0189204 loss)
I1107 03:59:53.982079 39477 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I1107 04:00:06.686862 39477 solver.cpp:266] Iteration 5450 (3.93568 iter/s, 12.7043s/50 iter), loss = 0.00190132
I1107 04:00:06.686893 39477 solver.cpp:285]     Train net output #0: loss = 0.00190132 (* 1 = 0.00190132 loss)
I1107 04:00:06.686898 39477 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I1107 04:00:19.333734 39477 solver.cpp:266] Iteration 5500 (3.95371 iter/s, 12.6464s/50 iter), loss = 0.00159957
I1107 04:00:19.333899 39477 solver.cpp:285]     Train net output #0: loss = 0.00159958 (* 1 = 0.00159958 loss)
I1107 04:00:19.333909 39477 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I1107 04:00:32.076748 39477 solver.cpp:266] Iteration 5550 (3.92392 iter/s, 12.7424s/50 iter), loss = 0.00440401
I1107 04:00:32.076779 39477 solver.cpp:285]     Train net output #0: loss = 0.00440402 (* 1 = 0.00440402 loss)
I1107 04:00:32.076786 39477 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I1107 04:00:44.773741 39477 solver.cpp:266] Iteration 5600 (3.9381 iter/s, 12.6965s/50 iter), loss = 0.0154661
I1107 04:00:44.773771 39477 solver.cpp:285]     Train net output #0: loss = 0.0154661 (* 1 = 0.0154661 loss)
I1107 04:00:44.773777 39477 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I1107 04:00:57.461838 39477 solver.cpp:266] Iteration 5650 (3.94086 iter/s, 12.6876s/50 iter), loss = 0.0043386
I1107 04:00:57.461982 39477 solver.cpp:285]     Train net output #0: loss = 0.0043386 (* 1 = 0.0043386 loss)
I1107 04:00:57.461990 39477 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I1107 04:01:10.177968 39477 solver.cpp:266] Iteration 5700 (3.93221 iter/s, 12.7155s/50 iter), loss = 0.0218408
I1107 04:01:10.178014 39477 solver.cpp:285]     Train net output #0: loss = 0.0218408 (* 1 = 0.0218408 loss)
I1107 04:01:10.178020 39477 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I1107 04:01:22.886549 39477 solver.cpp:266] Iteration 5750 (3.93452 iter/s, 12.708s/50 iter), loss = 0.0178435
I1107 04:01:22.886579 39477 solver.cpp:285]     Train net output #0: loss = 0.0178435 (* 1 = 0.0178435 loss)
I1107 04:01:22.886584 39477 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I1107 04:01:35.589295 39477 solver.cpp:266] Iteration 5800 (3.93632 iter/s, 12.7022s/50 iter), loss = 0.00767935
I1107 04:01:35.589601 39477 solver.cpp:285]     Train net output #0: loss = 0.00767936 (* 1 = 0.00767936 loss)
I1107 04:01:35.589610 39477 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I1107 04:01:48.299749 39477 solver.cpp:266] Iteration 5850 (3.93402 iter/s, 12.7097s/50 iter), loss = 0.00336207
I1107 04:01:48.299779 39477 solver.cpp:285]     Train net output #0: loss = 0.00336207 (* 1 = 0.00336207 loss)
I1107 04:01:48.299785 39477 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I1107 04:02:00.974082 39477 solver.cpp:266] Iteration 5900 (3.94514 iter/s, 12.6738s/50 iter), loss = 0.0118927
I1107 04:02:00.974112 39477 solver.cpp:285]     Train net output #0: loss = 0.0118927 (* 1 = 0.0118927 loss)
I1107 04:02:00.974118 39477 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I1107 04:02:13.684818 39477 solver.cpp:266] Iteration 5950 (3.93384 iter/s, 12.7102s/50 iter), loss = 0.0131203
I1107 04:02:13.684988 39477 solver.cpp:285]     Train net output #0: loss = 0.0131203 (* 1 = 0.0131203 loss)
I1107 04:02:13.684999 39477 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I1107 04:02:26.152273 39477 solver.cpp:418] Iteration 6000, Testing net (#0)
I1107 04:02:27.664441 39477 solver.cpp:517]     Test net output #0: loss = 0.15539 (* 1 = 0.15539 loss)
I1107 04:02:27.664459 39477 solver.cpp:517]     Test net output #1: top-1 = 0.95725
I1107 04:02:27.907359 39477 solver.cpp:266] Iteration 6000 (3.51572 iter/s, 14.2218s/50 iter), loss = 0.00169196
I1107 04:02:27.907389 39477 solver.cpp:285]     Train net output #0: loss = 0.00169198 (* 1 = 0.00169198 loss)
I1107 04:02:27.907395 39477 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I1107 04:02:40.584188 39477 solver.cpp:266] Iteration 6050 (3.94437 iter/s, 12.6763s/50 iter), loss = 0.00849682
I1107 04:02:40.584218 39477 solver.cpp:285]     Train net output #0: loss = 0.00849684 (* 1 = 0.00849684 loss)
I1107 04:02:40.584240 39477 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I1107 04:02:53.261503 39477 solver.cpp:266] Iteration 6100 (3.94421 iter/s, 12.6768s/50 iter), loss = 0.000828603
I1107 04:02:53.261660 39477 solver.cpp:285]     Train net output #0: loss = 0.000828622 (* 1 = 0.000828622 loss)
I1107 04:02:53.261669 39477 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I1107 04:03:05.999385 39477 solver.cpp:266] Iteration 6150 (3.9255 iter/s, 12.7372s/50 iter), loss = 0.00209265
I1107 04:03:05.999416 39477 solver.cpp:285]     Train net output #0: loss = 0.00209267 (* 1 = 0.00209267 loss)
I1107 04:03:05.999423 39477 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I1107 04:03:18.694025 39477 solver.cpp:266] Iteration 6200 (3.93883 iter/s, 12.6941s/50 iter), loss = 0.00164496
I1107 04:03:18.694066 39477 solver.cpp:285]     Train net output #0: loss = 0.00164498 (* 1 = 0.00164498 loss)
I1107 04:03:18.694072 39477 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I1107 04:03:31.372108 39477 solver.cpp:266] Iteration 6250 (3.94398 iter/s, 12.6776s/50 iter), loss = 0.00788861
I1107 04:03:31.372274 39477 solver.cpp:285]     Train net output #0: loss = 0.00788863 (* 1 = 0.00788863 loss)
I1107 04:03:31.372283 39477 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I1107 04:03:44.081485 39477 solver.cpp:266] Iteration 6300 (3.93431 iter/s, 12.7087s/50 iter), loss = 0.0139288
I1107 04:03:44.081512 39477 solver.cpp:285]     Train net output #0: loss = 0.0139288 (* 1 = 0.0139288 loss)
I1107 04:03:44.081518 39477 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I1107 04:03:56.779075 39477 solver.cpp:266] Iteration 6350 (3.93792 iter/s, 12.6971s/50 iter), loss = 0.00169455
I1107 04:03:56.779106 39477 solver.cpp:285]     Train net output #0: loss = 0.00169457 (* 1 = 0.00169457 loss)
I1107 04:03:56.779112 39477 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I1107 04:04:09.469880 39477 solver.cpp:266] Iteration 6400 (3.94002 iter/s, 12.6903s/50 iter), loss = 0.0123394
I1107 04:04:09.470072 39477 solver.cpp:285]     Train net output #0: loss = 0.0123395 (* 1 = 0.0123395 loss)
I1107 04:04:09.470120 39477 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I1107 04:04:22.180629 39477 solver.cpp:266] Iteration 6450 (3.9339 iter/s, 12.71s/50 iter), loss = 0.00835595
I1107 04:04:22.180670 39477 solver.cpp:285]     Train net output #0: loss = 0.00835597 (* 1 = 0.00835597 loss)
I1107 04:04:22.180692 39477 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I1107 04:04:34.910125 39477 solver.cpp:266] Iteration 6500 (3.92805 iter/s, 12.729s/50 iter), loss = 0.00440651
I1107 04:04:34.910154 39477 solver.cpp:285]     Train net output #0: loss = 0.00440654 (* 1 = 0.00440654 loss)
I1107 04:04:34.910161 39477 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I1107 04:04:47.628034 39477 solver.cpp:266] Iteration 6550 (3.93163 iter/s, 12.7174s/50 iter), loss = 0.0417249
I1107 04:04:47.628209 39477 solver.cpp:285]     Train net output #0: loss = 0.0417249 (* 1 = 0.0417249 loss)
I1107 04:04:47.628219 39477 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I1107 04:05:00.318477 39477 solver.cpp:266] Iteration 6600 (3.94018 iter/s, 12.6898s/50 iter), loss = 0.00696008
I1107 04:05:00.318506 39477 solver.cpp:285]     Train net output #0: loss = 0.0069601 (* 1 = 0.0069601 loss)
I1107 04:05:00.318512 39477 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I1107 04:05:13.025975 39477 solver.cpp:266] Iteration 6650 (3.93485 iter/s, 12.707s/50 iter), loss = 0.00159247
I1107 04:05:13.026005 39477 solver.cpp:285]     Train net output #0: loss = 0.00159249 (* 1 = 0.00159249 loss)
I1107 04:05:13.026011 39477 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I1107 04:05:25.762735 39477 solver.cpp:266] Iteration 6700 (3.92581 iter/s, 12.7362s/50 iter), loss = 0.00513544
I1107 04:05:25.762904 39477 solver.cpp:285]     Train net output #0: loss = 0.00513547 (* 1 = 0.00513547 loss)
I1107 04:05:25.762913 39477 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I1107 04:05:38.478780 39477 solver.cpp:266] Iteration 6750 (3.93224 iter/s, 12.7154s/50 iter), loss = 0.0118853
I1107 04:05:38.478813 39477 solver.cpp:285]     Train net output #0: loss = 0.0118853 (* 1 = 0.0118853 loss)
I1107 04:05:38.478821 39477 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I1107 04:05:51.190021 39477 solver.cpp:266] Iteration 6800 (3.93369 iter/s, 12.7107s/50 iter), loss = 0.00974293
I1107 04:05:51.190052 39477 solver.cpp:285]     Train net output #0: loss = 0.00974295 (* 1 = 0.00974295 loss)
I1107 04:05:51.190074 39477 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I1107 04:06:03.874903 39477 solver.cpp:266] Iteration 6850 (3.94186 iter/s, 12.6844s/50 iter), loss = 0.016992
I1107 04:06:03.875063 39477 solver.cpp:285]     Train net output #0: loss = 0.0169921 (* 1 = 0.0169921 loss)
I1107 04:06:03.875072 39477 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I1107 04:06:16.590492 39477 solver.cpp:266] Iteration 6900 (3.93238 iter/s, 12.7149s/50 iter), loss = 0.00131152
I1107 04:06:16.590523 39477 solver.cpp:285]     Train net output #0: loss = 0.00131154 (* 1 = 0.00131154 loss)
I1107 04:06:16.590544 39477 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I1107 04:06:29.279474 39477 solver.cpp:266] Iteration 6950 (3.94059 iter/s, 12.6885s/50 iter), loss = 0.0136717
I1107 04:06:29.279503 39477 solver.cpp:285]     Train net output #0: loss = 0.0136717 (* 1 = 0.0136717 loss)
I1107 04:06:29.279510 39477 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I1107 04:06:41.731863 39477 solver.cpp:418] Iteration 7000, Testing net (#0)
I1107 04:06:43.245880 39477 solver.cpp:517]     Test net output #0: loss = 0.172282 (* 1 = 0.172282 loss)
I1107 04:06:43.245896 39477 solver.cpp:517]     Test net output #1: top-1 = 0.957
I1107 04:06:43.494899 39477 solver.cpp:266] Iteration 7000 (3.51745 iter/s, 14.2148s/50 iter), loss = 0.00314984
I1107 04:06:43.494926 39477 solver.cpp:285]     Train net output #0: loss = 0.00314986 (* 1 = 0.00314986 loss)
I1107 04:06:43.494933 39477 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I1107 04:06:56.211563 39477 solver.cpp:266] Iteration 7050 (3.93201 iter/s, 12.7161s/50 iter), loss = 0.0018143
I1107 04:06:56.211593 39477 solver.cpp:285]     Train net output #0: loss = 0.00181432 (* 1 = 0.00181432 loss)
I1107 04:06:56.211599 39477 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I1107 04:07:08.918109 39477 solver.cpp:266] Iteration 7100 (3.93514 iter/s, 12.706s/50 iter), loss = 0.0109358
I1107 04:07:08.918150 39477 solver.cpp:285]     Train net output #0: loss = 0.0109358 (* 1 = 0.0109358 loss)
I1107 04:07:08.918156 39477 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I1107 04:07:21.607157 39477 solver.cpp:266] Iteration 7150 (3.94057 iter/s, 12.6885s/50 iter), loss = 0.0154631
I1107 04:07:21.607336 39477 solver.cpp:285]     Train net output #0: loss = 0.0154631 (* 1 = 0.0154631 loss)
I1107 04:07:21.607344 39477 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I1107 04:07:34.323261 39477 solver.cpp:266] Iteration 7200 (3.93223 iter/s, 12.7154s/50 iter), loss = 0.00162423
I1107 04:07:34.323290 39477 solver.cpp:285]     Train net output #0: loss = 0.00162424 (* 1 = 0.00162424 loss)
I1107 04:07:34.323312 39477 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I1107 04:07:47.040925 39477 solver.cpp:266] Iteration 7250 (3.9317 iter/s, 12.7171s/50 iter), loss = 0.00251621
I1107 04:07:47.040953 39477 solver.cpp:285]     Train net output #0: loss = 0.00251623 (* 1 = 0.00251623 loss)
I1107 04:07:47.040959 39477 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I1107 04:07:59.743988 39477 solver.cpp:266] Iteration 7300 (3.93622 iter/s, 12.7025s/50 iter), loss = 0.0159172
I1107 04:07:59.744143 39477 solver.cpp:285]     Train net output #0: loss = 0.0159172 (* 1 = 0.0159172 loss)
I1107 04:07:59.744151 39477 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I1107 04:08:12.452263 39477 solver.cpp:266] Iteration 7350 (3.93464 iter/s, 12.7076s/50 iter), loss = 0.000856257
I1107 04:08:12.452293 39477 solver.cpp:285]     Train net output #0: loss = 0.000856275 (* 1 = 0.000856275 loss)
I1107 04:08:12.452298 39477 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I1107 04:08:25.137346 39477 solver.cpp:266] Iteration 7400 (3.9418 iter/s, 12.6846s/50 iter), loss = 0.00647533
I1107 04:08:25.137384 39477 solver.cpp:285]     Train net output #0: loss = 0.00647535 (* 1 = 0.00647535 loss)
I1107 04:08:25.137408 39477 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I1107 04:08:37.835958 39477 solver.cpp:266] Iteration 7450 (3.9376 iter/s, 12.6981s/50 iter), loss = 0.00115881
I1107 04:08:37.836109 39477 solver.cpp:285]     Train net output #0: loss = 0.00115882 (* 1 = 0.00115882 loss)
I1107 04:08:37.836117 39477 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I1107 04:08:50.541672 39477 solver.cpp:266] Iteration 7500 (3.93544 iter/s, 12.7051s/50 iter), loss = 0.00763529
I1107 04:08:50.541702 39477 solver.cpp:285]     Train net output #0: loss = 0.0076353 (* 1 = 0.0076353 loss)
I1107 04:08:50.541707 39477 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I1107 04:09:03.243191 39477 solver.cpp:266] Iteration 7550 (3.9367 iter/s, 12.701s/50 iter), loss = 0.00313478
I1107 04:09:03.243221 39477 solver.cpp:285]     Train net output #0: loss = 0.00313479 (* 1 = 0.00313479 loss)
I1107 04:09:03.243227 39477 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I1107 04:09:15.964004 39477 solver.cpp:266] Iteration 7600 (3.93073 iter/s, 12.7203s/50 iter), loss = 0.00218751
I1107 04:09:15.964146 39477 solver.cpp:285]     Train net output #0: loss = 0.00218753 (* 1 = 0.00218753 loss)
I1107 04:09:15.964155 39477 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I1107 04:09:28.661419 39477 solver.cpp:266] Iteration 7650 (3.93801 iter/s, 12.6968s/50 iter), loss = 0.00802242
I1107 04:09:28.661448 39477 solver.cpp:285]     Train net output #0: loss = 0.00802244 (* 1 = 0.00802244 loss)
I1107 04:09:28.661453 39477 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I1107 04:09:41.385864 39477 solver.cpp:266] Iteration 7700 (3.92961 iter/s, 12.7239s/50 iter), loss = 0.0135118
I1107 04:09:41.385893 39477 solver.cpp:285]     Train net output #0: loss = 0.0135118 (* 1 = 0.0135118 loss)
I1107 04:09:41.385900 39477 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I1107 04:09:54.092049 39477 solver.cpp:266] Iteration 7750 (3.93525 iter/s, 12.7057s/50 iter), loss = 0.00439871
I1107 04:09:54.093156 39477 solver.cpp:285]     Train net output #0: loss = 0.00439873 (* 1 = 0.00439873 loss)
I1107 04:09:54.093165 39477 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I1107 04:10:06.778522 39477 solver.cpp:266] Iteration 7800 (3.9417 iter/s, 12.6849s/50 iter), loss = 0.00259426
I1107 04:10:06.778566 39477 solver.cpp:285]     Train net output #0: loss = 0.00259429 (* 1 = 0.00259429 loss)
I1107 04:10:06.778590 39477 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I1107 04:10:19.485002 39477 solver.cpp:266] Iteration 7850 (3.93516 iter/s, 12.706s/50 iter), loss = 0.0111992
I1107 04:10:19.485031 39477 solver.cpp:285]     Train net output #0: loss = 0.0111993 (* 1 = 0.0111993 loss)
I1107 04:10:19.485038 39477 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I1107 04:10:32.192800 39477 solver.cpp:266] Iteration 7900 (3.93475 iter/s, 12.7073s/50 iter), loss = 0.010007
I1107 04:10:32.193172 39477 solver.cpp:285]     Train net output #0: loss = 0.0100071 (* 1 = 0.0100071 loss)
I1107 04:10:32.193181 39477 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I1107 04:10:44.899497 39477 solver.cpp:266] Iteration 7950 (3.9352 iter/s, 12.7058s/50 iter), loss = 0.0023778
I1107 04:10:44.899525 39477 solver.cpp:285]     Train net output #0: loss = 0.00237783 (* 1 = 0.00237783 loss)
I1107 04:10:44.899531 39477 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I1107 04:10:57.355188 39477 solver.cpp:418] Iteration 8000, Testing net (#0)
I1107 04:10:58.868706 39477 solver.cpp:517]     Test net output #0: loss = 0.18415 (* 1 = 0.18415 loss)
I1107 04:10:58.868722 39477 solver.cpp:517]     Test net output #1: top-1 = 0.9565
I1107 04:10:59.114194 39477 solver.cpp:266] Iteration 8000 (3.51763 iter/s, 14.2141s/50 iter), loss = 0.0012467
I1107 04:10:59.114219 39477 solver.cpp:285]     Train net output #0: loss = 0.00124673 (* 1 = 0.00124673 loss)
I1107 04:10:59.114225 39477 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I1107 04:11:11.785737 39477 solver.cpp:266] Iteration 8050 (3.94601 iter/s, 12.671s/50 iter), loss = 0.00580272
I1107 04:11:11.785905 39477 solver.cpp:285]     Train net output #0: loss = 0.00580275 (* 1 = 0.00580275 loss)
I1107 04:11:11.785914 39477 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I1107 04:11:24.501307 39477 solver.cpp:266] Iteration 8100 (3.93239 iter/s, 12.7149s/50 iter), loss = 0.0104077
I1107 04:11:24.501338 39477 solver.cpp:285]     Train net output #0: loss = 0.0104078 (* 1 = 0.0104078 loss)
I1107 04:11:24.501360 39477 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I1107 04:11:37.207713 39477 solver.cpp:266] Iteration 8150 (3.93519 iter/s, 12.7059s/50 iter), loss = 0.00177084
I1107 04:11:37.207742 39477 solver.cpp:285]     Train net output #0: loss = 0.00177087 (* 1 = 0.00177087 loss)
I1107 04:11:37.207748 39477 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I1107 04:11:49.934283 39477 solver.cpp:266] Iteration 8200 (3.92895 iter/s, 12.726s/50 iter), loss = 0.016465
I1107 04:11:49.934433 39477 solver.cpp:285]     Train net output #0: loss = 0.0164651 (* 1 = 0.0164651 loss)
I1107 04:11:49.934442 39477 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I1107 04:12:02.653694 39477 solver.cpp:266] Iteration 8250 (3.9312 iter/s, 12.7188s/50 iter), loss = 0.00219369
I1107 04:12:02.653725 39477 solver.cpp:285]     Train net output #0: loss = 0.00219371 (* 1 = 0.00219371 loss)
I1107 04:12:02.653731 39477 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I1107 04:12:15.341684 39477 solver.cpp:266] Iteration 8300 (3.9409 iter/s, 12.6875s/50 iter), loss = 0.00165827
I1107 04:12:15.341715 39477 solver.cpp:285]     Train net output #0: loss = 0.00165829 (* 1 = 0.00165829 loss)
I1107 04:12:15.341722 39477 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I1107 04:12:28.023527 39477 solver.cpp:266] Iteration 8350 (3.94281 iter/s, 12.6813s/50 iter), loss = 0.00178905
I1107 04:12:28.023710 39477 solver.cpp:285]     Train net output #0: loss = 0.00178907 (* 1 = 0.00178907 loss)
I1107 04:12:28.023720 39477 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I1107 04:12:40.735019 39477 solver.cpp:266] Iteration 8400 (3.93366 iter/s, 12.7108s/50 iter), loss = 0.00248232
I1107 04:12:40.735049 39477 solver.cpp:285]     Train net output #0: loss = 0.00248234 (* 1 = 0.00248234 loss)
I1107 04:12:40.735055 39477 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I1107 04:12:53.427644 39477 solver.cpp:266] Iteration 8450 (3.93946 iter/s, 12.6921s/50 iter), loss = 0.00166083
I1107 04:12:53.427683 39477 solver.cpp:285]     Train net output #0: loss = 0.00166085 (* 1 = 0.00166085 loss)
I1107 04:12:53.427690 39477 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I1107 04:13:06.130477 39477 solver.cpp:266] Iteration 8500 (3.93629 iter/s, 12.7023s/50 iter), loss = 0.00181257
I1107 04:13:06.130625 39477 solver.cpp:285]     Train net output #0: loss = 0.0018126 (* 1 = 0.0018126 loss)
I1107 04:13:06.130632 39477 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I1107 04:13:18.838984 39477 solver.cpp:266] Iteration 8550 (3.93457 iter/s, 12.7079s/50 iter), loss = 0.00599545
I1107 04:13:18.839013 39477 solver.cpp:285]     Train net output #0: loss = 0.00599548 (* 1 = 0.00599548 loss)
I1107 04:13:18.839035 39477 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I1107 04:13:31.550487 39477 solver.cpp:266] Iteration 8600 (3.93361 iter/s, 12.711s/50 iter), loss = 0.00145173
I1107 04:13:31.550516 39477 solver.cpp:285]     Train net output #0: loss = 0.00145176 (* 1 = 0.00145176 loss)
I1107 04:13:31.550539 39477 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I1107 04:13:44.251626 39477 solver.cpp:266] Iteration 8650 (3.93682 iter/s, 12.7006s/50 iter), loss = 0.0070509
I1107 04:13:44.251765 39477 solver.cpp:285]     Train net output #0: loss = 0.00705092 (* 1 = 0.00705092 loss)
I1107 04:13:44.251775 39477 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I1107 04:13:56.933156 39477 solver.cpp:266] Iteration 8700 (3.94294 iter/s, 12.6809s/50 iter), loss = 0.00228713
I1107 04:13:56.933185 39477 solver.cpp:285]     Train net output #0: loss = 0.00228715 (* 1 = 0.00228715 loss)
I1107 04:13:56.933192 39477 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I1107 04:14:09.628290 39477 solver.cpp:266] Iteration 8750 (3.93868 iter/s, 12.6946s/50 iter), loss = 0.00655579
I1107 04:14:09.628336 39477 solver.cpp:285]     Train net output #0: loss = 0.00655581 (* 1 = 0.00655581 loss)
I1107 04:14:09.628343 39477 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I1107 04:14:22.371002 39477 solver.cpp:266] Iteration 8800 (3.92398 iter/s, 12.7422s/50 iter), loss = 0.00405586
I1107 04:14:22.371161 39477 solver.cpp:285]     Train net output #0: loss = 0.00405588 (* 1 = 0.00405588 loss)
I1107 04:14:22.371171 39477 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I1107 04:14:35.065011 39477 solver.cpp:266] Iteration 8850 (3.93907 iter/s, 12.6934s/50 iter), loss = 0.00152815
I1107 04:14:35.065042 39477 solver.cpp:285]     Train net output #0: loss = 0.00152817 (* 1 = 0.00152817 loss)
I1107 04:14:35.065048 39477 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I1107 04:14:47.800595 39477 solver.cpp:266] Iteration 8900 (3.92617 iter/s, 12.7351s/50 iter), loss = 0.0159877
I1107 04:14:47.800626 39477 solver.cpp:285]     Train net output #0: loss = 0.0159878 (* 1 = 0.0159878 loss)
I1107 04:14:47.800632 39477 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I1107 04:15:00.492836 39477 solver.cpp:266] Iteration 8950 (3.93958 iter/s, 12.6917s/50 iter), loss = 0.0030177
I1107 04:15:00.492987 39477 solver.cpp:285]     Train net output #0: loss = 0.00301772 (* 1 = 0.00301772 loss)
I1107 04:15:00.493005 39477 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I1107 04:15:12.931560 39477 solver.cpp:418] Iteration 9000, Testing net (#0)
I1107 04:15:14.415400 39477 solver.cpp:517]     Test net output #0: loss = 0.190623 (* 1 = 0.190623 loss)
I1107 04:15:14.415416 39477 solver.cpp:517]     Test net output #1: top-1 = 0.95725
I1107 04:15:14.660140 39477 solver.cpp:266] Iteration 9000 (3.52943 iter/s, 14.1666s/50 iter), loss = 0.00676624
I1107 04:15:14.660179 39477 solver.cpp:285]     Train net output #0: loss = 0.00676626 (* 1 = 0.00676626 loss)
I1107 04:15:14.660187 39477 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I1107 04:15:27.357893 39477 solver.cpp:266] Iteration 9050 (3.93787 iter/s, 12.6972s/50 iter), loss = 0.00316788
I1107 04:15:27.357923 39477 solver.cpp:285]     Train net output #0: loss = 0.0031679 (* 1 = 0.0031679 loss)
I1107 04:15:27.357928 39477 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I1107 04:15:40.095851 39477 solver.cpp:266] Iteration 9100 (3.92544 iter/s, 12.7374s/50 iter), loss = 0.00604477
I1107 04:15:40.096026 39477 solver.cpp:285]     Train net output #0: loss = 0.00604479 (* 1 = 0.00604479 loss)
I1107 04:15:40.096046 39477 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I1107 04:15:52.811962 39477 solver.cpp:266] Iteration 9150 (3.93223 iter/s, 12.7154s/50 iter), loss = 0.00988048
I1107 04:15:52.811992 39477 solver.cpp:285]     Train net output #0: loss = 0.0098805 (* 1 = 0.0098805 loss)
I1107 04:15:52.811998 39477 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I1107 04:16:05.518577 39477 solver.cpp:266] Iteration 9200 (3.93512 iter/s, 12.7061s/50 iter), loss = 0.00403266
I1107 04:16:05.518609 39477 solver.cpp:285]     Train net output #0: loss = 0.00403268 (* 1 = 0.00403268 loss)
I1107 04:16:05.518615 39477 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I1107 04:16:18.241420 39477 solver.cpp:266] Iteration 9250 (3.9301 iter/s, 12.7223s/50 iter), loss = 0.000997026
I1107 04:16:18.241612 39477 solver.cpp:285]     Train net output #0: loss = 0.000997049 (* 1 = 0.000997049 loss)
I1107 04:16:18.241621 39477 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I1107 04:16:30.964512 39477 solver.cpp:266] Iteration 9300 (3.93007 iter/s, 12.7224s/50 iter), loss = 0.00160751
I1107 04:16:30.964541 39477 solver.cpp:285]     Train net output #0: loss = 0.00160753 (* 1 = 0.00160753 loss)
I1107 04:16:30.964565 39477 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I1107 04:16:43.677423 39477 solver.cpp:266] Iteration 9350 (3.93317 iter/s, 12.7124s/50 iter), loss = 0.00222339
I1107 04:16:43.677453 39477 solver.cpp:285]     Train net output #0: loss = 0.00222341 (* 1 = 0.00222341 loss)
I1107 04:16:43.677470 39477 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I1107 04:16:56.375952 39477 solver.cpp:266] Iteration 9400 (3.93763 iter/s, 12.698s/50 iter), loss = 0.00103498
I1107 04:16:56.376111 39477 solver.cpp:285]     Train net output #0: loss = 0.00103499 (* 1 = 0.00103499 loss)
I1107 04:16:56.376121 39477 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I1107 04:17:09.101938 39477 solver.cpp:266] Iteration 9450 (3.92917 iter/s, 12.7253s/50 iter), loss = 0.0176068
I1107 04:17:09.101969 39477 solver.cpp:285]     Train net output #0: loss = 0.0176068 (* 1 = 0.0176068 loss)
I1107 04:17:09.101990 39477 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I1107 04:17:21.803787 39477 solver.cpp:266] Iteration 9500 (3.9366 iter/s, 12.7013s/50 iter), loss = 0.0077261
I1107 04:17:21.803817 39477 solver.cpp:285]     Train net output #0: loss = 0.00772612 (* 1 = 0.00772612 loss)
I1107 04:17:21.803839 39477 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I1107 04:17:34.522258 39477 solver.cpp:266] Iteration 9550 (3.93145 iter/s, 12.7179s/50 iter), loss = 0.00581013
I1107 04:17:34.522428 39477 solver.cpp:285]     Train net output #0: loss = 0.00581015 (* 1 = 0.00581015 loss)
I1107 04:17:34.522438 39477 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I1107 04:17:47.170265 39477 solver.cpp:266] Iteration 9600 (3.9534 iter/s, 12.6473s/50 iter), loss = 0.00121619
I1107 04:17:47.170305 39477 solver.cpp:285]     Train net output #0: loss = 0.00121621 (* 1 = 0.00121621 loss)
I1107 04:17:47.170327 39477 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I1107 04:17:59.898841 39477 solver.cpp:266] Iteration 9650 (3.92833 iter/s, 12.728s/50 iter), loss = 0.00929903
I1107 04:17:59.898871 39477 solver.cpp:285]     Train net output #0: loss = 0.00929905 (* 1 = 0.00929905 loss)
I1107 04:17:59.898878 39477 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I1107 04:18:12.623337 39477 solver.cpp:266] Iteration 9700 (3.92959 iter/s, 12.724s/50 iter), loss = 0.00233824
I1107 04:18:12.623528 39477 solver.cpp:285]     Train net output #0: loss = 0.00233826 (* 1 = 0.00233826 loss)
I1107 04:18:12.623539 39477 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I1107 04:18:25.363473 39477 solver.cpp:266] Iteration 9750 (3.92481 iter/s, 12.7395s/50 iter), loss = 0.00684947
I1107 04:18:25.363502 39477 solver.cpp:285]     Train net output #0: loss = 0.00684949 (* 1 = 0.00684949 loss)
I1107 04:18:25.363508 39477 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I1107 04:18:38.080323 39477 solver.cpp:266] Iteration 9800 (3.93195 iter/s, 12.7163s/50 iter), loss = 0.00184528
I1107 04:18:38.080353 39477 solver.cpp:285]     Train net output #0: loss = 0.00184531 (* 1 = 0.00184531 loss)
I1107 04:18:38.080375 39477 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I1107 04:18:50.789113 39477 solver.cpp:266] Iteration 9850 (3.93445 iter/s, 12.7083s/50 iter), loss = 0.00550683
I1107 04:18:50.789264 39477 solver.cpp:285]     Train net output #0: loss = 0.00550685 (* 1 = 0.00550685 loss)
I1107 04:18:50.789273 39477 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I1107 04:19:03.468886 39477 solver.cpp:266] Iteration 9900 (3.94349 iter/s, 12.6791s/50 iter), loss = 0.00308054
I1107 04:19:03.468914 39477 solver.cpp:285]     Train net output #0: loss = 0.00308057 (* 1 = 0.00308057 loss)
I1107 04:19:03.468921 39477 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I1107 04:19:16.217839 39477 solver.cpp:266] Iteration 9950 (3.92205 iter/s, 12.7484s/50 iter), loss = 0.0060893
I1107 04:19:16.217871 39477 solver.cpp:285]     Train net output #0: loss = 0.00608933 (* 1 = 0.00608933 loss)
I1107 04:19:16.217877 39477 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I1107 04:19:28.645202 39477 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_10000.caffemodel
I1107 04:19:30.933166 39477 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_10000.solverstate
I1107 04:19:31.412422 39477 solver.cpp:418] Iteration 10000, Testing net (#0)
I1107 04:19:32.877905 39477 solver.cpp:517]     Test net output #0: loss = 0.193762 (* 1 = 0.193762 loss)
I1107 04:19:32.877923 39477 solver.cpp:517]     Test net output #1: top-1 = 0.95675
I1107 04:19:33.119009 39477 solver.cpp:266] Iteration 10000 (2.95849 iter/s, 16.9005s/50 iter), loss = 0.00616943
I1107 04:19:33.119035 39477 solver.cpp:285]     Train net output #0: loss = 0.00616947 (* 1 = 0.00616947 loss)
I1107 04:19:33.119058 39477 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I1107 04:19:45.670647 39477 solver.cpp:266] Iteration 10050 (3.98371 iter/s, 12.5511s/50 iter), loss = 0.0179556
I1107 04:19:45.670678 39477 solver.cpp:285]     Train net output #0: loss = 0.0179557 (* 1 = 0.0179557 loss)
I1107 04:19:45.670686 39477 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I1107 04:19:58.280095 39477 solver.cpp:266] Iteration 10100 (3.96544 iter/s, 12.6089s/50 iter), loss = 0.00366929
I1107 04:19:58.280128 39477 solver.cpp:285]     Train net output #0: loss = 0.00366932 (* 1 = 0.00366932 loss)
I1107 04:19:58.280133 39477 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I1107 04:20:10.920065 39477 solver.cpp:266] Iteration 10150 (3.95587 iter/s, 12.6394s/50 iter), loss = 0.00140733
I1107 04:20:10.920212 39477 solver.cpp:285]     Train net output #0: loss = 0.00140735 (* 1 = 0.00140735 loss)
I1107 04:20:10.920220 39477 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I1107 04:20:23.630594 39477 solver.cpp:266] Iteration 10200 (3.93394 iter/s, 12.7099s/50 iter), loss = 0.00570114
I1107 04:20:23.630623 39477 solver.cpp:285]     Train net output #0: loss = 0.00570117 (* 1 = 0.00570117 loss)
I1107 04:20:23.630630 39477 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I1107 04:20:36.333981 39477 solver.cpp:266] Iteration 10250 (3.93612 iter/s, 12.7029s/50 iter), loss = 0.016509
I1107 04:20:36.334008 39477 solver.cpp:285]     Train net output #0: loss = 0.016509 (* 1 = 0.016509 loss)
I1107 04:20:36.334013 39477 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I1107 04:20:49.086555 39477 solver.cpp:266] Iteration 10300 (3.92094 iter/s, 12.7521s/50 iter), loss = 0.00216084
I1107 04:20:49.086771 39477 solver.cpp:285]     Train net output #0: loss = 0.00216087 (* 1 = 0.00216087 loss)
I1107 04:20:49.086798 39477 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I1107 04:21:01.787397 39477 solver.cpp:266] Iteration 10350 (3.93696 iter/s, 12.7002s/50 iter), loss = 0.00889145
I1107 04:21:01.787436 39477 solver.cpp:285]     Train net output #0: loss = 0.00889148 (* 1 = 0.00889148 loss)
I1107 04:21:01.787444 39477 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I1107 04:21:14.544281 39477 solver.cpp:266] Iteration 10400 (3.91962 iter/s, 12.7564s/50 iter), loss = 0.00133968
I1107 04:21:14.544309 39477 solver.cpp:285]     Train net output #0: loss = 0.00133971 (* 1 = 0.00133971 loss)
I1107 04:21:14.544315 39477 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I1107 04:21:27.254882 39477 solver.cpp:266] Iteration 10450 (3.93389 iter/s, 12.7101s/50 iter), loss = 0.00236905
I1107 04:21:27.255064 39477 solver.cpp:285]     Train net output #0: loss = 0.00236908 (* 1 = 0.00236908 loss)
I1107 04:21:27.255072 39477 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I1107 04:21:39.909657 39477 solver.cpp:266] Iteration 10500 (3.95129 iter/s, 12.6541s/50 iter), loss = 0.00434634
I1107 04:21:39.909688 39477 solver.cpp:285]     Train net output #0: loss = 0.00434638 (* 1 = 0.00434638 loss)
I1107 04:21:39.909694 39477 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I1107 04:21:52.664372 39477 solver.cpp:266] Iteration 10550 (3.92028 iter/s, 12.7542s/50 iter), loss = 0.00343478
I1107 04:21:52.664410 39477 solver.cpp:285]     Train net output #0: loss = 0.00343481 (* 1 = 0.00343481 loss)
I1107 04:21:52.664433 39477 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I1107 04:22:05.345633 39477 solver.cpp:266] Iteration 10600 (3.94299 iter/s, 12.6807s/50 iter), loss = 0.00407597
I1107 04:22:05.345801 39477 solver.cpp:285]     Train net output #0: loss = 0.004076 (* 1 = 0.004076 loss)
I1107 04:22:05.345810 39477 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I1107 04:22:18.064368 39477 solver.cpp:266] Iteration 10650 (3.93141 iter/s, 12.7181s/50 iter), loss = 0.00369383
I1107 04:22:18.064396 39477 solver.cpp:285]     Train net output #0: loss = 0.00369386 (* 1 = 0.00369386 loss)
I1107 04:22:18.064404 39477 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I1107 04:22:30.743877 39477 solver.cpp:266] Iteration 10700 (3.94353 iter/s, 12.679s/50 iter), loss = 0.00722013
I1107 04:22:30.743907 39477 solver.cpp:285]     Train net output #0: loss = 0.00722016 (* 1 = 0.00722016 loss)
I1107 04:22:30.743929 39477 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I1107 04:22:43.504709 39477 solver.cpp:266] Iteration 10750 (3.9184 iter/s, 12.7603s/50 iter), loss = 0.00396165
I1107 04:22:43.504863 39477 solver.cpp:285]     Train net output #0: loss = 0.00396167 (* 1 = 0.00396167 loss)
I1107 04:22:43.504873 39477 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I1107 04:22:56.220311 39477 solver.cpp:266] Iteration 10800 (3.93238 iter/s, 12.715s/50 iter), loss = 0.0120715
I1107 04:22:56.220340 39477 solver.cpp:285]     Train net output #0: loss = 0.0120715 (* 1 = 0.0120715 loss)
I1107 04:22:56.220346 39477 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I1107 04:23:08.882894 39477 solver.cpp:266] Iteration 10850 (3.9488 iter/s, 12.6621s/50 iter), loss = 0.0115826
I1107 04:23:08.882921 39477 solver.cpp:285]     Train net output #0: loss = 0.0115826 (* 1 = 0.0115826 loss)
I1107 04:23:08.882927 39477 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I1107 04:23:21.649686 39477 solver.cpp:266] Iteration 10900 (3.91657 iter/s, 12.7663s/50 iter), loss = 0.005483
I1107 04:23:21.649857 39477 solver.cpp:285]     Train net output #0: loss = 0.00548302 (* 1 = 0.00548302 loss)
I1107 04:23:21.649868 39477 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I1107 04:23:34.349973 39477 solver.cpp:266] Iteration 10950 (3.93712 iter/s, 12.6996s/50 iter), loss = 0.00117835
I1107 04:23:34.350003 39477 solver.cpp:285]     Train net output #0: loss = 0.00117837 (* 1 = 0.00117837 loss)
I1107 04:23:34.350009 39477 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I1107 04:23:46.760577 39477 solver.cpp:418] Iteration 11000, Testing net (#0)
I1107 04:23:48.280680 39477 solver.cpp:517]     Test net output #0: loss = 0.195827 (* 1 = 0.195827 loss)
I1107 04:23:48.280697 39477 solver.cpp:517]     Test net output #1: top-1 = 0.9565
I1107 04:23:48.533344 39477 solver.cpp:266] Iteration 11000 (3.5254 iter/s, 14.1828s/50 iter), loss = 0.00146786
I1107 04:23:48.533368 39477 solver.cpp:285]     Train net output #0: loss = 0.00146788 (* 1 = 0.00146788 loss)
I1107 04:23:48.533375 39477 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I1107 04:24:01.224051 39477 solver.cpp:266] Iteration 11050 (3.94005 iter/s, 12.6902s/50 iter), loss = 0.00475145
I1107 04:24:01.224416 39477 solver.cpp:285]     Train net output #0: loss = 0.00475148 (* 1 = 0.00475148 loss)
I1107 04:24:01.224436 39477 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I1107 04:24:13.940906 39477 solver.cpp:266] Iteration 11100 (3.93205 iter/s, 12.716s/50 iter), loss = 0.0106554
I1107 04:24:13.940934 39477 solver.cpp:285]     Train net output #0: loss = 0.0106555 (* 1 = 0.0106555 loss)
I1107 04:24:13.940940 39477 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I1107 04:24:26.661511 39477 solver.cpp:266] Iteration 11150 (3.93079 iter/s, 12.7201s/50 iter), loss = 0.00313047
I1107 04:24:26.661542 39477 solver.cpp:285]     Train net output #0: loss = 0.00313049 (* 1 = 0.00313049 loss)
I1107 04:24:26.661548 39477 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I1107 04:24:39.368800 39477 solver.cpp:266] Iteration 11200 (3.93491 iter/s, 12.7068s/50 iter), loss = 0.00559791
I1107 04:24:39.368970 39477 solver.cpp:285]     Train net output #0: loss = 0.00559793 (* 1 = 0.00559793 loss)
I1107 04:24:39.368978 39477 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I1107 04:24:52.083182 39477 solver.cpp:266] Iteration 11250 (3.93276 iter/s, 12.7137s/50 iter), loss = 0.00913567
I1107 04:24:52.083212 39477 solver.cpp:285]     Train net output #0: loss = 0.0091357 (* 1 = 0.0091357 loss)
I1107 04:24:52.083218 39477 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I1107 04:25:04.756917 39477 solver.cpp:266] Iteration 11300 (3.94533 iter/s, 12.6732s/50 iter), loss = 0.00165602
I1107 04:25:04.756947 39477 solver.cpp:285]     Train net output #0: loss = 0.00165605 (* 1 = 0.00165605 loss)
I1107 04:25:04.756953 39477 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I1107 04:25:17.463867 39477 solver.cpp:266] Iteration 11350 (3.93502 iter/s, 12.7064s/50 iter), loss = 0.0109503
I1107 04:25:17.464038 39477 solver.cpp:285]     Train net output #0: loss = 0.0109503 (* 1 = 0.0109503 loss)
I1107 04:25:17.464046 39477 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I1107 04:25:30.190250 39477 solver.cpp:266] Iteration 11400 (3.92905 iter/s, 12.7257s/50 iter), loss = 0.00771323
I1107 04:25:30.190279 39477 solver.cpp:285]     Train net output #0: loss = 0.00771326 (* 1 = 0.00771326 loss)
I1107 04:25:30.190285 39477 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I1107 04:25:42.898480 39477 solver.cpp:266] Iteration 11450 (3.93462 iter/s, 12.7077s/50 iter), loss = 0.00749487
I1107 04:25:42.898511 39477 solver.cpp:285]     Train net output #0: loss = 0.0074949 (* 1 = 0.0074949 loss)
I1107 04:25:42.898517 39477 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I1107 04:25:55.618376 39477 solver.cpp:266] Iteration 11500 (3.93101 iter/s, 12.7194s/50 iter), loss = 0.00292431
I1107 04:25:55.619138 39477 solver.cpp:285]     Train net output #0: loss = 0.00292434 (* 1 = 0.00292434 loss)
I1107 04:25:55.619145 39477 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I1107 04:26:08.345191 39477 solver.cpp:266] Iteration 11550 (3.9291 iter/s, 12.7256s/50 iter), loss = 0.00362515
I1107 04:26:08.345221 39477 solver.cpp:285]     Train net output #0: loss = 0.00362517 (* 1 = 0.00362517 loss)
I1107 04:26:08.345228 39477 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I1107 04:26:21.076407 39477 solver.cpp:266] Iteration 11600 (3.92752 iter/s, 12.7307s/50 iter), loss = 0.00230834
I1107 04:26:21.076436 39477 solver.cpp:285]     Train net output #0: loss = 0.00230837 (* 1 = 0.00230837 loss)
I1107 04:26:21.076442 39477 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I1107 04:26:33.778417 39477 solver.cpp:266] Iteration 11650 (3.93655 iter/s, 12.7015s/50 iter), loss = 0.00149564
I1107 04:26:33.778595 39477 solver.cpp:285]     Train net output #0: loss = 0.00149567 (* 1 = 0.00149567 loss)
I1107 04:26:33.778604 39477 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I1107 04:26:46.491778 39477 solver.cpp:266] Iteration 11700 (3.93308 iter/s, 12.7127s/50 iter), loss = 0.0026277
I1107 04:26:46.491808 39477 solver.cpp:285]     Train net output #0: loss = 0.00262774 (* 1 = 0.00262774 loss)
I1107 04:26:46.491832 39477 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I1107 04:26:59.184950 39477 solver.cpp:266] Iteration 11750 (3.93929 iter/s, 12.6926s/50 iter), loss = 0.00102662
I1107 04:26:59.184980 39477 solver.cpp:285]     Train net output #0: loss = 0.00102665 (* 1 = 0.00102665 loss)
I1107 04:26:59.184986 39477 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I1107 04:27:11.935552 39477 solver.cpp:266] Iteration 11800 (3.92154 iter/s, 12.7501s/50 iter), loss = 0.00676756
I1107 04:27:11.935703 39477 solver.cpp:285]     Train net output #0: loss = 0.00676759 (* 1 = 0.00676759 loss)
I1107 04:27:11.935710 39477 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I1107 04:27:24.631003 39477 solver.cpp:266] Iteration 11850 (3.93862 iter/s, 12.6948s/50 iter), loss = 0.00461929
I1107 04:27:24.631034 39477 solver.cpp:285]     Train net output #0: loss = 0.00461932 (* 1 = 0.00461932 loss)
I1107 04:27:24.631040 39477 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I1107 04:27:37.361826 39477 solver.cpp:266] Iteration 11900 (3.92764 iter/s, 12.7303s/50 iter), loss = 0.00278124
I1107 04:27:37.361860 39477 solver.cpp:285]     Train net output #0: loss = 0.00278127 (* 1 = 0.00278127 loss)
I1107 04:27:37.361866 39477 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I1107 04:27:50.041610 39477 solver.cpp:266] Iteration 11950 (3.94345 iter/s, 12.6793s/50 iter), loss = 0.0018179
I1107 04:27:50.042894 39477 solver.cpp:285]     Train net output #0: loss = 0.00181794 (* 1 = 0.00181794 loss)
I1107 04:27:50.042917 39477 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I1107 04:28:02.535071 39477 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_12000.caffemodel
I1107 04:28:04.796953 39477 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.2/snapshots/_iter_12000.solverstate
I1107 04:28:05.375967 39477 solver.cpp:378] Iteration 12000, loss = 0.00106078
I1107 04:28:05.375988 39477 solver.cpp:418] Iteration 12000, Testing net (#0)
I1107 04:28:06.834072 39477 solver.cpp:517]     Test net output #0: loss = 0.196597 (* 1 = 0.196597 loss)
I1107 04:28:06.834087 39477 solver.cpp:517]     Test net output #1: top-1 = 0.9565
I1107 04:28:06.834090 39477 solver.cpp:386] Optimization Done (3.91679 iter/s).
I1107 04:28:06.834094 39477 caffe_interface.cpp:530] Optimization Done.
I1107 04:28:07.802533 39802 pruning_runner.cpp:190] Sens info found, use it.
I1107 04:28:09.012120 39802 pruning_runner.cpp:217] Start compressing, please wait...
I1107 04:28:15.015238 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:21.093524 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:27.186271 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:33.419930 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:39.666854 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:45.717334 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:51.858948 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:28:57.983235 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:29:04.114984 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:29:10.410562 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:29:16.666692 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:29:22.785842 39802 pruning_runner.cpp:264] Compression complete 0%
I1107 04:29:28.944900 39802 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 04:29:28.945204 39802 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 04:29:28.945541 39802 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 04:29:28.945709 39802 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 04:29:28.945830 39802 layer_factory.hpp:77] Creating layer data
I1107 04:29:28.945871 39802 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:28.946224 39802 net.cpp:94] Creating Layer data
I1107 04:29:28.946233 39802 net.cpp:409] data -> data
I1107 04:29:28.946245 39802 net.cpp:409] data -> label
I1107 04:29:28.947350 41014 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 04:29:28.947381 41014 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 04:29:28.947603 39802 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 04:29:28.947672 39802 data_layer.cpp:83] output data size: 50,3,227,227
I1107 04:29:29.032008 39802 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:29.032078 39802 net.cpp:144] Setting up data
I1107 04:29:29.032085 39802 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 04:29:29.032090 39802 net.cpp:151] Top shape: 50 (50)
I1107 04:29:29.032093 39802 net.cpp:159] Memory required for data: 30917600
I1107 04:29:29.032097 39802 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 04:29:29.032106 39802 net.cpp:94] Creating Layer label_data_1_split
I1107 04:29:29.032114 39802 net.cpp:435] label_data_1_split <- label
I1107 04:29:29.032120 39802 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 04:29:29.032130 39802 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 04:29:29.032234 39802 net.cpp:144] Setting up label_data_1_split
I1107 04:29:29.032239 39802 net.cpp:151] Top shape: 50 (50)
I1107 04:29:29.032258 39802 net.cpp:151] Top shape: 50 (50)
I1107 04:29:29.032259 39802 net.cpp:159] Memory required for data: 30918000
I1107 04:29:29.032263 39802 layer_factory.hpp:77] Creating layer conv1
I1107 04:29:29.032272 39802 net.cpp:94] Creating Layer conv1
I1107 04:29:29.032276 39802 net.cpp:435] conv1 <- data
I1107 04:29:29.032281 39802 net.cpp:409] conv1 -> conv1
I1107 04:29:29.033947 39802 net.cpp:144] Setting up conv1
I1107 04:29:29.033958 39802 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:29.033960 39802 net.cpp:159] Memory required for data: 88998000
I1107 04:29:29.033987 39802 layer_factory.hpp:77] Creating layer bn1
I1107 04:29:29.033994 39802 net.cpp:94] Creating Layer bn1
I1107 04:29:29.033998 39802 net.cpp:435] bn1 <- conv1
I1107 04:29:29.034003 39802 net.cpp:409] bn1 -> scale1
I1107 04:29:29.034767 39802 net.cpp:144] Setting up bn1
I1107 04:29:29.034775 39802 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:29.034778 39802 net.cpp:159] Memory required for data: 147078000
I1107 04:29:29.034790 39802 layer_factory.hpp:77] Creating layer relu1
I1107 04:29:29.034796 39802 net.cpp:94] Creating Layer relu1
I1107 04:29:29.034801 39802 net.cpp:435] relu1 <- scale1
I1107 04:29:29.034806 39802 net.cpp:409] relu1 -> relu1
I1107 04:29:29.034871 39802 net.cpp:144] Setting up relu1
I1107 04:29:29.034878 39802 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:29.034880 39802 net.cpp:159] Memory required for data: 205158000
I1107 04:29:29.034883 39802 layer_factory.hpp:77] Creating layer pool1
I1107 04:29:29.034888 39802 net.cpp:94] Creating Layer pool1
I1107 04:29:29.034891 39802 net.cpp:435] pool1 <- relu1
I1107 04:29:29.034895 39802 net.cpp:409] pool1 -> pool1
I1107 04:29:29.034941 39802 net.cpp:144] Setting up pool1
I1107 04:29:29.034946 39802 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 04:29:29.034950 39802 net.cpp:159] Memory required for data: 219154800
I1107 04:29:29.034952 39802 layer_factory.hpp:77] Creating layer conv2
I1107 04:29:29.034960 39802 net.cpp:94] Creating Layer conv2
I1107 04:29:29.034965 39802 net.cpp:435] conv2 <- pool1
I1107 04:29:29.034970 39802 net.cpp:409] conv2 -> conv2
I1107 04:29:29.041877 39802 net.cpp:144] Setting up conv2
I1107 04:29:29.041893 39802 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:29.041896 39802 net.cpp:159] Memory required for data: 256479600
I1107 04:29:29.041908 39802 layer_factory.hpp:77] Creating layer bn2
I1107 04:29:29.041916 39802 net.cpp:94] Creating Layer bn2
I1107 04:29:29.041920 39802 net.cpp:435] bn2 <- conv2
I1107 04:29:29.041926 39802 net.cpp:409] bn2 -> scale2
I1107 04:29:29.044970 39802 net.cpp:144] Setting up bn2
I1107 04:29:29.044978 39802 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:29.044981 39802 net.cpp:159] Memory required for data: 293804400
I1107 04:29:29.044989 39802 layer_factory.hpp:77] Creating layer relu2
I1107 04:29:29.044996 39802 net.cpp:94] Creating Layer relu2
I1107 04:29:29.044999 39802 net.cpp:435] relu2 <- scale2
I1107 04:29:29.045003 39802 net.cpp:409] relu2 -> relu2
I1107 04:29:29.045023 39802 net.cpp:144] Setting up relu2
I1107 04:29:29.045028 39802 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:29.045030 39802 net.cpp:159] Memory required for data: 331129200
I1107 04:29:29.045033 39802 layer_factory.hpp:77] Creating layer pool2
I1107 04:29:29.045039 39802 net.cpp:94] Creating Layer pool2
I1107 04:29:29.045042 39802 net.cpp:435] pool2 <- relu2
I1107 04:29:29.045047 39802 net.cpp:409] pool2 -> pool2
I1107 04:29:29.047422 39802 net.cpp:144] Setting up pool2
I1107 04:29:29.047428 39802 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:29.047430 39802 net.cpp:159] Memory required for data: 339782000
I1107 04:29:29.047432 39802 layer_factory.hpp:77] Creating layer conv3
I1107 04:29:29.047448 39802 net.cpp:94] Creating Layer conv3
I1107 04:29:29.047451 39802 net.cpp:435] conv3 <- pool2
I1107 04:29:29.047456 39802 net.cpp:409] conv3 -> conv3
I1107 04:29:29.061333 39802 net.cpp:144] Setting up conv3
I1107 04:29:29.061357 39802 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:29.061379 39802 net.cpp:159] Memory required for data: 352761200
I1107 04:29:29.061388 39802 layer_factory.hpp:77] Creating layer relu3
I1107 04:29:29.061396 39802 net.cpp:94] Creating Layer relu3
I1107 04:29:29.061400 39802 net.cpp:435] relu3 <- conv3
I1107 04:29:29.061406 39802 net.cpp:409] relu3 -> relu3
I1107 04:29:29.061432 39802 net.cpp:144] Setting up relu3
I1107 04:29:29.061436 39802 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:29.061439 39802 net.cpp:159] Memory required for data: 365740400
I1107 04:29:29.061441 39802 layer_factory.hpp:77] Creating layer conv4
I1107 04:29:29.061451 39802 net.cpp:94] Creating Layer conv4
I1107 04:29:29.061455 39802 net.cpp:435] conv4 <- relu3
I1107 04:29:29.061460 39802 net.cpp:409] conv4 -> conv4
I1107 04:29:29.077631 39802 net.cpp:144] Setting up conv4
I1107 04:29:29.077675 39802 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:29.077682 39802 net.cpp:159] Memory required for data: 378719600
I1107 04:29:29.077699 39802 layer_factory.hpp:77] Creating layer relu4
I1107 04:29:29.077711 39802 net.cpp:94] Creating Layer relu4
I1107 04:29:29.077718 39802 net.cpp:435] relu4 <- conv4
I1107 04:29:29.077728 39802 net.cpp:409] relu4 -> relu4
I1107 04:29:29.077781 39802 net.cpp:144] Setting up relu4
I1107 04:29:29.077790 39802 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:29.077795 39802 net.cpp:159] Memory required for data: 391698800
I1107 04:29:29.077801 39802 layer_factory.hpp:77] Creating layer conv5
I1107 04:29:29.077817 39802 net.cpp:94] Creating Layer conv5
I1107 04:29:29.077833 39802 net.cpp:435] conv5 <- relu4
I1107 04:29:29.077842 39802 net.cpp:409] conv5 -> conv5
I1107 04:29:29.088325 39802 net.cpp:144] Setting up conv5
I1107 04:29:29.088351 39802 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:29.088353 39802 net.cpp:159] Memory required for data: 400351600
I1107 04:29:29.088363 39802 layer_factory.hpp:77] Creating layer relu5
I1107 04:29:29.088371 39802 net.cpp:94] Creating Layer relu5
I1107 04:29:29.088376 39802 net.cpp:435] relu5 <- conv5
I1107 04:29:29.088382 39802 net.cpp:409] relu5 -> relu5
I1107 04:29:29.088416 39802 net.cpp:144] Setting up relu5
I1107 04:29:29.088423 39802 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:29.088426 39802 net.cpp:159] Memory required for data: 409004400
I1107 04:29:29.088430 39802 layer_factory.hpp:77] Creating layer pool5
I1107 04:29:29.088441 39802 net.cpp:94] Creating Layer pool5
I1107 04:29:29.088445 39802 net.cpp:435] pool5 <- relu5
I1107 04:29:29.088451 39802 net.cpp:409] pool5 -> pool5
I1107 04:29:29.088486 39802 net.cpp:144] Setting up pool5
I1107 04:29:29.088493 39802 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 04:29:29.088497 39802 net.cpp:159] Memory required for data: 410847600
I1107 04:29:29.088501 39802 layer_factory.hpp:77] Creating layer fc6
I1107 04:29:29.088511 39802 net.cpp:94] Creating Layer fc6
I1107 04:29:29.088516 39802 net.cpp:435] fc6 <- pool5
I1107 04:29:29.088523 39802 net.cpp:409] fc6 -> fc6
I1107 04:29:29.387624 39802 net.cpp:144] Setting up fc6
I1107 04:29:29.387645 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.387648 39802 net.cpp:159] Memory required for data: 411666800
I1107 04:29:29.387657 39802 layer_factory.hpp:77] Creating layer relu6
I1107 04:29:29.387665 39802 net.cpp:94] Creating Layer relu6
I1107 04:29:29.387670 39802 net.cpp:435] relu6 <- fc6
I1107 04:29:29.387676 39802 net.cpp:409] relu6 -> relu6
I1107 04:29:29.387712 39802 net.cpp:144] Setting up relu6
I1107 04:29:29.387717 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.387719 39802 net.cpp:159] Memory required for data: 412486000
I1107 04:29:29.387723 39802 layer_factory.hpp:77] Creating layer drop6
I1107 04:29:29.387730 39802 net.cpp:94] Creating Layer drop6
I1107 04:29:29.387749 39802 net.cpp:435] drop6 <- relu6
I1107 04:29:29.387754 39802 net.cpp:409] drop6 -> drop6
I1107 04:29:29.387781 39802 net.cpp:144] Setting up drop6
I1107 04:29:29.387785 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.387789 39802 net.cpp:159] Memory required for data: 413305200
I1107 04:29:29.387802 39802 layer_factory.hpp:77] Creating layer fc7
I1107 04:29:29.387811 39802 net.cpp:94] Creating Layer fc7
I1107 04:29:29.387814 39802 net.cpp:435] fc7 <- drop6
I1107 04:29:29.387820 39802 net.cpp:409] fc7 -> fc7
I1107 04:29:29.519359 39802 net.cpp:144] Setting up fc7
I1107 04:29:29.519394 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.519397 39802 net.cpp:159] Memory required for data: 414124400
I1107 04:29:29.519405 39802 layer_factory.hpp:77] Creating layer bn7
I1107 04:29:29.519418 39802 net.cpp:94] Creating Layer bn7
I1107 04:29:29.519423 39802 net.cpp:435] bn7 <- fc7
I1107 04:29:29.519431 39802 net.cpp:409] bn7 -> scale7
I1107 04:29:29.519917 39802 net.cpp:144] Setting up bn7
I1107 04:29:29.519922 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.519927 39802 net.cpp:159] Memory required for data: 414943600
I1107 04:29:29.519937 39802 layer_factory.hpp:77] Creating layer relu7
I1107 04:29:29.519942 39802 net.cpp:94] Creating Layer relu7
I1107 04:29:29.519946 39802 net.cpp:435] relu7 <- scale7
I1107 04:29:29.519953 39802 net.cpp:409] relu7 -> relu7
I1107 04:29:29.519970 39802 net.cpp:144] Setting up relu7
I1107 04:29:29.519975 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.519979 39802 net.cpp:159] Memory required for data: 415762800
I1107 04:29:29.519981 39802 layer_factory.hpp:77] Creating layer drop7
I1107 04:29:29.519989 39802 net.cpp:94] Creating Layer drop7
I1107 04:29:29.519992 39802 net.cpp:435] drop7 <- relu7
I1107 04:29:29.519997 39802 net.cpp:409] drop7 -> drop7
I1107 04:29:29.520021 39802 net.cpp:144] Setting up drop7
I1107 04:29:29.520026 39802 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:29.520030 39802 net.cpp:159] Memory required for data: 416582000
I1107 04:29:29.520032 39802 layer_factory.hpp:77] Creating layer fc8
I1107 04:29:29.520040 39802 net.cpp:94] Creating Layer fc8
I1107 04:29:29.520043 39802 net.cpp:435] fc8 <- drop7
I1107 04:29:29.520048 39802 net.cpp:409] fc8 -> fc8
I1107 04:29:29.520869 39802 net.cpp:144] Setting up fc8
I1107 04:29:29.520880 39802 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:29.520884 39802 net.cpp:159] Memory required for data: 416582400
I1107 04:29:29.520890 39802 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 04:29:29.520898 39802 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 04:29:29.520903 39802 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 04:29:29.520910 39802 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 04:29:29.520918 39802 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 04:29:29.520946 39802 net.cpp:144] Setting up fc8_fc8_0_split
I1107 04:29:29.520951 39802 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:29.520954 39802 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:29.520957 39802 net.cpp:159] Memory required for data: 416583200
I1107 04:29:29.520961 39802 layer_factory.hpp:77] Creating layer loss
I1107 04:29:29.520968 39802 net.cpp:94] Creating Layer loss
I1107 04:29:29.520972 39802 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 04:29:29.520977 39802 net.cpp:435] loss <- label_data_1_split_0
I1107 04:29:29.520982 39802 net.cpp:409] loss -> loss
I1107 04:29:29.520992 39802 layer_factory.hpp:77] Creating layer loss
I1107 04:29:29.521056 39802 net.cpp:144] Setting up loss
I1107 04:29:29.521060 39802 net.cpp:151] Top shape: (1)
I1107 04:29:29.521064 39802 net.cpp:154]     with loss weight 1
I1107 04:29:29.521075 39802 net.cpp:159] Memory required for data: 416583204
I1107 04:29:29.521077 39802 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 04:29:29.521085 39802 net.cpp:94] Creating Layer accuracy-top1
I1107 04:29:29.521087 39802 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 04:29:29.521092 39802 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 04:29:29.521098 39802 net.cpp:409] accuracy-top1 -> top-1
I1107 04:29:29.521107 39802 net.cpp:144] Setting up accuracy-top1
I1107 04:29:29.521111 39802 net.cpp:151] Top shape: (1)
I1107 04:29:29.521114 39802 net.cpp:159] Memory required for data: 416583208
I1107 04:29:29.521126 39802 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 04:29:29.521131 39802 net.cpp:220] loss needs backward computation.
I1107 04:29:29.521134 39802 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 04:29:29.521139 39802 net.cpp:220] fc8 needs backward computation.
I1107 04:29:29.521143 39802 net.cpp:220] drop7 needs backward computation.
I1107 04:29:29.521147 39802 net.cpp:220] relu7 needs backward computation.
I1107 04:29:29.521150 39802 net.cpp:220] bn7 needs backward computation.
I1107 04:29:29.521155 39802 net.cpp:220] fc7 needs backward computation.
I1107 04:29:29.521158 39802 net.cpp:220] drop6 needs backward computation.
I1107 04:29:29.521162 39802 net.cpp:220] relu6 needs backward computation.
I1107 04:29:29.521167 39802 net.cpp:220] fc6 needs backward computation.
I1107 04:29:29.521172 39802 net.cpp:220] pool5 needs backward computation.
I1107 04:29:29.521175 39802 net.cpp:220] relu5 needs backward computation.
I1107 04:29:29.521178 39802 net.cpp:220] conv5 needs backward computation.
I1107 04:29:29.521183 39802 net.cpp:220] relu4 needs backward computation.
I1107 04:29:29.521186 39802 net.cpp:220] conv4 needs backward computation.
I1107 04:29:29.521190 39802 net.cpp:220] relu3 needs backward computation.
I1107 04:29:29.521193 39802 net.cpp:220] conv3 needs backward computation.
I1107 04:29:29.521198 39802 net.cpp:220] pool2 needs backward computation.
I1107 04:29:29.521201 39802 net.cpp:220] relu2 needs backward computation.
I1107 04:29:29.521206 39802 net.cpp:220] bn2 needs backward computation.
I1107 04:29:29.521210 39802 net.cpp:220] conv2 needs backward computation.
I1107 04:29:29.521214 39802 net.cpp:220] pool1 needs backward computation.
I1107 04:29:29.521219 39802 net.cpp:220] relu1 needs backward computation.
I1107 04:29:29.521221 39802 net.cpp:220] bn1 needs backward computation.
I1107 04:29:29.521225 39802 net.cpp:220] conv1 needs backward computation.
I1107 04:29:29.521229 39802 net.cpp:222] label_data_1_split does not need backward computation.
I1107 04:29:29.521234 39802 net.cpp:222] data does not need backward computation.
I1107 04:29:29.521237 39802 net.cpp:264] This network produces output loss
I1107 04:29:29.521241 39802 net.cpp:264] This network produces output top-1
I1107 04:29:29.521261 39802 net.cpp:284] Network initialization done.
I1107 04:29:29.592579 39802 caffe_interface.cpp:363] Running for 80 iterations.
I1107 04:29:29.636009 39802 caffe_interface.cpp:125] Batch 0, loss = 0.250654
I1107 04:29:29.636030 39802 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I1107 04:29:29.656177 39802 caffe_interface.cpp:125] Batch 1, loss = 0.0984157
I1107 04:29:29.656195 39802 caffe_interface.cpp:125] Batch 1, top-1 = 0.96
I1107 04:29:29.675148 39802 caffe_interface.cpp:125] Batch 2, loss = 0.42676
I1107 04:29:29.675165 39802 caffe_interface.cpp:125] Batch 2, top-1 = 0.9
I1107 04:29:29.694069 39802 caffe_interface.cpp:125] Batch 3, loss = 0.71919
I1107 04:29:29.694088 39802 caffe_interface.cpp:125] Batch 3, top-1 = 0.9
I1107 04:29:29.712169 39802 caffe_interface.cpp:125] Batch 4, loss = 0.248171
I1107 04:29:29.712186 39802 caffe_interface.cpp:125] Batch 4, top-1 = 0.96
I1107 04:29:29.730571 39802 caffe_interface.cpp:125] Batch 5, loss = 0.000460102
I1107 04:29:29.730589 39802 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 04:29:29.749657 39802 caffe_interface.cpp:125] Batch 6, loss = 0.317063
I1107 04:29:29.749675 39802 caffe_interface.cpp:125] Batch 6, top-1 = 0.94
I1107 04:29:29.769042 39802 caffe_interface.cpp:125] Batch 7, loss = 0.194159
I1107 04:29:29.769062 39802 caffe_interface.cpp:125] Batch 7, top-1 = 0.94
I1107 04:29:29.787140 39802 caffe_interface.cpp:125] Batch 8, loss = 0.00910675
I1107 04:29:29.787159 39802 caffe_interface.cpp:125] Batch 8, top-1 = 1
I1107 04:29:29.806464 39802 caffe_interface.cpp:125] Batch 9, loss = 0.367341
I1107 04:29:29.806483 39802 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I1107 04:29:29.825773 39802 caffe_interface.cpp:125] Batch 10, loss = 0.066852
I1107 04:29:29.825791 39802 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I1107 04:29:29.843878 39802 caffe_interface.cpp:125] Batch 11, loss = 0.310723
I1107 04:29:29.843895 39802 caffe_interface.cpp:125] Batch 11, top-1 = 0.94
I1107 04:29:29.862818 39802 caffe_interface.cpp:125] Batch 12, loss = 0.444067
I1107 04:29:29.862834 39802 caffe_interface.cpp:125] Batch 12, top-1 = 0.92
I1107 04:29:29.881120 39802 caffe_interface.cpp:125] Batch 13, loss = 0.190643
I1107 04:29:29.881137 39802 caffe_interface.cpp:125] Batch 13, top-1 = 0.94
I1107 04:29:29.899971 39802 caffe_interface.cpp:125] Batch 14, loss = 0.15108
I1107 04:29:29.899986 39802 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I1107 04:29:29.918023 39802 caffe_interface.cpp:125] Batch 15, loss = 0.265647
I1107 04:29:29.918040 39802 caffe_interface.cpp:125] Batch 15, top-1 = 0.92
I1107 04:29:29.936029 39802 caffe_interface.cpp:125] Batch 16, loss = 0.160123
I1107 04:29:29.936044 39802 caffe_interface.cpp:125] Batch 16, top-1 = 0.94
I1107 04:29:29.954103 39802 caffe_interface.cpp:125] Batch 17, loss = 0.0234658
I1107 04:29:29.954119 39802 caffe_interface.cpp:125] Batch 17, top-1 = 0.98
I1107 04:29:29.972986 39802 caffe_interface.cpp:125] Batch 18, loss = 0.194201
I1107 04:29:29.973001 39802 caffe_interface.cpp:125] Batch 18, top-1 = 0.94
I1107 04:29:29.991075 39802 caffe_interface.cpp:125] Batch 19, loss = 0.296043
I1107 04:29:29.991091 39802 caffe_interface.cpp:125] Batch 19, top-1 = 0.94
I1107 04:29:30.010288 39802 caffe_interface.cpp:125] Batch 20, loss = 0.0426528
I1107 04:29:30.010304 39802 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I1107 04:29:30.028420 39802 caffe_interface.cpp:125] Batch 21, loss = 0.102952
I1107 04:29:30.028440 39802 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I1107 04:29:30.047888 39802 caffe_interface.cpp:125] Batch 22, loss = 0.176379
I1107 04:29:30.047900 39802 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I1107 04:29:30.065925 39802 caffe_interface.cpp:125] Batch 23, loss = 0.0968665
I1107 04:29:30.065939 39802 caffe_interface.cpp:125] Batch 23, top-1 = 0.98
I1107 04:29:30.085732 39802 caffe_interface.cpp:125] Batch 24, loss = 0.332154
I1107 04:29:30.085744 39802 caffe_interface.cpp:125] Batch 24, top-1 = 0.92
I1107 04:29:30.103633 39802 caffe_interface.cpp:125] Batch 25, loss = 0.155584
I1107 04:29:30.103644 39802 caffe_interface.cpp:125] Batch 25, top-1 = 0.98
I1107 04:29:30.123288 39802 caffe_interface.cpp:125] Batch 26, loss = 0.191984
I1107 04:29:30.123298 39802 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I1107 04:29:30.141336 39802 caffe_interface.cpp:125] Batch 27, loss = 0.16799
I1107 04:29:30.141347 39802 caffe_interface.cpp:125] Batch 27, top-1 = 0.96
I1107 04:29:30.160372 39802 caffe_interface.cpp:125] Batch 28, loss = 0.250963
I1107 04:29:30.160383 39802 caffe_interface.cpp:125] Batch 28, top-1 = 0.92
I1107 04:29:30.179783 39802 caffe_interface.cpp:125] Batch 29, loss = 0.00938851
I1107 04:29:30.179795 39802 caffe_interface.cpp:125] Batch 29, top-1 = 1
I1107 04:29:30.198015 39802 caffe_interface.cpp:125] Batch 30, loss = 0.0215332
I1107 04:29:30.198026 39802 caffe_interface.cpp:125] Batch 30, top-1 = 1
I1107 04:29:30.216941 39802 caffe_interface.cpp:125] Batch 31, loss = 0.00263977
I1107 04:29:30.216953 39802 caffe_interface.cpp:125] Batch 31, top-1 = 1
I1107 04:29:30.235195 39802 caffe_interface.cpp:125] Batch 32, loss = 0.160852
I1107 04:29:30.235206 39802 caffe_interface.cpp:125] Batch 32, top-1 = 0.96
I1107 04:29:30.253545 39802 caffe_interface.cpp:125] Batch 33, loss = 0.120167
I1107 04:29:30.253556 39802 caffe_interface.cpp:125] Batch 33, top-1 = 0.96
I1107 04:29:30.271808 39802 caffe_interface.cpp:125] Batch 34, loss = 0.181656
I1107 04:29:30.271818 39802 caffe_interface.cpp:125] Batch 34, top-1 = 0.94
I1107 04:29:30.290657 39802 caffe_interface.cpp:125] Batch 35, loss = 0.0279289
I1107 04:29:30.290666 39802 caffe_interface.cpp:125] Batch 35, top-1 = 0.98
I1107 04:29:30.309140 39802 caffe_interface.cpp:125] Batch 36, loss = 0.368236
I1107 04:29:30.309154 39802 caffe_interface.cpp:125] Batch 36, top-1 = 0.94
I1107 04:29:30.328102 39802 caffe_interface.cpp:125] Batch 37, loss = 0.140839
I1107 04:29:30.328131 39802 caffe_interface.cpp:125] Batch 37, top-1 = 0.98
I1107 04:29:30.346396 39802 caffe_interface.cpp:125] Batch 38, loss = 0.247672
I1107 04:29:30.346407 39802 caffe_interface.cpp:125] Batch 38, top-1 = 0.94
I1107 04:29:30.365458 39802 caffe_interface.cpp:125] Batch 39, loss = 0.212001
I1107 04:29:30.365469 39802 caffe_interface.cpp:125] Batch 39, top-1 = 0.94
I1107 04:29:30.383497 39802 caffe_interface.cpp:125] Batch 40, loss = 0.191269
I1107 04:29:30.383507 39802 caffe_interface.cpp:125] Batch 40, top-1 = 0.98
I1107 04:29:30.402981 39802 caffe_interface.cpp:125] Batch 41, loss = 0.0839814
I1107 04:29:30.402993 39802 caffe_interface.cpp:125] Batch 41, top-1 = 0.98
I1107 04:29:30.422119 39802 caffe_interface.cpp:125] Batch 42, loss = 0.00412013
I1107 04:29:30.422129 39802 caffe_interface.cpp:125] Batch 42, top-1 = 1
I1107 04:29:30.439990 39802 caffe_interface.cpp:125] Batch 43, loss = 0.105466
I1107 04:29:30.440001 39802 caffe_interface.cpp:125] Batch 43, top-1 = 0.98
I1107 04:29:30.459118 39802 caffe_interface.cpp:125] Batch 44, loss = 0.494919
I1107 04:29:30.459128 39802 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I1107 04:29:30.476977 39802 caffe_interface.cpp:125] Batch 45, loss = 0.519593
I1107 04:29:30.476989 39802 caffe_interface.cpp:125] Batch 45, top-1 = 0.9
I1107 04:29:30.495796 39802 caffe_interface.cpp:125] Batch 46, loss = 0.123894
I1107 04:29:30.495806 39802 caffe_interface.cpp:125] Batch 46, top-1 = 0.98
I1107 04:29:30.514060 39802 caffe_interface.cpp:125] Batch 47, loss = 0.179156
I1107 04:29:30.514070 39802 caffe_interface.cpp:125] Batch 47, top-1 = 0.94
I1107 04:29:30.532171 39802 caffe_interface.cpp:125] Batch 48, loss = 0.211969
I1107 04:29:30.532179 39802 caffe_interface.cpp:125] Batch 48, top-1 = 0.96
I1107 04:29:30.551347 39802 caffe_interface.cpp:125] Batch 49, loss = 0.236093
I1107 04:29:30.551358 39802 caffe_interface.cpp:125] Batch 49, top-1 = 0.96
I1107 04:29:30.570031 39802 caffe_interface.cpp:125] Batch 50, loss = 0.183489
I1107 04:29:30.570042 39802 caffe_interface.cpp:125] Batch 50, top-1 = 0.94
I1107 04:29:30.588325 39802 caffe_interface.cpp:125] Batch 51, loss = 0.524838
I1107 04:29:30.588340 39802 caffe_interface.cpp:125] Batch 51, top-1 = 0.88
I1107 04:29:30.607676 39802 caffe_interface.cpp:125] Batch 52, loss = 0.0456706
I1107 04:29:30.607687 39802 caffe_interface.cpp:125] Batch 52, top-1 = 0.98
I1107 04:29:30.625941 39802 caffe_interface.cpp:125] Batch 53, loss = 0.128703
I1107 04:29:30.625952 39802 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I1107 04:29:30.644796 39802 caffe_interface.cpp:125] Batch 54, loss = 0.00465114
I1107 04:29:30.644806 39802 caffe_interface.cpp:125] Batch 54, top-1 = 1
I1107 04:29:30.663719 39802 caffe_interface.cpp:125] Batch 55, loss = 0.384954
I1107 04:29:30.663730 39802 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I1107 04:29:30.681762 39802 caffe_interface.cpp:125] Batch 56, loss = 0.105925
I1107 04:29:30.681773 39802 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 04:29:30.700742 39802 caffe_interface.cpp:125] Batch 57, loss = 0.0605423
I1107 04:29:30.700752 39802 caffe_interface.cpp:125] Batch 57, top-1 = 0.96
I1107 04:29:30.718765 39802 caffe_interface.cpp:125] Batch 58, loss = 0.244754
I1107 04:29:30.718775 39802 caffe_interface.cpp:125] Batch 58, top-1 = 0.92
I1107 04:29:30.736886 39802 caffe_interface.cpp:125] Batch 59, loss = 0.154953
I1107 04:29:30.736896 39802 caffe_interface.cpp:125] Batch 59, top-1 = 0.94
I1107 04:29:30.754920 39802 caffe_interface.cpp:125] Batch 60, loss = 0.144315
I1107 04:29:30.754931 39802 caffe_interface.cpp:125] Batch 60, top-1 = 0.98
I1107 04:29:30.773737 39802 caffe_interface.cpp:125] Batch 61, loss = 0.160027
I1107 04:29:30.773747 39802 caffe_interface.cpp:125] Batch 61, top-1 = 0.98
I1107 04:29:30.791975 39802 caffe_interface.cpp:125] Batch 62, loss = 0.0993556
I1107 04:29:30.791985 39802 caffe_interface.cpp:125] Batch 62, top-1 = 0.96
I1107 04:29:30.811064 39802 caffe_interface.cpp:125] Batch 63, loss = 0.381938
I1107 04:29:30.811075 39802 caffe_interface.cpp:125] Batch 63, top-1 = 0.94
I1107 04:29:30.830404 39802 caffe_interface.cpp:125] Batch 64, loss = 0.32965
I1107 04:29:30.830413 39802 caffe_interface.cpp:125] Batch 64, top-1 = 0.92
I1107 04:29:30.848292 39802 caffe_interface.cpp:125] Batch 65, loss = 0.243684
I1107 04:29:30.848304 39802 caffe_interface.cpp:125] Batch 65, top-1 = 0.96
I1107 04:29:30.867357 39802 caffe_interface.cpp:125] Batch 66, loss = 0.474471
I1107 04:29:30.867373 39802 caffe_interface.cpp:125] Batch 66, top-1 = 0.94
I1107 04:29:30.885388 39802 caffe_interface.cpp:125] Batch 67, loss = 0.213302
I1107 04:29:30.885399 39802 caffe_interface.cpp:125] Batch 67, top-1 = 0.94
I1107 04:29:30.904232 39802 caffe_interface.cpp:125] Batch 68, loss = 0.447143
I1107 04:29:30.904242 39802 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I1107 04:29:30.922508 39802 caffe_interface.cpp:125] Batch 69, loss = 0.135196
I1107 04:29:30.922518 39802 caffe_interface.cpp:125] Batch 69, top-1 = 0.98
I1107 04:29:30.941247 39802 caffe_interface.cpp:125] Batch 70, loss = 0.0113639
I1107 04:29:30.941258 39802 caffe_interface.cpp:125] Batch 70, top-1 = 1
I1107 04:29:30.959609 39802 caffe_interface.cpp:125] Batch 71, loss = 0.262932
I1107 04:29:30.959620 39802 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I1107 04:29:30.978628 39802 caffe_interface.cpp:125] Batch 72, loss = 0.12911
I1107 04:29:30.978639 39802 caffe_interface.cpp:125] Batch 72, top-1 = 0.98
I1107 04:29:30.996909 39802 caffe_interface.cpp:125] Batch 73, loss = 0.056963
I1107 04:29:30.996924 39802 caffe_interface.cpp:125] Batch 73, top-1 = 0.96
I1107 04:29:31.016011 39802 caffe_interface.cpp:125] Batch 74, loss = 0.0422868
I1107 04:29:31.016023 39802 caffe_interface.cpp:125] Batch 74, top-1 = 0.98
I1107 04:29:31.034070 39802 caffe_interface.cpp:125] Batch 75, loss = 0.135974
I1107 04:29:31.034082 39802 caffe_interface.cpp:125] Batch 75, top-1 = 0.98
I1107 04:29:31.053320 39802 caffe_interface.cpp:125] Batch 76, loss = 0.308842
I1107 04:29:31.053330 39802 caffe_interface.cpp:125] Batch 76, top-1 = 0.94
I1107 04:29:31.071818 39802 caffe_interface.cpp:125] Batch 77, loss = 0.376016
I1107 04:29:31.071830 39802 caffe_interface.cpp:125] Batch 77, top-1 = 0.94
I1107 04:29:31.091274 39802 caffe_interface.cpp:125] Batch 78, loss = 0.0192239
I1107 04:29:31.091284 39802 caffe_interface.cpp:125] Batch 78, top-1 = 1
I1107 04:29:31.110306 39802 caffe_interface.cpp:125] Batch 79, loss = 0.0224383
I1107 04:29:31.110317 39802 caffe_interface.cpp:125] Batch 79, top-1 = 1
I1107 04:29:31.110321 39802 caffe_interface.cpp:130] Loss: 0.196597
I1107 04:29:31.110327 39802 caffe_interface.cpp:142] loss = 0.196597 (* 1 = 0.196597 loss)
I1107 04:29:31.110333 39802 caffe_interface.cpp:142] top-1 = 0.9565
I1107 04:29:31.347638 39802 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/sparse.caffemodel
I1107 04:29:31.347666 39802 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.3:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.956499755    | 0.00974988937  |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 1526191        | -59.4636688%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 1397275624     | -35.1286659%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config3.prototxt
I1107 04:29:31.610764 41046 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/net_finetune.prototxt
I1107 04:29:31.786572 41046 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 04:29:31.787081 41046 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24848105472, dev_info[0]: total=25620447232 free=24848105472
I1107 04:29:31.787092 41046 caffe_interface.cpp:493] Using GPUs 0
I1107 04:29:31.787344 41046 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 04:29:32.372122 41046 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/net_finetune.prototxt"
type: "Adam"
I1107 04:29:32.372236 41046 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/net_finetune.prototxt
I1107 04:29:32.372463 41046 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 04:29:32.372476 41046 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 04:29:32.372622 41046 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 04:29:32.372686 41046 layer_factory.hpp:77] Creating layer data
I1107 04:29:32.372809 41046 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:32.373193 41046 net.cpp:94] Creating Layer data
I1107 04:29:32.373200 41046 net.cpp:409] data -> data
I1107 04:29:32.373208 41046 net.cpp:409] data -> label
I1107 04:29:32.374963 41085 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 04:29:32.375005 41085 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 04:29:32.375272 41046 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 04:29:32.375352 41046 data_layer.cpp:83] output data size: 256,3,227,227
I1107 04:29:32.751755 41046 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:32.751811 41046 net.cpp:144] Setting up data
I1107 04:29:32.751819 41046 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 04:29:32.751822 41046 net.cpp:151] Top shape: 256 (256)
I1107 04:29:32.751824 41046 net.cpp:159] Memory required for data: 158298112
I1107 04:29:32.751829 41046 layer_factory.hpp:77] Creating layer conv1
I1107 04:29:32.751844 41046 net.cpp:94] Creating Layer conv1
I1107 04:29:32.751863 41046 net.cpp:435] conv1 <- data
I1107 04:29:32.751879 41046 net.cpp:409] conv1 -> conv1
I1107 04:29:32.753834 41046 net.cpp:144] Setting up conv1
I1107 04:29:32.753844 41046 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 04:29:32.753847 41046 net.cpp:159] Memory required for data: 455667712
I1107 04:29:32.753876 41046 layer_factory.hpp:77] Creating layer bn1
I1107 04:29:32.753885 41046 net.cpp:94] Creating Layer bn1
I1107 04:29:32.753888 41046 net.cpp:435] bn1 <- conv1
I1107 04:29:32.753892 41046 net.cpp:409] bn1 -> scale1
I1107 04:29:32.755246 41046 net.cpp:144] Setting up bn1
I1107 04:29:32.755254 41046 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 04:29:32.755256 41046 net.cpp:159] Memory required for data: 753037312
I1107 04:29:32.755266 41046 layer_factory.hpp:77] Creating layer relu1
I1107 04:29:32.755270 41046 net.cpp:94] Creating Layer relu1
I1107 04:29:32.755273 41046 net.cpp:435] relu1 <- scale1
I1107 04:29:32.755277 41046 net.cpp:409] relu1 -> relu1
I1107 04:29:32.755313 41046 net.cpp:144] Setting up relu1
I1107 04:29:32.755318 41046 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 04:29:32.755321 41046 net.cpp:159] Memory required for data: 1050406912
I1107 04:29:32.755323 41046 layer_factory.hpp:77] Creating layer pool1
I1107 04:29:32.755329 41046 net.cpp:94] Creating Layer pool1
I1107 04:29:32.755331 41046 net.cpp:435] pool1 <- relu1
I1107 04:29:32.755336 41046 net.cpp:409] pool1 -> pool1
I1107 04:29:32.755401 41046 net.cpp:144] Setting up pool1
I1107 04:29:32.755406 41046 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 04:29:32.755409 41046 net.cpp:159] Memory required for data: 1122070528
I1107 04:29:32.755412 41046 layer_factory.hpp:77] Creating layer conv2
I1107 04:29:32.755419 41046 net.cpp:94] Creating Layer conv2
I1107 04:29:32.755424 41046 net.cpp:435] conv2 <- pool1
I1107 04:29:32.755429 41046 net.cpp:409] conv2 -> conv2
I1107 04:29:32.770382 41046 net.cpp:144] Setting up conv2
I1107 04:29:32.770401 41046 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 04:29:32.770404 41046 net.cpp:159] Memory required for data: 1313173504
I1107 04:29:32.770416 41046 layer_factory.hpp:77] Creating layer bn2
I1107 04:29:32.770426 41046 net.cpp:94] Creating Layer bn2
I1107 04:29:32.770437 41046 net.cpp:435] bn2 <- conv2
I1107 04:29:32.770458 41046 net.cpp:409] bn2 -> scale2
I1107 04:29:32.771030 41046 net.cpp:144] Setting up bn2
I1107 04:29:32.771039 41046 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 04:29:32.771044 41046 net.cpp:159] Memory required for data: 1504276480
I1107 04:29:32.771052 41046 layer_factory.hpp:77] Creating layer relu2
I1107 04:29:32.771057 41046 net.cpp:94] Creating Layer relu2
I1107 04:29:32.771060 41046 net.cpp:435] relu2 <- scale2
I1107 04:29:32.771065 41046 net.cpp:409] relu2 -> relu2
I1107 04:29:32.771083 41046 net.cpp:144] Setting up relu2
I1107 04:29:32.771090 41046 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 04:29:32.771093 41046 net.cpp:159] Memory required for data: 1695379456
I1107 04:29:32.771095 41046 layer_factory.hpp:77] Creating layer pool2
I1107 04:29:32.771101 41046 net.cpp:94] Creating Layer pool2
I1107 04:29:32.771104 41046 net.cpp:435] pool2 <- relu2
I1107 04:29:32.771124 41046 net.cpp:409] pool2 -> pool2
I1107 04:29:32.771153 41046 net.cpp:144] Setting up pool2
I1107 04:29:32.771158 41046 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 04:29:32.771162 41046 net.cpp:159] Memory required for data: 1739681792
I1107 04:29:32.771164 41046 layer_factory.hpp:77] Creating layer conv3
I1107 04:29:32.771173 41046 net.cpp:94] Creating Layer conv3
I1107 04:29:32.771176 41046 net.cpp:435] conv3 <- pool2
I1107 04:29:32.771181 41046 net.cpp:409] conv3 -> conv3
I1107 04:29:32.784708 41046 net.cpp:144] Setting up conv3
I1107 04:29:32.784736 41046 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 04:29:32.784740 41046 net.cpp:159] Memory required for data: 1806135296
I1107 04:29:32.784752 41046 layer_factory.hpp:77] Creating layer relu3
I1107 04:29:32.784765 41046 net.cpp:94] Creating Layer relu3
I1107 04:29:32.784780 41046 net.cpp:435] relu3 <- conv3
I1107 04:29:32.784793 41046 net.cpp:409] relu3 -> relu3
I1107 04:29:32.784828 41046 net.cpp:144] Setting up relu3
I1107 04:29:32.784835 41046 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 04:29:32.784839 41046 net.cpp:159] Memory required for data: 1872588800
I1107 04:29:32.784843 41046 layer_factory.hpp:77] Creating layer conv4
I1107 04:29:32.784857 41046 net.cpp:94] Creating Layer conv4
I1107 04:29:32.784863 41046 net.cpp:435] conv4 <- relu3
I1107 04:29:32.784871 41046 net.cpp:409] conv4 -> conv4
I1107 04:29:32.813417 41046 net.cpp:144] Setting up conv4
I1107 04:29:32.813449 41046 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 04:29:32.813457 41046 net.cpp:159] Memory required for data: 1939042304
I1107 04:29:32.813475 41046 layer_factory.hpp:77] Creating layer relu4
I1107 04:29:32.813488 41046 net.cpp:94] Creating Layer relu4
I1107 04:29:32.813498 41046 net.cpp:435] relu4 <- conv4
I1107 04:29:32.813510 41046 net.cpp:409] relu4 -> relu4
I1107 04:29:32.813561 41046 net.cpp:144] Setting up relu4
I1107 04:29:32.813586 41046 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 04:29:32.813604 41046 net.cpp:159] Memory required for data: 2005495808
I1107 04:29:32.813623 41046 layer_factory.hpp:77] Creating layer conv5
I1107 04:29:32.813650 41046 net.cpp:94] Creating Layer conv5
I1107 04:29:32.813668 41046 net.cpp:435] conv5 <- relu4
I1107 04:29:32.813688 41046 net.cpp:409] conv5 -> conv5
I1107 04:29:32.828333 41046 net.cpp:144] Setting up conv5
I1107 04:29:32.828356 41046 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 04:29:32.828359 41046 net.cpp:159] Memory required for data: 2049798144
I1107 04:29:32.828369 41046 layer_factory.hpp:77] Creating layer relu5
I1107 04:29:32.828380 41046 net.cpp:94] Creating Layer relu5
I1107 04:29:32.828384 41046 net.cpp:435] relu5 <- conv5
I1107 04:29:32.828392 41046 net.cpp:409] relu5 -> relu5
I1107 04:29:32.828424 41046 net.cpp:144] Setting up relu5
I1107 04:29:32.828430 41046 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 04:29:32.828433 41046 net.cpp:159] Memory required for data: 2094100480
I1107 04:29:32.828436 41046 layer_factory.hpp:77] Creating layer pool5
I1107 04:29:32.828444 41046 net.cpp:94] Creating Layer pool5
I1107 04:29:32.828449 41046 net.cpp:435] pool5 <- relu5
I1107 04:29:32.828455 41046 net.cpp:409] pool5 -> pool5
I1107 04:29:32.828487 41046 net.cpp:144] Setting up pool5
I1107 04:29:32.828493 41046 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 04:29:32.828496 41046 net.cpp:159] Memory required for data: 2103537664
I1107 04:29:32.828500 41046 layer_factory.hpp:77] Creating layer fc6
I1107 04:29:32.828510 41046 net.cpp:94] Creating Layer fc6
I1107 04:29:32.828514 41046 net.cpp:435] fc6 <- pool5
I1107 04:29:32.828521 41046 net.cpp:409] fc6 -> fc6
I1107 04:29:33.167485 41046 net.cpp:144] Setting up fc6
I1107 04:29:33.167508 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.167511 41046 net.cpp:159] Memory required for data: 2107731968
I1107 04:29:33.167518 41046 layer_factory.hpp:77] Creating layer relu6
I1107 04:29:33.167526 41046 net.cpp:94] Creating Layer relu6
I1107 04:29:33.167529 41046 net.cpp:435] relu6 <- fc6
I1107 04:29:33.167554 41046 net.cpp:409] relu6 -> relu6
I1107 04:29:33.167573 41046 net.cpp:144] Setting up relu6
I1107 04:29:33.167577 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.167579 41046 net.cpp:159] Memory required for data: 2111926272
I1107 04:29:33.167582 41046 layer_factory.hpp:77] Creating layer drop6
I1107 04:29:33.167589 41046 net.cpp:94] Creating Layer drop6
I1107 04:29:33.167593 41046 net.cpp:435] drop6 <- relu6
I1107 04:29:33.167598 41046 net.cpp:409] drop6 -> drop6
I1107 04:29:33.167623 41046 net.cpp:144] Setting up drop6
I1107 04:29:33.167629 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.167630 41046 net.cpp:159] Memory required for data: 2116120576
I1107 04:29:33.167634 41046 layer_factory.hpp:77] Creating layer fc7
I1107 04:29:33.167639 41046 net.cpp:94] Creating Layer fc7
I1107 04:29:33.167642 41046 net.cpp:435] fc7 <- drop6
I1107 04:29:33.167649 41046 net.cpp:409] fc7 -> fc7
I1107 04:29:33.308336 41046 net.cpp:144] Setting up fc7
I1107 04:29:33.308360 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.308363 41046 net.cpp:159] Memory required for data: 2120314880
I1107 04:29:33.308372 41046 layer_factory.hpp:77] Creating layer bn7
I1107 04:29:33.308380 41046 net.cpp:94] Creating Layer bn7
I1107 04:29:33.308383 41046 net.cpp:435] bn7 <- fc7
I1107 04:29:33.308399 41046 net.cpp:409] bn7 -> scale7
I1107 04:29:33.308899 41046 net.cpp:144] Setting up bn7
I1107 04:29:33.308907 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.308909 41046 net.cpp:159] Memory required for data: 2124509184
I1107 04:29:33.308918 41046 layer_factory.hpp:77] Creating layer relu7
I1107 04:29:33.308924 41046 net.cpp:94] Creating Layer relu7
I1107 04:29:33.308930 41046 net.cpp:435] relu7 <- scale7
I1107 04:29:33.308938 41046 net.cpp:409] relu7 -> relu7
I1107 04:29:33.308959 41046 net.cpp:144] Setting up relu7
I1107 04:29:33.308964 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.308965 41046 net.cpp:159] Memory required for data: 2128703488
I1107 04:29:33.308969 41046 layer_factory.hpp:77] Creating layer drop7
I1107 04:29:33.308974 41046 net.cpp:94] Creating Layer drop7
I1107 04:29:33.308977 41046 net.cpp:435] drop7 <- relu7
I1107 04:29:33.308984 41046 net.cpp:409] drop7 -> drop7
I1107 04:29:33.309010 41046 net.cpp:144] Setting up drop7
I1107 04:29:33.309016 41046 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 04:29:33.309018 41046 net.cpp:159] Memory required for data: 2132897792
I1107 04:29:33.309020 41046 layer_factory.hpp:77] Creating layer fc8
I1107 04:29:33.309026 41046 net.cpp:94] Creating Layer fc8
I1107 04:29:33.309031 41046 net.cpp:435] fc8 <- drop7
I1107 04:29:33.309036 41046 net.cpp:409] fc8 -> fc8
I1107 04:29:33.309911 41046 net.cpp:144] Setting up fc8
I1107 04:29:33.309923 41046 net.cpp:151] Top shape: 256 2 (512)
I1107 04:29:33.309926 41046 net.cpp:159] Memory required for data: 2132899840
I1107 04:29:33.309932 41046 layer_factory.hpp:77] Creating layer loss
I1107 04:29:33.309939 41046 net.cpp:94] Creating Layer loss
I1107 04:29:33.309945 41046 net.cpp:435] loss <- fc8
I1107 04:29:33.309952 41046 net.cpp:435] loss <- label
I1107 04:29:33.309957 41046 net.cpp:409] loss -> loss
I1107 04:29:33.309967 41046 layer_factory.hpp:77] Creating layer loss
I1107 04:29:33.310035 41046 net.cpp:144] Setting up loss
I1107 04:29:33.310041 41046 net.cpp:151] Top shape: (1)
I1107 04:29:33.310043 41046 net.cpp:154]     with loss weight 1
I1107 04:29:33.310056 41046 net.cpp:159] Memory required for data: 2132899844
I1107 04:29:33.310060 41046 net.cpp:220] loss needs backward computation.
I1107 04:29:33.310078 41046 net.cpp:220] fc8 needs backward computation.
I1107 04:29:33.310082 41046 net.cpp:220] drop7 needs backward computation.
I1107 04:29:33.310086 41046 net.cpp:220] relu7 needs backward computation.
I1107 04:29:33.310089 41046 net.cpp:220] bn7 needs backward computation.
I1107 04:29:33.310094 41046 net.cpp:220] fc7 needs backward computation.
I1107 04:29:33.310097 41046 net.cpp:220] drop6 needs backward computation.
I1107 04:29:33.310101 41046 net.cpp:220] relu6 needs backward computation.
I1107 04:29:33.310115 41046 net.cpp:220] fc6 needs backward computation.
I1107 04:29:33.310120 41046 net.cpp:220] pool5 needs backward computation.
I1107 04:29:33.310123 41046 net.cpp:220] relu5 needs backward computation.
I1107 04:29:33.310127 41046 net.cpp:220] conv5 needs backward computation.
I1107 04:29:33.310132 41046 net.cpp:220] relu4 needs backward computation.
I1107 04:29:33.310137 41046 net.cpp:220] conv4 needs backward computation.
I1107 04:29:33.310139 41046 net.cpp:220] relu3 needs backward computation.
I1107 04:29:33.310144 41046 net.cpp:220] conv3 needs backward computation.
I1107 04:29:33.310148 41046 net.cpp:220] pool2 needs backward computation.
I1107 04:29:33.310151 41046 net.cpp:220] relu2 needs backward computation.
I1107 04:29:33.310155 41046 net.cpp:220] bn2 needs backward computation.
I1107 04:29:33.310159 41046 net.cpp:220] conv2 needs backward computation.
I1107 04:29:33.310163 41046 net.cpp:220] pool1 needs backward computation.
I1107 04:29:33.310168 41046 net.cpp:220] relu1 needs backward computation.
I1107 04:29:33.310170 41046 net.cpp:220] bn1 needs backward computation.
I1107 04:29:33.310174 41046 net.cpp:220] conv1 needs backward computation.
I1107 04:29:33.310178 41046 net.cpp:222] data does not need backward computation.
I1107 04:29:33.310183 41046 net.cpp:264] This network produces output loss
I1107 04:29:33.310204 41046 net.cpp:284] Network initialization done.
I1107 04:29:33.310487 41046 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/net_finetune.prototxt
I1107 04:29:33.310523 41046 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 04:29:33.310693 41046 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 04:29:33.310797 41046 layer_factory.hpp:77] Creating layer data
I1107 04:29:33.310842 41046 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:33.311730 41046 net.cpp:94] Creating Layer data
I1107 04:29:33.311743 41046 net.cpp:409] data -> data
I1107 04:29:33.311751 41046 net.cpp:409] data -> label
I1107 04:29:33.313009 41115 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 04:29:33.313045 41115 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 04:29:33.313295 41046 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 04:29:33.313361 41046 data_layer.cpp:83] output data size: 50,3,227,227
I1107 04:29:33.389103 41046 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 04:29:33.389169 41046 net.cpp:144] Setting up data
I1107 04:29:33.389178 41046 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 04:29:33.389183 41046 net.cpp:151] Top shape: 50 (50)
I1107 04:29:33.389185 41046 net.cpp:159] Memory required for data: 30917600
I1107 04:29:33.389191 41046 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 04:29:33.389204 41046 net.cpp:94] Creating Layer label_data_1_split
I1107 04:29:33.389209 41046 net.cpp:435] label_data_1_split <- label
I1107 04:29:33.389217 41046 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 04:29:33.389227 41046 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 04:29:33.389358 41046 net.cpp:144] Setting up label_data_1_split
I1107 04:29:33.389362 41046 net.cpp:151] Top shape: 50 (50)
I1107 04:29:33.389365 41046 net.cpp:151] Top shape: 50 (50)
I1107 04:29:33.389367 41046 net.cpp:159] Memory required for data: 30918000
I1107 04:29:33.389370 41046 layer_factory.hpp:77] Creating layer conv1
I1107 04:29:33.389381 41046 net.cpp:94] Creating Layer conv1
I1107 04:29:33.389386 41046 net.cpp:435] conv1 <- data
I1107 04:29:33.389391 41046 net.cpp:409] conv1 -> conv1
I1107 04:29:33.389961 41046 net.cpp:144] Setting up conv1
I1107 04:29:33.389967 41046 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:33.389969 41046 net.cpp:159] Memory required for data: 88998000
I1107 04:29:33.389981 41046 layer_factory.hpp:77] Creating layer bn1
I1107 04:29:33.389992 41046 net.cpp:94] Creating Layer bn1
I1107 04:29:33.389997 41046 net.cpp:435] bn1 <- conv1
I1107 04:29:33.390002 41046 net.cpp:409] bn1 -> scale1
I1107 04:29:33.390574 41046 net.cpp:144] Setting up bn1
I1107 04:29:33.390583 41046 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:33.390584 41046 net.cpp:159] Memory required for data: 147078000
I1107 04:29:33.390597 41046 layer_factory.hpp:77] Creating layer relu1
I1107 04:29:33.390609 41046 net.cpp:94] Creating Layer relu1
I1107 04:29:33.390614 41046 net.cpp:435] relu1 <- scale1
I1107 04:29:33.390619 41046 net.cpp:409] relu1 -> relu1
I1107 04:29:33.390903 41046 net.cpp:144] Setting up relu1
I1107 04:29:33.390908 41046 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 04:29:33.390911 41046 net.cpp:159] Memory required for data: 205158000
I1107 04:29:33.390913 41046 layer_factory.hpp:77] Creating layer pool1
I1107 04:29:33.390920 41046 net.cpp:94] Creating Layer pool1
I1107 04:29:33.390925 41046 net.cpp:435] pool1 <- relu1
I1107 04:29:33.390933 41046 net.cpp:409] pool1 -> pool1
I1107 04:29:33.390995 41046 net.cpp:144] Setting up pool1
I1107 04:29:33.391000 41046 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 04:29:33.391001 41046 net.cpp:159] Memory required for data: 219154800
I1107 04:29:33.391005 41046 layer_factory.hpp:77] Creating layer conv2
I1107 04:29:33.391013 41046 net.cpp:94] Creating Layer conv2
I1107 04:29:33.391018 41046 net.cpp:435] conv2 <- pool1
I1107 04:29:33.391026 41046 net.cpp:409] conv2 -> conv2
I1107 04:29:33.397629 41046 net.cpp:144] Setting up conv2
I1107 04:29:33.397666 41046 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:33.397670 41046 net.cpp:159] Memory required for data: 256479600
I1107 04:29:33.397680 41046 layer_factory.hpp:77] Creating layer bn2
I1107 04:29:33.397696 41046 net.cpp:94] Creating Layer bn2
I1107 04:29:33.397703 41046 net.cpp:435] bn2 <- conv2
I1107 04:29:33.397713 41046 net.cpp:409] bn2 -> scale2
I1107 04:29:33.398385 41046 net.cpp:144] Setting up bn2
I1107 04:29:33.398394 41046 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:33.398397 41046 net.cpp:159] Memory required for data: 293804400
I1107 04:29:33.398411 41046 layer_factory.hpp:77] Creating layer relu2
I1107 04:29:33.398424 41046 net.cpp:94] Creating Layer relu2
I1107 04:29:33.398430 41046 net.cpp:435] relu2 <- scale2
I1107 04:29:33.398437 41046 net.cpp:409] relu2 -> relu2
I1107 04:29:33.398464 41046 net.cpp:144] Setting up relu2
I1107 04:29:33.398471 41046 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 04:29:33.398474 41046 net.cpp:159] Memory required for data: 331129200
I1107 04:29:33.398478 41046 layer_factory.hpp:77] Creating layer pool2
I1107 04:29:33.398485 41046 net.cpp:94] Creating Layer pool2
I1107 04:29:33.398489 41046 net.cpp:435] pool2 <- relu2
I1107 04:29:33.398496 41046 net.cpp:409] pool2 -> pool2
I1107 04:29:33.398540 41046 net.cpp:144] Setting up pool2
I1107 04:29:33.398548 41046 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:33.398551 41046 net.cpp:159] Memory required for data: 339782000
I1107 04:29:33.398556 41046 layer_factory.hpp:77] Creating layer conv3
I1107 04:29:33.398568 41046 net.cpp:94] Creating Layer conv3
I1107 04:29:33.398572 41046 net.cpp:435] conv3 <- pool2
I1107 04:29:33.398581 41046 net.cpp:409] conv3 -> conv3
I1107 04:29:33.408272 41046 net.cpp:144] Setting up conv3
I1107 04:29:33.408293 41046 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:33.408295 41046 net.cpp:159] Memory required for data: 352761200
I1107 04:29:33.408303 41046 layer_factory.hpp:77] Creating layer relu3
I1107 04:29:33.408313 41046 net.cpp:94] Creating Layer relu3
I1107 04:29:33.408316 41046 net.cpp:435] relu3 <- conv3
I1107 04:29:33.408325 41046 net.cpp:409] relu3 -> relu3
I1107 04:29:33.408375 41046 net.cpp:144] Setting up relu3
I1107 04:29:33.408382 41046 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:33.408385 41046 net.cpp:159] Memory required for data: 365740400
I1107 04:29:33.408388 41046 layer_factory.hpp:77] Creating layer conv4
I1107 04:29:33.408404 41046 net.cpp:94] Creating Layer conv4
I1107 04:29:33.408411 41046 net.cpp:435] conv4 <- relu3
I1107 04:29:33.408418 41046 net.cpp:409] conv4 -> conv4
I1107 04:29:33.423987 41046 net.cpp:144] Setting up conv4
I1107 04:29:33.424010 41046 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:33.424015 41046 net.cpp:159] Memory required for data: 378719600
I1107 04:29:33.424027 41046 layer_factory.hpp:77] Creating layer relu4
I1107 04:29:33.424037 41046 net.cpp:94] Creating Layer relu4
I1107 04:29:33.424057 41046 net.cpp:435] relu4 <- conv4
I1107 04:29:33.424105 41046 net.cpp:409] relu4 -> relu4
I1107 04:29:33.424151 41046 net.cpp:144] Setting up relu4
I1107 04:29:33.424160 41046 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 04:29:33.424163 41046 net.cpp:159] Memory required for data: 391698800
I1107 04:29:33.424168 41046 layer_factory.hpp:77] Creating layer conv5
I1107 04:29:33.424190 41046 net.cpp:94] Creating Layer conv5
I1107 04:29:33.424196 41046 net.cpp:435] conv5 <- relu4
I1107 04:29:33.424206 41046 net.cpp:409] conv5 -> conv5
I1107 04:29:33.433751 41046 net.cpp:144] Setting up conv5
I1107 04:29:33.433775 41046 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:33.433779 41046 net.cpp:159] Memory required for data: 400351600
I1107 04:29:33.433790 41046 layer_factory.hpp:77] Creating layer relu5
I1107 04:29:33.433802 41046 net.cpp:94] Creating Layer relu5
I1107 04:29:33.433809 41046 net.cpp:435] relu5 <- conv5
I1107 04:29:33.433823 41046 net.cpp:409] relu5 -> relu5
I1107 04:29:33.433863 41046 net.cpp:144] Setting up relu5
I1107 04:29:33.433872 41046 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 04:29:33.433889 41046 net.cpp:159] Memory required for data: 409004400
I1107 04:29:33.433893 41046 layer_factory.hpp:77] Creating layer pool5
I1107 04:29:33.433907 41046 net.cpp:94] Creating Layer pool5
I1107 04:29:33.433913 41046 net.cpp:435] pool5 <- relu5
I1107 04:29:33.433920 41046 net.cpp:409] pool5 -> pool5
I1107 04:29:33.433969 41046 net.cpp:144] Setting up pool5
I1107 04:29:33.433977 41046 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 04:29:33.433982 41046 net.cpp:159] Memory required for data: 410847600
I1107 04:29:33.433989 41046 layer_factory.hpp:77] Creating layer fc6
I1107 04:29:33.433998 41046 net.cpp:94] Creating Layer fc6
I1107 04:29:33.434005 41046 net.cpp:435] fc6 <- pool5
I1107 04:29:33.434011 41046 net.cpp:409] fc6 -> fc6
I1107 04:29:33.741324 41046 net.cpp:144] Setting up fc6
I1107 04:29:33.741349 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.741353 41046 net.cpp:159] Memory required for data: 411666800
I1107 04:29:33.741361 41046 layer_factory.hpp:77] Creating layer relu6
I1107 04:29:33.741371 41046 net.cpp:94] Creating Layer relu6
I1107 04:29:33.741375 41046 net.cpp:435] relu6 <- fc6
I1107 04:29:33.741398 41046 net.cpp:409] relu6 -> relu6
I1107 04:29:33.741426 41046 net.cpp:144] Setting up relu6
I1107 04:29:33.741433 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.741436 41046 net.cpp:159] Memory required for data: 412486000
I1107 04:29:33.741438 41046 layer_factory.hpp:77] Creating layer drop6
I1107 04:29:33.741444 41046 net.cpp:94] Creating Layer drop6
I1107 04:29:33.741447 41046 net.cpp:435] drop6 <- relu6
I1107 04:29:33.741452 41046 net.cpp:409] drop6 -> drop6
I1107 04:29:33.741494 41046 net.cpp:144] Setting up drop6
I1107 04:29:33.741498 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.741500 41046 net.cpp:159] Memory required for data: 413305200
I1107 04:29:33.741503 41046 layer_factory.hpp:77] Creating layer fc7
I1107 04:29:33.741509 41046 net.cpp:94] Creating Layer fc7
I1107 04:29:33.741513 41046 net.cpp:435] fc7 <- drop6
I1107 04:29:33.741518 41046 net.cpp:409] fc7 -> fc7
I1107 04:29:33.878011 41046 net.cpp:144] Setting up fc7
I1107 04:29:33.878034 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.878037 41046 net.cpp:159] Memory required for data: 414124400
I1107 04:29:33.878043 41046 layer_factory.hpp:77] Creating layer bn7
I1107 04:29:33.878053 41046 net.cpp:94] Creating Layer bn7
I1107 04:29:33.878057 41046 net.cpp:435] bn7 <- fc7
I1107 04:29:33.878062 41046 net.cpp:409] bn7 -> scale7
I1107 04:29:33.878748 41046 net.cpp:144] Setting up bn7
I1107 04:29:33.878756 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.878759 41046 net.cpp:159] Memory required for data: 414943600
I1107 04:29:33.878767 41046 layer_factory.hpp:77] Creating layer relu7
I1107 04:29:33.878772 41046 net.cpp:94] Creating Layer relu7
I1107 04:29:33.878775 41046 net.cpp:435] relu7 <- scale7
I1107 04:29:33.878782 41046 net.cpp:409] relu7 -> relu7
I1107 04:29:33.878810 41046 net.cpp:144] Setting up relu7
I1107 04:29:33.878816 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.878820 41046 net.cpp:159] Memory required for data: 415762800
I1107 04:29:33.878823 41046 layer_factory.hpp:77] Creating layer drop7
I1107 04:29:33.878829 41046 net.cpp:94] Creating Layer drop7
I1107 04:29:33.878831 41046 net.cpp:435] drop7 <- relu7
I1107 04:29:33.878839 41046 net.cpp:409] drop7 -> drop7
I1107 04:29:33.878875 41046 net.cpp:144] Setting up drop7
I1107 04:29:33.878880 41046 net.cpp:151] Top shape: 50 4096 (204800)
I1107 04:29:33.878882 41046 net.cpp:159] Memory required for data: 416582000
I1107 04:29:33.878885 41046 layer_factory.hpp:77] Creating layer fc8
I1107 04:29:33.878891 41046 net.cpp:94] Creating Layer fc8
I1107 04:29:33.878895 41046 net.cpp:435] fc8 <- drop7
I1107 04:29:33.878899 41046 net.cpp:409] fc8 -> fc8
I1107 04:29:33.879083 41046 net.cpp:144] Setting up fc8
I1107 04:29:33.879088 41046 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:33.879091 41046 net.cpp:159] Memory required for data: 416582400
I1107 04:29:33.879108 41046 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 04:29:33.879113 41046 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 04:29:33.879117 41046 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 04:29:33.879122 41046 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 04:29:33.879127 41046 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 04:29:33.879160 41046 net.cpp:144] Setting up fc8_fc8_0_split
I1107 04:29:33.879165 41046 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:33.879168 41046 net.cpp:151] Top shape: 50 2 (100)
I1107 04:29:33.879170 41046 net.cpp:159] Memory required for data: 416583200
I1107 04:29:33.879173 41046 layer_factory.hpp:77] Creating layer loss
I1107 04:29:33.879176 41046 net.cpp:94] Creating Layer loss
I1107 04:29:33.879179 41046 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 04:29:33.879184 41046 net.cpp:435] loss <- label_data_1_split_0
I1107 04:29:33.879187 41046 net.cpp:409] loss -> loss
I1107 04:29:33.879194 41046 layer_factory.hpp:77] Creating layer loss
I1107 04:29:33.879272 41046 net.cpp:144] Setting up loss
I1107 04:29:33.879277 41046 net.cpp:151] Top shape: (1)
I1107 04:29:33.879279 41046 net.cpp:154]     with loss weight 1
I1107 04:29:33.879290 41046 net.cpp:159] Memory required for data: 416583204
I1107 04:29:33.879293 41046 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 04:29:33.879298 41046 net.cpp:94] Creating Layer accuracy-top1
I1107 04:29:33.879302 41046 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 04:29:33.879304 41046 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 04:29:33.879309 41046 net.cpp:409] accuracy-top1 -> top-1
I1107 04:29:33.879315 41046 net.cpp:144] Setting up accuracy-top1
I1107 04:29:33.879320 41046 net.cpp:151] Top shape: (1)
I1107 04:29:33.879323 41046 net.cpp:159] Memory required for data: 416583208
I1107 04:29:33.879326 41046 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 04:29:33.879329 41046 net.cpp:220] loss needs backward computation.
I1107 04:29:33.879333 41046 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 04:29:33.879334 41046 net.cpp:220] fc8 needs backward computation.
I1107 04:29:33.879338 41046 net.cpp:220] drop7 needs backward computation.
I1107 04:29:33.879340 41046 net.cpp:220] relu7 needs backward computation.
I1107 04:29:33.879343 41046 net.cpp:220] bn7 needs backward computation.
I1107 04:29:33.879345 41046 net.cpp:220] fc7 needs backward computation.
I1107 04:29:33.879348 41046 net.cpp:220] drop6 needs backward computation.
I1107 04:29:33.879350 41046 net.cpp:220] relu6 needs backward computation.
I1107 04:29:33.879354 41046 net.cpp:220] fc6 needs backward computation.
I1107 04:29:33.879355 41046 net.cpp:220] pool5 needs backward computation.
I1107 04:29:33.879359 41046 net.cpp:220] relu5 needs backward computation.
I1107 04:29:33.879361 41046 net.cpp:220] conv5 needs backward computation.
I1107 04:29:33.879365 41046 net.cpp:220] relu4 needs backward computation.
I1107 04:29:33.879369 41046 net.cpp:220] conv4 needs backward computation.
I1107 04:29:33.879372 41046 net.cpp:220] relu3 needs backward computation.
I1107 04:29:33.879375 41046 net.cpp:220] conv3 needs backward computation.
I1107 04:29:33.879379 41046 net.cpp:220] pool2 needs backward computation.
I1107 04:29:33.879381 41046 net.cpp:220] relu2 needs backward computation.
I1107 04:29:33.879386 41046 net.cpp:220] bn2 needs backward computation.
I1107 04:29:33.879390 41046 net.cpp:220] conv2 needs backward computation.
I1107 04:29:33.879392 41046 net.cpp:220] pool1 needs backward computation.
I1107 04:29:33.879395 41046 net.cpp:220] relu1 needs backward computation.
I1107 04:29:33.879400 41046 net.cpp:220] bn1 needs backward computation.
I1107 04:29:33.879405 41046 net.cpp:220] conv1 needs backward computation.
I1107 04:29:33.879407 41046 net.cpp:222] label_data_1_split does not need backward computation.
I1107 04:29:33.879412 41046 net.cpp:222] data does not need backward computation.
I1107 04:29:33.879415 41046 net.cpp:264] This network produces output loss
I1107 04:29:33.879420 41046 net.cpp:264] This network produces output top-1
I1107 04:29:33.879446 41046 net.cpp:284] Network initialization done.
I1107 04:29:33.879541 41046 solver.cpp:63] Solver scaffolding done.
I1107 04:29:33.880725 41046 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/sparse.caffemodel
I1107 04:29:35.441136 41046 caffe_interface.cpp:527] Starting Optimization
I1107 04:29:35.441157 41046 solver.cpp:335] Solving 
I1107 04:29:35.441159 41046 solver.cpp:336] Learning Rate Policy: step
I1107 04:29:35.443150 41046 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 04:29:36.959194 41046 solver.cpp:517]     Test net output #0: loss = 0.196597 (* 1 = 0.196597 loss)
I1107 04:29:36.959216 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9565
I1107 04:29:37.213105 41046 solver.cpp:266] Iteration 0 (0 iter/s, 1.77184s/50 iter), loss = 0.00605711
I1107 04:29:37.213145 41046 solver.cpp:285]     Train net output #0: loss = 0.00605711 (* 1 = 0.00605711 loss)
I1107 04:29:37.213171 41046 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 04:29:49.544508 41046 solver.cpp:266] Iteration 50 (4.05488 iter/s, 12.3308s/50 iter), loss = 0.0516261
I1107 04:29:49.544565 41046 solver.cpp:285]     Train net output #0: loss = 0.0516261 (* 1 = 0.0516261 loss)
I1107 04:29:49.544571 41046 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 04:30:01.935678 41046 solver.cpp:266] Iteration 100 (4.03532 iter/s, 12.3906s/50 iter), loss = 0.0767356
I1107 04:30:01.935925 41046 solver.cpp:285]     Train net output #0: loss = 0.0767356 (* 1 = 0.0767356 loss)
I1107 04:30:01.935935 41046 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 04:30:14.398077 41046 solver.cpp:266] Iteration 150 (4.01232 iter/s, 12.4616s/50 iter), loss = 0.0296531
I1107 04:30:14.398108 41046 solver.cpp:285]     Train net output #0: loss = 0.0296531 (* 1 = 0.0296531 loss)
I1107 04:30:14.398113 41046 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 04:30:26.872115 41046 solver.cpp:266] Iteration 200 (4.00851 iter/s, 12.4735s/50 iter), loss = 0.0468316
I1107 04:30:26.872144 41046 solver.cpp:285]     Train net output #0: loss = 0.0468316 (* 1 = 0.0468316 loss)
I1107 04:30:26.872149 41046 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 04:30:39.384881 41046 solver.cpp:266] Iteration 250 (3.99611 iter/s, 12.5122s/50 iter), loss = 0.0598553
I1107 04:30:39.384948 41046 solver.cpp:285]     Train net output #0: loss = 0.0598553 (* 1 = 0.0598553 loss)
I1107 04:30:39.384954 41046 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 04:30:51.970293 41046 solver.cpp:266] Iteration 300 (3.97305 iter/s, 12.5848s/50 iter), loss = 0.0906635
I1107 04:30:51.970335 41046 solver.cpp:285]     Train net output #0: loss = 0.0906635 (* 1 = 0.0906635 loss)
I1107 04:30:51.970340 41046 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 04:31:04.683154 41046 solver.cpp:266] Iteration 350 (3.93321 iter/s, 12.7123s/50 iter), loss = 0.0899891
I1107 04:31:04.683185 41046 solver.cpp:285]     Train net output #0: loss = 0.089989 (* 1 = 0.089989 loss)
I1107 04:31:04.683192 41046 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 04:31:17.411036 41046 solver.cpp:266] Iteration 400 (3.92855 iter/s, 12.7273s/50 iter), loss = 0.0582507
I1107 04:31:17.411216 41046 solver.cpp:285]     Train net output #0: loss = 0.0582507 (* 1 = 0.0582507 loss)
I1107 04:31:17.411226 41046 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 04:31:30.142153 41046 solver.cpp:266] Iteration 450 (3.92759 iter/s, 12.7305s/50 iter), loss = 0.0455362
I1107 04:31:30.142185 41046 solver.cpp:285]     Train net output #0: loss = 0.0455362 (* 1 = 0.0455362 loss)
I1107 04:31:30.142190 41046 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 04:31:42.881180 41046 solver.cpp:266] Iteration 500 (3.9251 iter/s, 12.7385s/50 iter), loss = 0.0521699
I1107 04:31:42.881212 41046 solver.cpp:285]     Train net output #0: loss = 0.0521699 (* 1 = 0.0521699 loss)
I1107 04:31:42.881217 41046 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 04:31:55.597252 41046 solver.cpp:266] Iteration 550 (3.93219 iter/s, 12.7156s/50 iter), loss = 0.0464004
I1107 04:31:55.597448 41046 solver.cpp:285]     Train net output #0: loss = 0.0464004 (* 1 = 0.0464004 loss)
I1107 04:31:55.597457 41046 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 04:32:08.357322 41046 solver.cpp:266] Iteration 600 (3.91868 iter/s, 12.7594s/50 iter), loss = 0.0542506
I1107 04:32:08.357353 41046 solver.cpp:285]     Train net output #0: loss = 0.0542506 (* 1 = 0.0542506 loss)
I1107 04:32:08.357360 41046 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 04:32:21.030731 41046 solver.cpp:266] Iteration 650 (3.94543 iter/s, 12.6729s/50 iter), loss = 0.06201
I1107 04:32:21.030761 41046 solver.cpp:285]     Train net output #0: loss = 0.0620099 (* 1 = 0.0620099 loss)
I1107 04:32:21.030767 41046 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 04:32:33.782244 41046 solver.cpp:266] Iteration 700 (3.92126 iter/s, 12.751s/50 iter), loss = 0.0568604
I1107 04:32:33.782749 41046 solver.cpp:285]     Train net output #0: loss = 0.0568603 (* 1 = 0.0568603 loss)
I1107 04:32:33.782758 41046 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 04:32:46.544322 41046 solver.cpp:266] Iteration 750 (3.91816 iter/s, 12.7611s/50 iter), loss = 0.0568623
I1107 04:32:46.544354 41046 solver.cpp:285]     Train net output #0: loss = 0.0568623 (* 1 = 0.0568623 loss)
I1107 04:32:46.544359 41046 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 04:32:59.275295 41046 solver.cpp:266] Iteration 800 (3.92759 iter/s, 12.7305s/50 iter), loss = 0.0610141
I1107 04:32:59.275327 41046 solver.cpp:285]     Train net output #0: loss = 0.0610141 (* 1 = 0.0610141 loss)
I1107 04:32:59.275333 41046 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 04:33:12.007982 41046 solver.cpp:266] Iteration 850 (3.92706 iter/s, 12.7322s/50 iter), loss = 0.0373792
I1107 04:33:12.008147 41046 solver.cpp:285]     Train net output #0: loss = 0.0373792 (* 1 = 0.0373792 loss)
I1107 04:33:12.008154 41046 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 04:33:24.751049 41046 solver.cpp:266] Iteration 900 (3.9239 iter/s, 12.7424s/50 iter), loss = 0.0960405
I1107 04:33:24.751080 41046 solver.cpp:285]     Train net output #0: loss = 0.0960405 (* 1 = 0.0960405 loss)
I1107 04:33:24.751086 41046 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 04:33:37.531810 41046 solver.cpp:266] Iteration 950 (3.91229 iter/s, 12.7802s/50 iter), loss = 0.0644901
I1107 04:33:37.531841 41046 solver.cpp:285]     Train net output #0: loss = 0.0644901 (* 1 = 0.0644901 loss)
I1107 04:33:37.531846 41046 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 04:33:50.060252 41046 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 04:33:51.617332 41046 solver.cpp:517]     Test net output #0: loss = 0.251679 (* 1 = 0.251679 loss)
I1107 04:33:51.617348 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9125
I1107 04:33:51.862709 41046 solver.cpp:266] Iteration 1000 (3.4891 iter/s, 14.3303s/50 iter), loss = 0.0603213
I1107 04:33:51.862736 41046 solver.cpp:285]     Train net output #0: loss = 0.0603213 (* 1 = 0.0603213 loss)
I1107 04:33:51.862745 41046 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 04:34:04.584547 41046 solver.cpp:266] Iteration 1050 (3.93041 iter/s, 12.7213s/50 iter), loss = 0.0299845
I1107 04:34:04.584575 41046 solver.cpp:285]     Train net output #0: loss = 0.0299845 (* 1 = 0.0299845 loss)
I1107 04:34:04.584581 41046 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 04:34:17.352277 41046 solver.cpp:266] Iteration 1100 (3.91628 iter/s, 12.7672s/50 iter), loss = 0.0499635
I1107 04:34:17.352308 41046 solver.cpp:285]     Train net output #0: loss = 0.0499635 (* 1 = 0.0499635 loss)
I1107 04:34:17.352314 41046 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 04:34:30.120929 41046 solver.cpp:266] Iteration 1150 (3.916 iter/s, 12.7681s/50 iter), loss = 0.0508535
I1107 04:34:30.121101 41046 solver.cpp:285]     Train net output #0: loss = 0.0508535 (* 1 = 0.0508535 loss)
I1107 04:34:30.121110 41046 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 04:34:42.918613 41046 solver.cpp:266] Iteration 1200 (3.90716 iter/s, 12.797s/50 iter), loss = 0.102649
I1107 04:34:42.918643 41046 solver.cpp:285]     Train net output #0: loss = 0.102649 (* 1 = 0.102649 loss)
I1107 04:34:42.918649 41046 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 04:34:55.725962 41046 solver.cpp:266] Iteration 1250 (3.90417 iter/s, 12.8068s/50 iter), loss = 0.061287
I1107 04:34:55.725993 41046 solver.cpp:285]     Train net output #0: loss = 0.0612869 (* 1 = 0.0612869 loss)
I1107 04:34:55.725998 41046 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 04:35:08.463891 41046 solver.cpp:266] Iteration 1300 (3.92545 iter/s, 12.7374s/50 iter), loss = 0.080812
I1107 04:35:08.464056 41046 solver.cpp:285]     Train net output #0: loss = 0.080812 (* 1 = 0.080812 loss)
I1107 04:35:08.464062 41046 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 04:35:21.281240 41046 solver.cpp:266] Iteration 1350 (3.90116 iter/s, 12.8167s/50 iter), loss = 0.0345512
I1107 04:35:21.281281 41046 solver.cpp:285]     Train net output #0: loss = 0.0345511 (* 1 = 0.0345511 loss)
I1107 04:35:21.281287 41046 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 04:35:34.061069 41046 solver.cpp:266] Iteration 1400 (3.91258 iter/s, 12.7793s/50 iter), loss = 0.069731
I1107 04:35:34.061108 41046 solver.cpp:285]     Train net output #0: loss = 0.0697309 (* 1 = 0.0697309 loss)
I1107 04:35:34.061131 41046 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 04:35:46.837280 41046 solver.cpp:266] Iteration 1450 (3.91369 iter/s, 12.7757s/50 iter), loss = 0.054347
I1107 04:35:46.837432 41046 solver.cpp:285]     Train net output #0: loss = 0.054347 (* 1 = 0.054347 loss)
I1107 04:35:46.837440 41046 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 04:35:59.581496 41046 solver.cpp:266] Iteration 1500 (3.92355 iter/s, 12.7436s/50 iter), loss = 0.0363583
I1107 04:35:59.581526 41046 solver.cpp:285]     Train net output #0: loss = 0.0363583 (* 1 = 0.0363583 loss)
I1107 04:35:59.581532 41046 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 04:36:12.340262 41046 solver.cpp:266] Iteration 1550 (3.91903 iter/s, 12.7582s/50 iter), loss = 0.0753351
I1107 04:36:12.340304 41046 solver.cpp:285]     Train net output #0: loss = 0.0753351 (* 1 = 0.0753351 loss)
I1107 04:36:12.340310 41046 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 04:36:25.153102 41046 solver.cpp:266] Iteration 1600 (3.9025 iter/s, 12.8123s/50 iter), loss = 0.102185
I1107 04:36:25.153772 41046 solver.cpp:285]     Train net output #0: loss = 0.102185 (* 1 = 0.102185 loss)
I1107 04:36:25.153781 41046 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 04:36:37.881065 41046 solver.cpp:266] Iteration 1650 (3.92872 iter/s, 12.7268s/50 iter), loss = 0.0529691
I1107 04:36:37.881094 41046 solver.cpp:285]     Train net output #0: loss = 0.052969 (* 1 = 0.052969 loss)
I1107 04:36:37.881100 41046 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 04:36:50.667922 41046 solver.cpp:266] Iteration 1700 (3.91042 iter/s, 12.7863s/50 iter), loss = 0.0894047
I1107 04:36:50.667951 41046 solver.cpp:285]     Train net output #0: loss = 0.0894047 (* 1 = 0.0894047 loss)
I1107 04:36:50.667956 41046 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 04:37:03.435858 41046 solver.cpp:266] Iteration 1750 (3.91622 iter/s, 12.7674s/50 iter), loss = 0.0710947
I1107 04:37:03.436035 41046 solver.cpp:285]     Train net output #0: loss = 0.0710946 (* 1 = 0.0710946 loss)
I1107 04:37:03.436044 41046 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 04:37:16.186604 41046 solver.cpp:266] Iteration 1800 (3.92154 iter/s, 12.7501s/50 iter), loss = 0.0644347
I1107 04:37:16.186635 41046 solver.cpp:285]     Train net output #0: loss = 0.0644346 (* 1 = 0.0644346 loss)
I1107 04:37:16.186642 41046 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 04:37:28.894388 41046 solver.cpp:266] Iteration 1850 (3.93476 iter/s, 12.7073s/50 iter), loss = 0.0803343
I1107 04:37:28.894418 41046 solver.cpp:285]     Train net output #0: loss = 0.0803343 (* 1 = 0.0803343 loss)
I1107 04:37:28.894423 41046 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 04:37:41.666426 41046 solver.cpp:266] Iteration 1900 (3.91496 iter/s, 12.7715s/50 iter), loss = 0.0329858
I1107 04:37:41.666882 41046 solver.cpp:285]     Train net output #0: loss = 0.0329857 (* 1 = 0.0329857 loss)
I1107 04:37:41.666890 41046 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 04:37:54.454053 41046 solver.cpp:266] Iteration 1950 (3.91032 iter/s, 12.7867s/50 iter), loss = 0.0831278
I1107 04:37:54.454083 41046 solver.cpp:285]     Train net output #0: loss = 0.0831277 (* 1 = 0.0831277 loss)
I1107 04:37:54.454089 41046 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 04:38:06.913790 41046 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 04:38:08.437167 41046 solver.cpp:517]     Test net output #0: loss = 0.224294 (* 1 = 0.224294 loss)
I1107 04:38:08.437182 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9195
I1107 04:38:08.676986 41046 solver.cpp:266] Iteration 2000 (3.51559 iter/s, 14.2224s/50 iter), loss = 0.0314275
I1107 04:38:08.677021 41046 solver.cpp:285]     Train net output #0: loss = 0.0314275 (* 1 = 0.0314275 loss)
I1107 04:38:08.677042 41046 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 04:38:21.404848 41046 solver.cpp:266] Iteration 2050 (3.92855 iter/s, 12.7273s/50 iter), loss = 0.024092
I1107 04:38:21.405019 41046 solver.cpp:285]     Train net output #0: loss = 0.0240919 (* 1 = 0.0240919 loss)
I1107 04:38:21.405026 41046 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 04:38:34.164075 41046 solver.cpp:266] Iteration 2100 (3.91894 iter/s, 12.7586s/50 iter), loss = 0.0894515
I1107 04:38:34.164104 41046 solver.cpp:285]     Train net output #0: loss = 0.0894515 (* 1 = 0.0894515 loss)
I1107 04:38:34.164109 41046 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 04:38:46.868548 41046 solver.cpp:266] Iteration 2150 (3.93578 iter/s, 12.704s/50 iter), loss = 0.0412232
I1107 04:38:46.868578 41046 solver.cpp:285]     Train net output #0: loss = 0.0412231 (* 1 = 0.0412231 loss)
I1107 04:38:46.868585 41046 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 04:38:59.593894 41046 solver.cpp:266] Iteration 2200 (3.92933 iter/s, 12.7248s/50 iter), loss = 0.0480486
I1107 04:38:59.594048 41046 solver.cpp:285]     Train net output #0: loss = 0.0480485 (* 1 = 0.0480485 loss)
I1107 04:38:59.594056 41046 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 04:39:12.361369 41046 solver.cpp:266] Iteration 2250 (3.9164 iter/s, 12.7668s/50 iter), loss = 0.0399424
I1107 04:39:12.361399 41046 solver.cpp:285]     Train net output #0: loss = 0.0399423 (* 1 = 0.0399423 loss)
I1107 04:39:12.361404 41046 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 04:39:25.142925 41046 solver.cpp:266] Iteration 2300 (3.91205 iter/s, 12.781s/50 iter), loss = 0.048371
I1107 04:39:25.142952 41046 solver.cpp:285]     Train net output #0: loss = 0.0483709 (* 1 = 0.0483709 loss)
I1107 04:39:25.142958 41046 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 04:39:37.879180 41046 solver.cpp:266] Iteration 2350 (3.92596 iter/s, 12.7357s/50 iter), loss = 0.0336465
I1107 04:39:37.879341 41046 solver.cpp:285]     Train net output #0: loss = 0.0336464 (* 1 = 0.0336464 loss)
I1107 04:39:37.879350 41046 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 04:39:50.655598 41046 solver.cpp:266] Iteration 2400 (3.91366 iter/s, 12.7758s/50 iter), loss = 0.0778087
I1107 04:39:50.655628 41046 solver.cpp:285]     Train net output #0: loss = 0.0778087 (* 1 = 0.0778087 loss)
I1107 04:39:50.655633 41046 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 04:40:03.377938 41046 solver.cpp:266] Iteration 2450 (3.93026 iter/s, 12.7218s/50 iter), loss = 0.0513511
I1107 04:40:03.377967 41046 solver.cpp:285]     Train net output #0: loss = 0.0513511 (* 1 = 0.0513511 loss)
I1107 04:40:03.377974 41046 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 04:40:16.126236 41046 solver.cpp:266] Iteration 2500 (3.92225 iter/s, 12.7478s/50 iter), loss = 0.0681643
I1107 04:40:16.126418 41046 solver.cpp:285]     Train net output #0: loss = 0.0681643 (* 1 = 0.0681643 loss)
I1107 04:40:16.126426 41046 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 04:40:28.882454 41046 solver.cpp:266] Iteration 2550 (3.91986 iter/s, 12.7555s/50 iter), loss = 0.0269758
I1107 04:40:28.882484 41046 solver.cpp:285]     Train net output #0: loss = 0.0269758 (* 1 = 0.0269758 loss)
I1107 04:40:28.882489 41046 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 04:40:41.617323 41046 solver.cpp:266] Iteration 2600 (3.92639 iter/s, 12.7343s/50 iter), loss = 0.032835
I1107 04:40:41.617353 41046 solver.cpp:285]     Train net output #0: loss = 0.032835 (* 1 = 0.032835 loss)
I1107 04:40:41.617358 41046 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 04:40:54.338002 41046 solver.cpp:266] Iteration 2650 (3.93077 iter/s, 12.7202s/50 iter), loss = 0.00748082
I1107 04:40:54.338160 41046 solver.cpp:285]     Train net output #0: loss = 0.00748076 (* 1 = 0.00748076 loss)
I1107 04:40:54.338169 41046 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 04:41:07.132161 41046 solver.cpp:266] Iteration 2700 (3.90823 iter/s, 12.7935s/50 iter), loss = 0.0258762
I1107 04:41:07.132194 41046 solver.cpp:285]     Train net output #0: loss = 0.0258761 (* 1 = 0.0258761 loss)
I1107 04:41:07.132199 41046 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 04:41:19.827877 41046 solver.cpp:266] Iteration 2750 (3.9385 iter/s, 12.6952s/50 iter), loss = 0.0178593
I1107 04:41:19.827906 41046 solver.cpp:285]     Train net output #0: loss = 0.0178593 (* 1 = 0.0178593 loss)
I1107 04:41:19.827929 41046 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 04:41:32.571871 41046 solver.cpp:266] Iteration 2800 (3.92357 iter/s, 12.7435s/50 iter), loss = 0.01752
I1107 04:41:32.572041 41046 solver.cpp:285]     Train net output #0: loss = 0.01752 (* 1 = 0.01752 loss)
I1107 04:41:32.572051 41046 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 04:41:45.363060 41046 solver.cpp:266] Iteration 2850 (3.90913 iter/s, 12.7906s/50 iter), loss = 0.02149
I1107 04:41:45.363090 41046 solver.cpp:285]     Train net output #0: loss = 0.02149 (* 1 = 0.02149 loss)
I1107 04:41:45.363096 41046 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 04:41:58.050559 41046 solver.cpp:266] Iteration 2900 (3.94103 iter/s, 12.687s/50 iter), loss = 0.0297509
I1107 04:41:58.050590 41046 solver.cpp:285]     Train net output #0: loss = 0.0297509 (* 1 = 0.0297509 loss)
I1107 04:41:58.050595 41046 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 04:42:10.805879 41046 solver.cpp:266] Iteration 2950 (3.92008 iter/s, 12.7548s/50 iter), loss = 0.011118
I1107 04:42:10.806036 41046 solver.cpp:285]     Train net output #0: loss = 0.0111179 (* 1 = 0.0111179 loss)
I1107 04:42:10.806044 41046 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 04:42:23.299772 41046 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 04:42:24.794548 41046 solver.cpp:517]     Test net output #0: loss = 0.138784 (* 1 = 0.138784 loss)
I1107 04:42:24.794564 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I1107 04:42:25.042362 41046 solver.cpp:266] Iteration 3000 (3.51226 iter/s, 14.2358s/50 iter), loss = 0.014899
I1107 04:42:25.042390 41046 solver.cpp:285]     Train net output #0: loss = 0.0148989 (* 1 = 0.0148989 loss)
I1107 04:42:25.042395 41046 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 04:42:37.776679 41046 solver.cpp:266] Iteration 3050 (3.92654 iter/s, 12.7338s/50 iter), loss = 0.010899
I1107 04:42:37.776706 41046 solver.cpp:285]     Train net output #0: loss = 0.010899 (* 1 = 0.010899 loss)
I1107 04:42:37.776711 41046 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 04:42:50.524574 41046 solver.cpp:266] Iteration 3100 (3.92236 iter/s, 12.7474s/50 iter), loss = 0.0106564
I1107 04:42:50.524734 41046 solver.cpp:285]     Train net output #0: loss = 0.0106563 (* 1 = 0.0106563 loss)
I1107 04:42:50.524741 41046 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 04:43:03.226459 41046 solver.cpp:266] Iteration 3150 (3.93661 iter/s, 12.7013s/50 iter), loss = 0.0140913
I1107 04:43:03.226498 41046 solver.cpp:285]     Train net output #0: loss = 0.0140913 (* 1 = 0.0140913 loss)
I1107 04:43:03.226503 41046 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 04:43:15.932477 41046 solver.cpp:266] Iteration 3200 (3.93529 iter/s, 12.7055s/50 iter), loss = 0.0161098
I1107 04:43:15.932507 41046 solver.cpp:285]     Train net output #0: loss = 0.0161098 (* 1 = 0.0161098 loss)
I1107 04:43:15.932512 41046 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 04:43:28.700242 41046 solver.cpp:266] Iteration 3250 (3.91626 iter/s, 12.7673s/50 iter), loss = 0.00634472
I1107 04:43:28.700419 41046 solver.cpp:285]     Train net output #0: loss = 0.00634469 (* 1 = 0.00634469 loss)
I1107 04:43:28.700428 41046 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 04:43:41.401971 41046 solver.cpp:266] Iteration 3300 (3.93666 iter/s, 12.7011s/50 iter), loss = 0.0150772
I1107 04:43:41.402000 41046 solver.cpp:285]     Train net output #0: loss = 0.0150772 (* 1 = 0.0150772 loss)
I1107 04:43:41.402005 41046 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 04:43:54.147328 41046 solver.cpp:266] Iteration 3350 (3.92314 iter/s, 12.7449s/50 iter), loss = 0.0367888
I1107 04:43:54.147359 41046 solver.cpp:285]     Train net output #0: loss = 0.0367888 (* 1 = 0.0367888 loss)
I1107 04:43:54.147366 41046 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I1107 04:44:06.847901 41046 solver.cpp:266] Iteration 3400 (3.93698 iter/s, 12.7001s/50 iter), loss = 0.00355443
I1107 04:44:06.848068 41046 solver.cpp:285]     Train net output #0: loss = 0.00355438 (* 1 = 0.00355438 loss)
I1107 04:44:06.848075 41046 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I1107 04:44:19.599807 41046 solver.cpp:266] Iteration 3450 (3.92117 iter/s, 12.7513s/50 iter), loss = 0.0415227
I1107 04:44:19.599838 41046 solver.cpp:285]     Train net output #0: loss = 0.0415227 (* 1 = 0.0415227 loss)
I1107 04:44:19.599843 41046 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I1107 04:44:32.338099 41046 solver.cpp:266] Iteration 3500 (3.92532 iter/s, 12.7378s/50 iter), loss = 0.0189783
I1107 04:44:32.338129 41046 solver.cpp:285]     Train net output #0: loss = 0.0189782 (* 1 = 0.0189782 loss)
I1107 04:44:32.338135 41046 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I1107 04:44:45.059679 41046 solver.cpp:266] Iteration 3550 (3.93048 iter/s, 12.7211s/50 iter), loss = 0.0175618
I1107 04:44:45.059830 41046 solver.cpp:285]     Train net output #0: loss = 0.0175618 (* 1 = 0.0175618 loss)
I1107 04:44:45.059840 41046 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I1107 04:44:57.793455 41046 solver.cpp:266] Iteration 3600 (3.92675 iter/s, 12.7332s/50 iter), loss = 0.0287147
I1107 04:44:57.793486 41046 solver.cpp:285]     Train net output #0: loss = 0.0287147 (* 1 = 0.0287147 loss)
I1107 04:44:57.793493 41046 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I1107 04:45:10.496042 41046 solver.cpp:266] Iteration 3650 (3.93636 iter/s, 12.7021s/50 iter), loss = 0.0179328
I1107 04:45:10.496074 41046 solver.cpp:285]     Train net output #0: loss = 0.0179327 (* 1 = 0.0179327 loss)
I1107 04:45:10.496080 41046 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I1107 04:45:23.217641 41046 solver.cpp:266] Iteration 3700 (3.93047 iter/s, 12.7211s/50 iter), loss = 0.0463587
I1107 04:45:23.218056 41046 solver.cpp:285]     Train net output #0: loss = 0.0463587 (* 1 = 0.0463587 loss)
I1107 04:45:23.218065 41046 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I1107 04:45:35.911491 41046 solver.cpp:266] Iteration 3750 (3.93918 iter/s, 12.693s/50 iter), loss = 0.00349674
I1107 04:45:35.911522 41046 solver.cpp:285]     Train net output #0: loss = 0.00349672 (* 1 = 0.00349672 loss)
I1107 04:45:35.911528 41046 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I1107 04:45:48.575525 41046 solver.cpp:266] Iteration 3800 (3.94834 iter/s, 12.6635s/50 iter), loss = 0.0192242
I1107 04:45:48.575564 41046 solver.cpp:285]     Train net output #0: loss = 0.0192242 (* 1 = 0.0192242 loss)
I1107 04:45:48.575570 41046 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I1107 04:46:01.285787 41046 solver.cpp:266] Iteration 3850 (3.93398 iter/s, 12.7098s/50 iter), loss = 0.0158182
I1107 04:46:01.285985 41046 solver.cpp:285]     Train net output #0: loss = 0.0158181 (* 1 = 0.0158181 loss)
I1107 04:46:01.285995 41046 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I1107 04:46:13.943542 41046 solver.cpp:266] Iteration 3900 (3.95035 iter/s, 12.6571s/50 iter), loss = 0.00736173
I1107 04:46:13.943574 41046 solver.cpp:285]     Train net output #0: loss = 0.00736171 (* 1 = 0.00736171 loss)
I1107 04:46:13.943580 41046 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I1107 04:46:26.622793 41046 solver.cpp:266] Iteration 3950 (3.9436 iter/s, 12.6788s/50 iter), loss = 0.012007
I1107 04:46:26.622825 41046 solver.cpp:285]     Train net output #0: loss = 0.0120069 (* 1 = 0.0120069 loss)
I1107 04:46:26.622830 41046 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I1107 04:46:39.094357 41046 solver.cpp:418] Iteration 4000, Testing net (#0)
I1107 04:46:40.593807 41046 solver.cpp:517]     Test net output #0: loss = 0.138923 (* 1 = 0.138923 loss)
I1107 04:46:40.593824 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95075
I1107 04:46:40.840523 41046 solver.cpp:266] Iteration 4000 (3.51687 iter/s, 14.2172s/50 iter), loss = 0.00410062
I1107 04:46:40.840550 41046 solver.cpp:285]     Train net output #0: loss = 0.0041006 (* 1 = 0.0041006 loss)
I1107 04:46:40.840572 41046 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I1107 04:46:53.551520 41046 solver.cpp:266] Iteration 4050 (3.93375 iter/s, 12.7105s/50 iter), loss = 0.0114424
I1107 04:46:53.551551 41046 solver.cpp:285]     Train net output #0: loss = 0.0114424 (* 1 = 0.0114424 loss)
I1107 04:46:53.551573 41046 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I1107 04:47:06.224310 41046 solver.cpp:266] Iteration 4100 (3.94561 iter/s, 12.6723s/50 iter), loss = 0.00549289
I1107 04:47:06.224342 41046 solver.cpp:285]     Train net output #0: loss = 0.00549288 (* 1 = 0.00549288 loss)
I1107 04:47:06.224349 41046 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I1107 04:47:18.921875 41046 solver.cpp:266] Iteration 4150 (3.93791 iter/s, 12.6971s/50 iter), loss = 0.00704724
I1107 04:47:18.922026 41046 solver.cpp:285]     Train net output #0: loss = 0.00704723 (* 1 = 0.00704723 loss)
I1107 04:47:18.922034 41046 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I1107 04:47:31.651705 41046 solver.cpp:266] Iteration 4200 (3.92797 iter/s, 12.7292s/50 iter), loss = 0.00467252
I1107 04:47:31.651736 41046 solver.cpp:285]     Train net output #0: loss = 0.0046725 (* 1 = 0.0046725 loss)
I1107 04:47:31.651741 41046 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I1107 04:47:44.310459 41046 solver.cpp:266] Iteration 4250 (3.94999 iter/s, 12.6583s/50 iter), loss = 0.0071558
I1107 04:47:44.310490 41046 solver.cpp:285]     Train net output #0: loss = 0.00715578 (* 1 = 0.00715578 loss)
I1107 04:47:44.310497 41046 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I1107 04:47:56.994060 41046 solver.cpp:266] Iteration 4300 (3.94225 iter/s, 12.6831s/50 iter), loss = 0.0181815
I1107 04:47:56.994206 41046 solver.cpp:285]     Train net output #0: loss = 0.0181815 (* 1 = 0.0181815 loss)
I1107 04:47:56.994215 41046 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I1107 04:48:09.716172 41046 solver.cpp:266] Iteration 4350 (3.93035 iter/s, 12.7215s/50 iter), loss = 0.009178
I1107 04:48:09.716202 41046 solver.cpp:285]     Train net output #0: loss = 0.00917798 (* 1 = 0.00917798 loss)
I1107 04:48:09.716208 41046 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I1107 04:48:22.378396 41046 solver.cpp:266] Iteration 4400 (3.94891 iter/s, 12.6617s/50 iter), loss = 0.00837591
I1107 04:48:22.378427 41046 solver.cpp:285]     Train net output #0: loss = 0.00837589 (* 1 = 0.00837589 loss)
I1107 04:48:22.378433 41046 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I1107 04:48:35.051889 41046 solver.cpp:266] Iteration 4450 (3.94539 iter/s, 12.673s/50 iter), loss = 0.0187427
I1107 04:48:35.052062 41046 solver.cpp:285]     Train net output #0: loss = 0.0187426 (* 1 = 0.0187426 loss)
I1107 04:48:35.052072 41046 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I1107 04:48:47.795418 41046 solver.cpp:266] Iteration 4500 (3.92376 iter/s, 12.7429s/50 iter), loss = 0.00472958
I1107 04:48:47.795449 41046 solver.cpp:285]     Train net output #0: loss = 0.00472957 (* 1 = 0.00472957 loss)
I1107 04:48:47.795454 41046 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I1107 04:49:00.478152 41046 solver.cpp:266] Iteration 4550 (3.94252 iter/s, 12.6822s/50 iter), loss = 0.0125941
I1107 04:49:00.478183 41046 solver.cpp:285]     Train net output #0: loss = 0.012594 (* 1 = 0.012594 loss)
I1107 04:49:00.478188 41046 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I1107 04:49:13.172817 41046 solver.cpp:266] Iteration 4600 (3.93881 iter/s, 12.6942s/50 iter), loss = 0.00611395
I1107 04:49:13.172989 41046 solver.cpp:285]     Train net output #0: loss = 0.00611394 (* 1 = 0.00611394 loss)
I1107 04:49:13.172997 41046 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I1107 04:49:25.830126 41046 solver.cpp:266] Iteration 4650 (3.95048 iter/s, 12.6567s/50 iter), loss = 0.00526541
I1107 04:49:25.830153 41046 solver.cpp:285]     Train net output #0: loss = 0.0052654 (* 1 = 0.0052654 loss)
I1107 04:49:25.830160 41046 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I1107 04:49:38.520051 41046 solver.cpp:266] Iteration 4700 (3.94029 iter/s, 12.6894s/50 iter), loss = 0.0109159
I1107 04:49:38.520079 41046 solver.cpp:285]     Train net output #0: loss = 0.0109159 (* 1 = 0.0109159 loss)
I1107 04:49:38.520085 41046 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I1107 04:49:51.201455 41046 solver.cpp:266] Iteration 4750 (3.94293 iter/s, 12.6809s/50 iter), loss = 0.0132725
I1107 04:49:51.201647 41046 solver.cpp:285]     Train net output #0: loss = 0.0132725 (* 1 = 0.0132725 loss)
I1107 04:49:51.201655 41046 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I1107 04:50:03.899026 41046 solver.cpp:266] Iteration 4800 (3.93796 iter/s, 12.6969s/50 iter), loss = 0.0052886
I1107 04:50:03.899057 41046 solver.cpp:285]     Train net output #0: loss = 0.00528857 (* 1 = 0.00528857 loss)
I1107 04:50:03.899062 41046 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I1107 04:50:16.555660 41046 solver.cpp:266] Iteration 4850 (3.95065 iter/s, 12.6561s/50 iter), loss = 0.00232279
I1107 04:50:16.555691 41046 solver.cpp:285]     Train net output #0: loss = 0.00232275 (* 1 = 0.00232275 loss)
I1107 04:50:16.555696 41046 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I1107 04:50:29.261610 41046 solver.cpp:266] Iteration 4900 (3.93532 iter/s, 12.7055s/50 iter), loss = 0.00469749
I1107 04:50:29.261787 41046 solver.cpp:285]     Train net output #0: loss = 0.00469746 (* 1 = 0.00469746 loss)
I1107 04:50:29.261795 41046 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I1107 04:50:41.939357 41046 solver.cpp:266] Iteration 4950 (3.94412 iter/s, 12.6771s/50 iter), loss = 0.00812328
I1107 04:50:41.939385 41046 solver.cpp:285]     Train net output #0: loss = 0.00812325 (* 1 = 0.00812325 loss)
I1107 04:50:41.939391 41046 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I1107 04:50:54.356088 41046 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_5000.caffemodel
I1107 04:50:56.686296 41046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_5000.solverstate
I1107 04:50:57.127593 41046 solver.cpp:418] Iteration 5000, Testing net (#0)
I1107 04:50:58.599079 41046 solver.cpp:517]     Test net output #0: loss = 0.152634 (* 1 = 0.152634 loss)
I1107 04:50:58.599094 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95075
I1107 04:50:58.837991 41046 solver.cpp:266] Iteration 5000 (2.95893 iter/s, 16.898s/50 iter), loss = 0.00695799
I1107 04:50:58.838016 41046 solver.cpp:285]     Train net output #0: loss = 0.00695796 (* 1 = 0.00695796 loss)
I1107 04:50:58.838022 41046 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I1107 04:51:11.363737 41046 solver.cpp:266] Iteration 5050 (3.99193 iter/s, 12.5253s/50 iter), loss = 0.0142696
I1107 04:51:11.363936 41046 solver.cpp:285]     Train net output #0: loss = 0.0142695 (* 1 = 0.0142695 loss)
I1107 04:51:11.363955 41046 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I1107 04:51:23.925288 41046 solver.cpp:266] Iteration 5100 (3.98061 iter/s, 12.5609s/50 iter), loss = 0.0122118
I1107 04:51:23.925318 41046 solver.cpp:285]     Train net output #0: loss = 0.0122118 (* 1 = 0.0122118 loss)
I1107 04:51:23.925324 41046 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I1107 04:51:36.534525 41046 solver.cpp:266] Iteration 5150 (3.9655 iter/s, 12.6087s/50 iter), loss = 0.00344838
I1107 04:51:36.534555 41046 solver.cpp:285]     Train net output #0: loss = 0.00344836 (* 1 = 0.00344836 loss)
I1107 04:51:36.534560 41046 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I1107 04:51:49.206619 41046 solver.cpp:266] Iteration 5200 (3.94583 iter/s, 12.6716s/50 iter), loss = 0.0011181
I1107 04:51:49.206786 41046 solver.cpp:285]     Train net output #0: loss = 0.00111808 (* 1 = 0.00111808 loss)
I1107 04:51:49.206795 41046 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I1107 04:52:01.944944 41046 solver.cpp:266] Iteration 5250 (3.92536 iter/s, 12.7377s/50 iter), loss = 0.00316675
I1107 04:52:01.944975 41046 solver.cpp:285]     Train net output #0: loss = 0.00316673 (* 1 = 0.00316673 loss)
I1107 04:52:01.944981 41046 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I1107 04:52:14.613714 41046 solver.cpp:266] Iteration 5300 (3.94687 iter/s, 12.6683s/50 iter), loss = 0.0031228
I1107 04:52:14.613746 41046 solver.cpp:285]     Train net output #0: loss = 0.00312278 (* 1 = 0.00312278 loss)
I1107 04:52:14.613752 41046 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I1107 04:52:27.281582 41046 solver.cpp:266] Iteration 5350 (3.94715 iter/s, 12.6674s/50 iter), loss = 0.00646669
I1107 04:52:27.282567 41046 solver.cpp:285]     Train net output #0: loss = 0.00646668 (* 1 = 0.00646668 loss)
I1107 04:52:27.282573 41046 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I1107 04:52:39.962700 41046 solver.cpp:266] Iteration 5400 (3.94332 iter/s, 12.6797s/50 iter), loss = 0.00236359
I1107 04:52:39.962731 41046 solver.cpp:285]     Train net output #0: loss = 0.00236357 (* 1 = 0.00236357 loss)
I1107 04:52:39.962736 41046 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I1107 04:52:52.648046 41046 solver.cpp:266] Iteration 5450 (3.94171 iter/s, 12.6848s/50 iter), loss = 0.00162874
I1107 04:52:52.648074 41046 solver.cpp:285]     Train net output #0: loss = 0.00162873 (* 1 = 0.00162873 loss)
I1107 04:52:52.648079 41046 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I1107 04:53:05.318740 41046 solver.cpp:266] Iteration 5500 (3.94627 iter/s, 12.6702s/50 iter), loss = 0.00107365
I1107 04:53:05.318897 41046 solver.cpp:285]     Train net output #0: loss = 0.00107363 (* 1 = 0.00107363 loss)
I1107 04:53:05.318904 41046 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I1107 04:53:17.993482 41046 solver.cpp:266] Iteration 5550 (3.94505 iter/s, 12.6741s/50 iter), loss = 0.0158824
I1107 04:53:17.993511 41046 solver.cpp:285]     Train net output #0: loss = 0.0158824 (* 1 = 0.0158824 loss)
I1107 04:53:17.993516 41046 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I1107 04:53:30.700304 41046 solver.cpp:266] Iteration 5600 (3.93505 iter/s, 12.7063s/50 iter), loss = 0.00364083
I1107 04:53:30.700333 41046 solver.cpp:285]     Train net output #0: loss = 0.00364082 (* 1 = 0.00364082 loss)
I1107 04:53:30.700338 41046 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I1107 04:53:43.357636 41046 solver.cpp:266] Iteration 5650 (3.95044 iter/s, 12.6568s/50 iter), loss = 0.0242667
I1107 04:53:43.357798 41046 solver.cpp:285]     Train net output #0: loss = 0.0242666 (* 1 = 0.0242666 loss)
I1107 04:53:43.357807 41046 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I1107 04:53:56.065821 41046 solver.cpp:266] Iteration 5700 (3.93467 iter/s, 12.7076s/50 iter), loss = 0.00444558
I1107 04:53:56.065851 41046 solver.cpp:285]     Train net output #0: loss = 0.00444556 (* 1 = 0.00444556 loss)
I1107 04:53:56.065858 41046 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I1107 04:54:08.700574 41046 solver.cpp:266] Iteration 5750 (3.9575 iter/s, 12.6343s/50 iter), loss = 0.00462385
I1107 04:54:08.700603 41046 solver.cpp:285]     Train net output #0: loss = 0.00462383 (* 1 = 0.00462383 loss)
I1107 04:54:08.700609 41046 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I1107 04:54:21.388264 41046 solver.cpp:266] Iteration 5800 (3.94098 iter/s, 12.6872s/50 iter), loss = 0.00477699
I1107 04:54:21.388451 41046 solver.cpp:285]     Train net output #0: loss = 0.00477698 (* 1 = 0.00477698 loss)
I1107 04:54:21.388459 41046 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I1107 04:54:34.045933 41046 solver.cpp:266] Iteration 5850 (3.95038 iter/s, 12.657s/50 iter), loss = 0.00272431
I1107 04:54:34.045964 41046 solver.cpp:285]     Train net output #0: loss = 0.0027243 (* 1 = 0.0027243 loss)
I1107 04:54:34.045969 41046 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I1107 04:54:46.764626 41046 solver.cpp:266] Iteration 5900 (3.93138 iter/s, 12.7182s/50 iter), loss = 0.00641419
I1107 04:54:46.764655 41046 solver.cpp:285]     Train net output #0: loss = 0.00641418 (* 1 = 0.00641418 loss)
I1107 04:54:46.764662 41046 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I1107 04:54:59.419298 41046 solver.cpp:266] Iteration 5950 (3.95127 iter/s, 12.6542s/50 iter), loss = 0.0104073
I1107 04:54:59.419458 41046 solver.cpp:285]     Train net output #0: loss = 0.0104073 (* 1 = 0.0104073 loss)
I1107 04:54:59.419466 41046 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I1107 04:55:11.848762 41046 solver.cpp:418] Iteration 6000, Testing net (#0)
I1107 04:55:13.365358 41046 solver.cpp:517]     Test net output #0: loss = 0.170521 (* 1 = 0.170521 loss)
I1107 04:55:13.365375 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95375
I1107 04:55:13.611081 41046 solver.cpp:266] Iteration 6000 (3.52334 iter/s, 14.1911s/50 iter), loss = 0.000971061
I1107 04:55:13.611109 41046 solver.cpp:285]     Train net output #0: loss = 0.000971048 (* 1 = 0.000971048 loss)
I1107 04:55:13.611115 41046 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I1107 04:55:26.327062 41046 solver.cpp:266] Iteration 6050 (3.93221 iter/s, 12.7155s/50 iter), loss = 0.0232007
I1107 04:55:26.327091 41046 solver.cpp:285]     Train net output #0: loss = 0.0232007 (* 1 = 0.0232007 loss)
I1107 04:55:26.327097 41046 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I1107 04:55:38.977602 41046 solver.cpp:266] Iteration 6100 (3.95256 iter/s, 12.65s/50 iter), loss = 0.00140427
I1107 04:55:38.977761 41046 solver.cpp:285]     Train net output #0: loss = 0.00140425 (* 1 = 0.00140425 loss)
I1107 04:55:38.977768 41046 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I1107 04:55:51.615751 41046 solver.cpp:266] Iteration 6150 (3.95647 iter/s, 12.6375s/50 iter), loss = 0.0019197
I1107 04:55:51.615779 41046 solver.cpp:285]     Train net output #0: loss = 0.00191968 (* 1 = 0.00191968 loss)
I1107 04:55:51.615784 41046 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I1107 04:56:04.272965 41046 solver.cpp:266] Iteration 6200 (3.95047 iter/s, 12.6567s/50 iter), loss = 0.00308689
I1107 04:56:04.272996 41046 solver.cpp:285]     Train net output #0: loss = 0.00308688 (* 1 = 0.00308688 loss)
I1107 04:56:04.273002 41046 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I1107 04:56:16.988693 41046 solver.cpp:266] Iteration 6250 (3.93229 iter/s, 12.7152s/50 iter), loss = 0.00421118
I1107 04:56:16.988845 41046 solver.cpp:285]     Train net output #0: loss = 0.00421117 (* 1 = 0.00421117 loss)
I1107 04:56:16.988855 41046 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I1107 04:56:29.662959 41046 solver.cpp:266] Iteration 6300 (3.9452 iter/s, 12.6736s/50 iter), loss = 0.0073723
I1107 04:56:29.662991 41046 solver.cpp:285]     Train net output #0: loss = 0.00737229 (* 1 = 0.00737229 loss)
I1107 04:56:29.662995 41046 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I1107 04:56:42.338824 41046 solver.cpp:266] Iteration 6350 (3.94466 iter/s, 12.6754s/50 iter), loss = 0.00172346
I1107 04:56:42.338855 41046 solver.cpp:285]     Train net output #0: loss = 0.00172345 (* 1 = 0.00172345 loss)
I1107 04:56:42.338860 41046 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I1107 04:56:55.001384 41046 solver.cpp:266] Iteration 6400 (3.94881 iter/s, 12.6621s/50 iter), loss = 0.0190811
I1107 04:56:55.001564 41046 solver.cpp:285]     Train net output #0: loss = 0.0190811 (* 1 = 0.0190811 loss)
I1107 04:56:55.001572 41046 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I1107 04:57:07.634888 41046 solver.cpp:266] Iteration 6450 (3.95793 iter/s, 12.6329s/50 iter), loss = 0.00486121
I1107 04:57:07.634917 41046 solver.cpp:285]     Train net output #0: loss = 0.0048612 (* 1 = 0.0048612 loss)
I1107 04:57:07.634924 41046 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I1107 04:57:20.335171 41046 solver.cpp:266] Iteration 6500 (3.93708 iter/s, 12.6998s/50 iter), loss = 0.00777359
I1107 04:57:20.335203 41046 solver.cpp:285]     Train net output #0: loss = 0.00777358 (* 1 = 0.00777358 loss)
I1107 04:57:20.335224 41046 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I1107 04:57:33.023051 41046 solver.cpp:266] Iteration 6550 (3.94093 iter/s, 12.6874s/50 iter), loss = 0.0296388
I1107 04:57:33.023250 41046 solver.cpp:285]     Train net output #0: loss = 0.0296388 (* 1 = 0.0296388 loss)
I1107 04:57:33.023278 41046 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I1107 04:57:45.690878 41046 solver.cpp:266] Iteration 6600 (3.94721 iter/s, 12.6672s/50 iter), loss = 0.00395333
I1107 04:57:45.690912 41046 solver.cpp:285]     Train net output #0: loss = 0.00395333 (* 1 = 0.00395333 loss)
I1107 04:57:45.690917 41046 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I1107 04:57:58.379748 41046 solver.cpp:266] Iteration 6650 (3.94062 iter/s, 12.6884s/50 iter), loss = 0.000487895
I1107 04:57:58.379779 41046 solver.cpp:285]     Train net output #0: loss = 0.000487887 (* 1 = 0.000487887 loss)
I1107 04:57:58.379786 41046 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I1107 04:58:11.078708 41046 solver.cpp:266] Iteration 6700 (3.93749 iter/s, 12.6985s/50 iter), loss = 0.00166942
I1107 04:58:11.078852 41046 solver.cpp:285]     Train net output #0: loss = 0.00166942 (* 1 = 0.00166942 loss)
I1107 04:58:11.078871 41046 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I1107 04:58:23.696166 41046 solver.cpp:266] Iteration 6750 (3.96296 iter/s, 12.6168s/50 iter), loss = 0.018308
I1107 04:58:23.696197 41046 solver.cpp:285]     Train net output #0: loss = 0.018308 (* 1 = 0.018308 loss)
I1107 04:58:23.696202 41046 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I1107 04:58:36.376988 41046 solver.cpp:266] Iteration 6800 (3.94312 iter/s, 12.6803s/50 iter), loss = 0.00488291
I1107 04:58:36.377017 41046 solver.cpp:285]     Train net output #0: loss = 0.00488291 (* 1 = 0.00488291 loss)
I1107 04:58:36.377033 41046 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I1107 04:58:49.087785 41046 solver.cpp:266] Iteration 6850 (3.93382 iter/s, 12.7103s/50 iter), loss = 0.0100628
I1107 04:58:49.087944 41046 solver.cpp:285]     Train net output #0: loss = 0.0100628 (* 1 = 0.0100628 loss)
I1107 04:58:49.087963 41046 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I1107 04:59:01.756819 41046 solver.cpp:266] Iteration 6900 (3.94683 iter/s, 12.6684s/50 iter), loss = 0.0120079
I1107 04:59:01.756857 41046 solver.cpp:285]     Train net output #0: loss = 0.0120079 (* 1 = 0.0120079 loss)
I1107 04:59:01.756865 41046 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I1107 04:59:14.427201 41046 solver.cpp:266] Iteration 6950 (3.94637 iter/s, 12.6699s/50 iter), loss = 0.0213527
I1107 04:59:14.427242 41046 solver.cpp:285]     Train net output #0: loss = 0.0213527 (* 1 = 0.0213527 loss)
I1107 04:59:14.427248 41046 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I1107 04:59:26.872640 41046 solver.cpp:418] Iteration 7000, Testing net (#0)
I1107 04:59:28.375777 41046 solver.cpp:517]     Test net output #0: loss = 0.192697 (* 1 = 0.192697 loss)
I1107 04:59:28.375792 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9535
I1107 04:59:28.621381 41046 solver.cpp:266] Iteration 7000 (3.52271 iter/s, 14.1936s/50 iter), loss = 0.000537724
I1107 04:59:28.621418 41046 solver.cpp:285]     Train net output #0: loss = 0.000537721 (* 1 = 0.000537721 loss)
I1107 04:59:28.621425 41046 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I1107 04:59:41.271626 41046 solver.cpp:266] Iteration 7050 (3.95265 iter/s, 12.6497s/50 iter), loss = 0.00111425
I1107 04:59:41.271667 41046 solver.cpp:285]     Train net output #0: loss = 0.00111425 (* 1 = 0.00111425 loss)
I1107 04:59:41.271673 41046 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I1107 04:59:53.959296 41046 solver.cpp:266] Iteration 7100 (3.94099 iter/s, 12.6872s/50 iter), loss = 0.00931896
I1107 04:59:53.959338 41046 solver.cpp:285]     Train net output #0: loss = 0.00931896 (* 1 = 0.00931896 loss)
I1107 04:59:53.959345 41046 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I1107 05:00:06.636440 41046 solver.cpp:266] Iteration 7150 (3.94427 iter/s, 12.6766s/50 iter), loss = 0.00554324
I1107 05:00:06.636631 41046 solver.cpp:285]     Train net output #0: loss = 0.00554324 (* 1 = 0.00554324 loss)
I1107 05:00:06.636638 41046 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I1107 05:00:19.351382 41046 solver.cpp:266] Iteration 7200 (3.93259 iter/s, 12.7143s/50 iter), loss = 0.00563626
I1107 05:00:19.351424 41046 solver.cpp:285]     Train net output #0: loss = 0.00563626 (* 1 = 0.00563626 loss)
I1107 05:00:19.351433 41046 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I1107 05:00:32.014526 41046 solver.cpp:266] Iteration 7250 (3.94863 iter/s, 12.6626s/50 iter), loss = 0.00159796
I1107 05:00:32.014555 41046 solver.cpp:285]     Train net output #0: loss = 0.00159797 (* 1 = 0.00159797 loss)
I1107 05:00:32.014561 41046 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I1107 05:00:44.699247 41046 solver.cpp:266] Iteration 7300 (3.94191 iter/s, 12.6842s/50 iter), loss = 0.0286047
I1107 05:00:44.699403 41046 solver.cpp:285]     Train net output #0: loss = 0.0286047 (* 1 = 0.0286047 loss)
I1107 05:00:44.699410 41046 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I1107 05:00:57.380076 41046 solver.cpp:266] Iteration 7350 (3.94316 iter/s, 12.6802s/50 iter), loss = 0.000656008
I1107 05:00:57.380108 41046 solver.cpp:285]     Train net output #0: loss = 0.000656009 (* 1 = 0.000656009 loss)
I1107 05:00:57.380113 41046 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I1107 05:01:10.027410 41046 solver.cpp:266] Iteration 7400 (3.95356 iter/s, 12.6468s/50 iter), loss = 0.00687803
I1107 05:01:10.027439 41046 solver.cpp:285]     Train net output #0: loss = 0.00687803 (* 1 = 0.00687803 loss)
I1107 05:01:10.027446 41046 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I1107 05:01:22.699571 41046 solver.cpp:266] Iteration 7450 (3.94581 iter/s, 12.6717s/50 iter), loss = 0.000665671
I1107 05:01:22.699990 41046 solver.cpp:285]     Train net output #0: loss = 0.000665669 (* 1 = 0.000665669 loss)
I1107 05:01:22.699997 41046 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I1107 05:01:35.360230 41046 solver.cpp:266] Iteration 7500 (3.94952 iter/s, 12.6598s/50 iter), loss = 0.0024335
I1107 05:01:35.360260 41046 solver.cpp:285]     Train net output #0: loss = 0.0024335 (* 1 = 0.0024335 loss)
I1107 05:01:35.360265 41046 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I1107 05:01:47.995666 41046 solver.cpp:266] Iteration 7550 (3.95728 iter/s, 12.6349s/50 iter), loss = 0.00333237
I1107 05:01:47.995697 41046 solver.cpp:285]     Train net output #0: loss = 0.00333237 (* 1 = 0.00333237 loss)
I1107 05:01:47.995702 41046 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I1107 05:02:00.677902 41046 solver.cpp:266] Iteration 7600 (3.94268 iter/s, 12.6817s/50 iter), loss = 0.00339245
I1107 05:02:00.678037 41046 solver.cpp:285]     Train net output #0: loss = 0.00339246 (* 1 = 0.00339246 loss)
I1107 05:02:00.678045 41046 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I1107 05:02:13.340600 41046 solver.cpp:266] Iteration 7650 (3.9488 iter/s, 12.6621s/50 iter), loss = 0.00132931
I1107 05:02:13.340629 41046 solver.cpp:285]     Train net output #0: loss = 0.00132932 (* 1 = 0.00132932 loss)
I1107 05:02:13.340634 41046 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I1107 05:02:26.031443 41046 solver.cpp:266] Iteration 7700 (3.94001 iter/s, 12.6903s/50 iter), loss = 0.00827887
I1107 05:02:26.031473 41046 solver.cpp:285]     Train net output #0: loss = 0.00827887 (* 1 = 0.00827887 loss)
I1107 05:02:26.031479 41046 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I1107 05:02:38.682862 41046 solver.cpp:266] Iteration 7750 (3.95228 iter/s, 12.6509s/50 iter), loss = 0.00233914
I1107 05:02:38.683063 41046 solver.cpp:285]     Train net output #0: loss = 0.00233914 (* 1 = 0.00233914 loss)
I1107 05:02:38.683071 41046 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I1107 05:02:51.353353 41046 solver.cpp:266] Iteration 7800 (3.94639 iter/s, 12.6698s/50 iter), loss = 0.00603499
I1107 05:02:51.353381 41046 solver.cpp:285]     Train net output #0: loss = 0.00603499 (* 1 = 0.00603499 loss)
I1107 05:02:51.353389 41046 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I1107 05:03:04.067692 41046 solver.cpp:266] Iteration 7850 (3.93272 iter/s, 12.7138s/50 iter), loss = 0.019883
I1107 05:03:04.067721 41046 solver.cpp:285]     Train net output #0: loss = 0.019883 (* 1 = 0.019883 loss)
I1107 05:03:04.067728 41046 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I1107 05:03:16.722954 41046 solver.cpp:266] Iteration 7900 (3.95108 iter/s, 12.6548s/50 iter), loss = 0.00184832
I1107 05:03:16.723122 41046 solver.cpp:285]     Train net output #0: loss = 0.00184832 (* 1 = 0.00184832 loss)
I1107 05:03:16.723131 41046 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I1107 05:03:29.410200 41046 solver.cpp:266] Iteration 7950 (3.94117 iter/s, 12.6866s/50 iter), loss = 0.00283778
I1107 05:03:29.410248 41046 solver.cpp:285]     Train net output #0: loss = 0.00283779 (* 1 = 0.00283779 loss)
I1107 05:03:29.410257 41046 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I1107 05:03:41.830154 41046 solver.cpp:418] Iteration 8000, Testing net (#0)
I1107 05:03:43.321157 41046 solver.cpp:517]     Test net output #0: loss = 0.205372 (* 1 = 0.205372 loss)
I1107 05:03:43.321174 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95225
I1107 05:03:43.570061 41046 solver.cpp:266] Iteration 8000 (3.53125 iter/s, 14.1593s/50 iter), loss = 0.000799364
I1107 05:03:43.570092 41046 solver.cpp:285]     Train net output #0: loss = 0.000799373 (* 1 = 0.000799373 loss)
I1107 05:03:43.570101 41046 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I1107 05:03:56.232259 41046 solver.cpp:266] Iteration 8050 (3.94892 iter/s, 12.6617s/50 iter), loss = 0.00173898
I1107 05:03:56.232419 41046 solver.cpp:285]     Train net output #0: loss = 0.00173899 (* 1 = 0.00173899 loss)
I1107 05:03:56.232426 41046 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I1107 05:04:08.930770 41046 solver.cpp:266] Iteration 8100 (3.93767 iter/s, 12.6979s/50 iter), loss = 0.00324674
I1107 05:04:08.930809 41046 solver.cpp:285]     Train net output #0: loss = 0.00324675 (* 1 = 0.00324675 loss)
I1107 05:04:08.930815 41046 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I1107 05:04:21.613149 41046 solver.cpp:266] Iteration 8150 (3.94264 iter/s, 12.6819s/50 iter), loss = 0.0106584
I1107 05:04:21.613179 41046 solver.cpp:285]     Train net output #0: loss = 0.0106584 (* 1 = 0.0106584 loss)
I1107 05:04:21.613186 41046 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I1107 05:04:34.257517 41046 solver.cpp:266] Iteration 8200 (3.95449 iter/s, 12.6439s/50 iter), loss = 0.00470134
I1107 05:04:34.257680 41046 solver.cpp:285]     Train net output #0: loss = 0.00470135 (* 1 = 0.00470135 loss)
I1107 05:04:34.257688 41046 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I1107 05:04:46.934294 41046 solver.cpp:266] Iteration 8250 (3.94442 iter/s, 12.6761s/50 iter), loss = 0.00652365
I1107 05:04:46.934324 41046 solver.cpp:285]     Train net output #0: loss = 0.00652366 (* 1 = 0.00652366 loss)
I1107 05:04:46.934345 41046 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I1107 05:04:59.600926 41046 solver.cpp:266] Iteration 8300 (3.94754 iter/s, 12.6661s/50 iter), loss = 0.0019284
I1107 05:04:59.600956 41046 solver.cpp:285]     Train net output #0: loss = 0.00192841 (* 1 = 0.00192841 loss)
I1107 05:04:59.600963 41046 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I1107 05:05:12.244186 41046 solver.cpp:266] Iteration 8350 (3.95484 iter/s, 12.6428s/50 iter), loss = 0.000617418
I1107 05:05:12.244362 41046 solver.cpp:285]     Train net output #0: loss = 0.000617424 (* 1 = 0.000617424 loss)
I1107 05:05:12.244372 41046 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I1107 05:05:24.958822 41046 solver.cpp:266] Iteration 8400 (3.93268 iter/s, 12.714s/50 iter), loss = 0.00835299
I1107 05:05:24.958849 41046 solver.cpp:285]     Train net output #0: loss = 0.008353 (* 1 = 0.008353 loss)
I1107 05:05:24.958855 41046 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I1107 05:05:37.620877 41046 solver.cpp:266] Iteration 8450 (3.94896 iter/s, 12.6615s/50 iter), loss = 0.00362839
I1107 05:05:37.620906 41046 solver.cpp:285]     Train net output #0: loss = 0.0036284 (* 1 = 0.0036284 loss)
I1107 05:05:37.620913 41046 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I1107 05:05:50.258978 41046 solver.cpp:266] Iteration 8500 (3.95645 iter/s, 12.6376s/50 iter), loss = 0.00145377
I1107 05:05:50.259160 41046 solver.cpp:285]     Train net output #0: loss = 0.00145378 (* 1 = 0.00145378 loss)
I1107 05:05:50.259169 41046 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I1107 05:06:02.940806 41046 solver.cpp:266] Iteration 8550 (3.94285 iter/s, 12.6812s/50 iter), loss = 0.00377528
I1107 05:06:02.940846 41046 solver.cpp:285]     Train net output #0: loss = 0.00377529 (* 1 = 0.00377529 loss)
I1107 05:06:02.940852 41046 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I1107 05:06:15.637840 41046 solver.cpp:266] Iteration 8600 (3.93809 iter/s, 12.6965s/50 iter), loss = 0.000968038
I1107 05:06:15.637881 41046 solver.cpp:285]     Train net output #0: loss = 0.000968047 (* 1 = 0.000968047 loss)
I1107 05:06:15.637887 41046 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I1107 05:06:28.301734 41046 solver.cpp:266] Iteration 8650 (3.94839 iter/s, 12.6634s/50 iter), loss = 0.00599568
I1107 05:06:28.301908 41046 solver.cpp:285]     Train net output #0: loss = 0.00599569 (* 1 = 0.00599569 loss)
I1107 05:06:28.301916 41046 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I1107 05:06:41.014328 41046 solver.cpp:266] Iteration 8700 (3.93331 iter/s, 12.7119s/50 iter), loss = 0.00692276
I1107 05:06:41.014369 41046 solver.cpp:285]     Train net output #0: loss = 0.00692277 (* 1 = 0.00692277 loss)
I1107 05:06:41.014375 41046 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I1107 05:06:53.671941 41046 solver.cpp:266] Iteration 8750 (3.95035 iter/s, 12.6571s/50 iter), loss = 0.00229799
I1107 05:06:53.671983 41046 solver.cpp:285]     Train net output #0: loss = 0.002298 (* 1 = 0.002298 loss)
I1107 05:06:53.671989 41046 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I1107 05:07:06.323093 41046 solver.cpp:266] Iteration 8800 (3.95237 iter/s, 12.6506s/50 iter), loss = 0.00989738
I1107 05:07:06.323259 41046 solver.cpp:285]     Train net output #0: loss = 0.00989739 (* 1 = 0.00989739 loss)
I1107 05:07:06.323267 41046 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I1107 05:07:18.976584 41046 solver.cpp:266] Iteration 8850 (3.95168 iter/s, 12.6528s/50 iter), loss = 0.00100597
I1107 05:07:18.976614 41046 solver.cpp:285]     Train net output #0: loss = 0.00100598 (* 1 = 0.00100598 loss)
I1107 05:07:18.976620 41046 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I1107 05:07:31.683967 41046 solver.cpp:266] Iteration 8900 (3.93488 iter/s, 12.7069s/50 iter), loss = 0.0222053
I1107 05:07:31.683997 41046 solver.cpp:285]     Train net output #0: loss = 0.0222053 (* 1 = 0.0222053 loss)
I1107 05:07:31.684002 41046 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I1107 05:07:44.362254 41046 solver.cpp:266] Iteration 8950 (3.94391 iter/s, 12.6778s/50 iter), loss = 0.00222868
I1107 05:07:44.362406 41046 solver.cpp:285]     Train net output #0: loss = 0.00222869 (* 1 = 0.00222869 loss)
I1107 05:07:44.362414 41046 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I1107 05:07:56.769078 41046 solver.cpp:418] Iteration 9000, Testing net (#0)
I1107 05:07:58.274144 41046 solver.cpp:517]     Test net output #0: loss = 0.213792 (* 1 = 0.213792 loss)
I1107 05:07:58.274163 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9545
I1107 05:07:58.519469 41046 solver.cpp:266] Iteration 9000 (3.53194 iter/s, 14.1565s/50 iter), loss = 0.00437744
I1107 05:07:58.519496 41046 solver.cpp:285]     Train net output #0: loss = 0.00437745 (* 1 = 0.00437745 loss)
I1107 05:07:58.519503 41046 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I1107 05:08:11.225698 41046 solver.cpp:266] Iteration 9050 (3.93523 iter/s, 12.7057s/50 iter), loss = 0.00171859
I1107 05:08:11.225728 41046 solver.cpp:285]     Train net output #0: loss = 0.0017186 (* 1 = 0.0017186 loss)
I1107 05:08:11.225734 41046 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I1107 05:08:23.902098 41046 solver.cpp:266] Iteration 9100 (3.9445 iter/s, 12.6759s/50 iter), loss = 0.00336137
I1107 05:08:23.902292 41046 solver.cpp:285]     Train net output #0: loss = 0.00336139 (* 1 = 0.00336139 loss)
I1107 05:08:23.902302 41046 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I1107 05:08:36.596688 41046 solver.cpp:266] Iteration 9150 (3.93889 iter/s, 12.6939s/50 iter), loss = 0.00466507
I1107 05:08:36.596717 41046 solver.cpp:285]     Train net output #0: loss = 0.00466509 (* 1 = 0.00466509 loss)
I1107 05:08:36.596724 41046 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I1107 05:08:49.293799 41046 solver.cpp:266] Iteration 9200 (3.93806 iter/s, 12.6966s/50 iter), loss = 0.00214916
I1107 05:08:49.293829 41046 solver.cpp:285]     Train net output #0: loss = 0.00214918 (* 1 = 0.00214918 loss)
I1107 05:08:49.293835 41046 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I1107 05:09:01.943168 41046 solver.cpp:266] Iteration 9250 (3.95293 iter/s, 12.6489s/50 iter), loss = 0.000469903
I1107 05:09:01.943301 41046 solver.cpp:285]     Train net output #0: loss = 0.000469925 (* 1 = 0.000469925 loss)
I1107 05:09:01.943310 41046 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I1107 05:09:14.651010 41046 solver.cpp:266] Iteration 9300 (3.93477 iter/s, 12.7072s/50 iter), loss = 0.00681083
I1107 05:09:14.651041 41046 solver.cpp:285]     Train net output #0: loss = 0.00681085 (* 1 = 0.00681085 loss)
I1107 05:09:14.651046 41046 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I1107 05:09:27.309228 41046 solver.cpp:266] Iteration 9350 (3.95016 iter/s, 12.6577s/50 iter), loss = 0.00494293
I1107 05:09:27.309259 41046 solver.cpp:285]     Train net output #0: loss = 0.00494295 (* 1 = 0.00494295 loss)
I1107 05:09:27.309264 41046 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I1107 05:09:39.936807 41046 solver.cpp:266] Iteration 9400 (3.95975 iter/s, 12.6271s/50 iter), loss = 0.0032435
I1107 05:09:39.936950 41046 solver.cpp:285]     Train net output #0: loss = 0.00324352 (* 1 = 0.00324352 loss)
I1107 05:09:39.936959 41046 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I1107 05:09:52.630633 41046 solver.cpp:266] Iteration 9450 (3.93912 iter/s, 12.6932s/50 iter), loss = 0.0111107
I1107 05:09:52.630663 41046 solver.cpp:285]     Train net output #0: loss = 0.0111108 (* 1 = 0.0111108 loss)
I1107 05:09:52.630669 41046 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I1107 05:10:05.305207 41046 solver.cpp:266] Iteration 9500 (3.94506 iter/s, 12.6741s/50 iter), loss = 0.00679515
I1107 05:10:05.305238 41046 solver.cpp:285]     Train net output #0: loss = 0.00679518 (* 1 = 0.00679518 loss)
I1107 05:10:05.305244 41046 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I1107 05:10:17.998989 41046 solver.cpp:266] Iteration 9550 (3.93909 iter/s, 12.6933s/50 iter), loss = 0.0156969
I1107 05:10:17.999155 41046 solver.cpp:285]     Train net output #0: loss = 0.015697 (* 1 = 0.015697 loss)
I1107 05:10:17.999163 41046 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I1107 05:10:30.638933 41046 solver.cpp:266] Iteration 9600 (3.95591 iter/s, 12.6393s/50 iter), loss = 0.00237637
I1107 05:10:30.638963 41046 solver.cpp:285]     Train net output #0: loss = 0.0023764 (* 1 = 0.0023764 loss)
I1107 05:10:30.638969 41046 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I1107 05:10:43.340410 41046 solver.cpp:266] Iteration 9650 (3.93671 iter/s, 12.701s/50 iter), loss = 0.0151956
I1107 05:10:43.340440 41046 solver.cpp:285]     Train net output #0: loss = 0.0151957 (* 1 = 0.0151957 loss)
I1107 05:10:43.340445 41046 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I1107 05:10:56.041914 41046 solver.cpp:266] Iteration 9700 (3.9367 iter/s, 12.701s/50 iter), loss = 0.00350044
I1107 05:10:56.042088 41046 solver.cpp:285]     Train net output #0: loss = 0.00350046 (* 1 = 0.00350046 loss)
I1107 05:10:56.042098 41046 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I1107 05:11:08.669360 41046 solver.cpp:266] Iteration 9750 (3.95983 iter/s, 12.6268s/50 iter), loss = 0.00290888
I1107 05:11:08.669391 41046 solver.cpp:285]     Train net output #0: loss = 0.0029089 (* 1 = 0.0029089 loss)
I1107 05:11:08.669396 41046 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I1107 05:11:21.371063 41046 solver.cpp:266] Iteration 9800 (3.93664 iter/s, 12.7012s/50 iter), loss = 0.00234232
I1107 05:11:21.371093 41046 solver.cpp:285]     Train net output #0: loss = 0.00234233 (* 1 = 0.00234233 loss)
I1107 05:11:21.371098 41046 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I1107 05:11:34.035127 41046 solver.cpp:266] Iteration 9850 (3.94834 iter/s, 12.6636s/50 iter), loss = 0.000861563
I1107 05:11:34.035277 41046 solver.cpp:285]     Train net output #0: loss = 0.000861583 (* 1 = 0.000861583 loss)
I1107 05:11:34.035286 41046 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I1107 05:11:46.681993 41046 solver.cpp:266] Iteration 9900 (3.95374 iter/s, 12.6462s/50 iter), loss = 0.00171417
I1107 05:11:46.682024 41046 solver.cpp:285]     Train net output #0: loss = 0.00171419 (* 1 = 0.00171419 loss)
I1107 05:11:46.682029 41046 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I1107 05:11:59.358181 41046 solver.cpp:266] Iteration 9950 (3.94456 iter/s, 12.6757s/50 iter), loss = 0.0175831
I1107 05:11:59.358211 41046 solver.cpp:285]     Train net output #0: loss = 0.0175831 (* 1 = 0.0175831 loss)
I1107 05:11:59.358217 41046 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I1107 05:12:11.809713 41046 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_10000.caffemodel
I1107 05:12:14.073923 41046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_10000.solverstate
I1107 05:12:14.559317 41046 solver.cpp:418] Iteration 10000, Testing net (#0)
I1107 05:12:16.024238 41046 solver.cpp:517]     Test net output #0: loss = 0.217373 (* 1 = 0.217373 loss)
I1107 05:12:16.024253 41046 solver.cpp:517]     Test net output #1: top-1 = 0.95375
I1107 05:12:16.266129 41046 solver.cpp:266] Iteration 10000 (2.95731 iter/s, 16.9073s/50 iter), loss = 0.00456828
I1107 05:12:16.266157 41046 solver.cpp:285]     Train net output #0: loss = 0.0045683 (* 1 = 0.0045683 loss)
I1107 05:12:16.266163 41046 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I1107 05:12:28.806677 41046 solver.cpp:266] Iteration 10050 (3.98723 iter/s, 12.54s/50 iter), loss = 0.00489278
I1107 05:12:28.806707 41046 solver.cpp:285]     Train net output #0: loss = 0.00489279 (* 1 = 0.00489279 loss)
I1107 05:12:28.806730 41046 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I1107 05:12:41.366873 41046 solver.cpp:266] Iteration 10100 (3.98099 iter/s, 12.5597s/50 iter), loss = 0.00635186
I1107 05:12:41.366904 41046 solver.cpp:285]     Train net output #0: loss = 0.00635189 (* 1 = 0.00635189 loss)
I1107 05:12:41.366909 41046 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I1107 05:12:53.968379 41046 solver.cpp:266] Iteration 10150 (3.96794 iter/s, 12.601s/50 iter), loss = 0.00428921
I1107 05:12:53.968528 41046 solver.cpp:285]     Train net output #0: loss = 0.00428924 (* 1 = 0.00428924 loss)
I1107 05:12:53.968536 41046 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I1107 05:13:06.645988 41046 solver.cpp:266] Iteration 10200 (3.94416 iter/s, 12.677s/50 iter), loss = 0.00400911
I1107 05:13:06.646019 41046 solver.cpp:285]     Train net output #0: loss = 0.00400914 (* 1 = 0.00400914 loss)
I1107 05:13:06.646026 41046 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I1107 05:13:19.302037 41046 solver.cpp:266] Iteration 10250 (3.95084 iter/s, 12.6555s/50 iter), loss = 0.00247374
I1107 05:13:19.302067 41046 solver.cpp:285]     Train net output #0: loss = 0.00247377 (* 1 = 0.00247377 loss)
I1107 05:13:19.302072 41046 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I1107 05:13:31.997189 41046 solver.cpp:266] Iteration 10300 (3.93867 iter/s, 12.6946s/50 iter), loss = 0.00318831
I1107 05:13:31.997360 41046 solver.cpp:285]     Train net output #0: loss = 0.00318834 (* 1 = 0.00318834 loss)
I1107 05:13:31.997370 41046 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I1107 05:13:44.687405 41046 solver.cpp:266] Iteration 10350 (3.94024 iter/s, 12.6896s/50 iter), loss = 0.0179806
I1107 05:13:44.687438 41046 solver.cpp:285]     Train net output #0: loss = 0.0179806 (* 1 = 0.0179806 loss)
I1107 05:13:44.687444 41046 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I1107 05:13:57.377158 41046 solver.cpp:266] Iteration 10400 (3.94035 iter/s, 12.6892s/50 iter), loss = 0.000869082
I1107 05:13:57.377187 41046 solver.cpp:285]     Train net output #0: loss = 0.000869108 (* 1 = 0.000869108 loss)
I1107 05:13:57.377193 41046 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I1107 05:14:10.008522 41046 solver.cpp:266] Iteration 10450 (3.95856 iter/s, 12.6309s/50 iter), loss = 0.00299721
I1107 05:14:10.008738 41046 solver.cpp:285]     Train net output #0: loss = 0.00299723 (* 1 = 0.00299723 loss)
I1107 05:14:10.008745 41046 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I1107 05:14:22.712728 41046 solver.cpp:266] Iteration 10500 (3.93592 iter/s, 12.7035s/50 iter), loss = 0.00321222
I1107 05:14:22.712759 41046 solver.cpp:285]     Train net output #0: loss = 0.00321225 (* 1 = 0.00321225 loss)
I1107 05:14:22.712764 41046 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I1107 05:14:35.412897 41046 solver.cpp:266] Iteration 10550 (3.93711 iter/s, 12.6997s/50 iter), loss = 0.00237249
I1107 05:14:35.412927 41046 solver.cpp:285]     Train net output #0: loss = 0.00237251 (* 1 = 0.00237251 loss)
I1107 05:14:35.412933 41046 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I1107 05:14:48.109450 41046 solver.cpp:266] Iteration 10600 (3.93824 iter/s, 12.696s/50 iter), loss = 0.00302393
I1107 05:14:48.109606 41046 solver.cpp:285]     Train net output #0: loss = 0.00302396 (* 1 = 0.00302396 loss)
I1107 05:14:48.109613 41046 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I1107 05:15:00.744052 41046 solver.cpp:266] Iteration 10650 (3.95758 iter/s, 12.634s/50 iter), loss = 0.00973712
I1107 05:15:00.744082 41046 solver.cpp:285]     Train net output #0: loss = 0.00973715 (* 1 = 0.00973715 loss)
I1107 05:15:00.744088 41046 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I1107 05:15:13.490679 41046 solver.cpp:266] Iteration 10700 (3.92276 iter/s, 12.7461s/50 iter), loss = 0.00615688
I1107 05:15:13.490708 41046 solver.cpp:285]     Train net output #0: loss = 0.00615691 (* 1 = 0.00615691 loss)
I1107 05:15:13.490715 41046 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I1107 05:15:26.183359 41046 solver.cpp:266] Iteration 10750 (3.93944 iter/s, 12.6922s/50 iter), loss = 0.00690548
I1107 05:15:26.183522 41046 solver.cpp:285]     Train net output #0: loss = 0.00690551 (* 1 = 0.00690551 loss)
I1107 05:15:26.183531 41046 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I1107 05:15:38.838845 41046 solver.cpp:266] Iteration 10800 (3.95107 iter/s, 12.6548s/50 iter), loss = 0.0205299
I1107 05:15:38.838876 41046 solver.cpp:285]     Train net output #0: loss = 0.02053 (* 1 = 0.02053 loss)
I1107 05:15:38.838881 41046 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I1107 05:15:51.517040 41046 solver.cpp:266] Iteration 10850 (3.94396 iter/s, 12.6776s/50 iter), loss = 0.00553442
I1107 05:15:51.517068 41046 solver.cpp:285]     Train net output #0: loss = 0.00553444 (* 1 = 0.00553444 loss)
I1107 05:15:51.517073 41046 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I1107 05:16:04.159193 41046 solver.cpp:266] Iteration 10900 (3.9552 iter/s, 12.6416s/50 iter), loss = 0.00170124
I1107 05:16:04.159349 41046 solver.cpp:285]     Train net output #0: loss = 0.00170127 (* 1 = 0.00170127 loss)
I1107 05:16:04.159358 41046 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I1107 05:16:16.870134 41046 solver.cpp:266] Iteration 10950 (3.93383 iter/s, 12.7102s/50 iter), loss = 0.00304902
I1107 05:16:16.870164 41046 solver.cpp:285]     Train net output #0: loss = 0.00304904 (* 1 = 0.00304904 loss)
I1107 05:16:16.870172 41046 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I1107 05:16:29.299454 41046 solver.cpp:418] Iteration 11000, Testing net (#0)
I1107 05:16:30.801074 41046 solver.cpp:517]     Test net output #0: loss = 0.219391 (* 1 = 0.219391 loss)
I1107 05:16:30.801089 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9535
I1107 05:16:31.047863 41046 solver.cpp:266] Iteration 11000 (3.52681 iter/s, 14.1771s/50 iter), loss = 0.00423323
I1107 05:16:31.047888 41046 solver.cpp:285]     Train net output #0: loss = 0.00423326 (* 1 = 0.00423326 loss)
I1107 05:16:31.047910 41046 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I1107 05:16:43.692577 41046 solver.cpp:266] Iteration 11050 (3.9544 iter/s, 12.6442s/50 iter), loss = 0.00274011
I1107 05:16:43.692777 41046 solver.cpp:285]     Train net output #0: loss = 0.00274013 (* 1 = 0.00274013 loss)
I1107 05:16:43.692785 41046 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I1107 05:16:56.352757 41046 solver.cpp:266] Iteration 11100 (3.94962 iter/s, 12.6595s/50 iter), loss = 0.00933943
I1107 05:16:56.352787 41046 solver.cpp:285]     Train net output #0: loss = 0.00933945 (* 1 = 0.00933945 loss)
I1107 05:16:56.352793 41046 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I1107 05:17:09.029858 41046 solver.cpp:266] Iteration 11150 (3.94429 iter/s, 12.6765s/50 iter), loss = 0.000755978
I1107 05:17:09.029887 41046 solver.cpp:285]     Train net output #0: loss = 0.000756004 (* 1 = 0.000756004 loss)
I1107 05:17:09.029903 41046 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I1107 05:17:21.676260 41046 solver.cpp:266] Iteration 11200 (3.95387 iter/s, 12.6458s/50 iter), loss = 0.00278812
I1107 05:17:21.676425 41046 solver.cpp:285]     Train net output #0: loss = 0.00278814 (* 1 = 0.00278814 loss)
I1107 05:17:21.676434 41046 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I1107 05:17:34.396668 41046 solver.cpp:266] Iteration 11250 (3.93091 iter/s, 12.7197s/50 iter), loss = 0.00717152
I1107 05:17:34.396698 41046 solver.cpp:285]     Train net output #0: loss = 0.00717154 (* 1 = 0.00717154 loss)
I1107 05:17:34.396704 41046 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I1107 05:17:47.041852 41046 solver.cpp:266] Iteration 11300 (3.95425 iter/s, 12.6446s/50 iter), loss = 0.0053903
I1107 05:17:47.041883 41046 solver.cpp:285]     Train net output #0: loss = 0.00539032 (* 1 = 0.00539032 loss)
I1107 05:17:47.041889 41046 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I1107 05:17:59.715732 41046 solver.cpp:266] Iteration 11350 (3.9453 iter/s, 12.6733s/50 iter), loss = 0.00937889
I1107 05:17:59.715883 41046 solver.cpp:285]     Train net output #0: loss = 0.00937891 (* 1 = 0.00937891 loss)
I1107 05:17:59.715891 41046 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I1107 05:18:12.384991 41046 solver.cpp:266] Iteration 11400 (3.94677 iter/s, 12.6686s/50 iter), loss = 0.00394196
I1107 05:18:12.385020 41046 solver.cpp:285]     Train net output #0: loss = 0.00394198 (* 1 = 0.00394198 loss)
I1107 05:18:12.385113 41046 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I1107 05:18:25.085211 41046 solver.cpp:266] Iteration 11450 (3.93714 iter/s, 12.6996s/50 iter), loss = 0.0054048
I1107 05:18:25.085242 41046 solver.cpp:285]     Train net output #0: loss = 0.00540483 (* 1 = 0.00540483 loss)
I1107 05:18:25.085247 41046 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I1107 05:18:37.718456 41046 solver.cpp:266] Iteration 11500 (3.95799 iter/s, 12.6327s/50 iter), loss = 0.00212751
I1107 05:18:37.718621 41046 solver.cpp:285]     Train net output #0: loss = 0.00212753 (* 1 = 0.00212753 loss)
I1107 05:18:37.718631 41046 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I1107 05:18:50.414230 41046 solver.cpp:266] Iteration 11550 (3.93853 iter/s, 12.6951s/50 iter), loss = 0.00204147
I1107 05:18:50.414260 41046 solver.cpp:285]     Train net output #0: loss = 0.00204149 (* 1 = 0.00204149 loss)
I1107 05:18:50.414266 41046 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I1107 05:19:03.085999 41046 solver.cpp:266] Iteration 11600 (3.94595 iter/s, 12.6712s/50 iter), loss = 0.00122476
I1107 05:19:03.086030 41046 solver.cpp:285]     Train net output #0: loss = 0.00122478 (* 1 = 0.00122478 loss)
I1107 05:19:03.086036 41046 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I1107 05:19:15.767324 41046 solver.cpp:266] Iteration 11650 (3.94298 iter/s, 12.6808s/50 iter), loss = 0.000912954
I1107 05:19:15.767501 41046 solver.cpp:285]     Train net output #0: loss = 0.000912979 (* 1 = 0.000912979 loss)
I1107 05:19:15.767510 41046 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I1107 05:19:28.427131 41046 solver.cpp:266] Iteration 11700 (3.94972 iter/s, 12.6591s/50 iter), loss = 0.0011516
I1107 05:19:28.427160 41046 solver.cpp:285]     Train net output #0: loss = 0.00115163 (* 1 = 0.00115163 loss)
I1107 05:19:28.427166 41046 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I1107 05:19:41.089764 41046 solver.cpp:266] Iteration 11750 (3.9488 iter/s, 12.6621s/50 iter), loss = 0.000813083
I1107 05:19:41.089798 41046 solver.cpp:285]     Train net output #0: loss = 0.00081311 (* 1 = 0.00081311 loss)
I1107 05:19:41.089804 41046 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I1107 05:19:53.755703 41046 solver.cpp:266] Iteration 11800 (3.94777 iter/s, 12.6654s/50 iter), loss = 0.0102466
I1107 05:19:53.755867 41046 solver.cpp:285]     Train net output #0: loss = 0.0102467 (* 1 = 0.0102467 loss)
I1107 05:19:53.755875 41046 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I1107 05:20:06.429759 41046 solver.cpp:266] Iteration 11850 (3.94528 iter/s, 12.6734s/50 iter), loss = 0.00374293
I1107 05:20:06.429788 41046 solver.cpp:285]     Train net output #0: loss = 0.00374296 (* 1 = 0.00374296 loss)
I1107 05:20:06.429795 41046 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I1107 05:20:19.120707 41046 solver.cpp:266] Iteration 11900 (3.93999 iter/s, 12.6904s/50 iter), loss = 0.00520042
I1107 05:20:19.120736 41046 solver.cpp:285]     Train net output #0: loss = 0.00520045 (* 1 = 0.00520045 loss)
I1107 05:20:19.120743 41046 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I1107 05:20:31.795490 41046 solver.cpp:266] Iteration 11950 (3.94501 iter/s, 12.6742s/50 iter), loss = 0.001016
I1107 05:20:31.795629 41046 solver.cpp:285]     Train net output #0: loss = 0.00101603 (* 1 = 0.00101603 loss)
I1107 05:20:31.795637 41046 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I1107 05:20:44.241125 41046 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_12000.caffemodel
I1107 05:20:46.547441 41046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.3/snapshots/_iter_12000.solverstate
I1107 05:20:47.138311 41046 solver.cpp:378] Iteration 12000, loss = 0.00100234
I1107 05:20:47.138332 41046 solver.cpp:418] Iteration 12000, Testing net (#0)
I1107 05:20:48.596477 41046 solver.cpp:517]     Test net output #0: loss = 0.220153 (* 1 = 0.220153 loss)
I1107 05:20:48.596493 41046 solver.cpp:517]     Test net output #1: top-1 = 0.9535
I1107 05:20:48.596498 41046 solver.cpp:386] Optimization Done (3.92642 iter/s).
I1107 05:20:48.596500 41046 caffe_interface.cpp:530] Optimization Done.
I1107 05:20:49.555681 41413 pruning_runner.cpp:190] Sens info found, use it.
I1107 05:20:50.787968 41413 pruning_runner.cpp:217] Start compressing, please wait...
I1107 05:20:56.780956 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:02.755098 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:09.028666 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:15.211971 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:21.537878 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:27.726197 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:35.149972 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:41.373847 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:47.659366 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:21:53.849622 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:22:00.172930 41413 pruning_runner.cpp:264] Compression complete 0%
I1107 05:22:06.056586 41413 pruning_runner.cpp:264] Compression complete 50%
I1107 05:22:12.123981 41413 pruning_runner.cpp:264] Compression complete 96.9697%
I1107 05:22:18.166308 41413 pruning_runner.cpp:264] Compression complete 99.2424%
I1107 05:22:24.244269 41413 pruning_runner.cpp:264] Compression complete 99.9047%
I1107 05:22:30.242224 41413 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 05:22:30.242548 41413 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 05:22:30.242890 41413 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 05:22:30.243055 41413 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 05:22:30.243157 41413 layer_factory.hpp:77] Creating layer data
I1107 05:22:30.243193 41413 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:30.243577 41413 net.cpp:94] Creating Layer data
I1107 05:22:30.243589 41413 net.cpp:409] data -> data
I1107 05:22:30.243597 41413 net.cpp:409] data -> label
I1107 05:22:30.244614 42895 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 05:22:30.244647 42895 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 05:22:30.244820 41413 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 05:22:30.244889 41413 data_layer.cpp:83] output data size: 50,3,227,227
I1107 05:22:30.326159 41413 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:30.326211 41413 net.cpp:144] Setting up data
I1107 05:22:30.326220 41413 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 05:22:30.326243 41413 net.cpp:151] Top shape: 50 (50)
I1107 05:22:30.326246 41413 net.cpp:159] Memory required for data: 30917600
I1107 05:22:30.326249 41413 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 05:22:30.326261 41413 net.cpp:94] Creating Layer label_data_1_split
I1107 05:22:30.326265 41413 net.cpp:435] label_data_1_split <- label
I1107 05:22:30.326270 41413 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 05:22:30.326279 41413 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 05:22:30.326326 41413 net.cpp:144] Setting up label_data_1_split
I1107 05:22:30.326331 41413 net.cpp:151] Top shape: 50 (50)
I1107 05:22:30.326333 41413 net.cpp:151] Top shape: 50 (50)
I1107 05:22:30.326335 41413 net.cpp:159] Memory required for data: 30918000
I1107 05:22:30.326339 41413 layer_factory.hpp:77] Creating layer conv1
I1107 05:22:30.326362 41413 net.cpp:94] Creating Layer conv1
I1107 05:22:30.326365 41413 net.cpp:435] conv1 <- data
I1107 05:22:30.326370 41413 net.cpp:409] conv1 -> conv1
I1107 05:22:30.328212 41413 net.cpp:144] Setting up conv1
I1107 05:22:30.328227 41413 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:30.328228 41413 net.cpp:159] Memory required for data: 88998000
I1107 05:22:30.328239 41413 layer_factory.hpp:77] Creating layer bn1
I1107 05:22:30.328250 41413 net.cpp:94] Creating Layer bn1
I1107 05:22:30.328254 41413 net.cpp:435] bn1 <- conv1
I1107 05:22:30.328260 41413 net.cpp:409] bn1 -> scale1
I1107 05:22:30.328938 41413 net.cpp:144] Setting up bn1
I1107 05:22:30.328945 41413 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:30.328948 41413 net.cpp:159] Memory required for data: 147078000
I1107 05:22:30.328956 41413 layer_factory.hpp:77] Creating layer relu1
I1107 05:22:30.328963 41413 net.cpp:94] Creating Layer relu1
I1107 05:22:30.328968 41413 net.cpp:435] relu1 <- scale1
I1107 05:22:30.328971 41413 net.cpp:409] relu1 -> relu1
I1107 05:22:30.328997 41413 net.cpp:144] Setting up relu1
I1107 05:22:30.329002 41413 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:30.329005 41413 net.cpp:159] Memory required for data: 205158000
I1107 05:22:30.329008 41413 layer_factory.hpp:77] Creating layer pool1
I1107 05:22:30.329013 41413 net.cpp:94] Creating Layer pool1
I1107 05:22:30.329015 41413 net.cpp:435] pool1 <- relu1
I1107 05:22:30.329020 41413 net.cpp:409] pool1 -> pool1
I1107 05:22:30.329075 41413 net.cpp:144] Setting up pool1
I1107 05:22:30.329082 41413 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 05:22:30.329083 41413 net.cpp:159] Memory required for data: 219154800
I1107 05:22:30.329087 41413 layer_factory.hpp:77] Creating layer conv2
I1107 05:22:30.329093 41413 net.cpp:94] Creating Layer conv2
I1107 05:22:30.329095 41413 net.cpp:435] conv2 <- pool1
I1107 05:22:30.329099 41413 net.cpp:409] conv2 -> conv2
I1107 05:22:30.336174 41413 net.cpp:144] Setting up conv2
I1107 05:22:30.336191 41413 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:30.336195 41413 net.cpp:159] Memory required for data: 256479600
I1107 05:22:30.336207 41413 layer_factory.hpp:77] Creating layer bn2
I1107 05:22:30.336217 41413 net.cpp:94] Creating Layer bn2
I1107 05:22:30.336222 41413 net.cpp:435] bn2 <- conv2
I1107 05:22:30.336228 41413 net.cpp:409] bn2 -> scale2
I1107 05:22:30.336817 41413 net.cpp:144] Setting up bn2
I1107 05:22:30.336824 41413 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:30.336827 41413 net.cpp:159] Memory required for data: 293804400
I1107 05:22:30.336835 41413 layer_factory.hpp:77] Creating layer relu2
I1107 05:22:30.336843 41413 net.cpp:94] Creating Layer relu2
I1107 05:22:30.336848 41413 net.cpp:435] relu2 <- scale2
I1107 05:22:30.336853 41413 net.cpp:409] relu2 -> relu2
I1107 05:22:30.336890 41413 net.cpp:144] Setting up relu2
I1107 05:22:30.336896 41413 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:30.336899 41413 net.cpp:159] Memory required for data: 331129200
I1107 05:22:30.336902 41413 layer_factory.hpp:77] Creating layer pool2
I1107 05:22:30.336908 41413 net.cpp:94] Creating Layer pool2
I1107 05:22:30.336913 41413 net.cpp:435] pool2 <- relu2
I1107 05:22:30.336918 41413 net.cpp:409] pool2 -> pool2
I1107 05:22:30.336946 41413 net.cpp:144] Setting up pool2
I1107 05:22:30.336951 41413 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:30.336954 41413 net.cpp:159] Memory required for data: 339782000
I1107 05:22:30.336957 41413 layer_factory.hpp:77] Creating layer conv3
I1107 05:22:30.336966 41413 net.cpp:94] Creating Layer conv3
I1107 05:22:30.336969 41413 net.cpp:435] conv3 <- pool2
I1107 05:22:30.336974 41413 net.cpp:409] conv3 -> conv3
I1107 05:22:30.350399 41413 net.cpp:144] Setting up conv3
I1107 05:22:30.350425 41413 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:30.350428 41413 net.cpp:159] Memory required for data: 352761200
I1107 05:22:30.350437 41413 layer_factory.hpp:77] Creating layer relu3
I1107 05:22:30.350468 41413 net.cpp:94] Creating Layer relu3
I1107 05:22:30.350474 41413 net.cpp:435] relu3 <- conv3
I1107 05:22:30.350481 41413 net.cpp:409] relu3 -> relu3
I1107 05:22:30.350509 41413 net.cpp:144] Setting up relu3
I1107 05:22:30.350518 41413 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:30.350522 41413 net.cpp:159] Memory required for data: 365740400
I1107 05:22:30.350525 41413 layer_factory.hpp:77] Creating layer conv4
I1107 05:22:30.350539 41413 net.cpp:94] Creating Layer conv4
I1107 05:22:30.350548 41413 net.cpp:435] conv4 <- relu3
I1107 05:22:30.350556 41413 net.cpp:409] conv4 -> conv4
I1107 05:22:30.363888 41413 net.cpp:144] Setting up conv4
I1107 05:22:30.363910 41413 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:30.363914 41413 net.cpp:159] Memory required for data: 378719600
I1107 05:22:30.363926 41413 layer_factory.hpp:77] Creating layer relu4
I1107 05:22:30.363950 41413 net.cpp:94] Creating Layer relu4
I1107 05:22:30.363956 41413 net.cpp:435] relu4 <- conv4
I1107 05:22:30.363963 41413 net.cpp:409] relu4 -> relu4
I1107 05:22:30.363987 41413 net.cpp:144] Setting up relu4
I1107 05:22:30.363991 41413 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:30.363994 41413 net.cpp:159] Memory required for data: 391698800
I1107 05:22:30.363998 41413 layer_factory.hpp:77] Creating layer conv5
I1107 05:22:30.364008 41413 net.cpp:94] Creating Layer conv5
I1107 05:22:30.364020 41413 net.cpp:435] conv5 <- relu4
I1107 05:22:30.364027 41413 net.cpp:409] conv5 -> conv5
I1107 05:22:30.373641 41413 net.cpp:144] Setting up conv5
I1107 05:22:30.373663 41413 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:30.373667 41413 net.cpp:159] Memory required for data: 400351600
I1107 05:22:30.373674 41413 layer_factory.hpp:77] Creating layer relu5
I1107 05:22:30.373684 41413 net.cpp:94] Creating Layer relu5
I1107 05:22:30.373704 41413 net.cpp:435] relu5 <- conv5
I1107 05:22:30.373713 41413 net.cpp:409] relu5 -> relu5
I1107 05:22:30.373749 41413 net.cpp:144] Setting up relu5
I1107 05:22:30.373754 41413 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:30.373756 41413 net.cpp:159] Memory required for data: 409004400
I1107 05:22:30.373760 41413 layer_factory.hpp:77] Creating layer pool5
I1107 05:22:30.373769 41413 net.cpp:94] Creating Layer pool5
I1107 05:22:30.373773 41413 net.cpp:435] pool5 <- relu5
I1107 05:22:30.373780 41413 net.cpp:409] pool5 -> pool5
I1107 05:22:30.373807 41413 net.cpp:144] Setting up pool5
I1107 05:22:30.373812 41413 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 05:22:30.373816 41413 net.cpp:159] Memory required for data: 410847600
I1107 05:22:30.373819 41413 layer_factory.hpp:77] Creating layer fc6
I1107 05:22:30.373827 41413 net.cpp:94] Creating Layer fc6
I1107 05:22:30.373831 41413 net.cpp:435] fc6 <- pool5
I1107 05:22:30.373838 41413 net.cpp:409] fc6 -> fc6
I1107 05:22:30.686118 41413 net.cpp:144] Setting up fc6
I1107 05:22:30.686144 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.686148 41413 net.cpp:159] Memory required for data: 411666800
I1107 05:22:30.686158 41413 layer_factory.hpp:77] Creating layer relu6
I1107 05:22:30.686184 41413 net.cpp:94] Creating Layer relu6
I1107 05:22:30.686190 41413 net.cpp:435] relu6 <- fc6
I1107 05:22:30.686197 41413 net.cpp:409] relu6 -> relu6
I1107 05:22:30.686239 41413 net.cpp:144] Setting up relu6
I1107 05:22:30.686245 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.686249 41413 net.cpp:159] Memory required for data: 412486000
I1107 05:22:30.686251 41413 layer_factory.hpp:77] Creating layer drop6
I1107 05:22:30.686259 41413 net.cpp:94] Creating Layer drop6
I1107 05:22:30.686264 41413 net.cpp:435] drop6 <- relu6
I1107 05:22:30.686269 41413 net.cpp:409] drop6 -> drop6
I1107 05:22:30.686297 41413 net.cpp:144] Setting up drop6
I1107 05:22:30.686302 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.686305 41413 net.cpp:159] Memory required for data: 413305200
I1107 05:22:30.686308 41413 layer_factory.hpp:77] Creating layer fc7
I1107 05:22:30.686316 41413 net.cpp:94] Creating Layer fc7
I1107 05:22:30.686333 41413 net.cpp:435] fc7 <- drop6
I1107 05:22:30.686339 41413 net.cpp:409] fc7 -> fc7
I1107 05:22:30.824657 41413 net.cpp:144] Setting up fc7
I1107 05:22:30.824693 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.824697 41413 net.cpp:159] Memory required for data: 414124400
I1107 05:22:30.824707 41413 layer_factory.hpp:77] Creating layer bn7
I1107 05:22:30.824719 41413 net.cpp:94] Creating Layer bn7
I1107 05:22:30.824723 41413 net.cpp:435] bn7 <- fc7
I1107 05:22:30.824733 41413 net.cpp:409] bn7 -> scale7
I1107 05:22:30.825266 41413 net.cpp:144] Setting up bn7
I1107 05:22:30.825273 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.825276 41413 net.cpp:159] Memory required for data: 414943600
I1107 05:22:30.825286 41413 layer_factory.hpp:77] Creating layer relu7
I1107 05:22:30.825294 41413 net.cpp:94] Creating Layer relu7
I1107 05:22:30.825297 41413 net.cpp:435] relu7 <- scale7
I1107 05:22:30.825302 41413 net.cpp:409] relu7 -> relu7
I1107 05:22:30.825323 41413 net.cpp:144] Setting up relu7
I1107 05:22:30.825328 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.825331 41413 net.cpp:159] Memory required for data: 415762800
I1107 05:22:30.825335 41413 layer_factory.hpp:77] Creating layer drop7
I1107 05:22:30.825342 41413 net.cpp:94] Creating Layer drop7
I1107 05:22:30.825346 41413 net.cpp:435] drop7 <- relu7
I1107 05:22:30.825350 41413 net.cpp:409] drop7 -> drop7
I1107 05:22:30.825377 41413 net.cpp:144] Setting up drop7
I1107 05:22:30.825382 41413 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:30.825386 41413 net.cpp:159] Memory required for data: 416582000
I1107 05:22:30.825388 41413 layer_factory.hpp:77] Creating layer fc8
I1107 05:22:30.825397 41413 net.cpp:94] Creating Layer fc8
I1107 05:22:30.825400 41413 net.cpp:435] fc8 <- drop7
I1107 05:22:30.825407 41413 net.cpp:409] fc8 -> fc8
I1107 05:22:30.826288 41413 net.cpp:144] Setting up fc8
I1107 05:22:30.826301 41413 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:30.826305 41413 net.cpp:159] Memory required for data: 416582400
I1107 05:22:30.826313 41413 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 05:22:30.826320 41413 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 05:22:30.826324 41413 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 05:22:30.826333 41413 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 05:22:30.826341 41413 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 05:22:30.826372 41413 net.cpp:144] Setting up fc8_fc8_0_split
I1107 05:22:30.826377 41413 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:30.826381 41413 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:30.826385 41413 net.cpp:159] Memory required for data: 416583200
I1107 05:22:30.826390 41413 layer_factory.hpp:77] Creating layer loss
I1107 05:22:30.826395 41413 net.cpp:94] Creating Layer loss
I1107 05:22:30.826400 41413 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 05:22:30.826405 41413 net.cpp:435] loss <- label_data_1_split_0
I1107 05:22:30.826411 41413 net.cpp:409] loss -> loss
I1107 05:22:30.826421 41413 layer_factory.hpp:77] Creating layer loss
I1107 05:22:30.826493 41413 net.cpp:144] Setting up loss
I1107 05:22:30.826498 41413 net.cpp:151] Top shape: (1)
I1107 05:22:30.826500 41413 net.cpp:154]     with loss weight 1
I1107 05:22:30.826511 41413 net.cpp:159] Memory required for data: 416583204
I1107 05:22:30.826515 41413 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 05:22:30.826522 41413 net.cpp:94] Creating Layer accuracy-top1
I1107 05:22:30.826525 41413 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 05:22:30.826529 41413 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 05:22:30.826537 41413 net.cpp:409] accuracy-top1 -> top-1
I1107 05:22:30.826545 41413 net.cpp:144] Setting up accuracy-top1
I1107 05:22:30.826550 41413 net.cpp:151] Top shape: (1)
I1107 05:22:30.826552 41413 net.cpp:159] Memory required for data: 416583208
I1107 05:22:30.826557 41413 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 05:22:30.826562 41413 net.cpp:220] loss needs backward computation.
I1107 05:22:30.826576 41413 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 05:22:30.826581 41413 net.cpp:220] fc8 needs backward computation.
I1107 05:22:30.826584 41413 net.cpp:220] drop7 needs backward computation.
I1107 05:22:30.826588 41413 net.cpp:220] relu7 needs backward computation.
I1107 05:22:30.826592 41413 net.cpp:220] bn7 needs backward computation.
I1107 05:22:30.826596 41413 net.cpp:220] fc7 needs backward computation.
I1107 05:22:30.826602 41413 net.cpp:220] drop6 needs backward computation.
I1107 05:22:30.826607 41413 net.cpp:220] relu6 needs backward computation.
I1107 05:22:30.826611 41413 net.cpp:220] fc6 needs backward computation.
I1107 05:22:30.826616 41413 net.cpp:220] pool5 needs backward computation.
I1107 05:22:30.826620 41413 net.cpp:220] relu5 needs backward computation.
I1107 05:22:30.826624 41413 net.cpp:220] conv5 needs backward computation.
I1107 05:22:30.826629 41413 net.cpp:220] relu4 needs backward computation.
I1107 05:22:30.826633 41413 net.cpp:220] conv4 needs backward computation.
I1107 05:22:30.826638 41413 net.cpp:220] relu3 needs backward computation.
I1107 05:22:30.826642 41413 net.cpp:220] conv3 needs backward computation.
I1107 05:22:30.826647 41413 net.cpp:220] pool2 needs backward computation.
I1107 05:22:30.826650 41413 net.cpp:220] relu2 needs backward computation.
I1107 05:22:30.826655 41413 net.cpp:220] bn2 needs backward computation.
I1107 05:22:30.826659 41413 net.cpp:220] conv2 needs backward computation.
I1107 05:22:30.826663 41413 net.cpp:220] pool1 needs backward computation.
I1107 05:22:30.826668 41413 net.cpp:220] relu1 needs backward computation.
I1107 05:22:30.826671 41413 net.cpp:220] bn1 needs backward computation.
I1107 05:22:30.826675 41413 net.cpp:220] conv1 needs backward computation.
I1107 05:22:30.826680 41413 net.cpp:222] label_data_1_split does not need backward computation.
I1107 05:22:30.826685 41413 net.cpp:222] data does not need backward computation.
I1107 05:22:30.826689 41413 net.cpp:264] This network produces output loss
I1107 05:22:30.826694 41413 net.cpp:264] This network produces output top-1
I1107 05:22:30.826714 41413 net.cpp:284] Network initialization done.
I1107 05:22:30.900521 41413 caffe_interface.cpp:363] Running for 80 iterations.
I1107 05:22:30.943341 41413 caffe_interface.cpp:125] Batch 0, loss = 0.251929
I1107 05:22:30.943362 41413 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I1107 05:22:30.963436 41413 caffe_interface.cpp:125] Batch 1, loss = 0.226433
I1107 05:22:30.963449 41413 caffe_interface.cpp:125] Batch 1, top-1 = 0.92
I1107 05:22:30.982570 41413 caffe_interface.cpp:125] Batch 2, loss = 0.366968
I1107 05:22:30.982580 41413 caffe_interface.cpp:125] Batch 2, top-1 = 0.94
I1107 05:22:31.002544 41413 caffe_interface.cpp:125] Batch 3, loss = 0.622103
I1107 05:22:31.002553 41413 caffe_interface.cpp:125] Batch 3, top-1 = 0.92
I1107 05:22:31.021708 41413 caffe_interface.cpp:125] Batch 4, loss = 0.438126
I1107 05:22:31.021718 41413 caffe_interface.cpp:125] Batch 4, top-1 = 0.92
I1107 05:22:31.042009 41413 caffe_interface.cpp:125] Batch 5, loss = 0.000780586
I1107 05:22:31.042018 41413 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 05:22:31.061144 41413 caffe_interface.cpp:125] Batch 6, loss = 0.357857
I1107 05:22:31.061153 41413 caffe_interface.cpp:125] Batch 6, top-1 = 0.96
I1107 05:22:31.080485 41413 caffe_interface.cpp:125] Batch 7, loss = 0.12605
I1107 05:22:31.080493 41413 caffe_interface.cpp:125] Batch 7, top-1 = 0.96
I1107 05:22:31.100317 41413 caffe_interface.cpp:125] Batch 8, loss = 0.0216265
I1107 05:22:31.100327 41413 caffe_interface.cpp:125] Batch 8, top-1 = 0.98
I1107 05:22:31.118192 41413 caffe_interface.cpp:125] Batch 9, loss = 0.276424
I1107 05:22:31.118201 41413 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I1107 05:22:31.137291 41413 caffe_interface.cpp:125] Batch 10, loss = 0.102798
I1107 05:22:31.137300 41413 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I1107 05:22:31.155305 41413 caffe_interface.cpp:125] Batch 11, loss = 0.317738
I1107 05:22:31.155313 41413 caffe_interface.cpp:125] Batch 11, top-1 = 0.94
I1107 05:22:31.174281 41413 caffe_interface.cpp:125] Batch 12, loss = 0.404101
I1107 05:22:31.174290 41413 caffe_interface.cpp:125] Batch 12, top-1 = 0.92
I1107 05:22:31.192278 41413 caffe_interface.cpp:125] Batch 13, loss = 0.199866
I1107 05:22:31.192287 41413 caffe_interface.cpp:125] Batch 13, top-1 = 0.94
I1107 05:22:31.210366 41413 caffe_interface.cpp:125] Batch 14, loss = 0.329076
I1107 05:22:31.210373 41413 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I1107 05:22:31.228374 41413 caffe_interface.cpp:125] Batch 15, loss = 0.298398
I1107 05:22:31.228381 41413 caffe_interface.cpp:125] Batch 15, top-1 = 0.92
I1107 05:22:31.246877 41413 caffe_interface.cpp:125] Batch 16, loss = 0.198449
I1107 05:22:31.246887 41413 caffe_interface.cpp:125] Batch 16, top-1 = 0.96
I1107 05:22:31.264945 41413 caffe_interface.cpp:125] Batch 17, loss = 0.0596705
I1107 05:22:31.264955 41413 caffe_interface.cpp:125] Batch 17, top-1 = 0.98
I1107 05:22:31.283488 41413 caffe_interface.cpp:125] Batch 18, loss = 0.318379
I1107 05:22:31.283496 41413 caffe_interface.cpp:125] Batch 18, top-1 = 0.96
I1107 05:22:31.301589 41413 caffe_interface.cpp:125] Batch 19, loss = 0.365982
I1107 05:22:31.301595 41413 caffe_interface.cpp:125] Batch 19, top-1 = 0.94
I1107 05:22:31.320646 41413 caffe_interface.cpp:125] Batch 20, loss = 0.0690379
I1107 05:22:31.320654 41413 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I1107 05:22:31.338677 41413 caffe_interface.cpp:125] Batch 21, loss = 0.209574
I1107 05:22:31.338685 41413 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I1107 05:22:31.357818 41413 caffe_interface.cpp:125] Batch 22, loss = 0.250596
I1107 05:22:31.357826 41413 caffe_interface.cpp:125] Batch 22, top-1 = 0.96
I1107 05:22:31.377024 41413 caffe_interface.cpp:125] Batch 23, loss = 0.182695
I1107 05:22:31.377032 41413 caffe_interface.cpp:125] Batch 23, top-1 = 0.96
I1107 05:22:31.395115 41413 caffe_interface.cpp:125] Batch 24, loss = 0.325778
I1107 05:22:31.395123 41413 caffe_interface.cpp:125] Batch 24, top-1 = 0.92
I1107 05:22:31.413962 41413 caffe_interface.cpp:125] Batch 25, loss = 0.197992
I1107 05:22:31.413971 41413 caffe_interface.cpp:125] Batch 25, top-1 = 0.96
I1107 05:22:31.431968 41413 caffe_interface.cpp:125] Batch 26, loss = 0.176514
I1107 05:22:31.431977 41413 caffe_interface.cpp:125] Batch 26, top-1 = 0.98
I1107 05:22:31.450776 41413 caffe_interface.cpp:125] Batch 27, loss = 0.223998
I1107 05:22:31.450784 41413 caffe_interface.cpp:125] Batch 27, top-1 = 0.96
I1107 05:22:31.468657 41413 caffe_interface.cpp:125] Batch 28, loss = 0.393655
I1107 05:22:31.468665 41413 caffe_interface.cpp:125] Batch 28, top-1 = 0.92
I1107 05:22:31.487391 41413 caffe_interface.cpp:125] Batch 29, loss = 0.00659562
I1107 05:22:31.487399 41413 caffe_interface.cpp:125] Batch 29, top-1 = 1
I1107 05:22:31.505491 41413 caffe_interface.cpp:125] Batch 30, loss = 0.0431968
I1107 05:22:31.505499 41413 caffe_interface.cpp:125] Batch 30, top-1 = 0.98
I1107 05:22:31.524315 41413 caffe_interface.cpp:125] Batch 31, loss = 0.00065831
I1107 05:22:31.524327 41413 caffe_interface.cpp:125] Batch 31, top-1 = 1
I1107 05:22:31.542315 41413 caffe_interface.cpp:125] Batch 32, loss = 0.132355
I1107 05:22:31.542325 41413 caffe_interface.cpp:125] Batch 32, top-1 = 0.98
I1107 05:22:31.561635 41413 caffe_interface.cpp:125] Batch 33, loss = 0.110977
I1107 05:22:31.561645 41413 caffe_interface.cpp:125] Batch 33, top-1 = 0.94
I1107 05:22:31.579659 41413 caffe_interface.cpp:125] Batch 34, loss = 0.288888
I1107 05:22:31.579668 41413 caffe_interface.cpp:125] Batch 34, top-1 = 0.94
I1107 05:22:31.599071 41413 caffe_interface.cpp:125] Batch 35, loss = 0.0132304
I1107 05:22:31.599079 41413 caffe_interface.cpp:125] Batch 35, top-1 = 1
I1107 05:22:31.617334 41413 caffe_interface.cpp:125] Batch 36, loss = 0.399847
I1107 05:22:31.617343 41413 caffe_interface.cpp:125] Batch 36, top-1 = 0.94
I1107 05:22:31.636173 41413 caffe_interface.cpp:125] Batch 37, loss = 0.136325
I1107 05:22:31.636181 41413 caffe_interface.cpp:125] Batch 37, top-1 = 0.98
I1107 05:22:31.655107 41413 caffe_interface.cpp:125] Batch 38, loss = 0.322019
I1107 05:22:31.655130 41413 caffe_interface.cpp:125] Batch 38, top-1 = 0.92
I1107 05:22:31.673364 41413 caffe_interface.cpp:125] Batch 39, loss = 0.126702
I1107 05:22:31.673372 41413 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I1107 05:22:31.692474 41413 caffe_interface.cpp:125] Batch 40, loss = 0.295873
I1107 05:22:31.692481 41413 caffe_interface.cpp:125] Batch 40, top-1 = 0.94
I1107 05:22:31.710322 41413 caffe_interface.cpp:125] Batch 41, loss = 0.00125027
I1107 05:22:31.710332 41413 caffe_interface.cpp:125] Batch 41, top-1 = 1
I1107 05:22:31.729154 41413 caffe_interface.cpp:125] Batch 42, loss = 0.00805466
I1107 05:22:31.729162 41413 caffe_interface.cpp:125] Batch 42, top-1 = 1
I1107 05:22:31.747654 41413 caffe_interface.cpp:125] Batch 43, loss = 0.19586
I1107 05:22:31.747663 41413 caffe_interface.cpp:125] Batch 43, top-1 = 0.92
I1107 05:22:31.765597 41413 caffe_interface.cpp:125] Batch 44, loss = 0.531471
I1107 05:22:31.765606 41413 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I1107 05:22:31.783854 41413 caffe_interface.cpp:125] Batch 45, loss = 0.617401
I1107 05:22:31.783862 41413 caffe_interface.cpp:125] Batch 45, top-1 = 0.88
I1107 05:22:31.802479 41413 caffe_interface.cpp:125] Batch 46, loss = 0.0153335
I1107 05:22:31.802490 41413 caffe_interface.cpp:125] Batch 46, top-1 = 1
I1107 05:22:31.820744 41413 caffe_interface.cpp:125] Batch 47, loss = 0.263101
I1107 05:22:31.820755 41413 caffe_interface.cpp:125] Batch 47, top-1 = 0.94
I1107 05:22:31.839589 41413 caffe_interface.cpp:125] Batch 48, loss = 0.0912073
I1107 05:22:31.839598 41413 caffe_interface.cpp:125] Batch 48, top-1 = 0.96
I1107 05:22:31.858057 41413 caffe_interface.cpp:125] Batch 49, loss = 0.260227
I1107 05:22:31.858067 41413 caffe_interface.cpp:125] Batch 49, top-1 = 0.94
I1107 05:22:31.877331 41413 caffe_interface.cpp:125] Batch 50, loss = 0.182411
I1107 05:22:31.877339 41413 caffe_interface.cpp:125] Batch 50, top-1 = 0.94
I1107 05:22:31.895200 41413 caffe_interface.cpp:125] Batch 51, loss = 0.392546
I1107 05:22:31.895210 41413 caffe_interface.cpp:125] Batch 51, top-1 = 0.9
I1107 05:22:31.914641 41413 caffe_interface.cpp:125] Batch 52, loss = 0.100953
I1107 05:22:31.914649 41413 caffe_interface.cpp:125] Batch 52, top-1 = 0.98
I1107 05:22:31.933905 41413 caffe_interface.cpp:125] Batch 53, loss = 0.106025
I1107 05:22:31.933912 41413 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I1107 05:22:31.952379 41413 caffe_interface.cpp:125] Batch 54, loss = 0.000880239
I1107 05:22:31.952388 41413 caffe_interface.cpp:125] Batch 54, top-1 = 1
I1107 05:22:31.971716 41413 caffe_interface.cpp:125] Batch 55, loss = 0.460266
I1107 05:22:31.971726 41413 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I1107 05:22:31.990190 41413 caffe_interface.cpp:125] Batch 56, loss = 0.125098
I1107 05:22:31.990197 41413 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 05:22:32.009003 41413 caffe_interface.cpp:125] Batch 57, loss = 0.0586565
I1107 05:22:32.009011 41413 caffe_interface.cpp:125] Batch 57, top-1 = 0.98
I1107 05:22:32.027046 41413 caffe_interface.cpp:125] Batch 58, loss = 0.271071
I1107 05:22:32.027055 41413 caffe_interface.cpp:125] Batch 58, top-1 = 0.94
I1107 05:22:32.045150 41413 caffe_interface.cpp:125] Batch 59, loss = 0.0998428
I1107 05:22:32.045158 41413 caffe_interface.cpp:125] Batch 59, top-1 = 0.92
I1107 05:22:32.063169 41413 caffe_interface.cpp:125] Batch 60, loss = 0.168595
I1107 05:22:32.063177 41413 caffe_interface.cpp:125] Batch 60, top-1 = 0.98
I1107 05:22:32.081820 41413 caffe_interface.cpp:125] Batch 61, loss = 0.135061
I1107 05:22:32.081830 41413 caffe_interface.cpp:125] Batch 61, top-1 = 0.98
I1107 05:22:32.100129 41413 caffe_interface.cpp:125] Batch 62, loss = 0.153948
I1107 05:22:32.100144 41413 caffe_interface.cpp:125] Batch 62, top-1 = 0.96
I1107 05:22:32.119091 41413 caffe_interface.cpp:125] Batch 63, loss = 0.392053
I1107 05:22:32.119104 41413 caffe_interface.cpp:125] Batch 63, top-1 = 0.94
I1107 05:22:32.137393 41413 caffe_interface.cpp:125] Batch 64, loss = 0.545937
I1107 05:22:32.137409 41413 caffe_interface.cpp:125] Batch 64, top-1 = 0.9
I1107 05:22:32.156450 41413 caffe_interface.cpp:125] Batch 65, loss = 0.205065
I1107 05:22:32.156466 41413 caffe_interface.cpp:125] Batch 65, top-1 = 0.96
I1107 05:22:32.175340 41413 caffe_interface.cpp:125] Batch 66, loss = 0.489486
I1107 05:22:32.175356 41413 caffe_interface.cpp:125] Batch 66, top-1 = 0.9
I1107 05:22:32.193868 41413 caffe_interface.cpp:125] Batch 67, loss = 0.254691
I1107 05:22:32.193884 41413 caffe_interface.cpp:125] Batch 67, top-1 = 0.94
I1107 05:22:32.212868 41413 caffe_interface.cpp:125] Batch 68, loss = 0.369003
I1107 05:22:32.212885 41413 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I1107 05:22:32.230944 41413 caffe_interface.cpp:125] Batch 69, loss = 0.276444
I1107 05:22:32.230959 41413 caffe_interface.cpp:125] Batch 69, top-1 = 0.92
I1107 05:22:32.249085 41413 caffe_interface.cpp:125] Batch 70, loss = 0.0797701
I1107 05:22:32.249100 41413 caffe_interface.cpp:125] Batch 70, top-1 = 0.98
I1107 05:22:32.267140 41413 caffe_interface.cpp:125] Batch 71, loss = 0.268521
I1107 05:22:32.267155 41413 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I1107 05:22:32.285835 41413 caffe_interface.cpp:125] Batch 72, loss = 0.236586
I1107 05:22:32.285850 41413 caffe_interface.cpp:125] Batch 72, top-1 = 0.96
I1107 05:22:32.305001 41413 caffe_interface.cpp:125] Batch 73, loss = 0.0262577
I1107 05:22:32.305025 41413 caffe_interface.cpp:125] Batch 73, top-1 = 0.98
I1107 05:22:32.323982 41413 caffe_interface.cpp:125] Batch 74, loss = 0.136549
I1107 05:22:32.323997 41413 caffe_interface.cpp:125] Batch 74, top-1 = 0.96
I1107 05:22:32.342988 41413 caffe_interface.cpp:125] Batch 75, loss = 0.0658225
I1107 05:22:32.343014 41413 caffe_interface.cpp:125] Batch 75, top-1 = 0.96
I1107 05:22:32.361307 41413 caffe_interface.cpp:125] Batch 76, loss = 0.258193
I1107 05:22:32.361325 41413 caffe_interface.cpp:125] Batch 76, top-1 = 0.98
I1107 05:22:32.380481 41413 caffe_interface.cpp:125] Batch 77, loss = 0.445612
I1107 05:22:32.380496 41413 caffe_interface.cpp:125] Batch 77, top-1 = 0.92
I1107 05:22:32.398764 41413 caffe_interface.cpp:125] Batch 78, loss = 0.0631098
I1107 05:22:32.398772 41413 caffe_interface.cpp:125] Batch 78, top-1 = 0.98
I1107 05:22:32.417582 41413 caffe_interface.cpp:125] Batch 79, loss = 0.0706183
I1107 05:22:32.417589 41413 caffe_interface.cpp:125] Batch 79, top-1 = 0.98
I1107 05:22:32.417593 41413 caffe_interface.cpp:130] Loss: 0.220153
I1107 05:22:32.417600 41413 caffe_interface.cpp:142] loss = 0.220153 (* 1 = 0.220153 loss)
I1107 05:22:32.417606 41413 caffe_interface.cpp:142] top-1 = 0.9535
I1107 05:22:32.657920 41413 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/sparse.caffemodel
I1107 05:22:32.657950 41413 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.4:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.953499794    | 0.006749928    |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 1219367        | -67.6130524%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 1293592772     | -39.9423485%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config4.prototxt
I1107 05:22:32.917433 42927 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/net_finetune.prototxt
I1107 05:22:33.089538 42927 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 05:22:33.090047 42927 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24848105472, dev_info[0]: total=25620447232 free=24848105472
I1107 05:22:33.090057 42927 caffe_interface.cpp:493] Using GPUs 0
I1107 05:22:33.090314 42927 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 05:22:33.677403 42927 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/net_finetune.prototxt"
type: "Adam"
I1107 05:22:33.677536 42927 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/net_finetune.prototxt
I1107 05:22:33.677755 42927 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 05:22:33.677768 42927 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 05:22:33.677913 42927 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 05:22:33.677978 42927 layer_factory.hpp:77] Creating layer data
I1107 05:22:33.678100 42927 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:33.678714 42927 net.cpp:94] Creating Layer data
I1107 05:22:33.678722 42927 net.cpp:409] data -> data
I1107 05:22:33.678732 42927 net.cpp:409] data -> label
I1107 05:22:33.680078 42966 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 05:22:33.680122 42966 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 05:22:33.680379 42927 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 05:22:33.680445 42927 data_layer.cpp:83] output data size: 256,3,227,227
I1107 05:22:34.059234 42927 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:34.059285 42927 net.cpp:144] Setting up data
I1107 05:22:34.059293 42927 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 05:22:34.059312 42927 net.cpp:151] Top shape: 256 (256)
I1107 05:22:34.059314 42927 net.cpp:159] Memory required for data: 158298112
I1107 05:22:34.059320 42927 layer_factory.hpp:77] Creating layer conv1
I1107 05:22:34.059334 42927 net.cpp:94] Creating Layer conv1
I1107 05:22:34.059339 42927 net.cpp:435] conv1 <- data
I1107 05:22:34.059353 42927 net.cpp:409] conv1 -> conv1
I1107 05:22:34.061233 42927 net.cpp:144] Setting up conv1
I1107 05:22:34.061245 42927 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 05:22:34.061249 42927 net.cpp:159] Memory required for data: 455667712
I1107 05:22:34.061277 42927 layer_factory.hpp:77] Creating layer bn1
I1107 05:22:34.061287 42927 net.cpp:94] Creating Layer bn1
I1107 05:22:34.061290 42927 net.cpp:435] bn1 <- conv1
I1107 05:22:34.061295 42927 net.cpp:409] bn1 -> scale1
I1107 05:22:34.062652 42927 net.cpp:144] Setting up bn1
I1107 05:22:34.062660 42927 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 05:22:34.062664 42927 net.cpp:159] Memory required for data: 753037312
I1107 05:22:34.062674 42927 layer_factory.hpp:77] Creating layer relu1
I1107 05:22:34.062678 42927 net.cpp:94] Creating Layer relu1
I1107 05:22:34.062685 42927 net.cpp:435] relu1 <- scale1
I1107 05:22:34.062688 42927 net.cpp:409] relu1 -> relu1
I1107 05:22:34.062712 42927 net.cpp:144] Setting up relu1
I1107 05:22:34.062717 42927 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 05:22:34.062721 42927 net.cpp:159] Memory required for data: 1050406912
I1107 05:22:34.062722 42927 layer_factory.hpp:77] Creating layer pool1
I1107 05:22:34.062727 42927 net.cpp:94] Creating Layer pool1
I1107 05:22:34.062731 42927 net.cpp:435] pool1 <- relu1
I1107 05:22:34.062734 42927 net.cpp:409] pool1 -> pool1
I1107 05:22:34.062805 42927 net.cpp:144] Setting up pool1
I1107 05:22:34.062810 42927 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 05:22:34.062813 42927 net.cpp:159] Memory required for data: 1122070528
I1107 05:22:34.062816 42927 layer_factory.hpp:77] Creating layer conv2
I1107 05:22:34.062824 42927 net.cpp:94] Creating Layer conv2
I1107 05:22:34.062829 42927 net.cpp:435] conv2 <- pool1
I1107 05:22:34.062834 42927 net.cpp:409] conv2 -> conv2
I1107 05:22:34.077741 42927 net.cpp:144] Setting up conv2
I1107 05:22:34.077760 42927 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 05:22:34.077764 42927 net.cpp:159] Memory required for data: 1313173504
I1107 05:22:34.077777 42927 layer_factory.hpp:77] Creating layer bn2
I1107 05:22:34.077796 42927 net.cpp:94] Creating Layer bn2
I1107 05:22:34.077801 42927 net.cpp:435] bn2 <- conv2
I1107 05:22:34.077810 42927 net.cpp:409] bn2 -> scale2
I1107 05:22:34.078385 42927 net.cpp:144] Setting up bn2
I1107 05:22:34.078393 42927 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 05:22:34.078398 42927 net.cpp:159] Memory required for data: 1504276480
I1107 05:22:34.078410 42927 layer_factory.hpp:77] Creating layer relu2
I1107 05:22:34.078419 42927 net.cpp:94] Creating Layer relu2
I1107 05:22:34.078424 42927 net.cpp:435] relu2 <- scale2
I1107 05:22:34.078430 42927 net.cpp:409] relu2 -> relu2
I1107 05:22:34.078455 42927 net.cpp:144] Setting up relu2
I1107 05:22:34.078464 42927 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 05:22:34.078469 42927 net.cpp:159] Memory required for data: 1695379456
I1107 05:22:34.078474 42927 layer_factory.hpp:77] Creating layer pool2
I1107 05:22:34.078485 42927 net.cpp:94] Creating Layer pool2
I1107 05:22:34.078488 42927 net.cpp:435] pool2 <- relu2
I1107 05:22:34.078510 42927 net.cpp:409] pool2 -> pool2
I1107 05:22:34.078549 42927 net.cpp:144] Setting up pool2
I1107 05:22:34.078554 42927 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 05:22:34.078557 42927 net.cpp:159] Memory required for data: 1739681792
I1107 05:22:34.078562 42927 layer_factory.hpp:77] Creating layer conv3
I1107 05:22:34.078572 42927 net.cpp:94] Creating Layer conv3
I1107 05:22:34.078579 42927 net.cpp:435] conv3 <- pool2
I1107 05:22:34.078586 42927 net.cpp:409] conv3 -> conv3
I1107 05:22:34.095752 42927 net.cpp:144] Setting up conv3
I1107 05:22:34.095787 42927 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 05:22:34.095790 42927 net.cpp:159] Memory required for data: 1806135296
I1107 05:22:34.095803 42927 layer_factory.hpp:77] Creating layer relu3
I1107 05:22:34.095818 42927 net.cpp:94] Creating Layer relu3
I1107 05:22:34.095826 42927 net.cpp:435] relu3 <- conv3
I1107 05:22:34.095836 42927 net.cpp:409] relu3 -> relu3
I1107 05:22:34.095875 42927 net.cpp:144] Setting up relu3
I1107 05:22:34.095882 42927 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 05:22:34.095886 42927 net.cpp:159] Memory required for data: 1872588800
I1107 05:22:34.095891 42927 layer_factory.hpp:77] Creating layer conv4
I1107 05:22:34.095907 42927 net.cpp:94] Creating Layer conv4
I1107 05:22:34.095916 42927 net.cpp:435] conv4 <- relu3
I1107 05:22:34.095924 42927 net.cpp:409] conv4 -> conv4
I1107 05:22:34.125613 42927 net.cpp:144] Setting up conv4
I1107 05:22:34.125658 42927 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 05:22:34.125669 42927 net.cpp:159] Memory required for data: 1939042304
I1107 05:22:34.125712 42927 layer_factory.hpp:77] Creating layer relu4
I1107 05:22:34.125735 42927 net.cpp:94] Creating Layer relu4
I1107 05:22:34.125747 42927 net.cpp:435] relu4 <- conv4
I1107 05:22:34.125768 42927 net.cpp:409] relu4 -> relu4
I1107 05:22:34.125833 42927 net.cpp:144] Setting up relu4
I1107 05:22:34.125849 42927 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 05:22:34.125856 42927 net.cpp:159] Memory required for data: 2005495808
I1107 05:22:34.125871 42927 layer_factory.hpp:77] Creating layer conv5
I1107 05:22:34.125893 42927 net.cpp:94] Creating Layer conv5
I1107 05:22:34.125901 42927 net.cpp:435] conv5 <- relu4
I1107 05:22:34.125921 42927 net.cpp:409] conv5 -> conv5
I1107 05:22:34.145663 42927 net.cpp:144] Setting up conv5
I1107 05:22:34.145697 42927 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 05:22:34.145700 42927 net.cpp:159] Memory required for data: 2049798144
I1107 05:22:34.145725 42927 layer_factory.hpp:77] Creating layer relu5
I1107 05:22:34.145732 42927 net.cpp:94] Creating Layer relu5
I1107 05:22:34.145735 42927 net.cpp:435] relu5 <- conv5
I1107 05:22:34.145743 42927 net.cpp:409] relu5 -> relu5
I1107 05:22:34.145766 42927 net.cpp:144] Setting up relu5
I1107 05:22:34.145769 42927 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 05:22:34.145771 42927 net.cpp:159] Memory required for data: 2094100480
I1107 05:22:34.145774 42927 layer_factory.hpp:77] Creating layer pool5
I1107 05:22:34.145779 42927 net.cpp:94] Creating Layer pool5
I1107 05:22:34.145782 42927 net.cpp:435] pool5 <- relu5
I1107 05:22:34.145787 42927 net.cpp:409] pool5 -> pool5
I1107 05:22:34.145812 42927 net.cpp:144] Setting up pool5
I1107 05:22:34.145817 42927 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 05:22:34.145822 42927 net.cpp:159] Memory required for data: 2103537664
I1107 05:22:34.145823 42927 layer_factory.hpp:77] Creating layer fc6
I1107 05:22:34.145829 42927 net.cpp:94] Creating Layer fc6
I1107 05:22:34.145833 42927 net.cpp:435] fc6 <- pool5
I1107 05:22:34.145838 42927 net.cpp:409] fc6 -> fc6
I1107 05:22:34.493698 42927 net.cpp:144] Setting up fc6
I1107 05:22:34.493726 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.493727 42927 net.cpp:159] Memory required for data: 2107731968
I1107 05:22:34.493752 42927 layer_factory.hpp:77] Creating layer relu6
I1107 05:22:34.493758 42927 net.cpp:94] Creating Layer relu6
I1107 05:22:34.493762 42927 net.cpp:435] relu6 <- fc6
I1107 05:22:34.493788 42927 net.cpp:409] relu6 -> relu6
I1107 05:22:34.493804 42927 net.cpp:144] Setting up relu6
I1107 05:22:34.493808 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.493809 42927 net.cpp:159] Memory required for data: 2111926272
I1107 05:22:34.493810 42927 layer_factory.hpp:77] Creating layer drop6
I1107 05:22:34.493815 42927 net.cpp:94] Creating Layer drop6
I1107 05:22:34.493818 42927 net.cpp:435] drop6 <- relu6
I1107 05:22:34.493820 42927 net.cpp:409] drop6 -> drop6
I1107 05:22:34.493851 42927 net.cpp:144] Setting up drop6
I1107 05:22:34.493856 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.493858 42927 net.cpp:159] Memory required for data: 2116120576
I1107 05:22:34.493860 42927 layer_factory.hpp:77] Creating layer fc7
I1107 05:22:34.493882 42927 net.cpp:94] Creating Layer fc7
I1107 05:22:34.493887 42927 net.cpp:435] fc7 <- drop6
I1107 05:22:34.493891 42927 net.cpp:409] fc7 -> fc7
I1107 05:22:34.630839 42927 net.cpp:144] Setting up fc7
I1107 05:22:34.630863 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.630867 42927 net.cpp:159] Memory required for data: 2120314880
I1107 05:22:34.630874 42927 layer_factory.hpp:77] Creating layer bn7
I1107 05:22:34.630899 42927 net.cpp:94] Creating Layer bn7
I1107 05:22:34.630905 42927 net.cpp:435] bn7 <- fc7
I1107 05:22:34.630913 42927 net.cpp:409] bn7 -> scale7
I1107 05:22:34.631426 42927 net.cpp:144] Setting up bn7
I1107 05:22:34.631433 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.631434 42927 net.cpp:159] Memory required for data: 2124509184
I1107 05:22:34.631443 42927 layer_factory.hpp:77] Creating layer relu7
I1107 05:22:34.631448 42927 net.cpp:94] Creating Layer relu7
I1107 05:22:34.631449 42927 net.cpp:435] relu7 <- scale7
I1107 05:22:34.631453 42927 net.cpp:409] relu7 -> relu7
I1107 05:22:34.631470 42927 net.cpp:144] Setting up relu7
I1107 05:22:34.631475 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.631477 42927 net.cpp:159] Memory required for data: 2128703488
I1107 05:22:34.631479 42927 layer_factory.hpp:77] Creating layer drop7
I1107 05:22:34.631484 42927 net.cpp:94] Creating Layer drop7
I1107 05:22:34.631486 42927 net.cpp:435] drop7 <- relu7
I1107 05:22:34.631490 42927 net.cpp:409] drop7 -> drop7
I1107 05:22:34.631512 42927 net.cpp:144] Setting up drop7
I1107 05:22:34.631517 42927 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 05:22:34.631520 42927 net.cpp:159] Memory required for data: 2132897792
I1107 05:22:34.631522 42927 layer_factory.hpp:77] Creating layer fc8
I1107 05:22:34.631527 42927 net.cpp:94] Creating Layer fc8
I1107 05:22:34.631531 42927 net.cpp:435] fc8 <- drop7
I1107 05:22:34.631534 42927 net.cpp:409] fc8 -> fc8
I1107 05:22:34.632397 42927 net.cpp:144] Setting up fc8
I1107 05:22:34.632408 42927 net.cpp:151] Top shape: 256 2 (512)
I1107 05:22:34.632411 42927 net.cpp:159] Memory required for data: 2132899840
I1107 05:22:34.632416 42927 layer_factory.hpp:77] Creating layer loss
I1107 05:22:34.632422 42927 net.cpp:94] Creating Layer loss
I1107 05:22:34.632427 42927 net.cpp:435] loss <- fc8
I1107 05:22:34.632431 42927 net.cpp:435] loss <- label
I1107 05:22:34.632436 42927 net.cpp:409] loss -> loss
I1107 05:22:34.632445 42927 layer_factory.hpp:77] Creating layer loss
I1107 05:22:34.632509 42927 net.cpp:144] Setting up loss
I1107 05:22:34.632514 42927 net.cpp:151] Top shape: (1)
I1107 05:22:34.632516 42927 net.cpp:154]     with loss weight 1
I1107 05:22:34.632527 42927 net.cpp:159] Memory required for data: 2132899844
I1107 05:22:34.632529 42927 net.cpp:220] loss needs backward computation.
I1107 05:22:34.632544 42927 net.cpp:220] fc8 needs backward computation.
I1107 05:22:34.632545 42927 net.cpp:220] drop7 needs backward computation.
I1107 05:22:34.632549 42927 net.cpp:220] relu7 needs backward computation.
I1107 05:22:34.632550 42927 net.cpp:220] bn7 needs backward computation.
I1107 05:22:34.632553 42927 net.cpp:220] fc7 needs backward computation.
I1107 05:22:34.632556 42927 net.cpp:220] drop6 needs backward computation.
I1107 05:22:34.632560 42927 net.cpp:220] relu6 needs backward computation.
I1107 05:22:34.632573 42927 net.cpp:220] fc6 needs backward computation.
I1107 05:22:34.632577 42927 net.cpp:220] pool5 needs backward computation.
I1107 05:22:34.632580 42927 net.cpp:220] relu5 needs backward computation.
I1107 05:22:34.632582 42927 net.cpp:220] conv5 needs backward computation.
I1107 05:22:34.632585 42927 net.cpp:220] relu4 needs backward computation.
I1107 05:22:34.632587 42927 net.cpp:220] conv4 needs backward computation.
I1107 05:22:34.632591 42927 net.cpp:220] relu3 needs backward computation.
I1107 05:22:34.632592 42927 net.cpp:220] conv3 needs backward computation.
I1107 05:22:34.632596 42927 net.cpp:220] pool2 needs backward computation.
I1107 05:22:34.632598 42927 net.cpp:220] relu2 needs backward computation.
I1107 05:22:34.632601 42927 net.cpp:220] bn2 needs backward computation.
I1107 05:22:34.632602 42927 net.cpp:220] conv2 needs backward computation.
I1107 05:22:34.632606 42927 net.cpp:220] pool1 needs backward computation.
I1107 05:22:34.632608 42927 net.cpp:220] relu1 needs backward computation.
I1107 05:22:34.632611 42927 net.cpp:220] bn1 needs backward computation.
I1107 05:22:34.632613 42927 net.cpp:220] conv1 needs backward computation.
I1107 05:22:34.632616 42927 net.cpp:222] data does not need backward computation.
I1107 05:22:34.632619 42927 net.cpp:264] This network produces output loss
I1107 05:22:34.632637 42927 net.cpp:284] Network initialization done.
I1107 05:22:34.632912 42927 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/net_finetune.prototxt
I1107 05:22:34.632941 42927 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 05:22:34.633103 42927 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 05:22:34.633190 42927 layer_factory.hpp:77] Creating layer data
I1107 05:22:34.633230 42927 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:34.634189 42927 net.cpp:94] Creating Layer data
I1107 05:22:34.634202 42927 net.cpp:409] data -> data
I1107 05:22:34.634212 42927 net.cpp:409] data -> label
I1107 05:22:34.635417 42996 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 05:22:34.635453 42996 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 05:22:34.635757 42927 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 05:22:34.635846 42927 data_layer.cpp:83] output data size: 50,3,227,227
I1107 05:22:34.714027 42927 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 05:22:34.714081 42927 net.cpp:144] Setting up data
I1107 05:22:34.714088 42927 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 05:22:34.714092 42927 net.cpp:151] Top shape: 50 (50)
I1107 05:22:34.714093 42927 net.cpp:159] Memory required for data: 30917600
I1107 05:22:34.714107 42927 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 05:22:34.714116 42927 net.cpp:94] Creating Layer label_data_1_split
I1107 05:22:34.714120 42927 net.cpp:435] label_data_1_split <- label
I1107 05:22:34.714144 42927 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 05:22:34.714154 42927 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 05:22:34.714215 42927 net.cpp:144] Setting up label_data_1_split
I1107 05:22:34.714218 42927 net.cpp:151] Top shape: 50 (50)
I1107 05:22:34.714221 42927 net.cpp:151] Top shape: 50 (50)
I1107 05:22:34.714223 42927 net.cpp:159] Memory required for data: 30918000
I1107 05:22:34.714226 42927 layer_factory.hpp:77] Creating layer conv1
I1107 05:22:34.714239 42927 net.cpp:94] Creating Layer conv1
I1107 05:22:34.714242 42927 net.cpp:435] conv1 <- data
I1107 05:22:34.714247 42927 net.cpp:409] conv1 -> conv1
I1107 05:22:34.714859 42927 net.cpp:144] Setting up conv1
I1107 05:22:34.714866 42927 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:34.714869 42927 net.cpp:159] Memory required for data: 88998000
I1107 05:22:34.714877 42927 layer_factory.hpp:77] Creating layer bn1
I1107 05:22:34.714885 42927 net.cpp:94] Creating Layer bn1
I1107 05:22:34.714888 42927 net.cpp:435] bn1 <- conv1
I1107 05:22:34.714893 42927 net.cpp:409] bn1 -> scale1
I1107 05:22:34.715474 42927 net.cpp:144] Setting up bn1
I1107 05:22:34.715482 42927 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:34.715485 42927 net.cpp:159] Memory required for data: 147078000
I1107 05:22:34.715495 42927 layer_factory.hpp:77] Creating layer relu1
I1107 05:22:34.715503 42927 net.cpp:94] Creating Layer relu1
I1107 05:22:34.715507 42927 net.cpp:435] relu1 <- scale1
I1107 05:22:34.715512 42927 net.cpp:409] relu1 -> relu1
I1107 05:22:34.715528 42927 net.cpp:144] Setting up relu1
I1107 05:22:34.715533 42927 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 05:22:34.715535 42927 net.cpp:159] Memory required for data: 205158000
I1107 05:22:34.715538 42927 layer_factory.hpp:77] Creating layer pool1
I1107 05:22:34.715543 42927 net.cpp:94] Creating Layer pool1
I1107 05:22:34.715544 42927 net.cpp:435] pool1 <- relu1
I1107 05:22:34.715549 42927 net.cpp:409] pool1 -> pool1
I1107 05:22:34.715911 42927 net.cpp:144] Setting up pool1
I1107 05:22:34.715917 42927 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 05:22:34.715920 42927 net.cpp:159] Memory required for data: 219154800
I1107 05:22:34.715924 42927 layer_factory.hpp:77] Creating layer conv2
I1107 05:22:34.715945 42927 net.cpp:94] Creating Layer conv2
I1107 05:22:34.715950 42927 net.cpp:435] conv2 <- pool1
I1107 05:22:34.715955 42927 net.cpp:409] conv2 -> conv2
I1107 05:22:34.722414 42927 net.cpp:144] Setting up conv2
I1107 05:22:34.722453 42927 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:34.722457 42927 net.cpp:159] Memory required for data: 256479600
I1107 05:22:34.722474 42927 layer_factory.hpp:77] Creating layer bn2
I1107 05:22:34.722492 42927 net.cpp:94] Creating Layer bn2
I1107 05:22:34.722497 42927 net.cpp:435] bn2 <- conv2
I1107 05:22:34.722508 42927 net.cpp:409] bn2 -> scale2
I1107 05:22:34.723860 42927 net.cpp:144] Setting up bn2
I1107 05:22:34.723867 42927 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:34.723871 42927 net.cpp:159] Memory required for data: 293804400
I1107 05:22:34.723882 42927 layer_factory.hpp:77] Creating layer relu2
I1107 05:22:34.723893 42927 net.cpp:94] Creating Layer relu2
I1107 05:22:34.723901 42927 net.cpp:435] relu2 <- scale2
I1107 05:22:34.723906 42927 net.cpp:409] relu2 -> relu2
I1107 05:22:34.723932 42927 net.cpp:144] Setting up relu2
I1107 05:22:34.723948 42927 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 05:22:34.723953 42927 net.cpp:159] Memory required for data: 331129200
I1107 05:22:34.723958 42927 layer_factory.hpp:77] Creating layer pool2
I1107 05:22:34.723970 42927 net.cpp:94] Creating Layer pool2
I1107 05:22:34.723976 42927 net.cpp:435] pool2 <- relu2
I1107 05:22:34.723985 42927 net.cpp:409] pool2 -> pool2
I1107 05:22:34.724023 42927 net.cpp:144] Setting up pool2
I1107 05:22:34.724031 42927 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:34.724035 42927 net.cpp:159] Memory required for data: 339782000
I1107 05:22:34.724040 42927 layer_factory.hpp:77] Creating layer conv3
I1107 05:22:34.724052 42927 net.cpp:94] Creating Layer conv3
I1107 05:22:34.724056 42927 net.cpp:435] conv3 <- pool2
I1107 05:22:34.724066 42927 net.cpp:409] conv3 -> conv3
I1107 05:22:34.733839 42927 net.cpp:144] Setting up conv3
I1107 05:22:34.733860 42927 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:34.733865 42927 net.cpp:159] Memory required for data: 352761200
I1107 05:22:34.733875 42927 layer_factory.hpp:77] Creating layer relu3
I1107 05:22:34.733886 42927 net.cpp:94] Creating Layer relu3
I1107 05:22:34.733892 42927 net.cpp:435] relu3 <- conv3
I1107 05:22:34.733901 42927 net.cpp:409] relu3 -> relu3
I1107 05:22:34.733933 42927 net.cpp:144] Setting up relu3
I1107 05:22:34.733940 42927 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:34.733943 42927 net.cpp:159] Memory required for data: 365740400
I1107 05:22:34.733947 42927 layer_factory.hpp:77] Creating layer conv4
I1107 05:22:34.733960 42927 net.cpp:94] Creating Layer conv4
I1107 05:22:34.733965 42927 net.cpp:435] conv4 <- relu3
I1107 05:22:34.733973 42927 net.cpp:409] conv4 -> conv4
I1107 05:22:34.749400 42927 net.cpp:144] Setting up conv4
I1107 05:22:34.749440 42927 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:34.749444 42927 net.cpp:159] Memory required for data: 378719600
I1107 05:22:34.749461 42927 layer_factory.hpp:77] Creating layer relu4
I1107 05:22:34.749475 42927 net.cpp:94] Creating Layer relu4
I1107 05:22:34.749481 42927 net.cpp:435] relu4 <- conv4
I1107 05:22:34.749493 42927 net.cpp:409] relu4 -> relu4
I1107 05:22:34.749538 42927 net.cpp:144] Setting up relu4
I1107 05:22:34.749544 42927 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 05:22:34.749548 42927 net.cpp:159] Memory required for data: 391698800
I1107 05:22:34.749552 42927 layer_factory.hpp:77] Creating layer conv5
I1107 05:22:34.749565 42927 net.cpp:94] Creating Layer conv5
I1107 05:22:34.749569 42927 net.cpp:435] conv5 <- relu4
I1107 05:22:34.749578 42927 net.cpp:409] conv5 -> conv5
I1107 05:22:34.759757 42927 net.cpp:144] Setting up conv5
I1107 05:22:34.759784 42927 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:34.759788 42927 net.cpp:159] Memory required for data: 400351600
I1107 05:22:34.759797 42927 layer_factory.hpp:77] Creating layer relu5
I1107 05:22:34.759817 42927 net.cpp:94] Creating Layer relu5
I1107 05:22:34.759826 42927 net.cpp:435] relu5 <- conv5
I1107 05:22:34.759835 42927 net.cpp:409] relu5 -> relu5
I1107 05:22:34.759876 42927 net.cpp:144] Setting up relu5
I1107 05:22:34.759888 42927 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 05:22:34.759907 42927 net.cpp:159] Memory required for data: 409004400
I1107 05:22:34.759910 42927 layer_factory.hpp:77] Creating layer pool5
I1107 05:22:34.759922 42927 net.cpp:94] Creating Layer pool5
I1107 05:22:34.759928 42927 net.cpp:435] pool5 <- relu5
I1107 05:22:34.759935 42927 net.cpp:409] pool5 -> pool5
I1107 05:22:34.759984 42927 net.cpp:144] Setting up pool5
I1107 05:22:34.759991 42927 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 05:22:34.759996 42927 net.cpp:159] Memory required for data: 410847600
I1107 05:22:34.760002 42927 layer_factory.hpp:77] Creating layer fc6
I1107 05:22:34.760013 42927 net.cpp:94] Creating Layer fc6
I1107 05:22:34.760018 42927 net.cpp:435] fc6 <- pool5
I1107 05:22:34.760028 42927 net.cpp:409] fc6 -> fc6
I1107 05:22:35.062427 42927 net.cpp:144] Setting up fc6
I1107 05:22:35.062451 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.062454 42927 net.cpp:159] Memory required for data: 411666800
I1107 05:22:35.062464 42927 layer_factory.hpp:77] Creating layer relu6
I1107 05:22:35.062474 42927 net.cpp:94] Creating Layer relu6
I1107 05:22:35.062489 42927 net.cpp:435] relu6 <- fc6
I1107 05:22:35.062497 42927 net.cpp:409] relu6 -> relu6
I1107 05:22:35.062530 42927 net.cpp:144] Setting up relu6
I1107 05:22:35.062535 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.062537 42927 net.cpp:159] Memory required for data: 412486000
I1107 05:22:35.062541 42927 layer_factory.hpp:77] Creating layer drop6
I1107 05:22:35.062548 42927 net.cpp:94] Creating Layer drop6
I1107 05:22:35.062552 42927 net.cpp:435] drop6 <- relu6
I1107 05:22:35.062573 42927 net.cpp:409] drop6 -> drop6
I1107 05:22:35.062618 42927 net.cpp:144] Setting up drop6
I1107 05:22:35.062623 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.062625 42927 net.cpp:159] Memory required for data: 413305200
I1107 05:22:35.062628 42927 layer_factory.hpp:77] Creating layer fc7
I1107 05:22:35.062636 42927 net.cpp:94] Creating Layer fc7
I1107 05:22:35.062641 42927 net.cpp:435] fc7 <- drop6
I1107 05:22:35.062649 42927 net.cpp:409] fc7 -> fc7
I1107 05:22:35.197607 42927 net.cpp:144] Setting up fc7
I1107 05:22:35.197629 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.197633 42927 net.cpp:159] Memory required for data: 414124400
I1107 05:22:35.197641 42927 layer_factory.hpp:77] Creating layer bn7
I1107 05:22:35.197654 42927 net.cpp:94] Creating Layer bn7
I1107 05:22:35.197659 42927 net.cpp:435] bn7 <- fc7
I1107 05:22:35.197666 42927 net.cpp:409] bn7 -> scale7
I1107 05:22:35.198222 42927 net.cpp:144] Setting up bn7
I1107 05:22:35.198228 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.198231 42927 net.cpp:159] Memory required for data: 414943600
I1107 05:22:35.198246 42927 layer_factory.hpp:77] Creating layer relu7
I1107 05:22:35.198253 42927 net.cpp:94] Creating Layer relu7
I1107 05:22:35.198257 42927 net.cpp:435] relu7 <- scale7
I1107 05:22:35.198263 42927 net.cpp:409] relu7 -> relu7
I1107 05:22:35.198283 42927 net.cpp:144] Setting up relu7
I1107 05:22:35.198287 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.198290 42927 net.cpp:159] Memory required for data: 415762800
I1107 05:22:35.198293 42927 layer_factory.hpp:77] Creating layer drop7
I1107 05:22:35.198302 42927 net.cpp:94] Creating Layer drop7
I1107 05:22:35.198304 42927 net.cpp:435] drop7 <- relu7
I1107 05:22:35.198310 42927 net.cpp:409] drop7 -> drop7
I1107 05:22:35.198355 42927 net.cpp:144] Setting up drop7
I1107 05:22:35.198362 42927 net.cpp:151] Top shape: 50 4096 (204800)
I1107 05:22:35.198365 42927 net.cpp:159] Memory required for data: 416582000
I1107 05:22:35.198369 42927 layer_factory.hpp:77] Creating layer fc8
I1107 05:22:35.198376 42927 net.cpp:94] Creating Layer fc8
I1107 05:22:35.198380 42927 net.cpp:435] fc8 <- drop7
I1107 05:22:35.198387 42927 net.cpp:409] fc8 -> fc8
I1107 05:22:35.198560 42927 net.cpp:144] Setting up fc8
I1107 05:22:35.198565 42927 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:35.198568 42927 net.cpp:159] Memory required for data: 416582400
I1107 05:22:35.198586 42927 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 05:22:35.198593 42927 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 05:22:35.198597 42927 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 05:22:35.198606 42927 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 05:22:35.198613 42927 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 05:22:35.198644 42927 net.cpp:144] Setting up fc8_fc8_0_split
I1107 05:22:35.198648 42927 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:35.198653 42927 net.cpp:151] Top shape: 50 2 (100)
I1107 05:22:35.198657 42927 net.cpp:159] Memory required for data: 416583200
I1107 05:22:35.198660 42927 layer_factory.hpp:77] Creating layer loss
I1107 05:22:35.198668 42927 net.cpp:94] Creating Layer loss
I1107 05:22:35.198671 42927 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 05:22:35.198676 42927 net.cpp:435] loss <- label_data_1_split_0
I1107 05:22:35.198683 42927 net.cpp:409] loss -> loss
I1107 05:22:35.198693 42927 layer_factory.hpp:77] Creating layer loss
I1107 05:22:35.198771 42927 net.cpp:144] Setting up loss
I1107 05:22:35.198776 42927 net.cpp:151] Top shape: (1)
I1107 05:22:35.198779 42927 net.cpp:154]     with loss weight 1
I1107 05:22:35.198791 42927 net.cpp:159] Memory required for data: 416583204
I1107 05:22:35.198794 42927 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 05:22:35.198803 42927 net.cpp:94] Creating Layer accuracy-top1
I1107 05:22:35.198807 42927 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 05:22:35.198812 42927 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 05:22:35.198819 42927 net.cpp:409] accuracy-top1 -> top-1
I1107 05:22:35.198827 42927 net.cpp:144] Setting up accuracy-top1
I1107 05:22:35.198832 42927 net.cpp:151] Top shape: (1)
I1107 05:22:35.198835 42927 net.cpp:159] Memory required for data: 416583208
I1107 05:22:35.198839 42927 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 05:22:35.198844 42927 net.cpp:220] loss needs backward computation.
I1107 05:22:35.198849 42927 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 05:22:35.198853 42927 net.cpp:220] fc8 needs backward computation.
I1107 05:22:35.198858 42927 net.cpp:220] drop7 needs backward computation.
I1107 05:22:35.198860 42927 net.cpp:220] relu7 needs backward computation.
I1107 05:22:35.198864 42927 net.cpp:220] bn7 needs backward computation.
I1107 05:22:35.198868 42927 net.cpp:220] fc7 needs backward computation.
I1107 05:22:35.198873 42927 net.cpp:220] drop6 needs backward computation.
I1107 05:22:35.198876 42927 net.cpp:220] relu6 needs backward computation.
I1107 05:22:35.198880 42927 net.cpp:220] fc6 needs backward computation.
I1107 05:22:35.198884 42927 net.cpp:220] pool5 needs backward computation.
I1107 05:22:35.198889 42927 net.cpp:220] relu5 needs backward computation.
I1107 05:22:35.198892 42927 net.cpp:220] conv5 needs backward computation.
I1107 05:22:35.198897 42927 net.cpp:220] relu4 needs backward computation.
I1107 05:22:35.198900 42927 net.cpp:220] conv4 needs backward computation.
I1107 05:22:35.198905 42927 net.cpp:220] relu3 needs backward computation.
I1107 05:22:35.198909 42927 net.cpp:220] conv3 needs backward computation.
I1107 05:22:35.198912 42927 net.cpp:220] pool2 needs backward computation.
I1107 05:22:35.198916 42927 net.cpp:220] relu2 needs backward computation.
I1107 05:22:35.198921 42927 net.cpp:220] bn2 needs backward computation.
I1107 05:22:35.198925 42927 net.cpp:220] conv2 needs backward computation.
I1107 05:22:35.198930 42927 net.cpp:220] pool1 needs backward computation.
I1107 05:22:35.198933 42927 net.cpp:220] relu1 needs backward computation.
I1107 05:22:35.198938 42927 net.cpp:220] bn1 needs backward computation.
I1107 05:22:35.198941 42927 net.cpp:220] conv1 needs backward computation.
I1107 05:22:35.198946 42927 net.cpp:222] label_data_1_split does not need backward computation.
I1107 05:22:35.198951 42927 net.cpp:222] data does not need backward computation.
I1107 05:22:35.198956 42927 net.cpp:264] This network produces output loss
I1107 05:22:35.198958 42927 net.cpp:264] This network produces output top-1
I1107 05:22:35.198985 42927 net.cpp:284] Network initialization done.
I1107 05:22:35.199081 42927 solver.cpp:63] Solver scaffolding done.
I1107 05:22:35.200286 42927 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/sparse.caffemodel
I1107 05:22:36.757300 42927 caffe_interface.cpp:527] Starting Optimization
I1107 05:22:36.757330 42927 solver.cpp:335] Solving 
I1107 05:22:36.757333 42927 solver.cpp:336] Learning Rate Policy: step
I1107 05:22:36.759272 42927 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 05:22:38.278614 42927 solver.cpp:517]     Test net output #0: loss = 0.220153 (* 1 = 0.220153 loss)
I1107 05:22:38.278640 42927 solver.cpp:517]     Test net output #1: top-1 = 0.9535
I1107 05:22:38.533252 42927 solver.cpp:266] Iteration 0 (0 iter/s, 1.7758s/50 iter), loss = 0.00125134
I1107 05:22:38.533294 42927 solver.cpp:285]     Train net output #0: loss = 0.00125134 (* 1 = 0.00125134 loss)
I1107 05:22:38.533315 42927 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 05:22:50.843104 42927 solver.cpp:266] Iteration 50 (4.06199 iter/s, 12.3092s/50 iter), loss = 0.0616462
I1107 05:22:50.843133 42927 solver.cpp:285]     Train net output #0: loss = 0.0616462 (* 1 = 0.0616462 loss)
I1107 05:22:50.843139 42927 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 05:23:03.182792 42927 solver.cpp:266] Iteration 100 (4.05216 iter/s, 12.3391s/50 iter), loss = 0.0397525
I1107 05:23:03.183019 42927 solver.cpp:285]     Train net output #0: loss = 0.0397525 (* 1 = 0.0397525 loss)
I1107 05:23:03.183029 42927 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 05:23:15.570391 42927 solver.cpp:266] Iteration 150 (4.03655 iter/s, 12.3868s/50 iter), loss = 0.0415377
I1107 05:23:15.570420 42927 solver.cpp:285]     Train net output #0: loss = 0.0415378 (* 1 = 0.0415378 loss)
I1107 05:23:15.570425 42927 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 05:23:28.019011 42927 solver.cpp:266] Iteration 200 (4.0167 iter/s, 12.448s/50 iter), loss = 0.0364187
I1107 05:23:28.019038 42927 solver.cpp:285]     Train net output #0: loss = 0.0364187 (* 1 = 0.0364187 loss)
I1107 05:23:28.019042 42927 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 05:23:40.481660 42927 solver.cpp:266] Iteration 250 (4.01218 iter/s, 12.462s/50 iter), loss = 0.0294654
I1107 05:23:40.481716 42927 solver.cpp:285]     Train net output #0: loss = 0.0294655 (* 1 = 0.0294655 loss)
I1107 05:23:40.481724 42927 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 05:23:53.014792 42927 solver.cpp:266] Iteration 300 (3.98963 iter/s, 12.5325s/50 iter), loss = 0.0573624
I1107 05:23:53.014820 42927 solver.cpp:285]     Train net output #0: loss = 0.0573625 (* 1 = 0.0573625 loss)
I1107 05:23:53.014825 42927 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 05:24:05.662386 42927 solver.cpp:266] Iteration 350 (3.95351 iter/s, 12.647s/50 iter), loss = 0.066072
I1107 05:24:05.662413 42927 solver.cpp:285]     Train net output #0: loss = 0.066072 (* 1 = 0.066072 loss)
I1107 05:24:05.662420 42927 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 05:24:18.308286 42927 solver.cpp:266] Iteration 400 (3.95403 iter/s, 12.6453s/50 iter), loss = 0.0451402
I1107 05:24:18.308449 42927 solver.cpp:285]     Train net output #0: loss = 0.0451402 (* 1 = 0.0451402 loss)
I1107 05:24:18.308456 42927 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 05:24:31.000387 42927 solver.cpp:266] Iteration 450 (3.93967 iter/s, 12.6914s/50 iter), loss = 0.0704204
I1107 05:24:31.000414 42927 solver.cpp:285]     Train net output #0: loss = 0.0704204 (* 1 = 0.0704204 loss)
I1107 05:24:31.000421 42927 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 05:24:43.686223 42927 solver.cpp:266] Iteration 500 (3.94157 iter/s, 12.6853s/50 iter), loss = 0.0475911
I1107 05:24:43.686254 42927 solver.cpp:285]     Train net output #0: loss = 0.0475911 (* 1 = 0.0475911 loss)
I1107 05:24:43.686259 42927 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 05:24:56.362785 42927 solver.cpp:266] Iteration 550 (3.94445 iter/s, 12.676s/50 iter), loss = 0.0434145
I1107 05:24:56.362957 42927 solver.cpp:285]     Train net output #0: loss = 0.0434145 (* 1 = 0.0434145 loss)
I1107 05:24:56.362965 42927 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 05:25:09.057042 42927 solver.cpp:266] Iteration 600 (3.93899 iter/s, 12.6936s/50 iter), loss = 0.0472071
I1107 05:25:09.057081 42927 solver.cpp:285]     Train net output #0: loss = 0.0472071 (* 1 = 0.0472071 loss)
I1107 05:25:09.057102 42927 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 05:25:21.723665 42927 solver.cpp:266] Iteration 650 (3.94755 iter/s, 12.6661s/50 iter), loss = 0.0755828
I1107 05:25:21.723711 42927 solver.cpp:285]     Train net output #0: loss = 0.0755828 (* 1 = 0.0755828 loss)
I1107 05:25:21.723718 42927 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 05:25:34.450994 42927 solver.cpp:266] Iteration 700 (3.92872 iter/s, 12.7268s/50 iter), loss = 0.0623037
I1107 05:25:34.451164 42927 solver.cpp:285]     Train net output #0: loss = 0.0623037 (* 1 = 0.0623037 loss)
I1107 05:25:34.451175 42927 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 05:25:47.106573 42927 solver.cpp:266] Iteration 750 (3.95103 iter/s, 12.6549s/50 iter), loss = 0.0573095
I1107 05:25:47.106603 42927 solver.cpp:285]     Train net output #0: loss = 0.0573095 (* 1 = 0.0573095 loss)
I1107 05:25:47.106609 42927 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 05:25:59.769629 42927 solver.cpp:266] Iteration 800 (3.94866 iter/s, 12.6625s/50 iter), loss = 0.0489542
I1107 05:25:59.769657 42927 solver.cpp:285]     Train net output #0: loss = 0.0489542 (* 1 = 0.0489542 loss)
I1107 05:25:59.769662 42927 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 05:26:12.482192 42927 solver.cpp:266] Iteration 850 (3.93328 iter/s, 12.712s/50 iter), loss = 0.0395548
I1107 05:26:12.482352 42927 solver.cpp:285]     Train net output #0: loss = 0.0395548 (* 1 = 0.0395548 loss)
I1107 05:26:12.482360 42927 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 05:26:25.181985 42927 solver.cpp:266] Iteration 900 (3.93728 iter/s, 12.6991s/50 iter), loss = 0.0895443
I1107 05:26:25.182013 42927 solver.cpp:285]     Train net output #0: loss = 0.0895443 (* 1 = 0.0895443 loss)
I1107 05:26:25.182019 42927 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 05:26:37.918740 42927 solver.cpp:266] Iteration 950 (3.92581 iter/s, 12.7362s/50 iter), loss = 0.029866
I1107 05:26:37.918772 42927 solver.cpp:285]     Train net output #0: loss = 0.029866 (* 1 = 0.029866 loss)
I1107 05:26:37.918777 42927 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 05:26:50.420238 42927 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 05:26:51.921932 42927 solver.cpp:517]     Test net output #0: loss = 0.586777 (* 1 = 0.586777 loss)
I1107 05:26:51.921949 42927 solver.cpp:517]     Test net output #1: top-1 = 0.899
I1107 05:26:52.169513 42927 solver.cpp:266] Iteration 1000 (3.50873 iter/s, 14.2502s/50 iter), loss = 0.0563603
I1107 05:26:52.169539 42927 solver.cpp:285]     Train net output #0: loss = 0.0563603 (* 1 = 0.0563603 loss)
I1107 05:26:52.169545 42927 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 05:27:04.896394 42927 solver.cpp:266] Iteration 1050 (3.92886 iter/s, 12.7264s/50 iter), loss = 0.0409996
I1107 05:27:04.896421 42927 solver.cpp:285]     Train net output #0: loss = 0.0409996 (* 1 = 0.0409996 loss)
I1107 05:27:04.896427 42927 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 05:27:17.605933 42927 solver.cpp:266] Iteration 1100 (3.93422 iter/s, 12.709s/50 iter), loss = 0.0642048
I1107 05:27:17.605962 42927 solver.cpp:285]     Train net output #0: loss = 0.0642048 (* 1 = 0.0642048 loss)
I1107 05:27:17.605968 42927 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 05:27:30.374516 42927 solver.cpp:266] Iteration 1150 (3.91602 iter/s, 12.7681s/50 iter), loss = 0.0286597
I1107 05:27:30.374708 42927 solver.cpp:285]     Train net output #0: loss = 0.0286597 (* 1 = 0.0286597 loss)
I1107 05:27:30.374717 42927 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 05:27:43.146031 42927 solver.cpp:266] Iteration 1200 (3.91517 iter/s, 12.7708s/50 iter), loss = 0.0435458
I1107 05:27:43.146060 42927 solver.cpp:285]     Train net output #0: loss = 0.0435458 (* 1 = 0.0435458 loss)
I1107 05:27:43.146066 42927 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 05:27:55.837802 42927 solver.cpp:266] Iteration 1250 (3.93972 iter/s, 12.6912s/50 iter), loss = 0.0381906
I1107 05:27:55.837831 42927 solver.cpp:285]     Train net output #0: loss = 0.0381906 (* 1 = 0.0381906 loss)
I1107 05:27:55.837836 42927 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 05:28:08.583251 42927 solver.cpp:266] Iteration 1300 (3.92313 iter/s, 12.7449s/50 iter), loss = 0.0550362
I1107 05:28:08.583405 42927 solver.cpp:285]     Train net output #0: loss = 0.0550362 (* 1 = 0.0550362 loss)
I1107 05:28:08.583412 42927 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 05:28:21.334797 42927 solver.cpp:266] Iteration 1350 (3.92129 iter/s, 12.7509s/50 iter), loss = 0.0625324
I1107 05:28:21.334825 42927 solver.cpp:285]     Train net output #0: loss = 0.0625324 (* 1 = 0.0625324 loss)
I1107 05:28:21.334830 42927 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 05:28:34.084012 42927 solver.cpp:266] Iteration 1400 (3.92197 iter/s, 12.7487s/50 iter), loss = 0.0446735
I1107 05:28:34.084041 42927 solver.cpp:285]     Train net output #0: loss = 0.0446735 (* 1 = 0.0446735 loss)
I1107 05:28:34.084048 42927 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 05:28:46.814538 42927 solver.cpp:266] Iteration 1450 (3.92773 iter/s, 12.73s/50 iter), loss = 0.0497571
I1107 05:28:46.814687 42927 solver.cpp:285]     Train net output #0: loss = 0.0497571 (* 1 = 0.0497571 loss)
I1107 05:28:46.814695 42927 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 05:28:59.528607 42927 solver.cpp:266] Iteration 1500 (3.93285 iter/s, 12.7134s/50 iter), loss = 0.0496707
I1107 05:28:59.528637 42927 solver.cpp:285]     Train net output #0: loss = 0.0496707 (* 1 = 0.0496707 loss)
I1107 05:28:59.528643 42927 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 05:29:12.240612 42927 solver.cpp:266] Iteration 1550 (3.93345 iter/s, 12.7115s/50 iter), loss = 0.0619471
I1107 05:29:12.240641 42927 solver.cpp:285]     Train net output #0: loss = 0.0619471 (* 1 = 0.0619471 loss)
I1107 05:29:12.240645 42927 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 05:29:24.943634 42927 solver.cpp:266] Iteration 1600 (3.93623 iter/s, 12.7025s/50 iter), loss = 0.0904289
I1107 05:29:24.943783 42927 solver.cpp:285]     Train net output #0: loss = 0.0904289 (* 1 = 0.0904289 loss)
I1107 05:29:24.943791 42927 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 05:29:37.676898 42927 solver.cpp:266] Iteration 1650 (3.92692 iter/s, 12.7326s/50 iter), loss = 0.0722647
I1107 05:29:37.676926 42927 solver.cpp:285]     Train net output #0: loss = 0.0722647 (* 1 = 0.0722647 loss)
I1107 05:29:37.676931 42927 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 05:29:50.406380 42927 solver.cpp:266] Iteration 1700 (3.92805 iter/s, 12.729s/50 iter), loss = 0.0578308
I1107 05:29:50.406409 42927 solver.cpp:285]     Train net output #0: loss = 0.0578308 (* 1 = 0.0578308 loss)
I1107 05:29:50.406414 42927 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 05:30:03.175904 42927 solver.cpp:266] Iteration 1750 (3.91574 iter/s, 12.769s/50 iter), loss = 0.0701051
I1107 05:30:03.176044 42927 solver.cpp:285]     Train net output #0: loss = 0.0701051 (* 1 = 0.0701051 loss)
I1107 05:30:03.176051 42927 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 05:30:15.859213 42927 solver.cpp:266] Iteration 1800 (3.94239 iter/s, 12.6827s/50 iter), loss = 0.065765
I1107 05:30:15.859243 42927 solver.cpp:285]     Train net output #0: loss = 0.065765 (* 1 = 0.065765 loss)
I1107 05:30:15.859249 42927 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 05:30:28.581754 42927 solver.cpp:266] Iteration 1850 (3.9302 iter/s, 12.722s/50 iter), loss = 0.0650277
I1107 05:30:28.581784 42927 solver.cpp:285]     Train net output #0: loss = 0.0650276 (* 1 = 0.0650276 loss)
I1107 05:30:28.581790 42927 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 05:30:41.305802 42927 solver.cpp:266] Iteration 1900 (3.92973 iter/s, 12.7235s/50 iter), loss = 0.0532103
I1107 05:30:41.305979 42927 solver.cpp:285]     Train net output #0: loss = 0.0532103 (* 1 = 0.0532103 loss)
I1107 05:30:41.305989 42927 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 05:30:54.020520 42927 solver.cpp:266] Iteration 1950 (3.93266 iter/s, 12.714s/50 iter), loss = 0.0782742
I1107 05:30:54.020551 42927 solver.cpp:285]     Train net output #0: loss = 0.0782742 (* 1 = 0.0782742 loss)
I1107 05:30:54.020557 42927 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 05:31:06.521445 42927 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 05:31:08.028666 42927 solver.cpp:517]     Test net output #0: loss = 0.237337 (* 1 = 0.237337 loss)
I1107 05:31:08.028683 42927 solver.cpp:517]     Test net output #1: top-1 = 0.9335
I1107 05:31:08.272164 42927 solver.cpp:266] Iteration 2000 (3.50851 iter/s, 14.2511s/50 iter), loss = 0.0410029
I1107 05:31:08.272189 42927 solver.cpp:285]     Train net output #0: loss = 0.0410029 (* 1 = 0.0410029 loss)
I1107 05:31:08.272195 42927 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 05:31:20.979342 42927 solver.cpp:266] Iteration 2050 (3.93495 iter/s, 12.7067s/50 iter), loss = 0.0410984
I1107 05:31:20.979524 42927 solver.cpp:285]     Train net output #0: loss = 0.0410984 (* 1 = 0.0410984 loss)
I1107 05:31:20.979532 42927 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 05:31:33.672086 42927 solver.cpp:266] Iteration 2100 (3.93947 iter/s, 12.6921s/50 iter), loss = 0.0364083
I1107 05:31:33.672114 42927 solver.cpp:285]     Train net output #0: loss = 0.0364083 (* 1 = 0.0364083 loss)
I1107 05:31:33.672121 42927 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 05:31:46.426385 42927 solver.cpp:266] Iteration 2150 (3.92041 iter/s, 12.7538s/50 iter), loss = 0.0456307
I1107 05:31:46.426414 42927 solver.cpp:285]     Train net output #0: loss = 0.0456307 (* 1 = 0.0456307 loss)
I1107 05:31:46.426419 42927 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 05:31:59.141551 42927 solver.cpp:266] Iteration 2200 (3.93247 iter/s, 12.7146s/50 iter), loss = 0.052075
I1107 05:31:59.141703 42927 solver.cpp:285]     Train net output #0: loss = 0.052075 (* 1 = 0.052075 loss)
I1107 05:31:59.141711 42927 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 05:32:11.829840 42927 solver.cpp:266] Iteration 2250 (3.94084 iter/s, 12.6876s/50 iter), loss = 0.0259265
I1107 05:32:11.829869 42927 solver.cpp:285]     Train net output #0: loss = 0.0259265 (* 1 = 0.0259265 loss)
I1107 05:32:11.829874 42927 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 05:32:24.568135 42927 solver.cpp:266] Iteration 2300 (3.92533 iter/s, 12.7378s/50 iter), loss = 0.0484238
I1107 05:32:24.568166 42927 solver.cpp:285]     Train net output #0: loss = 0.0484238 (* 1 = 0.0484238 loss)
I1107 05:32:24.568171 42927 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 05:32:37.229238 42927 solver.cpp:266] Iteration 2350 (3.94927 iter/s, 12.6606s/50 iter), loss = 0.0403937
I1107 05:32:37.229399 42927 solver.cpp:285]     Train net output #0: loss = 0.0403937 (* 1 = 0.0403937 loss)
I1107 05:32:37.229408 42927 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 05:32:49.973740 42927 solver.cpp:266] Iteration 2400 (3.92346 iter/s, 12.7438s/50 iter), loss = 0.096042
I1107 05:32:49.973768 42927 solver.cpp:285]     Train net output #0: loss = 0.096042 (* 1 = 0.096042 loss)
I1107 05:32:49.973774 42927 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 05:33:02.684388 42927 solver.cpp:266] Iteration 2450 (3.93387 iter/s, 12.7101s/50 iter), loss = 0.0616191
I1107 05:33:02.684428 42927 solver.cpp:285]     Train net output #0: loss = 0.0616191 (* 1 = 0.0616191 loss)
I1107 05:33:02.684435 42927 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 05:33:15.381333 42927 solver.cpp:266] Iteration 2500 (3.93812 iter/s, 12.6964s/50 iter), loss = 0.060009
I1107 05:33:15.381536 42927 solver.cpp:285]     Train net output #0: loss = 0.060009 (* 1 = 0.060009 loss)
I1107 05:33:15.381546 42927 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 05:33:28.102170 42927 solver.cpp:266] Iteration 2550 (3.93077 iter/s, 12.7201s/50 iter), loss = 0.0265123
I1107 05:33:28.102198 42927 solver.cpp:285]     Train net output #0: loss = 0.0265123 (* 1 = 0.0265123 loss)
I1107 05:33:28.102205 42927 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 05:33:40.769013 42927 solver.cpp:266] Iteration 2600 (3.94748 iter/s, 12.6663s/50 iter), loss = 0.0278214
I1107 05:33:40.769042 42927 solver.cpp:285]     Train net output #0: loss = 0.0278214 (* 1 = 0.0278214 loss)
I1107 05:33:40.769047 42927 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 05:33:53.493566 42927 solver.cpp:266] Iteration 2650 (3.92957 iter/s, 12.724s/50 iter), loss = 0.00730687
I1107 05:33:53.493719 42927 solver.cpp:285]     Train net output #0: loss = 0.00730686 (* 1 = 0.00730686 loss)
I1107 05:33:53.493727 42927 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 05:34:06.222707 42927 solver.cpp:266] Iteration 2700 (3.92819 iter/s, 12.7285s/50 iter), loss = 0.0128558
I1107 05:34:06.222738 42927 solver.cpp:285]     Train net output #0: loss = 0.0128558 (* 1 = 0.0128558 loss)
I1107 05:34:06.222743 42927 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 05:34:18.923013 42927 solver.cpp:266] Iteration 2750 (3.93708 iter/s, 12.6998s/50 iter), loss = 0.0125248
I1107 05:34:18.923043 42927 solver.cpp:285]     Train net output #0: loss = 0.0125248 (* 1 = 0.0125248 loss)
I1107 05:34:18.923048 42927 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 05:34:31.607869 42927 solver.cpp:266] Iteration 2800 (3.94187 iter/s, 12.6843s/50 iter), loss = 0.0312506
I1107 05:34:31.608027 42927 solver.cpp:285]     Train net output #0: loss = 0.0312506 (* 1 = 0.0312506 loss)
I1107 05:34:31.608036 42927 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 05:34:44.333259 42927 solver.cpp:266] Iteration 2850 (3.92935 iter/s, 12.7247s/50 iter), loss = 0.0135839
I1107 05:34:44.333288 42927 solver.cpp:285]     Train net output #0: loss = 0.0135839 (* 1 = 0.0135839 loss)
I1107 05:34:44.333293 42927 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 05:34:57.029404 42927 solver.cpp:266] Iteration 2900 (3.93836 iter/s, 12.6956s/50 iter), loss = 0.0355614
I1107 05:34:57.029434 42927 solver.cpp:285]     Train net output #0: loss = 0.0355614 (* 1 = 0.0355614 loss)
I1107 05:34:57.029440 42927 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 05:35:09.734161 42927 solver.cpp:266] Iteration 2950 (3.9357 iter/s, 12.7042s/50 iter), loss = 0.0032481
I1107 05:35:09.734313 42927 solver.cpp:285]     Train net output #0: loss = 0.0032481 (* 1 = 0.0032481 loss)
I1107 05:35:09.734323 42927 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 05:35:22.172920 42927 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 05:35:23.697129 42927 solver.cpp:517]     Test net output #0: loss = 0.13856 (* 1 = 0.13856 loss)
I1107 05:35:23.697144 42927 solver.cpp:517]     Test net output #1: top-1 = 0.94925
I1107 05:35:23.944705 42927 solver.cpp:266] Iteration 3000 (3.51869 iter/s, 14.2098s/50 iter), loss = 0.0138555
I1107 05:35:23.944730 42927 solver.cpp:285]     Train net output #0: loss = 0.0138555 (* 1 = 0.0138555 loss)
I1107 05:35:23.944736 42927 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 05:35:36.679500 42927 solver.cpp:266] Iteration 3050 (3.92641 iter/s, 12.7343s/50 iter), loss = 0.00459265
I1107 05:35:36.679529 42927 solver.cpp:285]     Train net output #0: loss = 0.00459264 (* 1 = 0.00459264 loss)
I1107 05:35:36.679535 42927 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 05:35:49.345628 42927 solver.cpp:266] Iteration 3100 (3.9477 iter/s, 12.6656s/50 iter), loss = 0.0225247
I1107 05:35:49.345779 42927 solver.cpp:285]     Train net output #0: loss = 0.0225247 (* 1 = 0.0225247 loss)
I1107 05:35:49.345788 42927 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 05:36:02.060163 42927 solver.cpp:266] Iteration 3150 (3.93271 iter/s, 12.7139s/50 iter), loss = 0.00784631
I1107 05:36:02.060191 42927 solver.cpp:285]     Train net output #0: loss = 0.00784631 (* 1 = 0.00784631 loss)
I1107 05:36:02.060196 42927 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 05:36:14.764544 42927 solver.cpp:266] Iteration 3200 (3.93581 iter/s, 12.7039s/50 iter), loss = 0.0356855
I1107 05:36:14.764572 42927 solver.cpp:285]     Train net output #0: loss = 0.0356855 (* 1 = 0.0356855 loss)
I1107 05:36:14.764578 42927 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 05:36:27.475961 42927 solver.cpp:266] Iteration 3250 (3.93363 iter/s, 12.7109s/50 iter), loss = 0.00448848
I1107 05:36:27.476150 42927 solver.cpp:285]     Train net output #0: loss = 0.00448848 (* 1 = 0.00448848 loss)
I1107 05:36:27.476158 42927 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 05:36:40.155051 42927 solver.cpp:266] Iteration 3300 (3.94371 iter/s, 12.6784s/50 iter), loss = 0.00818519
I1107 05:36:40.155081 42927 solver.cpp:285]     Train net output #0: loss = 0.00818519 (* 1 = 0.00818519 loss)
I1107 05:36:40.155086 42927 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 05:36:52.847110 42927 solver.cpp:266] Iteration 3350 (3.93963 iter/s, 12.6915s/50 iter), loss = 0.0182048
I1107 05:36:52.847141 42927 solver.cpp:285]     Train net output #0: loss = 0.0182048 (* 1 = 0.0182048 loss)
I1107 05:36:52.847146 42927 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I1107 05:37:05.551102 42927 solver.cpp:266] Iteration 3400 (3.93593 iter/s, 12.7035s/50 iter), loss = 0.00794384
I1107 05:37:05.551260 42927 solver.cpp:285]     Train net output #0: loss = 0.00794383 (* 1 = 0.00794383 loss)
I1107 05:37:05.551268 42927 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I1107 05:37:18.240216 42927 solver.cpp:266] Iteration 3450 (3.94059 iter/s, 12.6885s/50 iter), loss = 0.0204228
I1107 05:37:18.240245 42927 solver.cpp:285]     Train net output #0: loss = 0.0204228 (* 1 = 0.0204228 loss)
I1107 05:37:18.240252 42927 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I1107 05:37:30.943297 42927 solver.cpp:266] Iteration 3500 (3.93621 iter/s, 12.7026s/50 iter), loss = 0.0193294
I1107 05:37:30.943326 42927 solver.cpp:285]     Train net output #0: loss = 0.0193294 (* 1 = 0.0193294 loss)
I1107 05:37:30.943332 42927 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I1107 05:37:43.634769 42927 solver.cpp:266] Iteration 3550 (3.93982 iter/s, 12.691s/50 iter), loss = 0.00467321
I1107 05:37:43.634919 42927 solver.cpp:285]     Train net output #0: loss = 0.0046732 (* 1 = 0.0046732 loss)
I1107 05:37:43.634927 42927 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I1107 05:37:56.310211 42927 solver.cpp:266] Iteration 3600 (3.94483 iter/s, 12.6748s/50 iter), loss = 0.028923
I1107 05:37:56.310242 42927 solver.cpp:285]     Train net output #0: loss = 0.028923 (* 1 = 0.028923 loss)
I1107 05:37:56.310250 42927 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I1107 05:38:09.034857 42927 solver.cpp:266] Iteration 3650 (3.92954 iter/s, 12.7241s/50 iter), loss = 0.0363844
I1107 05:38:09.034888 42927 solver.cpp:285]     Train net output #0: loss = 0.0363844 (* 1 = 0.0363844 loss)
I1107 05:38:09.034893 42927 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I1107 05:38:21.754968 42927 solver.cpp:266] Iteration 3700 (3.93094 iter/s, 12.7196s/50 iter), loss = 0.025118
I1107 05:38:21.755138 42927 solver.cpp:285]     Train net output #0: loss = 0.025118 (* 1 = 0.025118 loss)
I1107 05:38:21.755148 42927 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I1107 05:38:34.423557 42927 solver.cpp:266] Iteration 3750 (3.94697 iter/s, 12.6679s/50 iter), loss = 0.00839765
I1107 05:38:34.423588 42927 solver.cpp:285]     Train net output #0: loss = 0.00839765 (* 1 = 0.00839765 loss)
I1107 05:38:34.423593 42927 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I1107 05:38:47.065276 42927 solver.cpp:266] Iteration 3800 (3.95532 iter/s, 12.6412s/50 iter), loss = 0.00314865
I1107 05:38:47.065306 42927 solver.cpp:285]     Train net output #0: loss = 0.00314866 (* 1 = 0.00314866 loss)
I1107 05:38:47.065312 42927 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I1107 05:38:59.740473 42927 solver.cpp:266] Iteration 3850 (3.94487 iter/s, 12.6747s/50 iter), loss = 0.0185019
I1107 05:38:59.740649 42927 solver.cpp:285]     Train net output #0: loss = 0.0185019 (* 1 = 0.0185019 loss)
I1107 05:38:59.740658 42927 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I1107 05:39:12.345896 42927 solver.cpp:266] Iteration 3900 (3.96676 iter/s, 12.6048s/50 iter), loss = 0.00121777
I1107 05:39:12.345922 42927 solver.cpp:285]     Train net output #0: loss = 0.00121778 (* 1 = 0.00121778 loss)
I1107 05:39:12.345928 42927 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I1107 05:39:25.015373 42927 solver.cpp:266] Iteration 3950 (3.94665 iter/s, 12.669s/50 iter), loss = 0.0113117
I1107 05:39:25.015403 42927 solver.cpp:285]     Train net output #0: loss = 0.0113117 (* 1 = 0.0113117 loss)
I1107 05:39:25.015426 42927 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I1107 05:39:37.427130 42927 solver.cpp:418] Iteration 4000, Testing net (#0)
I1107 05:39:38.939894 42927 solver.cpp:517]     Test net output #0: loss = 0.145625 (* 1 = 0.145625 loss)
I1107 05:39:38.939910 42927 solver.cpp:517]     Test net output #1: top-1 = 0.9515
I1107 05:39:39.189415 42927 solver.cpp:266] Iteration 4000 (3.52772 iter/s, 14.1735s/50 iter), loss = 0.0109998
I1107 05:39:39.189442 42927 solver.cpp:285]     Train net output #0: loss = 0.0109998 (* 1 = 0.0109998 loss)
I1107 05:39:39.189448 42927 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I1107 05:39:51.855208 42927 solver.cpp:266] Iteration 4050 (3.9478 iter/s, 12.6653s/50 iter), loss = 0.0117588
I1107 05:39:51.855240 42927 solver.cpp:285]     Train net output #0: loss = 0.0117588 (* 1 = 0.0117588 loss)
I1107 05:39:51.855245 42927 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I1107 05:40:04.525763 42927 solver.cpp:266] Iteration 4100 (3.94632 iter/s, 12.67s/50 iter), loss = 0.00622594
I1107 05:40:04.525792 42927 solver.cpp:285]     Train net output #0: loss = 0.00622594 (* 1 = 0.00622594 loss)
I1107 05:40:04.525797 42927 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I1107 05:40:17.188097 42927 solver.cpp:266] Iteration 4150 (3.94888 iter/s, 12.6618s/50 iter), loss = 0.0153151
I1107 05:40:17.188256 42927 solver.cpp:285]     Train net output #0: loss = 0.0153151 (* 1 = 0.0153151 loss)
I1107 05:40:17.188266 42927 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I1107 05:40:29.806722 42927 solver.cpp:266] Iteration 4200 (3.9626 iter/s, 12.618s/50 iter), loss = 0.00327638
I1107 05:40:29.806751 42927 solver.cpp:285]     Train net output #0: loss = 0.00327639 (* 1 = 0.00327639 loss)
I1107 05:40:29.806756 42927 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I1107 05:40:42.464174 42927 solver.cpp:266] Iteration 4250 (3.9504 iter/s, 12.6569s/50 iter), loss = 0.0112071
I1107 05:40:42.464203 42927 solver.cpp:285]     Train net output #0: loss = 0.0112071 (* 1 = 0.0112071 loss)
I1107 05:40:42.464210 42927 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I1107 05:40:55.098404 42927 solver.cpp:266] Iteration 4300 (3.95766 iter/s, 12.6337s/50 iter), loss = 0.00246887
I1107 05:40:55.098564 42927 solver.cpp:285]     Train net output #0: loss = 0.00246888 (* 1 = 0.00246888 loss)
I1107 05:40:55.098572 42927 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I1107 05:41:07.797973 42927 solver.cpp:266] Iteration 4350 (3.93734 iter/s, 12.6989s/50 iter), loss = 0.00520309
I1107 05:41:07.798003 42927 solver.cpp:285]     Train net output #0: loss = 0.00520308 (* 1 = 0.00520308 loss)
I1107 05:41:07.798009 42927 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I1107 05:41:20.463522 42927 solver.cpp:266] Iteration 4400 (3.94788 iter/s, 12.665s/50 iter), loss = 0.0118335
I1107 05:41:20.463551 42927 solver.cpp:285]     Train net output #0: loss = 0.0118335 (* 1 = 0.0118335 loss)
I1107 05:41:20.463557 42927 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I1107 05:41:33.097167 42927 solver.cpp:266] Iteration 4450 (3.95785 iter/s, 12.6331s/50 iter), loss = 0.0143667
I1107 05:41:33.097319 42927 solver.cpp:285]     Train net output #0: loss = 0.0143667 (* 1 = 0.0143667 loss)
I1107 05:41:33.097327 42927 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I1107 05:41:45.742957 42927 solver.cpp:266] Iteration 4500 (3.95408 iter/s, 12.6452s/50 iter), loss = 0.00296405
I1107 05:41:45.742987 42927 solver.cpp:285]     Train net output #0: loss = 0.00296404 (* 1 = 0.00296404 loss)
I1107 05:41:45.742992 42927 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I1107 05:41:58.391774 42927 solver.cpp:266] Iteration 4550 (3.9531 iter/s, 12.6483s/50 iter), loss = 0.0141452
I1107 05:41:58.391803 42927 solver.cpp:285]     Train net output #0: loss = 0.0141452 (* 1 = 0.0141452 loss)
I1107 05:41:58.391808 42927 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I1107 05:42:10.979228 42927 solver.cpp:266] Iteration 4600 (3.97237 iter/s, 12.5869s/50 iter), loss = 0.00543527
I1107 05:42:10.979418 42927 solver.cpp:285]     Train net output #0: loss = 0.00543526 (* 1 = 0.00543526 loss)
I1107 05:42:10.979426 42927 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I1107 05:42:23.655278 42927 solver.cpp:266] Iteration 4650 (3.94466 iter/s, 12.6754s/50 iter), loss = 0.00306815
I1107 05:42:23.655308 42927 solver.cpp:285]     Train net output #0: loss = 0.00306814 (* 1 = 0.00306814 loss)
I1107 05:42:23.655314 42927 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I1107 05:42:36.300801 42927 solver.cpp:266] Iteration 4700 (3.95413 iter/s, 12.645s/50 iter), loss = 0.00830289
I1107 05:42:36.300828 42927 solver.cpp:285]     Train net output #0: loss = 0.00830288 (* 1 = 0.00830288 loss)
I1107 05:42:36.300833 42927 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I1107 05:42:48.976529 42927 solver.cpp:266] Iteration 4750 (3.94471 iter/s, 12.6752s/50 iter), loss = 0.0211773
I1107 05:42:48.976683 42927 solver.cpp:285]     Train net output #0: loss = 0.0211773 (* 1 = 0.0211773 loss)
I1107 05:42:48.976691 42927 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I1107 05:43:01.645988 42927 solver.cpp:266] Iteration 4800 (3.9467 iter/s, 12.6688s/50 iter), loss = 0.00350287
I1107 05:43:01.646016 42927 solver.cpp:285]     Train net output #0: loss = 0.00350286 (* 1 = 0.00350286 loss)
I1107 05:43:01.646023 42927 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I1107 05:43:14.276249 42927 solver.cpp:266] Iteration 4850 (3.95891 iter/s, 12.6297s/50 iter), loss = 0.0015088
I1107 05:43:14.276279 42927 solver.cpp:285]     Train net output #0: loss = 0.00150878 (* 1 = 0.00150878 loss)
I1107 05:43:14.276285 42927 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I1107 05:43:26.935032 42927 solver.cpp:266] Iteration 4900 (3.94999 iter/s, 12.6583s/50 iter), loss = 0.00368475
I1107 05:43:26.935179 42927 solver.cpp:285]     Train net output #0: loss = 0.00368473 (* 1 = 0.00368473 loss)
I1107 05:43:26.935187 42927 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I1107 05:43:39.545529 42927 solver.cpp:266] Iteration 4950 (3.96515 iter/s, 12.6099s/50 iter), loss = 0.0201693
I1107 05:43:39.545557 42927 solver.cpp:285]     Train net output #0: loss = 0.0201692 (* 1 = 0.0201692 loss)
I1107 05:43:39.545562 42927 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I1107 05:43:51.960465 42927 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_5000.caffemodel
I1107 05:43:54.345229 42927 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_5000.solverstate
I1107 05:43:54.841238 42927 solver.cpp:418] Iteration 5000, Testing net (#0)
I1107 05:43:56.309790 42927 solver.cpp:517]     Test net output #0: loss = 0.183086 (* 1 = 0.183086 loss)
I1107 05:43:56.309806 42927 solver.cpp:517]     Test net output #1: top-1 = 0.943
I1107 05:43:56.547809 42927 solver.cpp:266] Iteration 5000 (2.9409 iter/s, 17.0016s/50 iter), loss = 0.00595198
I1107 05:43:56.547833 42927 solver.cpp:285]     Train net output #0: loss = 0.00595197 (* 1 = 0.00595197 loss)
I1107 05:43:56.547839 42927 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I1107 05:44:09.070329 42927 solver.cpp:266] Iteration 5050 (3.99297 iter/s, 12.522s/50 iter), loss = 0.0174421
I1107 05:44:09.070499 42927 solver.cpp:285]     Train net output #0: loss = 0.0174421 (* 1 = 0.0174421 loss)
I1107 05:44:09.070509 42927 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I1107 05:44:21.595959 42927 solver.cpp:266] Iteration 5100 (3.99202 iter/s, 12.525s/50 iter), loss = 0.0113839
I1107 05:44:21.595989 42927 solver.cpp:285]     Train net output #0: loss = 0.0113839 (* 1 = 0.0113839 loss)
I1107 05:44:21.595998 42927 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I1107 05:44:34.193343 42927 solver.cpp:266] Iteration 5150 (3.96924 iter/s, 12.5969s/50 iter), loss = 0.00239552
I1107 05:44:34.193372 42927 solver.cpp:285]     Train net output #0: loss = 0.00239551 (* 1 = 0.00239551 loss)
I1107 05:44:34.193377 42927 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I1107 05:44:46.809592 42927 solver.cpp:266] Iteration 5200 (3.96331 iter/s, 12.6157s/50 iter), loss = 0.00547352
I1107 05:44:46.809762 42927 solver.cpp:285]     Train net output #0: loss = 0.0054735 (* 1 = 0.0054735 loss)
I1107 05:44:46.809770 42927 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I1107 05:44:59.452380 42927 solver.cpp:266] Iteration 5250 (3.95503 iter/s, 12.6421s/50 iter), loss = 0.00257918
I1107 05:44:59.452410 42927 solver.cpp:285]     Train net output #0: loss = 0.00257916 (* 1 = 0.00257916 loss)
I1107 05:44:59.452416 42927 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I1107 05:45:12.114833 42927 solver.cpp:266] Iteration 5300 (3.94884 iter/s, 12.6619s/50 iter), loss = 0.00217297
I1107 05:45:12.114862 42927 solver.cpp:285]     Train net output #0: loss = 0.00217295 (* 1 = 0.00217295 loss)
I1107 05:45:12.114868 42927 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I1107 05:45:24.722519 42927 solver.cpp:266] Iteration 5350 (3.966 iter/s, 12.6072s/50 iter), loss = 0.00338264
I1107 05:45:24.722666 42927 solver.cpp:285]     Train net output #0: loss = 0.00338262 (* 1 = 0.00338262 loss)
I1107 05:45:24.722674 42927 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I1107 05:45:37.348210 42927 solver.cpp:266] Iteration 5400 (3.96038 iter/s, 12.6251s/50 iter), loss = 0.00125902
I1107 05:45:37.348240 42927 solver.cpp:285]     Train net output #0: loss = 0.001259 (* 1 = 0.001259 loss)
I1107 05:45:37.348246 42927 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I1107 05:45:50.003093 42927 solver.cpp:266] Iteration 5450 (3.95121 iter/s, 12.6544s/50 iter), loss = 0.00448974
I1107 05:45:50.003120 42927 solver.cpp:285]     Train net output #0: loss = 0.00448971 (* 1 = 0.00448971 loss)
I1107 05:45:50.003126 42927 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I1107 05:46:02.642297 42927 solver.cpp:266] Iteration 5500 (3.95611 iter/s, 12.6387s/50 iter), loss = 0.00206031
I1107 05:46:02.642452 42927 solver.cpp:285]     Train net output #0: loss = 0.00206029 (* 1 = 0.00206029 loss)
I1107 05:46:02.642460 42927 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I1107 05:46:15.282276 42927 solver.cpp:266] Iteration 5550 (3.9559 iter/s, 12.6393s/50 iter), loss = 0.0029784
I1107 05:46:15.282307 42927 solver.cpp:285]     Train net output #0: loss = 0.00297838 (* 1 = 0.00297838 loss)
I1107 05:46:15.282313 42927 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I1107 05:46:27.918807 42927 solver.cpp:266] Iteration 5600 (3.95694 iter/s, 12.636s/50 iter), loss = 0.0020655
I1107 05:46:27.918838 42927 solver.cpp:285]     Train net output #0: loss = 0.00206548 (* 1 = 0.00206548 loss)
I1107 05:46:27.918843 42927 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I1107 05:46:40.557198 42927 solver.cpp:266] Iteration 5650 (3.95636 iter/s, 12.6379s/50 iter), loss = 0.0160816
I1107 05:46:40.557360 42927 solver.cpp:285]     Train net output #0: loss = 0.0160816 (* 1 = 0.0160816 loss)
I1107 05:46:40.557368 42927 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I1107 05:46:53.187798 42927 solver.cpp:266] Iteration 5700 (3.95884 iter/s, 12.63s/50 iter), loss = 0.00254972
I1107 05:46:53.187826 42927 solver.cpp:285]     Train net output #0: loss = 0.0025497 (* 1 = 0.0025497 loss)
I1107 05:46:53.187849 42927 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I1107 05:47:05.824983 42927 solver.cpp:266] Iteration 5750 (3.95674 iter/s, 12.6367s/50 iter), loss = 0.00191788
I1107 05:47:05.825012 42927 solver.cpp:285]     Train net output #0: loss = 0.00191786 (* 1 = 0.00191786 loss)
I1107 05:47:05.825018 42927 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I1107 05:47:18.478925 42927 solver.cpp:266] Iteration 5800 (3.9515 iter/s, 12.6534s/50 iter), loss = 0.00326555
I1107 05:47:18.479120 42927 solver.cpp:285]     Train net output #0: loss = 0.00326553 (* 1 = 0.00326553 loss)
I1107 05:47:18.479127 42927 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I1107 05:47:31.125233 42927 solver.cpp:266] Iteration 5850 (3.95394 iter/s, 12.6456s/50 iter), loss = 0.000532754
I1107 05:47:31.125262 42927 solver.cpp:285]     Train net output #0: loss = 0.000532734 (* 1 = 0.000532734 loss)
I1107 05:47:31.125267 42927 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I1107 05:47:43.765504 42927 solver.cpp:266] Iteration 5900 (3.95577 iter/s, 12.6398s/50 iter), loss = 0.00335183
I1107 05:47:43.765533 42927 solver.cpp:285]     Train net output #0: loss = 0.00335181 (* 1 = 0.00335181 loss)
I1107 05:47:43.765539 42927 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I1107 05:47:56.380156 42927 solver.cpp:266] Iteration 5950 (3.96381 iter/s, 12.6141s/50 iter), loss = 0.00616357
I1107 05:47:56.380322 42927 solver.cpp:285]     Train net output #0: loss = 0.00616356 (* 1 = 0.00616356 loss)
I1107 05:47:56.380331 42927 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I1107 05:48:08.793336 42927 solver.cpp:418] Iteration 6000, Testing net (#0)
I1107 05:48:10.289027 42927 solver.cpp:517]     Test net output #0: loss = 0.183425 (* 1 = 0.183425 loss)
I1107 05:48:10.289044 42927 solver.cpp:517]     Test net output #1: top-1 = 0.95075
I1107 05:48:10.531317 42927 solver.cpp:266] Iteration 6000 (3.53346 iter/s, 14.1505s/50 iter), loss = 0.00112784
I1107 05:48:10.531343 42927 solver.cpp:285]     Train net output #0: loss = 0.00112781 (* 1 = 0.00112781 loss)
I1107 05:48:10.531349 42927 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I1107 05:48:23.142895 42927 solver.cpp:266] Iteration 6050 (3.96477 iter/s, 12.6111s/50 iter), loss = 0.00621958
I1107 05:48:23.142922 42927 solver.cpp:285]     Train net output #0: loss = 0.00621956 (* 1 = 0.00621956 loss)
I1107 05:48:23.142944 42927 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I1107 05:48:35.776295 42927 solver.cpp:266] Iteration 6100 (3.95792 iter/s, 12.6329s/50 iter), loss = 0.000839955
I1107 05:48:35.776458 42927 solver.cpp:285]     Train net output #0: loss = 0.000839935 (* 1 = 0.000839935 loss)
I1107 05:48:35.776465 42927 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I1107 05:48:48.386220 42927 solver.cpp:266] Iteration 6150 (3.96533 iter/s, 12.6093s/50 iter), loss = 0.00131162
I1107 05:48:48.386251 42927 solver.cpp:285]     Train net output #0: loss = 0.0013116 (* 1 = 0.0013116 loss)
I1107 05:48:48.386258 42927 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I1107 05:49:01.020498 42927 solver.cpp:266] Iteration 6200 (3.95765 iter/s, 12.6338s/50 iter), loss = 0.00779437
I1107 05:49:01.020527 42927 solver.cpp:285]     Train net output #0: loss = 0.00779436 (* 1 = 0.00779436 loss)
I1107 05:49:01.020550 42927 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I1107 05:49:13.672210 42927 solver.cpp:266] Iteration 6250 (3.9522 iter/s, 12.6512s/50 iter), loss = 0.0165465
I1107 05:49:13.672363 42927 solver.cpp:285]     Train net output #0: loss = 0.0165465 (* 1 = 0.0165465 loss)
I1107 05:49:13.672371 42927 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I1107 05:49:26.305886 42927 solver.cpp:266] Iteration 6300 (3.95788 iter/s, 12.633s/50 iter), loss = 0.00135827
I1107 05:49:26.305915 42927 solver.cpp:285]     Train net output #0: loss = 0.00135826 (* 1 = 0.00135826 loss)
I1107 05:49:26.305922 42927 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I1107 05:49:38.914727 42927 solver.cpp:266] Iteration 6350 (3.96563 iter/s, 12.6083s/50 iter), loss = 0.0041941
I1107 05:49:38.914754 42927 solver.cpp:285]     Train net output #0: loss = 0.00419408 (* 1 = 0.00419408 loss)
I1107 05:49:38.914760 42927 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I1107 05:49:51.556411 42927 solver.cpp:266] Iteration 6400 (3.95534 iter/s, 12.6411s/50 iter), loss = 0.0117824
I1107 05:49:51.556588 42927 solver.cpp:285]     Train net output #0: loss = 0.0117824 (* 1 = 0.0117824 loss)
I1107 05:49:51.556596 42927 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I1107 05:50:04.231211 42927 solver.cpp:266] Iteration 6450 (3.94505 iter/s, 12.6741s/50 iter), loss = 0.00466012
I1107 05:50:04.231240 42927 solver.cpp:285]     Train net output #0: loss = 0.00466011 (* 1 = 0.00466011 loss)
I1107 05:50:04.231246 42927 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I1107 05:50:16.905361 42927 solver.cpp:266] Iteration 6500 (3.94521 iter/s, 12.6736s/50 iter), loss = 0.00565918
I1107 05:50:16.905393 42927 solver.cpp:285]     Train net output #0: loss = 0.00565917 (* 1 = 0.00565917 loss)
I1107 05:50:16.905398 42927 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I1107 05:50:29.523051 42927 solver.cpp:266] Iteration 6550 (3.96286 iter/s, 12.6171s/50 iter), loss = 0.00354215
I1107 05:50:29.523218 42927 solver.cpp:285]     Train net output #0: loss = 0.00354214 (* 1 = 0.00354214 loss)
I1107 05:50:29.523227 42927 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I1107 05:50:42.141841 42927 solver.cpp:266] Iteration 6600 (3.96256 iter/s, 12.6181s/50 iter), loss = 0.00992437
I1107 05:50:42.141872 42927 solver.cpp:285]     Train net output #0: loss = 0.00992436 (* 1 = 0.00992436 loss)
I1107 05:50:42.141878 42927 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I1107 05:50:54.825671 42927 solver.cpp:266] Iteration 6650 (3.9422 iter/s, 12.6833s/50 iter), loss = 0.00145571
I1107 05:50:54.825700 42927 solver.cpp:285]     Train net output #0: loss = 0.0014557 (* 1 = 0.0014557 loss)
I1107 05:50:54.825706 42927 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I1107 05:51:07.469019 42927 solver.cpp:266] Iteration 6700 (3.95482 iter/s, 12.6428s/50 iter), loss = 0.0019025
I1107 05:51:07.469190 42927 solver.cpp:285]     Train net output #0: loss = 0.00190249 (* 1 = 0.00190249 loss)
I1107 05:51:07.469199 42927 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I1107 05:51:20.124819 42927 solver.cpp:266] Iteration 6750 (3.95097 iter/s, 12.6551s/50 iter), loss = 0.00500166
I1107 05:51:20.124848 42927 solver.cpp:285]     Train net output #0: loss = 0.00500165 (* 1 = 0.00500165 loss)
I1107 05:51:20.124855 42927 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I1107 05:51:32.761616 42927 solver.cpp:266] Iteration 6800 (3.95687 iter/s, 12.6363s/50 iter), loss = 0.000855393
I1107 05:51:32.761646 42927 solver.cpp:285]     Train net output #0: loss = 0.000855378 (* 1 = 0.000855378 loss)
I1107 05:51:32.761651 42927 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I1107 05:51:45.343762 42927 solver.cpp:266] Iteration 6850 (3.97405 iter/s, 12.5816s/50 iter), loss = 0.00139357
I1107 05:51:45.343922 42927 solver.cpp:285]     Train net output #0: loss = 0.00139356 (* 1 = 0.00139356 loss)
I1107 05:51:45.343932 42927 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I1107 05:51:57.986413 42927 solver.cpp:266] Iteration 6900 (3.95508 iter/s, 12.642s/50 iter), loss = 0.00890792
I1107 05:51:57.986444 42927 solver.cpp:285]     Train net output #0: loss = 0.0089079 (* 1 = 0.0089079 loss)
I1107 05:51:57.986450 42927 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I1107 05:52:10.624941 42927 solver.cpp:266] Iteration 6950 (3.95633 iter/s, 12.638s/50 iter), loss = 0.0137172
I1107 05:52:10.624969 42927 solver.cpp:285]     Train net output #0: loss = 0.0137172 (* 1 = 0.0137172 loss)
I1107 05:52:10.624974 42927 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I1107 05:52:23.008657 42927 solver.cpp:418] Iteration 7000, Testing net (#0)
I1107 05:52:24.501199 42927 solver.cpp:517]     Test net output #0: loss = 0.20148 (* 1 = 0.20148 loss)
I1107 05:52:24.501215 42927 solver.cpp:517]     Test net output #1: top-1 = 0.94975
I1107 05:52:24.750807 42927 solver.cpp:266] Iteration 7000 (3.53975 iter/s, 14.1253s/50 iter), loss = 0.00391749
I1107 05:52:24.750831 42927 solver.cpp:285]     Train net output #0: loss = 0.00391747 (* 1 = 0.00391747 loss)
I1107 05:52:24.750838 42927 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I1107 05:52:37.419066 42927 solver.cpp:266] Iteration 7050 (3.94704 iter/s, 12.6677s/50 iter), loss = 0.00231547
I1107 05:52:37.419095 42927 solver.cpp:285]     Train net output #0: loss = 0.00231546 (* 1 = 0.00231546 loss)
I1107 05:52:37.419117 42927 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I1107 05:52:50.035194 42927 solver.cpp:266] Iteration 7100 (3.96335 iter/s, 12.6156s/50 iter), loss = 0.00477604
I1107 05:52:50.035234 42927 solver.cpp:285]     Train net output #0: loss = 0.00477603 (* 1 = 0.00477603 loss)
I1107 05:52:50.035241 42927 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I1107 05:53:02.643451 42927 solver.cpp:266] Iteration 7150 (3.96582 iter/s, 12.6077s/50 iter), loss = 0.00860665
I1107 05:53:02.643640 42927 solver.cpp:285]     Train net output #0: loss = 0.00860663 (* 1 = 0.00860663 loss)
I1107 05:53:02.643649 42927 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I1107 05:53:15.311755 42927 solver.cpp:266] Iteration 7200 (3.94707 iter/s, 12.6676s/50 iter), loss = 0.00119469
I1107 05:53:15.311787 42927 solver.cpp:285]     Train net output #0: loss = 0.00119468 (* 1 = 0.00119468 loss)
I1107 05:53:15.311794 42927 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I1107 05:53:27.951913 42927 solver.cpp:266] Iteration 7250 (3.95582 iter/s, 12.6396s/50 iter), loss = 0.00183248
I1107 05:53:27.951943 42927 solver.cpp:285]     Train net output #0: loss = 0.00183247 (* 1 = 0.00183247 loss)
I1107 05:53:27.951966 42927 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I1107 05:53:40.560971 42927 solver.cpp:266] Iteration 7300 (3.96557 iter/s, 12.6085s/50 iter), loss = 0.00351013
I1107 05:53:40.561126 42927 solver.cpp:285]     Train net output #0: loss = 0.00351012 (* 1 = 0.00351012 loss)
I1107 05:53:40.561134 42927 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I1107 05:53:53.215212 42927 solver.cpp:266] Iteration 7350 (3.95145 iter/s, 12.6536s/50 iter), loss = 0.00202387
I1107 05:53:53.215243 42927 solver.cpp:285]     Train net output #0: loss = 0.00202386 (* 1 = 0.00202386 loss)
I1107 05:53:53.215248 42927 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I1107 05:54:05.852593 42927 solver.cpp:266] Iteration 7400 (3.95668 iter/s, 12.6368s/50 iter), loss = 0.0078337
I1107 05:54:05.852624 42927 solver.cpp:285]     Train net output #0: loss = 0.0078337 (* 1 = 0.0078337 loss)
I1107 05:54:05.852632 42927 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I1107 05:54:18.479020 42927 solver.cpp:266] Iteration 7450 (3.96012 iter/s, 12.6259s/50 iter), loss = 0.000896738
I1107 05:54:18.479182 42927 solver.cpp:285]     Train net output #0: loss = 0.000896734 (* 1 = 0.000896734 loss)
I1107 05:54:18.479190 42927 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I1107 05:54:31.106230 42927 solver.cpp:266] Iteration 7500 (3.95991 iter/s, 12.6265s/50 iter), loss = 0.0038357
I1107 05:54:31.106259 42927 solver.cpp:285]     Train net output #0: loss = 0.00383569 (* 1 = 0.00383569 loss)
I1107 05:54:31.106266 42927 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I1107 05:54:43.707995 42927 solver.cpp:266] Iteration 7550 (3.96787 iter/s, 12.6012s/50 iter), loss = 0.00338399
I1107 05:54:43.708025 42927 solver.cpp:285]     Train net output #0: loss = 0.00338399 (* 1 = 0.00338399 loss)
I1107 05:54:43.708031 42927 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I1107 05:54:56.356945 42927 solver.cpp:266] Iteration 7600 (3.95306 iter/s, 12.6484s/50 iter), loss = 0.00409577
I1107 05:54:56.357120 42927 solver.cpp:285]     Train net output #0: loss = 0.00409577 (* 1 = 0.00409577 loss)
I1107 05:54:56.357128 42927 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I1107 05:55:09.026386 42927 solver.cpp:266] Iteration 7650 (3.94672 iter/s, 12.6688s/50 iter), loss = 0.00352073
I1107 05:55:09.026414 42927 solver.cpp:285]     Train net output #0: loss = 0.00352072 (* 1 = 0.00352072 loss)
I1107 05:55:09.026420 42927 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I1107 05:55:21.657287 42927 solver.cpp:266] Iteration 7700 (3.95871 iter/s, 12.6304s/50 iter), loss = 0.0134899
I1107 05:55:21.657317 42927 solver.cpp:285]     Train net output #0: loss = 0.0134899 (* 1 = 0.0134899 loss)
I1107 05:55:21.657323 42927 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I1107 05:55:34.301401 42927 solver.cpp:266] Iteration 7750 (3.95458 iter/s, 12.6436s/50 iter), loss = 0.000869217
I1107 05:55:34.301568 42927 solver.cpp:285]     Train net output #0: loss = 0.000869212 (* 1 = 0.000869212 loss)
I1107 05:55:34.301576 42927 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I1107 05:55:46.921254 42927 solver.cpp:266] Iteration 7800 (3.96222 iter/s, 12.6192s/50 iter), loss = 0.00340592
I1107 05:55:46.921283 42927 solver.cpp:285]     Train net output #0: loss = 0.00340592 (* 1 = 0.00340592 loss)
I1107 05:55:46.921289 42927 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I1107 05:55:59.534278 42927 solver.cpp:266] Iteration 7850 (3.96432 iter/s, 12.6125s/50 iter), loss = 0.00944647
I1107 05:55:59.534307 42927 solver.cpp:285]     Train net output #0: loss = 0.00944647 (* 1 = 0.00944647 loss)
I1107 05:55:59.534312 42927 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I1107 05:56:12.224818 42927 solver.cpp:266] Iteration 7900 (3.94011 iter/s, 12.69s/50 iter), loss = 0.00228915
I1107 05:56:12.224966 42927 solver.cpp:285]     Train net output #0: loss = 0.00228914 (* 1 = 0.00228914 loss)
I1107 05:56:12.224973 42927 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I1107 05:56:24.848012 42927 solver.cpp:266] Iteration 7950 (3.96117 iter/s, 12.6225s/50 iter), loss = 0.00111469
I1107 05:56:24.848042 42927 solver.cpp:285]     Train net output #0: loss = 0.00111469 (* 1 = 0.00111469 loss)
I1107 05:56:24.848047 42927 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I1107 05:56:37.222213 42927 solver.cpp:418] Iteration 8000, Testing net (#0)
I1107 05:56:38.719777 42927 solver.cpp:517]     Test net output #0: loss = 0.20789 (* 1 = 0.20789 loss)
I1107 05:56:38.719794 42927 solver.cpp:517]     Test net output #1: top-1 = 0.95375
I1107 05:56:38.964763 42927 solver.cpp:266] Iteration 8000 (3.54204 iter/s, 14.1162s/50 iter), loss = 0.00136706
I1107 05:56:38.964787 42927 solver.cpp:285]     Train net output #0: loss = 0.00136706 (* 1 = 0.00136706 loss)
I1107 05:56:38.964793 42927 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I1107 05:56:51.572760 42927 solver.cpp:266] Iteration 8050 (3.9659 iter/s, 12.6075s/50 iter), loss = 0.00181375
I1107 05:56:51.572922 42927 solver.cpp:285]     Train net output #0: loss = 0.00181375 (* 1 = 0.00181375 loss)
I1107 05:56:51.572932 42927 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I1107 05:57:04.212906 42927 solver.cpp:266] Iteration 8100 (3.95586 iter/s, 12.6395s/50 iter), loss = 0.00508459
I1107 05:57:04.212936 42927 solver.cpp:285]     Train net output #0: loss = 0.00508459 (* 1 = 0.00508459 loss)
I1107 05:57:04.212942 42927 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I1107 05:57:16.872421 42927 solver.cpp:266] Iteration 8150 (3.94977 iter/s, 12.659s/50 iter), loss = 0.00219649
I1107 05:57:16.872450 42927 solver.cpp:285]     Train net output #0: loss = 0.00219649 (* 1 = 0.00219649 loss)
I1107 05:57:16.872457 42927 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I1107 05:57:29.512913 42927 solver.cpp:266] Iteration 8200 (3.95571 iter/s, 12.64s/50 iter), loss = 0.00613212
I1107 05:57:29.513100 42927 solver.cpp:285]     Train net output #0: loss = 0.00613212 (* 1 = 0.00613212 loss)
I1107 05:57:29.513108 42927 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I1107 05:57:42.164929 42927 solver.cpp:266] Iteration 8250 (3.95215 iter/s, 12.6513s/50 iter), loss = 0.00189856
I1107 05:57:42.164958 42927 solver.cpp:285]     Train net output #0: loss = 0.00189856 (* 1 = 0.00189856 loss)
I1107 05:57:42.164965 42927 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I1107 05:57:54.778054 42927 solver.cpp:266] Iteration 8300 (3.96429 iter/s, 12.6126s/50 iter), loss = 0.00159231
I1107 05:57:54.778085 42927 solver.cpp:285]     Train net output #0: loss = 0.00159231 (* 1 = 0.00159231 loss)
I1107 05:57:54.778091 42927 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I1107 05:58:07.447710 42927 solver.cpp:266] Iteration 8350 (3.9466 iter/s, 12.6691s/50 iter), loss = 0.00322952
I1107 05:58:07.447921 42927 solver.cpp:285]     Train net output #0: loss = 0.00322952 (* 1 = 0.00322952 loss)
I1107 05:58:07.447932 42927 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I1107 05:58:20.091241 42927 solver.cpp:266] Iteration 8400 (3.95481 iter/s, 12.6428s/50 iter), loss = 0.00224328
I1107 05:58:20.091270 42927 solver.cpp:285]     Train net output #0: loss = 0.00224328 (* 1 = 0.00224328 loss)
I1107 05:58:20.091276 42927 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I1107 05:58:32.755017 42927 solver.cpp:266] Iteration 8450 (3.94844 iter/s, 12.6632s/50 iter), loss = 0.00797718
I1107 05:58:32.755045 42927 solver.cpp:285]     Train net output #0: loss = 0.00797718 (* 1 = 0.00797718 loss)
I1107 05:58:32.755051 42927 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I1107 05:58:45.397148 42927 solver.cpp:266] Iteration 8500 (3.95519 iter/s, 12.6416s/50 iter), loss = 0.00200312
I1107 05:58:45.397315 42927 solver.cpp:285]     Train net output #0: loss = 0.00200311 (* 1 = 0.00200311 loss)
I1107 05:58:45.397323 42927 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I1107 05:58:58.008260 42927 solver.cpp:266] Iteration 8550 (3.96497 iter/s, 12.6104s/50 iter), loss = 0.00550344
I1107 05:58:58.008291 42927 solver.cpp:285]     Train net output #0: loss = 0.00550343 (* 1 = 0.00550343 loss)
I1107 05:58:58.008296 42927 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I1107 05:59:10.620765 42927 solver.cpp:266] Iteration 8600 (3.96449 iter/s, 12.612s/50 iter), loss = 0.00128267
I1107 05:59:10.620795 42927 solver.cpp:285]     Train net output #0: loss = 0.00128267 (* 1 = 0.00128267 loss)
I1107 05:59:10.620802 42927 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I1107 05:59:23.283151 42927 solver.cpp:266] Iteration 8650 (3.94887 iter/s, 12.6619s/50 iter), loss = 0.00316504
I1107 05:59:23.283313 42927 solver.cpp:285]     Train net output #0: loss = 0.00316504 (* 1 = 0.00316504 loss)
I1107 05:59:23.283321 42927 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I1107 05:59:35.936434 42927 solver.cpp:266] Iteration 8700 (3.95175 iter/s, 12.6526s/50 iter), loss = 0.00162336
I1107 05:59:35.936463 42927 solver.cpp:285]     Train net output #0: loss = 0.00162336 (* 1 = 0.00162336 loss)
I1107 05:59:35.936470 42927 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I1107 05:59:48.521158 42927 solver.cpp:266] Iteration 8750 (3.97324 iter/s, 12.5842s/50 iter), loss = 0.000847328
I1107 05:59:48.521186 42927 solver.cpp:285]     Train net output #0: loss = 0.000847326 (* 1 = 0.000847326 loss)
I1107 05:59:48.521193 42927 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I1107 06:00:01.178231 42927 solver.cpp:266] Iteration 8800 (3.95053 iter/s, 12.6565s/50 iter), loss = 0.0082818
I1107 06:00:01.178372 42927 solver.cpp:285]     Train net output #0: loss = 0.0082818 (* 1 = 0.0082818 loss)
I1107 06:00:01.178380 42927 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I1107 06:00:13.815430 42927 solver.cpp:266] Iteration 8850 (3.95677 iter/s, 12.6366s/50 iter), loss = 0.0011143
I1107 06:00:13.815461 42927 solver.cpp:285]     Train net output #0: loss = 0.0011143 (* 1 = 0.0011143 loss)
I1107 06:00:13.815467 42927 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I1107 06:00:26.440147 42927 solver.cpp:266] Iteration 8900 (3.96065 iter/s, 12.6242s/50 iter), loss = 0.00711293
I1107 06:00:26.440176 42927 solver.cpp:285]     Train net output #0: loss = 0.00711293 (* 1 = 0.00711293 loss)
I1107 06:00:26.440182 42927 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I1107 06:00:39.063308 42927 solver.cpp:266] Iteration 8950 (3.96114 iter/s, 12.6226s/50 iter), loss = 0.000977513
I1107 06:00:39.063455 42927 solver.cpp:285]     Train net output #0: loss = 0.000977501 (* 1 = 0.000977501 loss)
I1107 06:00:39.063464 42927 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I1107 06:00:51.425297 42927 solver.cpp:418] Iteration 9000, Testing net (#0)
I1107 06:00:52.973937 42927 solver.cpp:517]     Test net output #0: loss = 0.217504 (* 1 = 0.217504 loss)
I1107 06:00:52.973951 42927 solver.cpp:517]     Test net output #1: top-1 = 0.95175
I1107 06:00:53.217950 42927 solver.cpp:266] Iteration 9000 (3.53258 iter/s, 14.1539s/50 iter), loss = 0.00734123
I1107 06:00:53.217978 42927 solver.cpp:285]     Train net output #0: loss = 0.00734121 (* 1 = 0.00734121 loss)
I1107 06:00:53.217985 42927 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I1107 06:01:05.846454 42927 solver.cpp:266] Iteration 9050 (3.95946 iter/s, 12.628s/50 iter), loss = 0.00188147
I1107 06:01:05.846487 42927 solver.cpp:285]     Train net output #0: loss = 0.00188145 (* 1 = 0.00188145 loss)
I1107 06:01:05.846493 42927 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I1107 06:01:18.478718 42927 solver.cpp:266] Iteration 9100 (3.95828 iter/s, 12.6317s/50 iter), loss = 0.0062586
I1107 06:01:18.479593 42927 solver.cpp:285]     Train net output #0: loss = 0.00625859 (* 1 = 0.00625859 loss)
I1107 06:01:18.479604 42927 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I1107 06:01:31.142144 42927 solver.cpp:266] Iteration 9150 (3.94881 iter/s, 12.6621s/50 iter), loss = 0.00500307
I1107 06:01:31.142175 42927 solver.cpp:285]     Train net output #0: loss = 0.00500305 (* 1 = 0.00500305 loss)
I1107 06:01:31.142181 42927 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I1107 06:01:43.785485 42927 solver.cpp:266] Iteration 9200 (3.95482 iter/s, 12.6428s/50 iter), loss = 0.00522312
I1107 06:01:43.785516 42927 solver.cpp:285]     Train net output #0: loss = 0.00522311 (* 1 = 0.00522311 loss)
I1107 06:01:43.785522 42927 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I1107 06:01:56.408324 42927 solver.cpp:266] Iteration 9250 (3.96124 iter/s, 12.6223s/50 iter), loss = 0.00286445
I1107 06:01:56.408475 42927 solver.cpp:285]     Train net output #0: loss = 0.00286443 (* 1 = 0.00286443 loss)
I1107 06:01:56.408484 42927 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I1107 06:02:09.054194 42927 solver.cpp:266] Iteration 9300 (3.95406 iter/s, 12.6452s/50 iter), loss = 0.0056724
I1107 06:02:09.054222 42927 solver.cpp:285]     Train net output #0: loss = 0.00567238 (* 1 = 0.00567238 loss)
I1107 06:02:09.054229 42927 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I1107 06:02:21.727174 42927 solver.cpp:266] Iteration 9350 (3.94557 iter/s, 12.6725s/50 iter), loss = 0.00390163
I1107 06:02:21.727206 42927 solver.cpp:285]     Train net output #0: loss = 0.00390161 (* 1 = 0.00390161 loss)
I1107 06:02:21.727213 42927 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I1107 06:02:34.381718 42927 solver.cpp:266] Iteration 9400 (3.95131 iter/s, 12.654s/50 iter), loss = 0.000760907
I1107 06:02:34.381871 42927 solver.cpp:285]     Train net output #0: loss = 0.000760891 (* 1 = 0.000760891 loss)
I1107 06:02:34.381878 42927 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I1107 06:02:47.022266 42927 solver.cpp:266] Iteration 9450 (3.95573 iter/s, 12.6399s/50 iter), loss = 0.0168804
I1107 06:02:47.022295 42927 solver.cpp:285]     Train net output #0: loss = 0.0168804 (* 1 = 0.0168804 loss)
I1107 06:02:47.022301 42927 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I1107 06:02:59.667234 42927 solver.cpp:266] Iteration 9500 (3.95431 iter/s, 12.6444s/50 iter), loss = 0.0205588
I1107 06:02:59.667265 42927 solver.cpp:285]     Train net output #0: loss = 0.0205588 (* 1 = 0.0205588 loss)
I1107 06:02:59.667271 42927 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I1107 06:03:12.310338 42927 solver.cpp:266] Iteration 9550 (3.95489 iter/s, 12.6426s/50 iter), loss = 0.0010609
I1107 06:03:12.310499 42927 solver.cpp:285]     Train net output #0: loss = 0.00106089 (* 1 = 0.00106089 loss)
I1107 06:03:12.310508 42927 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I1107 06:03:24.949862 42927 solver.cpp:266] Iteration 9600 (3.95605 iter/s, 12.6389s/50 iter), loss = 0.002304
I1107 06:03:24.949892 42927 solver.cpp:285]     Train net output #0: loss = 0.00230398 (* 1 = 0.00230398 loss)
I1107 06:03:24.949896 42927 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I1107 06:03:37.615010 42927 solver.cpp:266] Iteration 9650 (3.94801 iter/s, 12.6646s/50 iter), loss = 0.00442765
I1107 06:03:37.615039 42927 solver.cpp:285]     Train net output #0: loss = 0.00442763 (* 1 = 0.00442763 loss)
I1107 06:03:37.615046 42927 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I1107 06:03:50.266266 42927 solver.cpp:266] Iteration 9700 (3.95234 iter/s, 12.6507s/50 iter), loss = 0.00215502
I1107 06:03:50.266451 42927 solver.cpp:285]     Train net output #0: loss = 0.00215501 (* 1 = 0.00215501 loss)
I1107 06:03:50.266461 42927 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I1107 06:04:02.898675 42927 solver.cpp:266] Iteration 9750 (3.95829 iter/s, 12.6317s/50 iter), loss = 0.00679457
I1107 06:04:02.898705 42927 solver.cpp:285]     Train net output #0: loss = 0.00679455 (* 1 = 0.00679455 loss)
I1107 06:04:02.898711 42927 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I1107 06:04:15.515241 42927 solver.cpp:266] Iteration 9800 (3.96321 iter/s, 12.616s/50 iter), loss = 0.00558897
I1107 06:04:15.515270 42927 solver.cpp:285]     Train net output #0: loss = 0.00558896 (* 1 = 0.00558896 loss)
I1107 06:04:15.515276 42927 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I1107 06:04:28.182416 42927 solver.cpp:266] Iteration 9850 (3.94737 iter/s, 12.6667s/50 iter), loss = 0.000555019
I1107 06:04:28.182559 42927 solver.cpp:285]     Train net output #0: loss = 0.000555002 (* 1 = 0.000555002 loss)
I1107 06:04:28.182567 42927 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I1107 06:04:40.812665 42927 solver.cpp:266] Iteration 9900 (3.95895 iter/s, 12.6296s/50 iter), loss = 0.000734462
I1107 06:04:40.812695 42927 solver.cpp:285]     Train net output #0: loss = 0.000734447 (* 1 = 0.000734447 loss)
I1107 06:04:40.812718 42927 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I1107 06:04:53.469319 42927 solver.cpp:266] Iteration 9950 (3.95066 iter/s, 12.6561s/50 iter), loss = 0.0150045
I1107 06:04:53.469349 42927 solver.cpp:285]     Train net output #0: loss = 0.0150045 (* 1 = 0.0150045 loss)
I1107 06:04:53.469372 42927 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I1107 06:05:05.905292 42927 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_10000.caffemodel
I1107 06:05:08.223839 42927 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_10000.solverstate
I1107 06:05:08.708781 42927 solver.cpp:418] Iteration 10000, Testing net (#0)
I1107 06:05:10.172313 42927 solver.cpp:517]     Test net output #0: loss = 0.220006 (* 1 = 0.220006 loss)
I1107 06:05:10.172329 42927 solver.cpp:517]     Test net output #1: top-1 = 0.9525
I1107 06:05:10.412741 42927 solver.cpp:266] Iteration 10000 (2.95112 iter/s, 16.9427s/50 iter), loss = 0.00702628
I1107 06:05:10.412777 42927 solver.cpp:285]     Train net output #0: loss = 0.00702626 (* 1 = 0.00702626 loss)
I1107 06:05:10.412784 42927 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I1107 06:05:22.906777 42927 solver.cpp:266] Iteration 10050 (4.00208 iter/s, 12.4935s/50 iter), loss = 0.0150565
I1107 06:05:22.906807 42927 solver.cpp:285]     Train net output #0: loss = 0.0150565 (* 1 = 0.0150565 loss)
I1107 06:05:22.906813 42927 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I1107 06:05:35.418195 42927 solver.cpp:266] Iteration 10100 (3.99651 iter/s, 12.5109s/50 iter), loss = 0.0013955
I1107 06:05:35.418226 42927 solver.cpp:285]     Train net output #0: loss = 0.00139549 (* 1 = 0.00139549 loss)
I1107 06:05:35.418256 42927 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I1107 06:05:48.008779 42927 solver.cpp:266] Iteration 10150 (3.97139 iter/s, 12.5901s/50 iter), loss = 0.00332746
I1107 06:05:48.008921 42927 solver.cpp:285]     Train net output #0: loss = 0.00332745 (* 1 = 0.00332745 loss)
I1107 06:05:48.008929 42927 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I1107 06:06:00.629539 42927 solver.cpp:266] Iteration 10200 (3.96193 iter/s, 12.6201s/50 iter), loss = 0.00666105
I1107 06:06:00.629568 42927 solver.cpp:285]     Train net output #0: loss = 0.00666104 (* 1 = 0.00666104 loss)
I1107 06:06:00.629575 42927 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I1107 06:06:13.266371 42927 solver.cpp:266] Iteration 10250 (3.95685 iter/s, 12.6363s/50 iter), loss = 0.0048904
I1107 06:06:13.266403 42927 solver.cpp:285]     Train net output #0: loss = 0.0048904 (* 1 = 0.0048904 loss)
I1107 06:06:13.266409 42927 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I1107 06:06:25.894093 42927 solver.cpp:266] Iteration 10300 (3.95971 iter/s, 12.6272s/50 iter), loss = 0.00163384
I1107 06:06:25.894289 42927 solver.cpp:285]     Train net output #0: loss = 0.00163384 (* 1 = 0.00163384 loss)
I1107 06:06:25.894299 42927 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I1107 06:06:38.579211 42927 solver.cpp:266] Iteration 10350 (3.94184 iter/s, 12.6844s/50 iter), loss = 0.0239412
I1107 06:06:38.579241 42927 solver.cpp:285]     Train net output #0: loss = 0.0239412 (* 1 = 0.0239412 loss)
I1107 06:06:38.579246 42927 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I1107 06:06:51.254523 42927 solver.cpp:266] Iteration 10400 (3.94484 iter/s, 12.6748s/50 iter), loss = 0.00389197
I1107 06:06:51.254552 42927 solver.cpp:285]     Train net output #0: loss = 0.00389197 (* 1 = 0.00389197 loss)
I1107 06:06:51.254559 42927 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I1107 06:07:03.846153 42927 solver.cpp:266] Iteration 10450 (3.97106 iter/s, 12.5911s/50 iter), loss = 0.00108243
I1107 06:07:03.846312 42927 solver.cpp:285]     Train net output #0: loss = 0.00108244 (* 1 = 0.00108244 loss)
I1107 06:07:03.846321 42927 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I1107 06:07:16.524811 42927 solver.cpp:266] Iteration 10500 (3.94384 iter/s, 12.678s/50 iter), loss = 0.000855707
I1107 06:07:16.524839 42927 solver.cpp:285]     Train net output #0: loss = 0.000855713 (* 1 = 0.000855713 loss)
I1107 06:07:16.524844 42927 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I1107 06:07:29.131177 42927 solver.cpp:266] Iteration 10550 (3.96641 iter/s, 12.6058s/50 iter), loss = 0.000930401
I1107 06:07:29.131203 42927 solver.cpp:285]     Train net output #0: loss = 0.000930407 (* 1 = 0.000930407 loss)
I1107 06:07:29.131209 42927 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I1107 06:07:41.736132 42927 solver.cpp:266] Iteration 10600 (3.96686 iter/s, 12.6044s/50 iter), loss = 0.00481802
I1107 06:07:41.736294 42927 solver.cpp:285]     Train net output #0: loss = 0.00481803 (* 1 = 0.00481803 loss)
I1107 06:07:41.736301 42927 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I1107 06:07:54.384337 42927 solver.cpp:266] Iteration 10650 (3.95333 iter/s, 12.6476s/50 iter), loss = 0.0038979
I1107 06:07:54.384366 42927 solver.cpp:285]     Train net output #0: loss = 0.0038979 (* 1 = 0.0038979 loss)
I1107 06:07:54.384371 42927 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I1107 06:08:07.040310 42927 solver.cpp:266] Iteration 10700 (3.95087 iter/s, 12.6554s/50 iter), loss = 0.0112258
I1107 06:08:07.040341 42927 solver.cpp:285]     Train net output #0: loss = 0.0112258 (* 1 = 0.0112258 loss)
I1107 06:08:07.040347 42927 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I1107 06:08:19.691993 42927 solver.cpp:266] Iteration 10750 (3.95221 iter/s, 12.6512s/50 iter), loss = 0.00879597
I1107 06:08:19.692173 42927 solver.cpp:285]     Train net output #0: loss = 0.00879597 (* 1 = 0.00879597 loss)
I1107 06:08:19.692181 42927 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I1107 06:08:32.316152 42927 solver.cpp:266] Iteration 10800 (3.96087 iter/s, 12.6235s/50 iter), loss = 0.00604346
I1107 06:08:32.316182 42927 solver.cpp:285]     Train net output #0: loss = 0.00604347 (* 1 = 0.00604347 loss)
I1107 06:08:32.316205 42927 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I1107 06:08:44.956344 42927 solver.cpp:266] Iteration 10850 (3.9558 iter/s, 12.6397s/50 iter), loss = 0.00306402
I1107 06:08:44.956373 42927 solver.cpp:285]     Train net output #0: loss = 0.00306402 (* 1 = 0.00306402 loss)
I1107 06:08:44.956379 42927 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I1107 06:08:57.606988 42927 solver.cpp:266] Iteration 10900 (3.95253 iter/s, 12.6501s/50 iter), loss = 0.00409347
I1107 06:08:57.607143 42927 solver.cpp:285]     Train net output #0: loss = 0.00409347 (* 1 = 0.00409347 loss)
I1107 06:08:57.607151 42927 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I1107 06:09:10.219295 42927 solver.cpp:266] Iteration 10950 (3.96458 iter/s, 12.6117s/50 iter), loss = 0.000841021
I1107 06:09:10.219324 42927 solver.cpp:285]     Train net output #0: loss = 0.000841022 (* 1 = 0.000841022 loss)
I1107 06:09:10.219331 42927 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I1107 06:09:22.619719 42927 solver.cpp:418] Iteration 11000, Testing net (#0)
I1107 06:09:24.116145 42927 solver.cpp:517]     Test net output #0: loss = 0.222074 (* 1 = 0.222074 loss)
I1107 06:09:24.116160 42927 solver.cpp:517]     Test net output #1: top-1 = 0.95225
I1107 06:09:24.364552 42927 solver.cpp:266] Iteration 11000 (3.5349 iter/s, 14.1447s/50 iter), loss = 0.00968981
I1107 06:09:24.364574 42927 solver.cpp:285]     Train net output #0: loss = 0.00968982 (* 1 = 0.00968982 loss)
I1107 06:09:24.364581 42927 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I1107 06:09:37.010035 42927 solver.cpp:266] Iteration 11050 (3.95414 iter/s, 12.645s/50 iter), loss = 0.00419199
I1107 06:09:37.010205 42927 solver.cpp:285]     Train net output #0: loss = 0.00419199 (* 1 = 0.00419199 loss)
I1107 06:09:37.010213 42927 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I1107 06:09:49.650713 42927 solver.cpp:266] Iteration 11100 (3.95569 iter/s, 12.64s/50 iter), loss = 0.00323201
I1107 06:09:49.650743 42927 solver.cpp:285]     Train net output #0: loss = 0.00323202 (* 1 = 0.00323202 loss)
I1107 06:09:49.650749 42927 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I1107 06:10:02.304689 42927 solver.cpp:266] Iteration 11150 (3.95149 iter/s, 12.6535s/50 iter), loss = 0.000421239
I1107 06:10:02.304718 42927 solver.cpp:285]     Train net output #0: loss = 0.000421252 (* 1 = 0.000421252 loss)
I1107 06:10:02.304723 42927 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I1107 06:10:14.896749 42927 solver.cpp:266] Iteration 11200 (3.97092 iter/s, 12.5915s/50 iter), loss = 0.000657353
I1107 06:10:14.896903 42927 solver.cpp:285]     Train net output #0: loss = 0.000657364 (* 1 = 0.000657364 loss)
I1107 06:10:14.896912 42927 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I1107 06:10:27.527901 42927 solver.cpp:266] Iteration 11250 (3.95867 iter/s, 12.6305s/50 iter), loss = 0.00691098
I1107 06:10:27.527930 42927 solver.cpp:285]     Train net output #0: loss = 0.00691099 (* 1 = 0.00691099 loss)
I1107 06:10:27.527936 42927 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I1107 06:10:40.114316 42927 solver.cpp:266] Iteration 11300 (3.9727 iter/s, 12.5859s/50 iter), loss = 0.000981908
I1107 06:10:40.114346 42927 solver.cpp:285]     Train net output #0: loss = 0.000981919 (* 1 = 0.000981919 loss)
I1107 06:10:40.114351 42927 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I1107 06:10:52.763360 42927 solver.cpp:266] Iteration 11350 (3.95303 iter/s, 12.6485s/50 iter), loss = 0.00595267
I1107 06:10:52.763494 42927 solver.cpp:285]     Train net output #0: loss = 0.00595268 (* 1 = 0.00595268 loss)
I1107 06:10:52.763502 42927 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I1107 06:11:05.389101 42927 solver.cpp:266] Iteration 11400 (3.96036 iter/s, 12.6251s/50 iter), loss = 0.00497626
I1107 06:11:05.389129 42927 solver.cpp:285]     Train net output #0: loss = 0.00497627 (* 1 = 0.00497627 loss)
I1107 06:11:05.389137 42927 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I1107 06:11:18.039723 42927 solver.cpp:266] Iteration 11450 (3.95254 iter/s, 12.6501s/50 iter), loss = 0.0128593
I1107 06:11:18.039753 42927 solver.cpp:285]     Train net output #0: loss = 0.0128593 (* 1 = 0.0128593 loss)
I1107 06:11:18.039759 42927 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I1107 06:11:30.671684 42927 solver.cpp:266] Iteration 11500 (3.95838 iter/s, 12.6314s/50 iter), loss = 0.00289993
I1107 06:11:30.671846 42927 solver.cpp:285]     Train net output #0: loss = 0.00289994 (* 1 = 0.00289994 loss)
I1107 06:11:30.671855 42927 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I1107 06:11:43.283046 42927 solver.cpp:266] Iteration 11550 (3.96489 iter/s, 12.6107s/50 iter), loss = 0.000744481
I1107 06:11:43.283076 42927 solver.cpp:285]     Train net output #0: loss = 0.000744493 (* 1 = 0.000744493 loss)
I1107 06:11:43.283083 42927 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I1107 06:11:55.937822 42927 solver.cpp:266] Iteration 11600 (3.95124 iter/s, 12.6543s/50 iter), loss = 0.00923353
I1107 06:11:55.937853 42927 solver.cpp:285]     Train net output #0: loss = 0.00923354 (* 1 = 0.00923354 loss)
I1107 06:11:55.937860 42927 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I1107 06:12:08.552812 42927 solver.cpp:266] Iteration 11650 (3.9637 iter/s, 12.6145s/50 iter), loss = 0.000691894
I1107 06:12:08.553016 42927 solver.cpp:285]     Train net output #0: loss = 0.00069191 (* 1 = 0.00069191 loss)
I1107 06:12:08.553025 42927 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I1107 06:12:21.193769 42927 solver.cpp:266] Iteration 11700 (3.95561 iter/s, 12.6403s/50 iter), loss = 0.000958024
I1107 06:12:21.193799 42927 solver.cpp:285]     Train net output #0: loss = 0.000958044 (* 1 = 0.000958044 loss)
I1107 06:12:21.193820 42927 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I1107 06:12:33.798189 42927 solver.cpp:266] Iteration 11750 (3.96703 iter/s, 12.6039s/50 iter), loss = 0.00100083
I1107 06:12:33.798216 42927 solver.cpp:285]     Train net output #0: loss = 0.00100085 (* 1 = 0.00100085 loss)
I1107 06:12:33.798223 42927 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I1107 06:12:46.483526 42927 solver.cpp:266] Iteration 11800 (3.94172 iter/s, 12.6848s/50 iter), loss = 0.00203461
I1107 06:12:46.483683 42927 solver.cpp:285]     Train net output #0: loss = 0.00203463 (* 1 = 0.00203463 loss)
I1107 06:12:46.483691 42927 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I1107 06:12:59.098193 42927 solver.cpp:266] Iteration 11850 (3.96384 iter/s, 12.614s/50 iter), loss = 0.00570819
I1107 06:12:59.098222 42927 solver.cpp:285]     Train net output #0: loss = 0.00570822 (* 1 = 0.00570822 loss)
I1107 06:12:59.098228 42927 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I1107 06:13:11.763978 42927 solver.cpp:266] Iteration 11900 (3.94781 iter/s, 12.6653s/50 iter), loss = 0.00603842
I1107 06:13:11.764006 42927 solver.cpp:285]     Train net output #0: loss = 0.00603844 (* 1 = 0.00603844 loss)
I1107 06:13:11.764029 42927 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I1107 06:13:24.408252 42927 solver.cpp:266] Iteration 11950 (3.95452 iter/s, 12.6438s/50 iter), loss = 0.0012751
I1107 06:13:24.408406 42927 solver.cpp:285]     Train net output #0: loss = 0.00127512 (* 1 = 0.00127512 loss)
I1107 06:13:24.408414 42927 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I1107 06:13:36.812052 42927 solver.cpp:929] Snapshotting to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_12000.caffemodel
I1107 06:13:39.091909 42927 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.4/snapshots/_iter_12000.solverstate
I1107 06:13:39.647239 42927 solver.cpp:378] Iteration 12000, loss = 0.00116863
I1107 06:13:39.647261 42927 solver.cpp:418] Iteration 12000, Testing net (#0)
I1107 06:13:41.110474 42927 solver.cpp:517]     Test net output #0: loss = 0.222874 (* 1 = 0.222874 loss)
I1107 06:13:41.110489 42927 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I1107 06:13:41.110493 42927 solver.cpp:386] Optimization Done (3.93764 iter/s).
I1107 06:13:41.110496 42927 caffe_interface.cpp:530] Optimization Done.
I1107 06:13:42.078378 44065 pruning_runner.cpp:190] Sens info found, use it.
I1107 06:13:43.319689 44065 pruning_runner.cpp:217] Start compressing, please wait...
I1107 06:13:49.313045 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:13:55.324182 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:01.303232 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:07.540269 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:13.588899 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:19.612398 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:25.891800 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:32.006938 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:38.220814 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:44.516598 44065 pruning_runner.cpp:264] Compression complete 0%
I1107 06:14:50.858959 44065 pruning_runner.cpp:264] Compression complete 50%
I1107 06:14:57.086789 44065 pruning_runner.cpp:264] Compression complete 66.6667%
I1107 06:15:03.476184 44065 pruning_runner.cpp:264] Compression complete 96.9697%
I1107 06:15:09.533551 44065 pruning_runner.cpp:264] Compression complete 99.2424%
I1107 06:15:15.523736 44065 pruning_runner.cpp:264] Compression complete 99.9047%
I1107 06:15:21.807132 44065 pruning_runner.cpp:264] Compression complete 99.9523%
I1107 06:15:27.864929 44065 pruning_runner.cpp:264] Compression complete 99.9762%
I1107 06:15:33.922063 44065 pruning_runner.cpp:264] Compression complete 99.9881%
I1107 06:15:39.970479 44065 pruning_runner.cpp:264] Compression complete 99.9998%
I1107 06:15:46.034966 44065 pruning_runner.cpp:264] Compression complete 99.9999%
I1107 06:15:52.168298 44065 pruning_runner.cpp:264] Compression complete 100%
I1107 06:15:58.319573 44065 caffe_interface.cpp:66] Use GPU with device ID 0
I1107 06:15:58.319888 44065 caffe_interface.cpp:70] GPU device name: Quadro P6000
I1107 06:15:58.320226 44065 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 06:15:58.320394 44065 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 06:15:58.320494 44065 layer_factory.hpp:77] Creating layer data
I1107 06:15:58.320531 44065 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:15:58.320909 44065 net.cpp:94] Creating Layer data
I1107 06:15:58.320917 44065 net.cpp:409] data -> data
I1107 06:15:58.320926 44065 net.cpp:409] data -> label
I1107 06:15:58.321995 46106 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 06:15:58.322023 46106 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 06:15:58.322245 44065 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 06:15:58.322346 44065 data_layer.cpp:83] output data size: 50,3,227,227
I1107 06:15:58.398654 44065 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:15:58.398710 44065 net.cpp:144] Setting up data
I1107 06:15:58.398717 44065 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 06:15:58.398720 44065 net.cpp:151] Top shape: 50 (50)
I1107 06:15:58.398723 44065 net.cpp:159] Memory required for data: 30917600
I1107 06:15:58.398727 44065 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 06:15:58.398737 44065 net.cpp:94] Creating Layer label_data_1_split
I1107 06:15:58.398741 44065 net.cpp:435] label_data_1_split <- label
I1107 06:15:58.398762 44065 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 06:15:58.398773 44065 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 06:15:58.398875 44065 net.cpp:144] Setting up label_data_1_split
I1107 06:15:58.398880 44065 net.cpp:151] Top shape: 50 (50)
I1107 06:15:58.398885 44065 net.cpp:151] Top shape: 50 (50)
I1107 06:15:58.398886 44065 net.cpp:159] Memory required for data: 30918000
I1107 06:15:58.398903 44065 layer_factory.hpp:77] Creating layer conv1
I1107 06:15:58.398914 44065 net.cpp:94] Creating Layer conv1
I1107 06:15:58.398918 44065 net.cpp:435] conv1 <- data
I1107 06:15:58.398923 44065 net.cpp:409] conv1 -> conv1
I1107 06:15:58.400611 44065 net.cpp:144] Setting up conv1
I1107 06:15:58.400624 44065 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:15:58.400626 44065 net.cpp:159] Memory required for data: 88998000
I1107 06:15:58.400651 44065 layer_factory.hpp:77] Creating layer bn1
I1107 06:15:58.400660 44065 net.cpp:94] Creating Layer bn1
I1107 06:15:58.400662 44065 net.cpp:435] bn1 <- conv1
I1107 06:15:58.400668 44065 net.cpp:409] bn1 -> scale1
I1107 06:15:58.401371 44065 net.cpp:144] Setting up bn1
I1107 06:15:58.401378 44065 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:15:58.401381 44065 net.cpp:159] Memory required for data: 147078000
I1107 06:15:58.401391 44065 layer_factory.hpp:77] Creating layer relu1
I1107 06:15:58.401397 44065 net.cpp:94] Creating Layer relu1
I1107 06:15:58.401401 44065 net.cpp:435] relu1 <- scale1
I1107 06:15:58.401405 44065 net.cpp:409] relu1 -> relu1
I1107 06:15:58.401427 44065 net.cpp:144] Setting up relu1
I1107 06:15:58.401432 44065 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:15:58.401434 44065 net.cpp:159] Memory required for data: 205158000
I1107 06:15:58.401437 44065 layer_factory.hpp:77] Creating layer pool1
I1107 06:15:58.401443 44065 net.cpp:94] Creating Layer pool1
I1107 06:15:58.401444 44065 net.cpp:435] pool1 <- relu1
I1107 06:15:58.401449 44065 net.cpp:409] pool1 -> pool1
I1107 06:15:58.401504 44065 net.cpp:144] Setting up pool1
I1107 06:15:58.401510 44065 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 06:15:58.401511 44065 net.cpp:159] Memory required for data: 219154800
I1107 06:15:58.401513 44065 layer_factory.hpp:77] Creating layer conv2
I1107 06:15:58.401520 44065 net.cpp:94] Creating Layer conv2
I1107 06:15:58.401523 44065 net.cpp:435] conv2 <- pool1
I1107 06:15:58.401528 44065 net.cpp:409] conv2 -> conv2
I1107 06:15:58.408393 44065 net.cpp:144] Setting up conv2
I1107 06:15:58.408411 44065 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:15:58.408413 44065 net.cpp:159] Memory required for data: 256479600
I1107 06:15:58.408424 44065 layer_factory.hpp:77] Creating layer bn2
I1107 06:15:58.408433 44065 net.cpp:94] Creating Layer bn2
I1107 06:15:58.408437 44065 net.cpp:435] bn2 <- conv2
I1107 06:15:58.408443 44065 net.cpp:409] bn2 -> scale2
I1107 06:15:58.411481 44065 net.cpp:144] Setting up bn2
I1107 06:15:58.411489 44065 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:15:58.411494 44065 net.cpp:159] Memory required for data: 293804400
I1107 06:15:58.411502 44065 layer_factory.hpp:77] Creating layer relu2
I1107 06:15:58.411509 44065 net.cpp:94] Creating Layer relu2
I1107 06:15:58.411512 44065 net.cpp:435] relu2 <- scale2
I1107 06:15:58.411517 44065 net.cpp:409] relu2 -> relu2
I1107 06:15:58.411538 44065 net.cpp:144] Setting up relu2
I1107 06:15:58.411543 44065 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:15:58.411546 44065 net.cpp:159] Memory required for data: 331129200
I1107 06:15:58.411550 44065 layer_factory.hpp:77] Creating layer pool2
I1107 06:15:58.411556 44065 net.cpp:94] Creating Layer pool2
I1107 06:15:58.411558 44065 net.cpp:435] pool2 <- relu2
I1107 06:15:58.411562 44065 net.cpp:409] pool2 -> pool2
I1107 06:15:58.413933 44065 net.cpp:144] Setting up pool2
I1107 06:15:58.413938 44065 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:15:58.413940 44065 net.cpp:159] Memory required for data: 339782000
I1107 06:15:58.413942 44065 layer_factory.hpp:77] Creating layer conv3
I1107 06:15:58.413949 44065 net.cpp:94] Creating Layer conv3
I1107 06:15:58.413951 44065 net.cpp:435] conv3 <- pool2
I1107 06:15:58.413955 44065 net.cpp:409] conv3 -> conv3
I1107 06:15:58.424891 44065 net.cpp:144] Setting up conv3
I1107 06:15:58.424917 44065 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:15:58.424921 44065 net.cpp:159] Memory required for data: 352761200
I1107 06:15:58.424964 44065 layer_factory.hpp:77] Creating layer relu3
I1107 06:15:58.424983 44065 net.cpp:94] Creating Layer relu3
I1107 06:15:58.424998 44065 net.cpp:435] relu3 <- conv3
I1107 06:15:58.425015 44065 net.cpp:409] relu3 -> relu3
I1107 06:15:58.425076 44065 net.cpp:144] Setting up relu3
I1107 06:15:58.425091 44065 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:15:58.425103 44065 net.cpp:159] Memory required for data: 365740400
I1107 06:15:58.425117 44065 layer_factory.hpp:77] Creating layer conv4
I1107 06:15:58.425138 44065 net.cpp:94] Creating Layer conv4
I1107 06:15:58.425151 44065 net.cpp:435] conv4 <- relu3
I1107 06:15:58.425168 44065 net.cpp:409] conv4 -> conv4
I1107 06:15:58.439177 44065 net.cpp:144] Setting up conv4
I1107 06:15:58.439200 44065 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:15:58.439203 44065 net.cpp:159] Memory required for data: 378719600
I1107 06:15:58.439213 44065 layer_factory.hpp:77] Creating layer relu4
I1107 06:15:58.439221 44065 net.cpp:94] Creating Layer relu4
I1107 06:15:58.439224 44065 net.cpp:435] relu4 <- conv4
I1107 06:15:58.439230 44065 net.cpp:409] relu4 -> relu4
I1107 06:15:58.439252 44065 net.cpp:144] Setting up relu4
I1107 06:15:58.439257 44065 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:15:58.439260 44065 net.cpp:159] Memory required for data: 391698800
I1107 06:15:58.439265 44065 layer_factory.hpp:77] Creating layer conv5
I1107 06:15:58.439275 44065 net.cpp:94] Creating Layer conv5
I1107 06:15:58.439282 44065 net.cpp:435] conv5 <- relu4
I1107 06:15:58.439290 44065 net.cpp:409] conv5 -> conv5
I1107 06:15:58.452426 44065 net.cpp:144] Setting up conv5
I1107 06:15:58.452469 44065 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:15:58.452474 44065 net.cpp:159] Memory required for data: 400351600
I1107 06:15:58.452484 44065 layer_factory.hpp:77] Creating layer relu5
I1107 06:15:58.452495 44065 net.cpp:94] Creating Layer relu5
I1107 06:15:58.452502 44065 net.cpp:435] relu5 <- conv5
I1107 06:15:58.452515 44065 net.cpp:409] relu5 -> relu5
I1107 06:15:58.452589 44065 net.cpp:144] Setting up relu5
I1107 06:15:58.452620 44065 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:15:58.452637 44065 net.cpp:159] Memory required for data: 409004400
I1107 06:15:58.452651 44065 layer_factory.hpp:77] Creating layer pool5
I1107 06:15:58.452675 44065 net.cpp:94] Creating Layer pool5
I1107 06:15:58.452695 44065 net.cpp:435] pool5 <- relu5
I1107 06:15:58.452715 44065 net.cpp:409] pool5 -> pool5
I1107 06:15:58.452785 44065 net.cpp:144] Setting up pool5
I1107 06:15:58.452801 44065 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 06:15:58.452816 44065 net.cpp:159] Memory required for data: 410847600
I1107 06:15:58.452829 44065 layer_factory.hpp:77] Creating layer fc6
I1107 06:15:58.452849 44065 net.cpp:94] Creating Layer fc6
I1107 06:15:58.452864 44065 net.cpp:435] fc6 <- pool5
I1107 06:15:58.452883 44065 net.cpp:409] fc6 -> fc6
I1107 06:15:58.771507 44065 net.cpp:144] Setting up fc6
I1107 06:15:58.771529 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.771531 44065 net.cpp:159] Memory required for data: 411666800
I1107 06:15:58.771554 44065 layer_factory.hpp:77] Creating layer relu6
I1107 06:15:58.771562 44065 net.cpp:94] Creating Layer relu6
I1107 06:15:58.771566 44065 net.cpp:435] relu6 <- fc6
I1107 06:15:58.771572 44065 net.cpp:409] relu6 -> relu6
I1107 06:15:58.771605 44065 net.cpp:144] Setting up relu6
I1107 06:15:58.771608 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.771612 44065 net.cpp:159] Memory required for data: 412486000
I1107 06:15:58.771615 44065 layer_factory.hpp:77] Creating layer drop6
I1107 06:15:58.771620 44065 net.cpp:94] Creating Layer drop6
I1107 06:15:58.771621 44065 net.cpp:435] drop6 <- relu6
I1107 06:15:58.771626 44065 net.cpp:409] drop6 -> drop6
I1107 06:15:58.771649 44065 net.cpp:144] Setting up drop6
I1107 06:15:58.771653 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.771656 44065 net.cpp:159] Memory required for data: 413305200
I1107 06:15:58.771675 44065 layer_factory.hpp:77] Creating layer fc7
I1107 06:15:58.771701 44065 net.cpp:94] Creating Layer fc7
I1107 06:15:58.771703 44065 net.cpp:435] fc7 <- drop6
I1107 06:15:58.771708 44065 net.cpp:409] fc7 -> fc7
I1107 06:15:58.904633 44065 net.cpp:144] Setting up fc7
I1107 06:15:58.904659 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.904660 44065 net.cpp:159] Memory required for data: 414124400
I1107 06:15:58.904667 44065 layer_factory.hpp:77] Creating layer bn7
I1107 06:15:58.904676 44065 net.cpp:94] Creating Layer bn7
I1107 06:15:58.904680 44065 net.cpp:435] bn7 <- fc7
I1107 06:15:58.904685 44065 net.cpp:409] bn7 -> scale7
I1107 06:15:58.905175 44065 net.cpp:144] Setting up bn7
I1107 06:15:58.905181 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.905184 44065 net.cpp:159] Memory required for data: 414943600
I1107 06:15:58.905190 44065 layer_factory.hpp:77] Creating layer relu7
I1107 06:15:58.905194 44065 net.cpp:94] Creating Layer relu7
I1107 06:15:58.905197 44065 net.cpp:435] relu7 <- scale7
I1107 06:15:58.905201 44065 net.cpp:409] relu7 -> relu7
I1107 06:15:58.905217 44065 net.cpp:144] Setting up relu7
I1107 06:15:58.905222 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.905225 44065 net.cpp:159] Memory required for data: 415762800
I1107 06:15:58.905226 44065 layer_factory.hpp:77] Creating layer drop7
I1107 06:15:58.905231 44065 net.cpp:94] Creating Layer drop7
I1107 06:15:58.905233 44065 net.cpp:435] drop7 <- relu7
I1107 06:15:58.905253 44065 net.cpp:409] drop7 -> drop7
I1107 06:15:58.905277 44065 net.cpp:144] Setting up drop7
I1107 06:15:58.905282 44065 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:15:58.905284 44065 net.cpp:159] Memory required for data: 416582000
I1107 06:15:58.905287 44065 layer_factory.hpp:77] Creating layer fc8
I1107 06:15:58.905293 44065 net.cpp:94] Creating Layer fc8
I1107 06:15:58.905295 44065 net.cpp:435] fc8 <- drop7
I1107 06:15:58.905299 44065 net.cpp:409] fc8 -> fc8
I1107 06:15:58.906159 44065 net.cpp:144] Setting up fc8
I1107 06:15:58.906172 44065 net.cpp:151] Top shape: 50 2 (100)
I1107 06:15:58.906173 44065 net.cpp:159] Memory required for data: 416582400
I1107 06:15:58.906179 44065 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 06:15:58.906184 44065 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 06:15:58.906188 44065 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 06:15:58.906193 44065 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 06:15:58.906199 44065 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 06:15:58.906225 44065 net.cpp:144] Setting up fc8_fc8_0_split
I1107 06:15:58.906229 44065 net.cpp:151] Top shape: 50 2 (100)
I1107 06:15:58.906232 44065 net.cpp:151] Top shape: 50 2 (100)
I1107 06:15:58.906241 44065 net.cpp:159] Memory required for data: 416583200
I1107 06:15:58.906244 44065 layer_factory.hpp:77] Creating layer loss
I1107 06:15:58.906249 44065 net.cpp:94] Creating Layer loss
I1107 06:15:58.906250 44065 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 06:15:58.906255 44065 net.cpp:435] loss <- label_data_1_split_0
I1107 06:15:58.906260 44065 net.cpp:409] loss -> loss
I1107 06:15:58.906266 44065 layer_factory.hpp:77] Creating layer loss
I1107 06:15:58.906332 44065 net.cpp:144] Setting up loss
I1107 06:15:58.906337 44065 net.cpp:151] Top shape: (1)
I1107 06:15:58.906338 44065 net.cpp:154]     with loss weight 1
I1107 06:15:58.906349 44065 net.cpp:159] Memory required for data: 416583204
I1107 06:15:58.906352 44065 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 06:15:58.906358 44065 net.cpp:94] Creating Layer accuracy-top1
I1107 06:15:58.906360 44065 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 06:15:58.906363 44065 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 06:15:58.906368 44065 net.cpp:409] accuracy-top1 -> top-1
I1107 06:15:58.906373 44065 net.cpp:144] Setting up accuracy-top1
I1107 06:15:58.906378 44065 net.cpp:151] Top shape: (1)
I1107 06:15:58.906378 44065 net.cpp:159] Memory required for data: 416583208
I1107 06:15:58.906381 44065 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 06:15:58.906402 44065 net.cpp:220] loss needs backward computation.
I1107 06:15:58.906406 44065 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 06:15:58.906409 44065 net.cpp:220] fc8 needs backward computation.
I1107 06:15:58.906411 44065 net.cpp:220] drop7 needs backward computation.
I1107 06:15:58.906414 44065 net.cpp:220] relu7 needs backward computation.
I1107 06:15:58.906417 44065 net.cpp:220] bn7 needs backward computation.
I1107 06:15:58.906419 44065 net.cpp:220] fc7 needs backward computation.
I1107 06:15:58.906422 44065 net.cpp:220] drop6 needs backward computation.
I1107 06:15:58.906424 44065 net.cpp:220] relu6 needs backward computation.
I1107 06:15:58.906428 44065 net.cpp:220] fc6 needs backward computation.
I1107 06:15:58.906430 44065 net.cpp:220] pool5 needs backward computation.
I1107 06:15:58.906433 44065 net.cpp:220] relu5 needs backward computation.
I1107 06:15:58.906435 44065 net.cpp:220] conv5 needs backward computation.
I1107 06:15:58.906440 44065 net.cpp:220] relu4 needs backward computation.
I1107 06:15:58.906441 44065 net.cpp:220] conv4 needs backward computation.
I1107 06:15:58.906445 44065 net.cpp:220] relu3 needs backward computation.
I1107 06:15:58.906446 44065 net.cpp:220] conv3 needs backward computation.
I1107 06:15:58.906450 44065 net.cpp:220] pool2 needs backward computation.
I1107 06:15:58.906452 44065 net.cpp:220] relu2 needs backward computation.
I1107 06:15:58.906455 44065 net.cpp:220] bn2 needs backward computation.
I1107 06:15:58.906457 44065 net.cpp:220] conv2 needs backward computation.
I1107 06:15:58.906461 44065 net.cpp:220] pool1 needs backward computation.
I1107 06:15:58.906462 44065 net.cpp:220] relu1 needs backward computation.
I1107 06:15:58.906466 44065 net.cpp:220] bn1 needs backward computation.
I1107 06:15:58.906467 44065 net.cpp:220] conv1 needs backward computation.
I1107 06:15:58.906471 44065 net.cpp:222] label_data_1_split does not need backward computation.
I1107 06:15:58.906476 44065 net.cpp:222] data does not need backward computation.
I1107 06:15:58.906478 44065 net.cpp:264] This network produces output loss
I1107 06:15:58.906481 44065 net.cpp:264] This network produces output top-1
I1107 06:15:58.906500 44065 net.cpp:284] Network initialization done.
I1107 06:15:58.978425 44065 caffe_interface.cpp:363] Running for 80 iterations.
I1107 06:15:59.022240 44065 caffe_interface.cpp:125] Batch 0, loss = 0.175796
I1107 06:15:59.022260 44065 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I1107 06:15:59.042636 44065 caffe_interface.cpp:125] Batch 1, loss = 0.318053
I1107 06:15:59.042655 44065 caffe_interface.cpp:125] Batch 1, top-1 = 0.92
I1107 06:15:59.062113 44065 caffe_interface.cpp:125] Batch 2, loss = 0.377503
I1107 06:15:59.062132 44065 caffe_interface.cpp:125] Batch 2, top-1 = 0.92
I1107 06:15:59.082396 44065 caffe_interface.cpp:125] Batch 3, loss = 0.573879
I1107 06:15:59.082417 44065 caffe_interface.cpp:125] Batch 3, top-1 = 0.9
I1107 06:15:59.101809 44065 caffe_interface.cpp:125] Batch 4, loss = 0.234008
I1107 06:15:59.101828 44065 caffe_interface.cpp:125] Batch 4, top-1 = 0.96
I1107 06:15:59.122148 44065 caffe_interface.cpp:125] Batch 5, loss = 0.000301776
I1107 06:15:59.122165 44065 caffe_interface.cpp:125] Batch 5, top-1 = 1
I1107 06:15:59.141290 44065 caffe_interface.cpp:125] Batch 6, loss = 0.298408
I1107 06:15:59.141309 44065 caffe_interface.cpp:125] Batch 6, top-1 = 0.94
I1107 06:15:59.161664 44065 caffe_interface.cpp:125] Batch 7, loss = 0.190715
I1107 06:15:59.161681 44065 caffe_interface.cpp:125] Batch 7, top-1 = 0.94
I1107 06:15:59.181987 44065 caffe_interface.cpp:125] Batch 8, loss = 0.0417552
I1107 06:15:59.182004 44065 caffe_interface.cpp:125] Batch 8, top-1 = 0.98
I1107 06:15:59.201421 44065 caffe_interface.cpp:125] Batch 9, loss = 0.254291
I1107 06:15:59.201438 44065 caffe_interface.cpp:125] Batch 9, top-1 = 0.96
I1107 06:15:59.220320 44065 caffe_interface.cpp:125] Batch 10, loss = 0.139136
I1107 06:15:59.220336 44065 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I1107 06:15:59.238253 44065 caffe_interface.cpp:125] Batch 11, loss = 0.178368
I1107 06:15:59.238287 44065 caffe_interface.cpp:125] Batch 11, top-1 = 0.98
I1107 06:15:59.256256 44065 caffe_interface.cpp:125] Batch 12, loss = 0.408257
I1107 06:15:59.256275 44065 caffe_interface.cpp:125] Batch 12, top-1 = 0.92
I1107 06:15:59.274885 44065 caffe_interface.cpp:125] Batch 13, loss = 0.182658
I1107 06:15:59.274905 44065 caffe_interface.cpp:125] Batch 13, top-1 = 0.94
I1107 06:15:59.294343 44065 caffe_interface.cpp:125] Batch 14, loss = 0.306457
I1107 06:15:59.294353 44065 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I1107 06:15:59.312029 44065 caffe_interface.cpp:125] Batch 15, loss = 0.373318
I1107 06:15:59.312037 44065 caffe_interface.cpp:125] Batch 15, top-1 = 0.88
I1107 06:15:59.331037 44065 caffe_interface.cpp:125] Batch 16, loss = 0.158739
I1107 06:15:59.331046 44065 caffe_interface.cpp:125] Batch 16, top-1 = 0.94
I1107 06:15:59.349292 44065 caffe_interface.cpp:125] Batch 17, loss = 0.00726255
I1107 06:15:59.349300 44065 caffe_interface.cpp:125] Batch 17, top-1 = 1
I1107 06:15:59.368145 44065 caffe_interface.cpp:125] Batch 18, loss = 0.293982
I1107 06:15:59.368152 44065 caffe_interface.cpp:125] Batch 18, top-1 = 0.96
I1107 06:15:59.386624 44065 caffe_interface.cpp:125] Batch 19, loss = 0.280664
I1107 06:15:59.386631 44065 caffe_interface.cpp:125] Batch 19, top-1 = 0.94
I1107 06:15:59.405812 44065 caffe_interface.cpp:125] Batch 20, loss = 0.0728294
I1107 06:15:59.405820 44065 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I1107 06:15:59.423822 44065 caffe_interface.cpp:125] Batch 21, loss = 0.0717798
I1107 06:15:59.423830 44065 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I1107 06:15:59.443315 44065 caffe_interface.cpp:125] Batch 22, loss = 0.198068
I1107 06:15:59.443322 44065 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I1107 06:15:59.462040 44065 caffe_interface.cpp:125] Batch 23, loss = 0.191492
I1107 06:15:59.462049 44065 caffe_interface.cpp:125] Batch 23, top-1 = 0.96
I1107 06:15:59.480523 44065 caffe_interface.cpp:125] Batch 24, loss = 0.336875
I1107 06:15:59.480531 44065 caffe_interface.cpp:125] Batch 24, top-1 = 0.94
I1107 06:15:59.499855 44065 caffe_interface.cpp:125] Batch 25, loss = 0.149484
I1107 06:15:59.499862 44065 caffe_interface.cpp:125] Batch 25, top-1 = 0.98
I1107 06:15:59.517849 44065 caffe_interface.cpp:125] Batch 26, loss = 0.129195
I1107 06:15:59.517856 44065 caffe_interface.cpp:125] Batch 26, top-1 = 0.98
I1107 06:15:59.536675 44065 caffe_interface.cpp:125] Batch 27, loss = 0.184868
I1107 06:15:59.536685 44065 caffe_interface.cpp:125] Batch 27, top-1 = 0.96
I1107 06:15:59.555141 44065 caffe_interface.cpp:125] Batch 28, loss = 0.291825
I1107 06:15:59.555152 44065 caffe_interface.cpp:125] Batch 28, top-1 = 0.94
I1107 06:15:59.573040 44065 caffe_interface.cpp:125] Batch 29, loss = 0.00818854
I1107 06:15:59.573050 44065 caffe_interface.cpp:125] Batch 29, top-1 = 1
I1107 06:15:59.591017 44065 caffe_interface.cpp:125] Batch 30, loss = 0.175277
I1107 06:15:59.591024 44065 caffe_interface.cpp:125] Batch 30, top-1 = 0.98
I1107 06:15:59.609659 44065 caffe_interface.cpp:125] Batch 31, loss = 0.00104797
I1107 06:15:59.609668 44065 caffe_interface.cpp:125] Batch 31, top-1 = 1
I1107 06:15:59.627490 44065 caffe_interface.cpp:125] Batch 32, loss = 0.0864403
I1107 06:15:59.627496 44065 caffe_interface.cpp:125] Batch 32, top-1 = 0.98
I1107 06:15:59.646389 44065 caffe_interface.cpp:125] Batch 33, loss = 0.137238
I1107 06:15:59.646395 44065 caffe_interface.cpp:125] Batch 33, top-1 = 0.96
I1107 06:15:59.664551 44065 caffe_interface.cpp:125] Batch 34, loss = 0.24999
I1107 06:15:59.664557 44065 caffe_interface.cpp:125] Batch 34, top-1 = 0.94
I1107 06:15:59.685004 44065 caffe_interface.cpp:125] Batch 35, loss = 0.0104361
I1107 06:15:59.685012 44065 caffe_interface.cpp:125] Batch 35, top-1 = 1
I1107 06:15:59.703634 44065 caffe_interface.cpp:125] Batch 36, loss = 0.37576
I1107 06:15:59.703642 44065 caffe_interface.cpp:125] Batch 36, top-1 = 0.92
I1107 06:15:59.721488 44065 caffe_interface.cpp:125] Batch 37, loss = 0.150116
I1107 06:15:59.721495 44065 caffe_interface.cpp:125] Batch 37, top-1 = 0.98
I1107 06:15:59.740592 44065 caffe_interface.cpp:125] Batch 38, loss = 0.474582
I1107 06:15:59.740600 44065 caffe_interface.cpp:125] Batch 38, top-1 = 0.9
I1107 06:15:59.758424 44065 caffe_interface.cpp:125] Batch 39, loss = 0.141679
I1107 06:15:59.758431 44065 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I1107 06:15:59.777139 44065 caffe_interface.cpp:125] Batch 40, loss = 0.212902
I1107 06:15:59.777146 44065 caffe_interface.cpp:125] Batch 40, top-1 = 0.94
I1107 06:15:59.795348 44065 caffe_interface.cpp:125] Batch 41, loss = 0.00425197
I1107 06:15:59.795357 44065 caffe_interface.cpp:125] Batch 41, top-1 = 1
I1107 06:15:59.813284 44065 caffe_interface.cpp:125] Batch 42, loss = 0.0850423
I1107 06:15:59.813293 44065 caffe_interface.cpp:125] Batch 42, top-1 = 0.98
I1107 06:15:59.831528 44065 caffe_interface.cpp:125] Batch 43, loss = 0.14255
I1107 06:15:59.831538 44065 caffe_interface.cpp:125] Batch 43, top-1 = 0.96
I1107 06:15:59.850564 44065 caffe_interface.cpp:125] Batch 44, loss = 0.478125
I1107 06:15:59.850570 44065 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I1107 06:15:59.868549 44065 caffe_interface.cpp:125] Batch 45, loss = 0.549294
I1107 06:15:59.868556 44065 caffe_interface.cpp:125] Batch 45, top-1 = 0.92
I1107 06:15:59.887439 44065 caffe_interface.cpp:125] Batch 46, loss = 0.0175137
I1107 06:15:59.887446 44065 caffe_interface.cpp:125] Batch 46, top-1 = 1
I1107 06:15:59.905441 44065 caffe_interface.cpp:125] Batch 47, loss = 0.434726
I1107 06:15:59.905449 44065 caffe_interface.cpp:125] Batch 47, top-1 = 0.88
I1107 06:15:59.924475 44065 caffe_interface.cpp:125] Batch 48, loss = 0.132144
I1107 06:15:59.924482 44065 caffe_interface.cpp:125] Batch 48, top-1 = 0.96
I1107 06:15:59.942337 44065 caffe_interface.cpp:125] Batch 49, loss = 0.337532
I1107 06:15:59.942345 44065 caffe_interface.cpp:125] Batch 49, top-1 = 0.94
I1107 06:15:59.961261 44065 caffe_interface.cpp:125] Batch 50, loss = 0.392693
I1107 06:15:59.961271 44065 caffe_interface.cpp:125] Batch 50, top-1 = 0.9
I1107 06:15:59.980219 44065 caffe_interface.cpp:125] Batch 51, loss = 0.631142
I1107 06:15:59.980226 44065 caffe_interface.cpp:125] Batch 51, top-1 = 0.84
I1107 06:15:59.998193 44065 caffe_interface.cpp:125] Batch 52, loss = 0.00294803
I1107 06:15:59.998199 44065 caffe_interface.cpp:125] Batch 52, top-1 = 1
I1107 06:16:00.017060 44065 caffe_interface.cpp:125] Batch 53, loss = 0.17554
I1107 06:16:00.017066 44065 caffe_interface.cpp:125] Batch 53, top-1 = 0.96
I1107 06:16:00.034901 44065 caffe_interface.cpp:125] Batch 54, loss = 0.000378029
I1107 06:16:00.034909 44065 caffe_interface.cpp:125] Batch 54, top-1 = 1
I1107 06:16:00.053704 44065 caffe_interface.cpp:125] Batch 55, loss = 0.457735
I1107 06:16:00.053710 44065 caffe_interface.cpp:125] Batch 55, top-1 = 0.88
I1107 06:16:00.071800 44065 caffe_interface.cpp:125] Batch 56, loss = 0.150508
I1107 06:16:00.071807 44065 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I1107 06:16:00.090772 44065 caffe_interface.cpp:125] Batch 57, loss = 0.0181097
I1107 06:16:00.090782 44065 caffe_interface.cpp:125] Batch 57, top-1 = 1
I1107 06:16:00.109745 44065 caffe_interface.cpp:125] Batch 58, loss = 0.23395
I1107 06:16:00.109753 44065 caffe_interface.cpp:125] Batch 58, top-1 = 0.96
I1107 06:16:00.129112 44065 caffe_interface.cpp:125] Batch 59, loss = 0.0826177
I1107 06:16:00.129120 44065 caffe_interface.cpp:125] Batch 59, top-1 = 0.94
I1107 06:16:00.147395 44065 caffe_interface.cpp:125] Batch 60, loss = 0.194262
I1107 06:16:00.147405 44065 caffe_interface.cpp:125] Batch 60, top-1 = 0.96
I1107 06:16:00.167127 44065 caffe_interface.cpp:125] Batch 61, loss = 0.246267
I1107 06:16:00.167135 44065 caffe_interface.cpp:125] Batch 61, top-1 = 0.96
I1107 06:16:00.185130 44065 caffe_interface.cpp:125] Batch 62, loss = 0.070524
I1107 06:16:00.185137 44065 caffe_interface.cpp:125] Batch 62, top-1 = 0.98
I1107 06:16:00.204138 44065 caffe_interface.cpp:125] Batch 63, loss = 0.477687
I1107 06:16:00.204146 44065 caffe_interface.cpp:125] Batch 63, top-1 = 0.92
I1107 06:16:00.222390 44065 caffe_interface.cpp:125] Batch 64, loss = 0.376801
I1107 06:16:00.222415 44065 caffe_interface.cpp:125] Batch 64, top-1 = 0.92
I1107 06:16:00.241883 44065 caffe_interface.cpp:125] Batch 65, loss = 0.267819
I1107 06:16:00.241889 44065 caffe_interface.cpp:125] Batch 65, top-1 = 0.94
I1107 06:16:00.260668 44065 caffe_interface.cpp:125] Batch 66, loss = 0.48302
I1107 06:16:00.260673 44065 caffe_interface.cpp:125] Batch 66, top-1 = 0.94
I1107 06:16:00.278491 44065 caffe_interface.cpp:125] Batch 67, loss = 0.3189
I1107 06:16:00.278498 44065 caffe_interface.cpp:125] Batch 67, top-1 = 0.92
I1107 06:16:00.297569 44065 caffe_interface.cpp:125] Batch 68, loss = 0.354762
I1107 06:16:00.297576 44065 caffe_interface.cpp:125] Batch 68, top-1 = 0.94
I1107 06:16:00.315791 44065 caffe_interface.cpp:125] Batch 69, loss = 0.372791
I1107 06:16:00.315799 44065 caffe_interface.cpp:125] Batch 69, top-1 = 0.9
I1107 06:16:00.334522 44065 caffe_interface.cpp:125] Batch 70, loss = 0.0233242
I1107 06:16:00.334529 44065 caffe_interface.cpp:125] Batch 70, top-1 = 0.98
I1107 06:16:00.352613 44065 caffe_interface.cpp:125] Batch 71, loss = 0.334519
I1107 06:16:00.352620 44065 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I1107 06:16:00.370542 44065 caffe_interface.cpp:125] Batch 72, loss = 0.242187
I1107 06:16:00.370550 44065 caffe_interface.cpp:125] Batch 72, top-1 = 0.94
I1107 06:16:00.388783 44065 caffe_interface.cpp:125] Batch 73, loss = 0.071936
I1107 06:16:00.388792 44065 caffe_interface.cpp:125] Batch 73, top-1 = 0.98
I1107 06:16:00.407837 44065 caffe_interface.cpp:125] Batch 74, loss = 0.308291
I1107 06:16:00.407845 44065 caffe_interface.cpp:125] Batch 74, top-1 = 0.94
I1107 06:16:00.425803 44065 caffe_interface.cpp:125] Batch 75, loss = 0.0346209
I1107 06:16:00.425812 44065 caffe_interface.cpp:125] Batch 75, top-1 = 0.98
I1107 06:16:00.444707 44065 caffe_interface.cpp:125] Batch 76, loss = 0.302854
I1107 06:16:00.444715 44065 caffe_interface.cpp:125] Batch 76, top-1 = 0.96
I1107 06:16:00.462940 44065 caffe_interface.cpp:125] Batch 77, loss = 0.417949
I1107 06:16:00.462949 44065 caffe_interface.cpp:125] Batch 77, top-1 = 0.94
I1107 06:16:00.482899 44065 caffe_interface.cpp:125] Batch 78, loss = 0.0164078
I1107 06:16:00.482908 44065 caffe_interface.cpp:125] Batch 78, top-1 = 1
I1107 06:16:00.501106 44065 caffe_interface.cpp:125] Batch 79, loss = 0.144607
I1107 06:16:00.501112 44065 caffe_interface.cpp:125] Batch 79, top-1 = 0.96
I1107 06:16:00.501116 44065 caffe_interface.cpp:130] Loss: 0.222874
I1107 06:16:00.501121 44065 caffe_interface.cpp:142] loss = 0.222874 (* 1 = 0.222874 loss)
I1107 06:16:00.501127 44065 caffe_interface.cpp:142] top-1 = 0.95275
I1107 06:16:00.740296 44065 pruning_runner.cpp:306] pruning done, output model: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/sparse.caffemodel
I1107 06:16:00.740321 44065 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.5:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.946749866    | 0.952749908    | 0.00600004196  |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 806031         | -78.5914459%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 1153937256     | -46.4261398%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config5.prototxt
I1107 06:16:01.002537 46137 deephi_compress.cpp:236] /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/net_finetune.prototxt
I1107 06:16:01.172879 46137 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I1107 06:16:01.173409 46137 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24848171008, dev_info[0]: total=25620447232 free=24848171008
I1107 06:16:01.173421 46137 caffe_interface.cpp:493] Using GPUs 0
I1107 06:16:01.173673 46137 caffe_interface.cpp:498] GPU 0: Quadro P6000
I1107 06:16:01.762610 46137 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 5000
snapshot_prefix: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/net_finetune.prototxt"
type: "Adam"
I1107 06:16:01.762713 46137 solver.cpp:99] Creating training net from net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/net_finetune.prototxt
I1107 06:16:01.762930 46137 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1107 06:16:01.762944 46137 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I1107 06:16:01.763088 46137 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1107 06:16:01.763152 46137 layer_factory.hpp:77] Creating layer data
I1107 06:16:01.763274 46137 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:16:01.763697 46137 net.cpp:94] Creating Layer data
I1107 06:16:01.763705 46137 net.cpp:409] data -> data
I1107 06:16:01.763715 46137 net.cpp:409] data -> label
I1107 06:16:01.765126 46176 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/train_lmdb
I1107 06:16:01.765167 46176 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I1107 06:16:01.765441 46137 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I1107 06:16:01.765516 46137 data_layer.cpp:83] output data size: 256,3,227,227
I1107 06:16:02.145031 46137 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:16:02.145084 46137 net.cpp:144] Setting up data
I1107 06:16:02.145093 46137 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I1107 06:16:02.145097 46137 net.cpp:151] Top shape: 256 (256)
I1107 06:16:02.145099 46137 net.cpp:159] Memory required for data: 158298112
I1107 06:16:02.145104 46137 layer_factory.hpp:77] Creating layer conv1
I1107 06:16:02.145121 46137 net.cpp:94] Creating Layer conv1
I1107 06:16:02.145125 46137 net.cpp:435] conv1 <- data
I1107 06:16:02.145141 46137 net.cpp:409] conv1 -> conv1
I1107 06:16:02.147090 46137 net.cpp:144] Setting up conv1
I1107 06:16:02.147101 46137 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 06:16:02.147104 46137 net.cpp:159] Memory required for data: 455667712
I1107 06:16:02.147133 46137 layer_factory.hpp:77] Creating layer bn1
I1107 06:16:02.147142 46137 net.cpp:94] Creating Layer bn1
I1107 06:16:02.147145 46137 net.cpp:435] bn1 <- conv1
I1107 06:16:02.147150 46137 net.cpp:409] bn1 -> scale1
I1107 06:16:02.148375 46137 net.cpp:144] Setting up bn1
I1107 06:16:02.148380 46137 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 06:16:02.148382 46137 net.cpp:159] Memory required for data: 753037312
I1107 06:16:02.148391 46137 layer_factory.hpp:77] Creating layer relu1
I1107 06:16:02.148394 46137 net.cpp:94] Creating Layer relu1
I1107 06:16:02.148397 46137 net.cpp:435] relu1 <- scale1
I1107 06:16:02.148401 46137 net.cpp:409] relu1 -> relu1
I1107 06:16:02.148432 46137 net.cpp:144] Setting up relu1
I1107 06:16:02.148435 46137 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I1107 06:16:02.148437 46137 net.cpp:159] Memory required for data: 1050406912
I1107 06:16:02.148439 46137 layer_factory.hpp:77] Creating layer pool1
I1107 06:16:02.148460 46137 net.cpp:94] Creating Layer pool1
I1107 06:16:02.148465 46137 net.cpp:435] pool1 <- relu1
I1107 06:16:02.148469 46137 net.cpp:409] pool1 -> pool1
I1107 06:16:02.148524 46137 net.cpp:144] Setting up pool1
I1107 06:16:02.148530 46137 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I1107 06:16:02.148531 46137 net.cpp:159] Memory required for data: 1122070528
I1107 06:16:02.148535 46137 layer_factory.hpp:77] Creating layer conv2
I1107 06:16:02.148540 46137 net.cpp:94] Creating Layer conv2
I1107 06:16:02.148543 46137 net.cpp:435] conv2 <- pool1
I1107 06:16:02.148547 46137 net.cpp:409] conv2 -> conv2
I1107 06:16:02.163496 46137 net.cpp:144] Setting up conv2
I1107 06:16:02.163514 46137 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 06:16:02.163517 46137 net.cpp:159] Memory required for data: 1313173504
I1107 06:16:02.163530 46137 layer_factory.hpp:77] Creating layer bn2
I1107 06:16:02.163540 46137 net.cpp:94] Creating Layer bn2
I1107 06:16:02.163544 46137 net.cpp:435] bn2 <- conv2
I1107 06:16:02.163550 46137 net.cpp:409] bn2 -> scale2
I1107 06:16:02.164121 46137 net.cpp:144] Setting up bn2
I1107 06:16:02.164129 46137 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 06:16:02.164131 46137 net.cpp:159] Memory required for data: 1504276480
I1107 06:16:02.164140 46137 layer_factory.hpp:77] Creating layer relu2
I1107 06:16:02.164146 46137 net.cpp:94] Creating Layer relu2
I1107 06:16:02.164149 46137 net.cpp:435] relu2 <- scale2
I1107 06:16:02.164155 46137 net.cpp:409] relu2 -> relu2
I1107 06:16:02.164173 46137 net.cpp:144] Setting up relu2
I1107 06:16:02.164180 46137 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I1107 06:16:02.164182 46137 net.cpp:159] Memory required for data: 1695379456
I1107 06:16:02.164186 46137 layer_factory.hpp:77] Creating layer pool2
I1107 06:16:02.164191 46137 net.cpp:94] Creating Layer pool2
I1107 06:16:02.164194 46137 net.cpp:435] pool2 <- relu2
I1107 06:16:02.164213 46137 net.cpp:409] pool2 -> pool2
I1107 06:16:02.164242 46137 net.cpp:144] Setting up pool2
I1107 06:16:02.164247 46137 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 06:16:02.164249 46137 net.cpp:159] Memory required for data: 1739681792
I1107 06:16:02.164253 46137 layer_factory.hpp:77] Creating layer conv3
I1107 06:16:02.164261 46137 net.cpp:94] Creating Layer conv3
I1107 06:16:02.164263 46137 net.cpp:435] conv3 <- pool2
I1107 06:16:02.164268 46137 net.cpp:409] conv3 -> conv3
I1107 06:16:02.179029 46137 net.cpp:144] Setting up conv3
I1107 06:16:02.179059 46137 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 06:16:02.179064 46137 net.cpp:159] Memory required for data: 1806135296
I1107 06:16:02.179076 46137 layer_factory.hpp:77] Creating layer relu3
I1107 06:16:02.179088 46137 net.cpp:94] Creating Layer relu3
I1107 06:16:02.179096 46137 net.cpp:435] relu3 <- conv3
I1107 06:16:02.179105 46137 net.cpp:409] relu3 -> relu3
I1107 06:16:02.179141 46137 net.cpp:144] Setting up relu3
I1107 06:16:02.179150 46137 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 06:16:02.179154 46137 net.cpp:159] Memory required for data: 1872588800
I1107 06:16:02.179159 46137 layer_factory.hpp:77] Creating layer conv4
I1107 06:16:02.179172 46137 net.cpp:94] Creating Layer conv4
I1107 06:16:02.179178 46137 net.cpp:435] conv4 <- relu3
I1107 06:16:02.179186 46137 net.cpp:409] conv4 -> conv4
I1107 06:16:02.209622 46137 net.cpp:144] Setting up conv4
I1107 06:16:02.209648 46137 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 06:16:02.209652 46137 net.cpp:159] Memory required for data: 1939042304
I1107 06:16:02.209666 46137 layer_factory.hpp:77] Creating layer relu4
I1107 06:16:02.209673 46137 net.cpp:94] Creating Layer relu4
I1107 06:16:02.209677 46137 net.cpp:435] relu4 <- conv4
I1107 06:16:02.209700 46137 net.cpp:409] relu4 -> relu4
I1107 06:16:02.209725 46137 net.cpp:144] Setting up relu4
I1107 06:16:02.209731 46137 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I1107 06:16:02.209734 46137 net.cpp:159] Memory required for data: 2005495808
I1107 06:16:02.209738 46137 layer_factory.hpp:77] Creating layer conv5
I1107 06:16:02.209746 46137 net.cpp:94] Creating Layer conv5
I1107 06:16:02.209753 46137 net.cpp:435] conv5 <- relu4
I1107 06:16:02.209758 46137 net.cpp:409] conv5 -> conv5
I1107 06:16:02.226483 46137 net.cpp:144] Setting up conv5
I1107 06:16:02.226505 46137 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 06:16:02.226508 46137 net.cpp:159] Memory required for data: 2049798144
I1107 06:16:02.226531 46137 layer_factory.hpp:77] Creating layer relu5
I1107 06:16:02.226539 46137 net.cpp:94] Creating Layer relu5
I1107 06:16:02.226542 46137 net.cpp:435] relu5 <- conv5
I1107 06:16:02.226548 46137 net.cpp:409] relu5 -> relu5
I1107 06:16:02.226572 46137 net.cpp:144] Setting up relu5
I1107 06:16:02.226583 46137 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I1107 06:16:02.226584 46137 net.cpp:159] Memory required for data: 2094100480
I1107 06:16:02.226586 46137 layer_factory.hpp:77] Creating layer pool5
I1107 06:16:02.226591 46137 net.cpp:94] Creating Layer pool5
I1107 06:16:02.226594 46137 net.cpp:435] pool5 <- relu5
I1107 06:16:02.226598 46137 net.cpp:409] pool5 -> pool5
I1107 06:16:02.226624 46137 net.cpp:144] Setting up pool5
I1107 06:16:02.226629 46137 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I1107 06:16:02.226632 46137 net.cpp:159] Memory required for data: 2103537664
I1107 06:16:02.226635 46137 layer_factory.hpp:77] Creating layer fc6
I1107 06:16:02.226641 46137 net.cpp:94] Creating Layer fc6
I1107 06:16:02.226646 46137 net.cpp:435] fc6 <- pool5
I1107 06:16:02.226651 46137 net.cpp:409] fc6 -> fc6
I1107 06:16:02.554307 46137 net.cpp:144] Setting up fc6
I1107 06:16:02.554330 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.554332 46137 net.cpp:159] Memory required for data: 2107731968
I1107 06:16:02.554340 46137 layer_factory.hpp:77] Creating layer relu6
I1107 06:16:02.554347 46137 net.cpp:94] Creating Layer relu6
I1107 06:16:02.554350 46137 net.cpp:435] relu6 <- fc6
I1107 06:16:02.554385 46137 net.cpp:409] relu6 -> relu6
I1107 06:16:02.554402 46137 net.cpp:144] Setting up relu6
I1107 06:16:02.554421 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.554424 46137 net.cpp:159] Memory required for data: 2111926272
I1107 06:16:02.554426 46137 layer_factory.hpp:77] Creating layer drop6
I1107 06:16:02.554431 46137 net.cpp:94] Creating Layer drop6
I1107 06:16:02.554433 46137 net.cpp:435] drop6 <- relu6
I1107 06:16:02.554437 46137 net.cpp:409] drop6 -> drop6
I1107 06:16:02.554458 46137 net.cpp:144] Setting up drop6
I1107 06:16:02.554463 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.554466 46137 net.cpp:159] Memory required for data: 2116120576
I1107 06:16:02.554468 46137 layer_factory.hpp:77] Creating layer fc7
I1107 06:16:02.554473 46137 net.cpp:94] Creating Layer fc7
I1107 06:16:02.554476 46137 net.cpp:435] fc7 <- drop6
I1107 06:16:02.554481 46137 net.cpp:409] fc7 -> fc7
I1107 06:16:02.686151 46137 net.cpp:144] Setting up fc7
I1107 06:16:02.686177 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.686178 46137 net.cpp:159] Memory required for data: 2120314880
I1107 06:16:02.686185 46137 layer_factory.hpp:77] Creating layer bn7
I1107 06:16:02.686194 46137 net.cpp:94] Creating Layer bn7
I1107 06:16:02.686197 46137 net.cpp:435] bn7 <- fc7
I1107 06:16:02.686203 46137 net.cpp:409] bn7 -> scale7
I1107 06:16:02.686738 46137 net.cpp:144] Setting up bn7
I1107 06:16:02.686745 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.686748 46137 net.cpp:159] Memory required for data: 2124509184
I1107 06:16:02.686755 46137 layer_factory.hpp:77] Creating layer relu7
I1107 06:16:02.686759 46137 net.cpp:94] Creating Layer relu7
I1107 06:16:02.686763 46137 net.cpp:435] relu7 <- scale7
I1107 06:16:02.686765 46137 net.cpp:409] relu7 -> relu7
I1107 06:16:02.686782 46137 net.cpp:144] Setting up relu7
I1107 06:16:02.686787 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.686789 46137 net.cpp:159] Memory required for data: 2128703488
I1107 06:16:02.686791 46137 layer_factory.hpp:77] Creating layer drop7
I1107 06:16:02.686796 46137 net.cpp:94] Creating Layer drop7
I1107 06:16:02.686799 46137 net.cpp:435] drop7 <- relu7
I1107 06:16:02.686802 46137 net.cpp:409] drop7 -> drop7
I1107 06:16:02.686825 46137 net.cpp:144] Setting up drop7
I1107 06:16:02.686830 46137 net.cpp:151] Top shape: 256 4096 (1048576)
I1107 06:16:02.686831 46137 net.cpp:159] Memory required for data: 2132897792
I1107 06:16:02.686833 46137 layer_factory.hpp:77] Creating layer fc8
I1107 06:16:02.686839 46137 net.cpp:94] Creating Layer fc8
I1107 06:16:02.686842 46137 net.cpp:435] fc8 <- drop7
I1107 06:16:02.686846 46137 net.cpp:409] fc8 -> fc8
I1107 06:16:02.687695 46137 net.cpp:144] Setting up fc8
I1107 06:16:02.687706 46137 net.cpp:151] Top shape: 256 2 (512)
I1107 06:16:02.687710 46137 net.cpp:159] Memory required for data: 2132899840
I1107 06:16:02.687716 46137 layer_factory.hpp:77] Creating layer loss
I1107 06:16:02.687721 46137 net.cpp:94] Creating Layer loss
I1107 06:16:02.687726 46137 net.cpp:435] loss <- fc8
I1107 06:16:02.687731 46137 net.cpp:435] loss <- label
I1107 06:16:02.687736 46137 net.cpp:409] loss -> loss
I1107 06:16:02.687742 46137 layer_factory.hpp:77] Creating layer loss
I1107 06:16:02.687805 46137 net.cpp:144] Setting up loss
I1107 06:16:02.687810 46137 net.cpp:151] Top shape: (1)
I1107 06:16:02.687813 46137 net.cpp:154]     with loss weight 1
I1107 06:16:02.687824 46137 net.cpp:159] Memory required for data: 2132899844
I1107 06:16:02.687826 46137 net.cpp:220] loss needs backward computation.
I1107 06:16:02.687839 46137 net.cpp:220] fc8 needs backward computation.
I1107 06:16:02.687842 46137 net.cpp:220] drop7 needs backward computation.
I1107 06:16:02.687844 46137 net.cpp:220] relu7 needs backward computation.
I1107 06:16:02.687847 46137 net.cpp:220] bn7 needs backward computation.
I1107 06:16:02.687850 46137 net.cpp:220] fc7 needs backward computation.
I1107 06:16:02.687853 46137 net.cpp:220] drop6 needs backward computation.
I1107 06:16:02.687855 46137 net.cpp:220] relu6 needs backward computation.
I1107 06:16:02.687870 46137 net.cpp:220] fc6 needs backward computation.
I1107 06:16:02.687873 46137 net.cpp:220] pool5 needs backward computation.
I1107 06:16:02.687875 46137 net.cpp:220] relu5 needs backward computation.
I1107 06:16:02.687878 46137 net.cpp:220] conv5 needs backward computation.
I1107 06:16:02.687880 46137 net.cpp:220] relu4 needs backward computation.
I1107 06:16:02.687885 46137 net.cpp:220] conv4 needs backward computation.
I1107 06:16:02.687886 46137 net.cpp:220] relu3 needs backward computation.
I1107 06:16:02.687891 46137 net.cpp:220] conv3 needs backward computation.
I1107 06:16:02.687892 46137 net.cpp:220] pool2 needs backward computation.
I1107 06:16:02.687896 46137 net.cpp:220] relu2 needs backward computation.
I1107 06:16:02.687898 46137 net.cpp:220] bn2 needs backward computation.
I1107 06:16:02.687901 46137 net.cpp:220] conv2 needs backward computation.
I1107 06:16:02.687903 46137 net.cpp:220] pool1 needs backward computation.
I1107 06:16:02.687906 46137 net.cpp:220] relu1 needs backward computation.
I1107 06:16:02.687909 46137 net.cpp:220] bn1 needs backward computation.
I1107 06:16:02.687911 46137 net.cpp:220] conv1 needs backward computation.
I1107 06:16:02.687916 46137 net.cpp:222] data does not need backward computation.
I1107 06:16:02.687918 46137 net.cpp:264] This network produces output loss
I1107 06:16:02.687933 46137 net.cpp:284] Network initialization done.
I1107 06:16:02.688207 46137 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/net_finetune.prototxt
I1107 06:16:02.688235 46137 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1107 06:16:02.688400 46137 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "/home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I1107 06:16:02.688483 46137 layer_factory.hpp:77] Creating layer data
I1107 06:16:02.688519 46137 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:16:02.689543 46137 net.cpp:94] Creating Layer data
I1107 06:16:02.689558 46137 net.cpp:409] data -> data
I1107 06:16:02.689566 46137 net.cpp:409] data -> label
I1107 06:16:02.690775 46207 db_lmdb.cpp:35] Opened lmdb /home/danieleb/ML/cats-vs-dogs/input/lmdb/valid_lmdb
I1107 06:16:02.690811 46207 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I1107 06:16:02.691092 46137 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I1107 06:16:02.691170 46137 data_layer.cpp:83] output data size: 50,3,227,227
I1107 06:16:02.768685 46137 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I1107 06:16:02.768752 46137 net.cpp:144] Setting up data
I1107 06:16:02.768759 46137 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I1107 06:16:02.768764 46137 net.cpp:151] Top shape: 50 (50)
I1107 06:16:02.768765 46137 net.cpp:159] Memory required for data: 30917600
I1107 06:16:02.768769 46137 layer_factory.hpp:77] Creating layer label_data_1_split
I1107 06:16:02.768779 46137 net.cpp:94] Creating Layer label_data_1_split
I1107 06:16:02.768781 46137 net.cpp:435] label_data_1_split <- label
I1107 06:16:02.768805 46137 net.cpp:409] label_data_1_split -> label_data_1_split_0
I1107 06:16:02.768811 46137 net.cpp:409] label_data_1_split -> label_data_1_split_1
I1107 06:16:02.768872 46137 net.cpp:144] Setting up label_data_1_split
I1107 06:16:02.768877 46137 net.cpp:151] Top shape: 50 (50)
I1107 06:16:02.768879 46137 net.cpp:151] Top shape: 50 (50)
I1107 06:16:02.768880 46137 net.cpp:159] Memory required for data: 30918000
I1107 06:16:02.768883 46137 layer_factory.hpp:77] Creating layer conv1
I1107 06:16:02.768893 46137 net.cpp:94] Creating Layer conv1
I1107 06:16:02.768896 46137 net.cpp:435] conv1 <- data
I1107 06:16:02.768900 46137 net.cpp:409] conv1 -> conv1
I1107 06:16:02.769510 46137 net.cpp:144] Setting up conv1
I1107 06:16:02.769517 46137 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:16:02.769520 46137 net.cpp:159] Memory required for data: 88998000
I1107 06:16:02.769529 46137 layer_factory.hpp:77] Creating layer bn1
I1107 06:16:02.769538 46137 net.cpp:94] Creating Layer bn1
I1107 06:16:02.769541 46137 net.cpp:435] bn1 <- conv1
I1107 06:16:02.769546 46137 net.cpp:409] bn1 -> scale1
I1107 06:16:02.770129 46137 net.cpp:144] Setting up bn1
I1107 06:16:02.770138 46137 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:16:02.770140 46137 net.cpp:159] Memory required for data: 147078000
I1107 06:16:02.770150 46137 layer_factory.hpp:77] Creating layer relu1
I1107 06:16:02.770159 46137 net.cpp:94] Creating Layer relu1
I1107 06:16:02.770164 46137 net.cpp:435] relu1 <- scale1
I1107 06:16:02.770170 46137 net.cpp:409] relu1 -> relu1
I1107 06:16:02.770190 46137 net.cpp:144] Setting up relu1
I1107 06:16:02.770195 46137 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I1107 06:16:02.770200 46137 net.cpp:159] Memory required for data: 205158000
I1107 06:16:02.770201 46137 layer_factory.hpp:77] Creating layer pool1
I1107 06:16:02.770207 46137 net.cpp:94] Creating Layer pool1
I1107 06:16:02.770210 46137 net.cpp:435] pool1 <- relu1
I1107 06:16:02.770216 46137 net.cpp:409] pool1 -> pool1
I1107 06:16:02.770524 46137 net.cpp:144] Setting up pool1
I1107 06:16:02.770529 46137 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I1107 06:16:02.770532 46137 net.cpp:159] Memory required for data: 219154800
I1107 06:16:02.770534 46137 layer_factory.hpp:77] Creating layer conv2
I1107 06:16:02.770541 46137 net.cpp:94] Creating Layer conv2
I1107 06:16:02.770545 46137 net.cpp:435] conv2 <- pool1
I1107 06:16:02.770548 46137 net.cpp:409] conv2 -> conv2
I1107 06:16:02.776903 46137 net.cpp:144] Setting up conv2
I1107 06:16:02.776937 46137 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:16:02.776940 46137 net.cpp:159] Memory required for data: 256479600
I1107 06:16:02.776952 46137 layer_factory.hpp:77] Creating layer bn2
I1107 06:16:02.776973 46137 net.cpp:94] Creating Layer bn2
I1107 06:16:02.776980 46137 net.cpp:435] bn2 <- conv2
I1107 06:16:02.776990 46137 net.cpp:409] bn2 -> scale2
I1107 06:16:02.777642 46137 net.cpp:144] Setting up bn2
I1107 06:16:02.777650 46137 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:16:02.777653 46137 net.cpp:159] Memory required for data: 293804400
I1107 06:16:02.777662 46137 layer_factory.hpp:77] Creating layer relu2
I1107 06:16:02.777670 46137 net.cpp:94] Creating Layer relu2
I1107 06:16:02.777675 46137 net.cpp:435] relu2 <- scale2
I1107 06:16:02.777680 46137 net.cpp:409] relu2 -> relu2
I1107 06:16:02.777700 46137 net.cpp:144] Setting up relu2
I1107 06:16:02.777705 46137 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I1107 06:16:02.777709 46137 net.cpp:159] Memory required for data: 331129200
I1107 06:16:02.777711 46137 layer_factory.hpp:77] Creating layer pool2
I1107 06:16:02.777717 46137 net.cpp:94] Creating Layer pool2
I1107 06:16:02.777720 46137 net.cpp:435] pool2 <- relu2
I1107 06:16:02.777726 46137 net.cpp:409] pool2 -> pool2
I1107 06:16:02.777763 46137 net.cpp:144] Setting up pool2
I1107 06:16:02.777768 46137 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:16:02.777772 46137 net.cpp:159] Memory required for data: 339782000
I1107 06:16:02.777776 46137 layer_factory.hpp:77] Creating layer conv3
I1107 06:16:02.777786 46137 net.cpp:94] Creating Layer conv3
I1107 06:16:02.777791 46137 net.cpp:435] conv3 <- pool2
I1107 06:16:02.777799 46137 net.cpp:409] conv3 -> conv3
I1107 06:16:02.787446 46137 net.cpp:144] Setting up conv3
I1107 06:16:02.787467 46137 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:16:02.787470 46137 net.cpp:159] Memory required for data: 352761200
I1107 06:16:02.787478 46137 layer_factory.hpp:77] Creating layer relu3
I1107 06:16:02.787487 46137 net.cpp:94] Creating Layer relu3
I1107 06:16:02.787490 46137 net.cpp:435] relu3 <- conv3
I1107 06:16:02.787497 46137 net.cpp:409] relu3 -> relu3
I1107 06:16:02.787535 46137 net.cpp:144] Setting up relu3
I1107 06:16:02.787546 46137 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:16:02.787550 46137 net.cpp:159] Memory required for data: 365740400
I1107 06:16:02.787554 46137 layer_factory.hpp:77] Creating layer conv4
I1107 06:16:02.787566 46137 net.cpp:94] Creating Layer conv4
I1107 06:16:02.787575 46137 net.cpp:435] conv4 <- relu3
I1107 06:16:02.787581 46137 net.cpp:409] conv4 -> conv4
I1107 06:16:02.803880 46137 net.cpp:144] Setting up conv4
I1107 06:16:02.803910 46137 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:16:02.803915 46137 net.cpp:159] Memory required for data: 378719600
I1107 06:16:02.803936 46137 layer_factory.hpp:77] Creating layer relu4
I1107 06:16:02.803948 46137 net.cpp:94] Creating Layer relu4
I1107 06:16:02.803961 46137 net.cpp:435] relu4 <- conv4
I1107 06:16:02.803972 46137 net.cpp:409] relu4 -> relu4
I1107 06:16:02.804015 46137 net.cpp:144] Setting up relu4
I1107 06:16:02.804023 46137 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I1107 06:16:02.804028 46137 net.cpp:159] Memory required for data: 391698800
I1107 06:16:02.804033 46137 layer_factory.hpp:77] Creating layer conv5
I1107 06:16:02.804045 46137 net.cpp:94] Creating Layer conv5
I1107 06:16:02.804056 46137 net.cpp:435] conv5 <- relu4
I1107 06:16:02.804066 46137 net.cpp:409] conv5 -> conv5
I1107 06:16:02.814147 46137 net.cpp:144] Setting up conv5
I1107 06:16:02.814172 46137 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:16:02.814174 46137 net.cpp:159] Memory required for data: 400351600
I1107 06:16:02.814184 46137 layer_factory.hpp:77] Creating layer relu5
I1107 06:16:02.814198 46137 net.cpp:94] Creating Layer relu5
I1107 06:16:02.814205 46137 net.cpp:435] relu5 <- conv5
I1107 06:16:02.814216 46137 net.cpp:409] relu5 -> relu5
I1107 06:16:02.814262 46137 net.cpp:144] Setting up relu5
I1107 06:16:02.814268 46137 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I1107 06:16:02.814282 46137 net.cpp:159] Memory required for data: 409004400
I1107 06:16:02.814286 46137 layer_factory.hpp:77] Creating layer pool5
I1107 06:16:02.814298 46137 net.cpp:94] Creating Layer pool5
I1107 06:16:02.814303 46137 net.cpp:435] pool5 <- relu5
I1107 06:16:02.814309 46137 net.cpp:409] pool5 -> pool5
I1107 06:16:02.814355 46137 net.cpp:144] Setting up pool5
I1107 06:16:02.814363 46137 net.cpp:151] Top shape: 50 256 6 6 (460800)
I1107 06:16:02.814368 46137 net.cpp:159] Memory required for data: 410847600
I1107 06:16:02.814373 46137 layer_factory.hpp:77] Creating layer fc6
I1107 06:16:02.814383 46137 net.cpp:94] Creating Layer fc6
I1107 06:16:02.814388 46137 net.cpp:435] fc6 <- pool5
I1107 06:16:02.814396 46137 net.cpp:409] fc6 -> fc6
I1107 06:16:03.116662 46137 net.cpp:144] Setting up fc6
I1107 06:16:03.116685 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.116689 46137 net.cpp:159] Memory required for data: 411666800
I1107 06:16:03.116698 46137 layer_factory.hpp:77] Creating layer relu6
I1107 06:16:03.116708 46137 net.cpp:94] Creating Layer relu6
I1107 06:16:03.116727 46137 net.cpp:435] relu6 <- fc6
I1107 06:16:03.116735 46137 net.cpp:409] relu6 -> relu6
I1107 06:16:03.116762 46137 net.cpp:144] Setting up relu6
I1107 06:16:03.116767 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.116770 46137 net.cpp:159] Memory required for data: 412486000
I1107 06:16:03.116772 46137 layer_factory.hpp:77] Creating layer drop6
I1107 06:16:03.116780 46137 net.cpp:94] Creating Layer drop6
I1107 06:16:03.116783 46137 net.cpp:435] drop6 <- relu6
I1107 06:16:03.116788 46137 net.cpp:409] drop6 -> drop6
I1107 06:16:03.116817 46137 net.cpp:144] Setting up drop6
I1107 06:16:03.116822 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.116840 46137 net.cpp:159] Memory required for data: 413305200
I1107 06:16:03.116842 46137 layer_factory.hpp:77] Creating layer fc7
I1107 06:16:03.116859 46137 net.cpp:94] Creating Layer fc7
I1107 06:16:03.116863 46137 net.cpp:435] fc7 <- drop6
I1107 06:16:03.116869 46137 net.cpp:409] fc7 -> fc7
I1107 06:16:03.251902 46137 net.cpp:144] Setting up fc7
I1107 06:16:03.251927 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.251931 46137 net.cpp:159] Memory required for data: 414124400
I1107 06:16:03.251941 46137 layer_factory.hpp:77] Creating layer bn7
I1107 06:16:03.251953 46137 net.cpp:94] Creating Layer bn7
I1107 06:16:03.251960 46137 net.cpp:435] bn7 <- fc7
I1107 06:16:03.251971 46137 net.cpp:409] bn7 -> scale7
I1107 06:16:03.252586 46137 net.cpp:144] Setting up bn7
I1107 06:16:03.252593 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.252596 46137 net.cpp:159] Memory required for data: 414943600
I1107 06:16:03.252607 46137 layer_factory.hpp:77] Creating layer relu7
I1107 06:16:03.252614 46137 net.cpp:94] Creating Layer relu7
I1107 06:16:03.252619 46137 net.cpp:435] relu7 <- scale7
I1107 06:16:03.252627 46137 net.cpp:409] relu7 -> relu7
I1107 06:16:03.252650 46137 net.cpp:144] Setting up relu7
I1107 06:16:03.252655 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.252658 46137 net.cpp:159] Memory required for data: 415762800
I1107 06:16:03.252661 46137 layer_factory.hpp:77] Creating layer drop7
I1107 06:16:03.252669 46137 net.cpp:94] Creating Layer drop7
I1107 06:16:03.252674 46137 net.cpp:435] drop7 <- relu7
I1107 06:16:03.252681 46137 net.cpp:409] drop7 -> drop7
I1107 06:16:03.252710 46137 net.cpp:144] Setting up drop7
I1107 06:16:03.252715 46137 net.cpp:151] Top shape: 50 4096 (204800)
I1107 06:16:03.252718 46137 net.cpp:159] Memory required for data: 416582000
I1107 06:16:03.252722 46137 layer_factory.hpp:77] Creating layer fc8
I1107 06:16:03.252732 46137 net.cpp:94] Creating Layer fc8
I1107 06:16:03.252735 46137 net.cpp:435] fc8 <- drop7
I1107 06:16:03.252743 46137 net.cpp:409] fc8 -> fc8
I1107 06:16:03.252918 46137 net.cpp:144] Setting up fc8
I1107 06:16:03.252923 46137 net.cpp:151] Top shape: 50 2 (100)
I1107 06:16:03.252928 46137 net.cpp:159] Memory required for data: 416582400
I1107 06:16:03.252945 46137 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1107 06:16:03.252952 46137 net.cpp:94] Creating Layer fc8_fc8_0_split
I1107 06:16:03.252956 46137 net.cpp:435] fc8_fc8_0_split <- fc8
I1107 06:16:03.252964 46137 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1107 06:16:03.252971 46137 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1107 06:16:03.253001 46137 net.cpp:144] Setting up fc8_fc8_0_split
I1107 06:16:03.253006 46137 net.cpp:151] Top shape: 50 2 (100)
I1107 06:16:03.253010 46137 net.cpp:151] Top shape: 50 2 (100)
I1107 06:16:03.253015 46137 net.cpp:159] Memory required for data: 416583200
I1107 06:16:03.253018 46137 layer_factory.hpp:77] Creating layer loss
I1107 06:16:03.253024 46137 net.cpp:94] Creating Layer loss
I1107 06:16:03.253029 46137 net.cpp:435] loss <- fc8_fc8_0_split_0
I1107 06:16:03.253033 46137 net.cpp:435] loss <- label_data_1_split_0
I1107 06:16:03.253041 46137 net.cpp:409] loss -> loss
I1107 06:16:03.253051 46137 layer_factory.hpp:77] Creating layer loss
I1107 06:16:03.253127 46137 net.cpp:144] Setting up loss
I1107 06:16:03.253132 46137 net.cpp:151] Top shape: (1)
I1107 06:16:03.253134 46137 net.cpp:154]     with loss weight 1
I1107 06:16:03.253146 46137 net.cpp:159] Memory required for data: 416583204
I1107 06:16:03.253150 46137 layer_factory.hpp:77] Creating layer accuracy-top1
I1107 06:16:03.253159 46137 net.cpp:94] Creating Layer accuracy-top1
I1107 06:16:03.253163 46137 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I1107 06:16:03.253167 46137 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I1107 06:16:03.253175 46137 net.cpp:409] accuracy-top1 -> top-1
I1107 06:16:03.253183 46137 net.cpp:144] Setting up accuracy-top1
I1107 06:16:03.253187 46137 net.cpp:151] Top shape: (1)
I1107 06:16:03.253190 46137 net.cpp:159] Memory required for data: 416583208
I1107 06:16:03.253196 46137 net.cpp:222] accuracy-top1 does not need backward computation.
I1107 06:16:03.253201 46137 net.cpp:220] loss needs backward computation.
I1107 06:16:03.253206 46137 net.cpp:220] fc8_fc8_0_split needs backward computation.
I1107 06:16:03.253209 46137 net.cpp:220] fc8 needs backward computation.
I1107 06:16:03.253214 46137 net.cpp:220] drop7 needs backward computation.
I1107 06:16:03.253218 46137 net.cpp:220] relu7 needs backward computation.
I1107 06:16:03.253221 46137 net.cpp:220] bn7 needs backward computation.
I1107 06:16:03.253226 46137 net.cpp:220] fc7 needs backward computation.
I1107 06:16:03.253232 46137 net.cpp:220] drop6 needs backward computation.
I1107 06:16:03.253237 46137 net.cpp:220] relu6 needs backward computation.
I1107 06:16:03.253242 46137 net.cpp:220] fc6 needs backward computation.
I1107 06:16:03.253245 46137 net.cpp:220] pool5 needs backward computation.
I1107 06:16:03.253250 46137 net.cpp:220] relu5 needs backward computation.
I1107 06:16:03.253254 46137 net.cpp:220] conv5 needs backward computation.
I1107 06:16:03.253257 46137 net.cpp:220] relu4 needs backward computation.
I1107 06:16:03.253262 46137 net.cpp:220] conv4 needs backward computation.
I1107 06:16:03.253268 46137 net.cpp:220] relu3 needs backward computation.
I1107 06:16:03.253271 46137 net.cpp:220] conv3 needs backward computation.
I1107 06:16:03.253275 46137 net.cpp:220] pool2 needs backward computation.
I1107 06:16:03.253279 46137 net.cpp:220] relu2 needs backward computation.
I1107 06:16:03.253284 46137 net.cpp:220] bn2 needs backward computation.
I1107 06:16:03.253288 46137 net.cpp:220] conv2 needs backward computation.
I1107 06:16:03.253291 46137 net.cpp:220] pool1 needs backward computation.
I1107 06:16:03.253295 46137 net.cpp:220] relu1 needs backward computation.
I1107 06:16:03.253300 46137 net.cpp:220] bn1 needs backward computation.
I1107 06:16:03.253304 46137 net.cpp:220] conv1 needs backward computation.
I1107 06:16:03.253307 46137 net.cpp:222] label_data_1_split does not need backward computation.
I1107 06:16:03.253312 46137 net.cpp:222] data does not need backward computation.
I1107 06:16:03.253317 46137 net.cpp:264] This network produces output loss
I1107 06:16:03.253321 46137 net.cpp:264] This network produces output top-1
I1107 06:16:03.253350 46137 net.cpp:284] Network initialization done.
I1107 06:16:03.253446 46137 solver.cpp:63] Solver scaffolding done.
I1107 06:16:03.254693 46137 caffe_interface.cpp:93] Finetuning from /home/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/sparse.caffemodel
I1107 06:16:04.783308 46137 caffe_interface.cpp:527] Starting Optimization
I1107 06:16:04.783329 46137 solver.cpp:335] Solving 
I1107 06:16:04.783334 46137 solver.cpp:336] Learning Rate Policy: step
I1107 06:16:04.785336 46137 solver.cpp:418] Iteration 0, Testing net (#0)
I1107 06:16:06.302763 46137 solver.cpp:517]     Test net output #0: loss = 0.222874 (* 1 = 0.222874 loss)
I1107 06:16:06.302788 46137 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I1107 06:16:06.557677 46137 solver.cpp:266] Iteration 0 (0 iter/s, 1.77423s/50 iter), loss = 0.00134931
I1107 06:16:06.557718 46137 solver.cpp:285]     Train net output #0: loss = 0.00134931 (* 1 = 0.00134931 loss)
I1107 06:16:06.557739 46137 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1107 06:16:18.862947 46137 solver.cpp:266] Iteration 50 (4.06349 iter/s, 12.3047s/50 iter), loss = 0.0849894
I1107 06:16:18.862978 46137 solver.cpp:285]     Train net output #0: loss = 0.0849894 (* 1 = 0.0849894 loss)
I1107 06:16:18.862987 46137 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I1107 06:16:31.209048 46137 solver.cpp:266] Iteration 100 (4.05005 iter/s, 12.3455s/50 iter), loss = 0.0274362
I1107 06:16:31.209275 46137 solver.cpp:285]     Train net output #0: loss = 0.0274362 (* 1 = 0.0274362 loss)
I1107 06:16:31.209286 46137 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1107 06:16:43.590339 46137 solver.cpp:266] Iteration 150 (4.0386 iter/s, 12.3805s/50 iter), loss = 0.0296795
I1107 06:16:43.590369 46137 solver.cpp:285]     Train net output #0: loss = 0.0296795 (* 1 = 0.0296795 loss)
I1107 06:16:43.590375 46137 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I1107 06:16:56.009543 46137 solver.cpp:266] Iteration 200 (4.02621 iter/s, 12.4186s/50 iter), loss = 0.0514083
I1107 06:16:56.009572 46137 solver.cpp:285]     Train net output #0: loss = 0.0514083 (* 1 = 0.0514083 loss)
I1107 06:16:56.009577 46137 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1107 06:17:08.490383 46137 solver.cpp:266] Iteration 250 (4.00633 iter/s, 12.4803s/50 iter), loss = 0.0726708
I1107 06:17:08.490440 46137 solver.cpp:285]     Train net output #0: loss = 0.0726708 (* 1 = 0.0726708 loss)
I1107 06:17:08.490463 46137 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I1107 06:17:20.974556 46137 solver.cpp:266] Iteration 300 (4.00527 iter/s, 12.4836s/50 iter), loss = 0.0866118
I1107 06:17:20.974586 46137 solver.cpp:285]     Train net output #0: loss = 0.0866118 (* 1 = 0.0866118 loss)
I1107 06:17:20.974593 46137 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1107 06:17:33.496273 46137 solver.cpp:266] Iteration 350 (3.99325 iter/s, 12.5211s/50 iter), loss = 0.0649604
I1107 06:17:33.496304 46137 solver.cpp:285]     Train net output #0: loss = 0.0649604 (* 1 = 0.0649604 loss)
I1107 06:17:33.496310 46137 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I1107 06:17:46.049945 46137 solver.cpp:266] Iteration 400 (3.98308 iter/s, 12.5531s/50 iter), loss = 0.0954156
I1107 06:17:46.050097 46137 solver.cpp:285]     Train net output #0: loss = 0.0954156 (* 1 = 0.0954156 loss)
I1107 06:17:46.050104 46137 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1107 06:17:58.628423 46137 solver.cpp:266] Iteration 450 (3.97526 iter/s, 12.5778s/50 iter), loss = 0.0411652
I1107 06:17:58.628469 46137 solver.cpp:285]     Train net output #0: loss = 0.0411651 (* 1 = 0.0411651 loss)
I1107 06:17:58.628485 46137 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I1107 06:18:11.250002 46137 solver.cpp:266] Iteration 500 (3.96165 iter/s, 12.621s/50 iter), loss = 0.0330087
I1107 06:18:11.250031 46137 solver.cpp:285]     Train net output #0: loss = 0.0330087 (* 1 = 0.0330087 loss)
I1107 06:18:11.250039 46137 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1107 06:18:23.837787 46137 solver.cpp:266] Iteration 550 (3.97227 iter/s, 12.5873s/50 iter), loss = 0.0358079
I1107 06:18:23.837977 46137 solver.cpp:285]     Train net output #0: loss = 0.0358079 (* 1 = 0.0358079 loss)
I1107 06:18:23.837986 46137 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I1107 06:18:36.470718 46137 solver.cpp:266] Iteration 600 (3.95812 iter/s, 12.6323s/50 iter), loss = 0.0838322
I1107 06:18:36.470747 46137 solver.cpp:285]     Train net output #0: loss = 0.0838322 (* 1 = 0.0838322 loss)
I1107 06:18:36.470753 46137 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1107 06:18:49.094276 46137 solver.cpp:266] Iteration 650 (3.96101 iter/s, 12.6231s/50 iter), loss = 0.0439272
I1107 06:18:49.094305 46137 solver.cpp:285]     Train net output #0: loss = 0.0439272 (* 1 = 0.0439272 loss)
I1107 06:18:49.094328 46137 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I1107 06:19:01.702244 46137 solver.cpp:266] Iteration 700 (3.9659 iter/s, 12.6075s/50 iter), loss = 0.0504042
I1107 06:19:01.702412 46137 solver.cpp:285]     Train net output #0: loss = 0.0504042 (* 1 = 0.0504042 loss)
I1107 06:19:01.702421 46137 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1107 06:19:14.274755 46137 solver.cpp:266] Iteration 750 (3.97713 iter/s, 12.5719s/50 iter), loss = 0.0444305
I1107 06:19:14.274785 46137 solver.cpp:285]     Train net output #0: loss = 0.0444305 (* 1 = 0.0444305 loss)
I1107 06:19:14.274806 46137 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I1107 06:19:26.895594 46137 solver.cpp:266] Iteration 800 (3.96186 iter/s, 12.6203s/50 iter), loss = 0.0321151
I1107 06:19:26.895623 46137 solver.cpp:285]     Train net output #0: loss = 0.0321151 (* 1 = 0.0321151 loss)
I1107 06:19:26.895629 46137 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1107 06:19:39.545168 46137 solver.cpp:266] Iteration 850 (3.95286 iter/s, 12.6491s/50 iter), loss = 0.0597306
I1107 06:19:39.545316 46137 solver.cpp:285]     Train net output #0: loss = 0.0597306 (* 1 = 0.0597306 loss)
I1107 06:19:39.545325 46137 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I1107 06:19:52.193552 46137 solver.cpp:266] Iteration 900 (3.95327 iter/s, 12.6478s/50 iter), loss = 0.0580751
I1107 06:19:52.193583 46137 solver.cpp:285]     Train net output #0: loss = 0.0580751 (* 1 = 0.0580751 loss)
I1107 06:19:52.193605 46137 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1107 06:20:04.846323 46137 solver.cpp:266] Iteration 950 (3.95186 iter/s, 12.6523s/50 iter), loss = 0.0246453
I1107 06:20:04.846352 46137 solver.cpp:285]     Train net output #0: loss = 0.0246453 (* 1 = 0.0246453 loss)
I1107 06:20:04.846357 46137 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I1107 06:20:17.263490 46137 solver.cpp:418] Iteration 1000, Testing net (#0)
I1107 06:20:18.777415 46137 solver.cpp:517]     Test net output #0: loss = 0.239681 (* 1 = 0.239681 loss)
I1107 06:20:18.777431 46137 solver.cpp:517]     Test net output #1: top-1 = 0.931
I1107 06:20:19.017225 46137 solver.cpp:266] Iteration 1000 (3.5285 iter/s, 14.1703s/50 iter), loss = 0.0228722
I1107 06:20:19.017252 46137 solver.cpp:285]     Train net output #0: loss = 0.0228722 (* 1 = 0.0228722 loss)
I1107 06:20:19.017259 46137 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1107 06:20:31.699942 46137 solver.cpp:266] Iteration 1050 (3.94253 iter/s, 12.6822s/50 iter), loss = 0.0303474
I1107 06:20:31.699973 46137 solver.cpp:285]     Train net output #0: loss = 0.0303474 (* 1 = 0.0303474 loss)
I1107 06:20:31.699980 46137 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I1107 06:20:44.398174 46137 solver.cpp:266] Iteration 1100 (3.93772 iter/s, 12.6977s/50 iter), loss = 0.0355322
I1107 06:20:44.398205 46137 solver.cpp:285]     Train net output #0: loss = 0.0355322 (* 1 = 0.0355322 loss)
I1107 06:20:44.398210 46137 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1107 06:20:57.065130 46137 solver.cpp:266] Iteration 1150 (3.94744 iter/s, 12.6664s/50 iter), loss = 0.0590264
I1107 06:20:57.065305 46137 solver.cpp:285]     Train net output #0: loss = 0.0590264 (* 1 = 0.0590264 loss)
I1107 06:20:57.065315 46137 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I1107 06:21:09.758566 46137 solver.cpp:266] Iteration 1200 (3.93925 iter/s, 12.6928s/50 iter), loss = 0.0324122
I1107 06:21:09.758595 46137 solver.cpp:285]     Train net output #0: loss = 0.0324122 (* 1 = 0.0324122 loss)
I1107 06:21:09.758617 46137 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1107 06:21:22.442817 46137 solver.cpp:266] Iteration 1250 (3.94206 iter/s, 12.6837s/50 iter), loss = 0.0254848
I1107 06:21:22.442848 46137 solver.cpp:285]     Train net output #0: loss = 0.0254848 (* 1 = 0.0254848 loss)
I1107 06:21:22.442853 46137 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I1107 06:21:35.128528 46137 solver.cpp:266] Iteration 1300 (3.9416 iter/s, 12.6852s/50 iter), loss = 0.078095
I1107 06:21:35.128675 46137 solver.cpp:285]     Train net output #0: loss = 0.078095 (* 1 = 0.078095 loss)
I1107 06:21:35.128684 46137 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1107 06:21:47.818382 46137 solver.cpp:266] Iteration 1350 (3.94035 iter/s, 12.6892s/50 iter), loss = 0.0612465
I1107 06:21:47.818414 46137 solver.cpp:285]     Train net output #0: loss = 0.0612464 (* 1 = 0.0612464 loss)
I1107 06:21:47.818421 46137 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I1107 06:22:00.471750 46137 solver.cpp:266] Iteration 1400 (3.95168 iter/s, 12.6528s/50 iter), loss = 0.0463796
I1107 06:22:00.471779 46137 solver.cpp:285]     Train net output #0: loss = 0.0463796 (* 1 = 0.0463796 loss)
I1107 06:22:00.471786 46137 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1107 06:22:13.138378 46137 solver.cpp:266] Iteration 1450 (3.94754 iter/s, 12.6661s/50 iter), loss = 0.026992
I1107 06:22:13.138520 46137 solver.cpp:285]     Train net output #0: loss = 0.026992 (* 1 = 0.026992 loss)
I1107 06:22:13.138527 46137 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I1107 06:22:25.810536 46137 solver.cpp:266] Iteration 1500 (3.94585 iter/s, 12.6715s/50 iter), loss = 0.0280019
I1107 06:22:25.810564 46137 solver.cpp:285]     Train net output #0: loss = 0.0280019 (* 1 = 0.0280019 loss)
I1107 06:22:25.810570 46137 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1107 06:22:38.490183 46137 solver.cpp:266] Iteration 1550 (3.94349 iter/s, 12.6791s/50 iter), loss = 0.0219802
I1107 06:22:38.490213 46137 solver.cpp:285]     Train net output #0: loss = 0.0219802 (* 1 = 0.0219802 loss)
I1107 06:22:38.490219 46137 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I1107 06:22:51.111768 46137 solver.cpp:266] Iteration 1600 (3.96163 iter/s, 12.6211s/50 iter), loss = 0.13259
I1107 06:22:51.111907 46137 solver.cpp:285]     Train net output #0: loss = 0.13259 (* 1 = 0.13259 loss)
I1107 06:22:51.111917 46137 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1107 06:23:03.831601 46137 solver.cpp:266] Iteration 1650 (3.93106 iter/s, 12.7192s/50 iter), loss = 0.0844354
I1107 06:23:03.831632 46137 solver.cpp:285]     Train net output #0: loss = 0.0844354 (* 1 = 0.0844354 loss)
I1107 06:23:03.831637 46137 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I1107 06:23:16.491761 46137 solver.cpp:266] Iteration 1700 (3.94956 iter/s, 12.6596s/50 iter), loss = 0.0816527
I1107 06:23:16.491791 46137 solver.cpp:285]     Train net output #0: loss = 0.0816527 (* 1 = 0.0816527 loss)
I1107 06:23:16.491797 46137 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1107 06:23:29.176961 46137 solver.cpp:266] Iteration 1750 (3.94176 iter/s, 12.6847s/50 iter), loss = 0.0650936
I1107 06:23:29.177120 46137 solver.cpp:285]     Train net output #0: loss = 0.0650936 (* 1 = 0.0650936 loss)
I1107 06:23:29.177129 46137 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I1107 06:23:41.865968 46137 solver.cpp:266] Iteration 1800 (3.94062 iter/s, 12.6884s/50 iter), loss = 0.0214002
I1107 06:23:41.865998 46137 solver.cpp:285]     Train net output #0: loss = 0.0214002 (* 1 = 0.0214002 loss)
I1107 06:23:41.866004 46137 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1107 06:23:54.520437 46137 solver.cpp:266] Iteration 1850 (3.95133 iter/s, 12.654s/50 iter), loss = 0.0878365
I1107 06:23:54.520467 46137 solver.cpp:285]     Train net output #0: loss = 0.0878365 (* 1 = 0.0878365 loss)
I1107 06:23:54.520471 46137 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I1107 06:24:07.185253 46137 solver.cpp:266] Iteration 1900 (3.94811 iter/s, 12.6643s/50 iter), loss = 0.059908
I1107 06:24:07.185436 46137 solver.cpp:285]     Train net output #0: loss = 0.059908 (* 1 = 0.059908 loss)
I1107 06:24:07.185443 46137 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1107 06:24:19.849956 46137 solver.cpp:266] Iteration 1950 (3.94819 iter/s, 12.664s/50 iter), loss = 0.0452425
I1107 06:24:19.849985 46137 solver.cpp:285]     Train net output #0: loss = 0.0452425 (* 1 = 0.0452425 loss)
I1107 06:24:19.849990 46137 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I1107 06:24:32.316884 46137 solver.cpp:418] Iteration 2000, Testing net (#0)
I1107 06:24:33.800283 46137 solver.cpp:517]     Test net output #0: loss = 0.209473 (* 1 = 0.209473 loss)
I1107 06:24:33.800300 46137 solver.cpp:517]     Test net output #1: top-1 = 0.92925
I1107 06:24:34.043825 46137 solver.cpp:266] Iteration 2000 (3.52277 iter/s, 14.1934s/50 iter), loss = 0.0674591
I1107 06:24:34.043851 46137 solver.cpp:285]     Train net output #0: loss = 0.0674591 (* 1 = 0.0674591 loss)
I1107 06:24:34.043857 46137 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1107 06:24:46.677914 46137 solver.cpp:266] Iteration 2050 (3.95768 iter/s, 12.6337s/50 iter), loss = 0.0436452
I1107 06:24:46.678057 46137 solver.cpp:285]     Train net output #0: loss = 0.0436452 (* 1 = 0.0436452 loss)
I1107 06:24:46.678066 46137 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I1107 06:24:59.321043 46137 solver.cpp:266] Iteration 2100 (3.95489 iter/s, 12.6426s/50 iter), loss = 0.0474783
I1107 06:24:59.321071 46137 solver.cpp:285]     Train net output #0: loss = 0.0474783 (* 1 = 0.0474783 loss)
I1107 06:24:59.321076 46137 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1107 06:25:11.957279 46137 solver.cpp:266] Iteration 2150 (3.95701 iter/s, 12.6358s/50 iter), loss = 0.0678204
I1107 06:25:11.957309 46137 solver.cpp:285]     Train net output #0: loss = 0.0678204 (* 1 = 0.0678204 loss)
I1107 06:25:11.957315 46137 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I1107 06:25:24.641139 46137 solver.cpp:266] Iteration 2200 (3.94215 iter/s, 12.6834s/50 iter), loss = 0.0267006
I1107 06:25:24.641309 46137 solver.cpp:285]     Train net output #0: loss = 0.0267006 (* 1 = 0.0267006 loss)
I1107 06:25:24.641316 46137 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1107 06:25:37.275399 46137 solver.cpp:266] Iteration 2250 (3.95767 iter/s, 12.6337s/50 iter), loss = 0.0770892
I1107 06:25:37.275431 46137 solver.cpp:285]     Train net output #0: loss = 0.0770892 (* 1 = 0.0770892 loss)
I1107 06:25:37.275452 46137 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I1107 06:25:49.919286 46137 solver.cpp:266] Iteration 2300 (3.95462 iter/s, 12.6434s/50 iter), loss = 0.0308065
I1107 06:25:49.919315 46137 solver.cpp:285]     Train net output #0: loss = 0.0308065 (* 1 = 0.0308065 loss)
I1107 06:25:49.919322 46137 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1107 06:26:02.565207 46137 solver.cpp:266] Iteration 2350 (3.95398 iter/s, 12.6455s/50 iter), loss = 0.0705627
I1107 06:26:02.565384 46137 solver.cpp:285]     Train net output #0: loss = 0.0705627 (* 1 = 0.0705627 loss)
I1107 06:26:02.565393 46137 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I1107 06:26:15.193698 46137 solver.cpp:266] Iteration 2400 (3.95949 iter/s, 12.6279s/50 iter), loss = 0.0609277
I1107 06:26:15.193727 46137 solver.cpp:285]     Train net output #0: loss = 0.0609277 (* 1 = 0.0609277 loss)
I1107 06:26:15.193732 46137 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1107 06:26:27.853463 46137 solver.cpp:266] Iteration 2450 (3.94966 iter/s, 12.6593s/50 iter), loss = 0.0604875
I1107 06:26:27.853493 46137 solver.cpp:285]     Train net output #0: loss = 0.0604875 (* 1 = 0.0604875 loss)
I1107 06:26:27.853500 46137 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I1107 06:26:40.488601 46137 solver.cpp:266] Iteration 2500 (3.95736 iter/s, 12.6347s/50 iter), loss = 0.0741616
I1107 06:26:40.489923 46137 solver.cpp:285]     Train net output #0: loss = 0.0741616 (* 1 = 0.0741616 loss)
I1107 06:26:40.489931 46137 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1107 06:26:53.166918 46137 solver.cpp:266] Iteration 2550 (3.94428 iter/s, 12.6766s/50 iter), loss = 0.0257055
I1107 06:26:53.166947 46137 solver.cpp:285]     Train net output #0: loss = 0.0257055 (* 1 = 0.0257055 loss)
I1107 06:26:53.166954 46137 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I1107 06:27:05.804513 46137 solver.cpp:266] Iteration 2600 (3.95659 iter/s, 12.6371s/50 iter), loss = 0.0180205
I1107 06:27:05.804543 46137 solver.cpp:285]     Train net output #0: loss = 0.0180205 (* 1 = 0.0180205 loss)
I1107 06:27:05.804548 46137 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1107 06:27:18.436426 46137 solver.cpp:266] Iteration 2650 (3.95837 iter/s, 12.6315s/50 iter), loss = 0.0177748
I1107 06:27:18.436564 46137 solver.cpp:285]     Train net output #0: loss = 0.0177748 (* 1 = 0.0177748 loss)
I1107 06:27:18.436573 46137 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I1107 06:27:31.115407 46137 solver.cpp:266] Iteration 2700 (3.94371 iter/s, 12.6784s/50 iter), loss = 0.013347
I1107 06:27:31.115439 46137 solver.cpp:285]     Train net output #0: loss = 0.013347 (* 1 = 0.013347 loss)
I1107 06:27:31.115447 46137 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1107 06:27:43.767112 46137 solver.cpp:266] Iteration 2750 (3.95218 iter/s, 12.6512s/50 iter), loss = 0.00617344
I1107 06:27:43.767141 46137 solver.cpp:285]     Train net output #0: loss = 0.00617345 (* 1 = 0.00617345 loss)
I1107 06:27:43.767146 46137 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I1107 06:27:56.436738 46137 solver.cpp:266] Iteration 2800 (3.94659 iter/s, 12.6692s/50 iter), loss = 0.0202791
I1107 06:27:56.436892 46137 solver.cpp:285]     Train net output #0: loss = 0.0202791 (* 1 = 0.0202791 loss)
I1107 06:27:56.436900 46137 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1107 06:28:09.087117 46137 solver.cpp:266] Iteration 2850 (3.95263 iter/s, 12.6498s/50 iter), loss = 0.0208577
I1107 06:28:09.087146 46137 solver.cpp:285]     Train net output #0: loss = 0.0208577 (* 1 = 0.0208577 loss)
I1107 06:28:09.087152 46137 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I1107 06:28:21.755708 46137 solver.cpp:266] Iteration 2900 (3.94692 iter/s, 12.6681s/50 iter), loss = 0.0290437
I1107 06:28:21.755738 46137 solver.cpp:285]     Train net output #0: loss = 0.0290437 (* 1 = 0.0290437 loss)
I1107 06:28:21.755744 46137 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1107 06:28:34.408757 46137 solver.cpp:266] Iteration 2950 (3.95176 iter/s, 12.6526s/50 iter), loss = 0.0122744
I1107 06:28:34.408919 46137 solver.cpp:285]     Train net output #0: loss = 0.0122744 (* 1 = 0.0122744 loss)
I1107 06:28:34.408928 46137 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I1107 06:28:46.771682 46137 solver.cpp:418] Iteration 3000, Testing net (#0)
I1107 06:28:48.272356 46137 solver.cpp:517]     Test net output #0: loss = 0.151541 (* 1 = 0.151541 loss)
I1107 06:28:48.272372 46137 solver.cpp:517]     Test net output #1: top-1 = 0.9495
I1107 06:28:48.513679 46137 solver.cpp:266] Iteration 3000 (3.54502 iter/s, 14.1043s/50 iter), loss = 0.0051717
I1107 06:28:48.513705 46137 solver.cpp:285]     Train net output #0: loss = 0.00517172 (* 1 = 0.00517172 loss)
I1107 06:28:48.513711 46137 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1107 06:29:01.161847 46137 solver.cpp:266] Iteration 3050 (3.95329 iter/s, 12.6477s/50 iter), loss = 0.00641064
I1107 06:29:01.161877 46137 solver.cpp:285]     Train net output #0: loss = 0.00641066 (* 1 = 0.00641066 loss)
I1107 06:29:01.161883 46137 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I1107 06:29:13.791298 46137 solver.cpp:266] Iteration 3100 (3.95915 iter/s, 12.629s/50 iter), loss = 0.00360973
I1107 06:29:13.791453 46137 solver.cpp:285]     Train net output #0: loss = 0.00360974 (* 1 = 0.00360974 loss)
I1107 06:29:13.791462 46137 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I1107 06:29:26.429160 46137 solver.cpp:266] Iteration 3150 (3.95655 iter/s, 12.6373s/50 iter), loss = 0.00701299
I1107 06:29:26.429189 46137 solver.cpp:285]     Train net output #0: loss = 0.007013 (* 1 = 0.007013 loss)
I1107 06:29:26.429195 46137 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I1107 06:29:39.067095 46137 solver.cpp:266] Iteration 3200 (3.95649 iter/s, 12.6375s/50 iter), loss = 0.0520292
I1107 06:29:39.067123 46137 solver.cpp:285]     Train net output #0: loss = 0.0520292 (* 1 = 0.0520292 loss)
I1107 06:29:39.067144 46137 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I1107 06:29:51.690786 46137 solver.cpp:266] Iteration 3250 (3.96095 iter/s, 12.6232s/50 iter), loss = 0.0093858
I1107 06:29:51.690989 46137 solver.cpp:285]     Train net output #0: loss = 0.00938582 (* 1 = 0.00938582 loss)
I1107 06:29:51.690996 46137 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I1107 06:30:04.388900 46137 solver.cpp:266] Iteration 3300 (3.93779 iter/s, 12.6975s/50 iter), loss = 0.0157779
I1107 06:30:04.388927 46137 solver.cpp:285]     Train net output #0: loss = 0.0157779 (* 1 = 0.0157779 loss)
I1107 06:30:04.388949 46137 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I1107 06:30:17.015341 46137 solver.cpp:266] Iteration 3350 (3.96009 iter/s, 12.626s/50 iter), loss = 0.0292429
I1107 06:30:17.015372 46137 solver.cpp:285]     Train net output #0: loss = 0.0292429 (* 1 = 0.0292429 loss)

I0130 22:46:13.627694 110538 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I0130 22:46:38.048804 110538 solver.cpp:266] Iteration 3400 (2.04748 iter/s, 24.4202s/50 iter), loss = 0.0142591
I0130 22:46:38.048830 110538 solver.cpp:285]     Train net output #0: loss = 0.0142591 (* 1 = 0.0142591 loss)
I0130 22:46:38.051059 110538 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0130 22:46:56.296818 110538 solver.cpp:266] Iteration 3450 (2.74046 iter/s, 18.2451s/50 iter), loss = 0.00533208
I0130 22:46:56.296948 110538 solver.cpp:285]     Train net output #0: loss = 0.00533207 (* 1 = 0.00533207 loss)
I0130 22:46:56.299064 110538 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I0130 22:47:14.556406 110538 solver.cpp:266] Iteration 3500 (2.73873 iter/s, 18.2567s/50 iter), loss = 0.00631617
I0130 22:47:14.556435 110538 solver.cpp:285]     Train net output #0: loss = 0.00631616 (* 1 = 0.00631616 loss)
I0130 22:47:14.556442 110538 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0130 22:47:32.825877 110538 solver.cpp:266] Iteration 3550 (2.73691 iter/s, 18.2688s/50 iter), loss = 0.00500668
I0130 22:47:32.825994 110538 solver.cpp:285]     Train net output #0: loss = 0.00500666 (* 1 = 0.00500666 loss)
I0130 22:47:32.828128 110538 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I0130 22:47:50.948537 110538 solver.cpp:266] Iteration 3600 (2.75942 iter/s, 18.1197s/50 iter), loss = 0.00300893
I0130 22:47:50.948565 110538 solver.cpp:285]     Train net output #0: loss = 0.00300891 (* 1 = 0.00300891 loss)
I0130 22:47:50.949389 110538 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0130 22:48:09.140694 110538 solver.cpp:266] Iteration 3650 (2.74867 iter/s, 18.1906s/50 iter), loss = 0.013187
I0130 22:48:09.140826 110538 solver.cpp:285]     Train net output #0: loss = 0.013187 (* 1 = 0.013187 loss)
I0130 22:48:09.141065 110538 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I0130 22:48:27.347995 110538 solver.cpp:266] Iteration 3700 (2.74631 iter/s, 18.2063s/50 iter), loss = 0.0208206
I0130 22:48:27.348022 110538 solver.cpp:285]     Train net output #0: loss = 0.0208206 (* 1 = 0.0208206 loss)
I0130 22:48:27.350227 110538 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0130 22:48:45.642072 110538 solver.cpp:266] Iteration 3750 (2.73356 iter/s, 18.2912s/50 iter), loss = 0.00608849
I0130 22:48:45.642223 110538 solver.cpp:285]     Train net output #0: loss = 0.00608847 (* 1 = 0.00608847 loss)
I0130 22:48:45.642231 110538 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I0130 22:49:03.915346 110538 solver.cpp:266] Iteration 3800 (2.73636 iter/s, 18.2725s/50 iter), loss = 0.0188894
I0130 22:49:03.915372 110538 solver.cpp:285]     Train net output #0: loss = 0.0188894 (* 1 = 0.0188894 loss)
I0130 22:49:03.916438 110538 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0130 22:49:17.188710 110538 solver.cpp:266] Iteration 3850 (3.76739 iter/s, 13.2718s/50 iter), loss = 0.00918146
I0130 22:49:17.188766 110538 solver.cpp:285]     Train net output #0: loss = 0.00918144 (* 1 = 0.00918144 loss)
I0130 22:49:17.190968 110538 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I0130 22:49:41.388257 110538 solver.cpp:266] Iteration 3900 (2.06642 iter/s, 24.1964s/50 iter), loss = 0.00160718
I0130 22:49:41.388285 110538 solver.cpp:285]     Train net output #0: loss = 0.00160716 (* 1 = 0.00160716 loss)
I0130 22:49:41.390516 110538 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0130 22:50:05.648581 110538 solver.cpp:266] Iteration 3950 (2.06125 iter/s, 24.2572s/50 iter), loss = 0.0111993
I0130 22:50:05.648715 110538 solver.cpp:285]     Train net output #0: loss = 0.0111993 (* 1 = 0.0111993 loss)
I0130 22:50:05.650830 110538 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I0130 22:50:29.644134 110538 solver.cpp:418] Iteration 4000, Testing net (#0)
I0130 22:50:33.056825 110538 solver.cpp:517]     Test net output #0: loss = 0.159618 (* 1 = 0.159618 loss)
I0130 22:50:33.056843 110538 solver.cpp:517]     Test net output #1: top-1 = 0.9475
I0130 22:50:33.613488 110538 solver.cpp:266] Iteration 4000 (1.78816 iter/s, 27.9616s/50 iter), loss = 0.00804663
I0130 22:50:33.613513 110538 solver.cpp:285]     Train net output #0: loss = 0.0080466 (* 1 = 0.0080466 loss)
I0130 22:50:33.615738 110538 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0130 22:50:57.758535 110538 solver.cpp:266] Iteration 4050 (2.07109 iter/s, 24.1419s/50 iter), loss = 0.00154604
I0130 22:50:57.758662 110538 solver.cpp:285]     Train net output #0: loss = 0.00154601 (* 1 = 0.00154601 loss)
I0130 22:50:57.760785 110538 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I0130 22:51:22.193528 110538 solver.cpp:266] Iteration 4100 (2.04651 iter/s, 24.4318s/50 iter), loss = 0.0118351
I0130 22:51:22.193558 110538 solver.cpp:285]     Train net output #0: loss = 0.0118351 (* 1 = 0.0118351 loss)
I0130 22:51:22.193603 110538 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0130 22:51:46.424134 110538 solver.cpp:266] Iteration 4150 (2.06359 iter/s, 24.2296s/50 iter), loss = 0.00787918
I0130 22:51:46.424232 110538 solver.cpp:285]     Train net output #0: loss = 0.00787916 (* 1 = 0.00787916 loss)
I0130 22:51:46.424275 110538 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I0130 22:52:10.697162 110538 solver.cpp:266] Iteration 4200 (2.05999 iter/s, 24.272s/50 iter), loss = 0.00182371
I0130 22:52:10.697191 110538 solver.cpp:285]     Train net output #0: loss = 0.00182369 (* 1 = 0.00182369 loss)
I0130 22:52:10.699414 110538 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0130 22:52:34.952977 110538 solver.cpp:266] Iteration 4250 (2.06163 iter/s, 24.2527s/50 iter), loss = 0.00402742
I0130 22:52:34.953096 110538 solver.cpp:285]     Train net output #0: loss = 0.00402741 (* 1 = 0.00402741 loss)
I0130 22:52:34.953104 110538 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I0130 22:52:59.418275 110538 solver.cpp:266] Iteration 4300 (2.0438 iter/s, 24.4643s/50 iter), loss = 0.0143248
I0130 22:52:59.418306 110538 solver.cpp:285]     Train net output #0: loss = 0.0143247 (* 1 = 0.0143247 loss)
I0130 22:52:59.420528 110538 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0130 22:53:23.733449 110538 solver.cpp:266] Iteration 4350 (2.0566 iter/s, 24.312s/50 iter), loss = 0.00675675
I0130 22:53:23.733567 110538 solver.cpp:285]     Train net output #0: loss = 0.00675673 (* 1 = 0.00675673 loss)
I0130 22:53:23.735704 110538 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I0130 22:53:48.066740 110538 solver.cpp:266] Iteration 4400 (2.05506 iter/s, 24.3301s/50 iter), loss = 0.0221264
I0130 22:53:48.066769 110538 solver.cpp:285]     Train net output #0: loss = 0.0221264 (* 1 = 0.0221264 loss)
I0130 22:53:48.068994 110538 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0130 22:54:12.403048 110538 solver.cpp:266] Iteration 4450 (2.05481 iter/s, 24.3332s/50 iter), loss = 0.0255276
I0130 22:54:12.403187 110538 solver.cpp:285]     Train net output #0: loss = 0.0255276 (* 1 = 0.0255276 loss)
I0130 22:54:12.403194 110538 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I0130 22:54:36.942420 110538 solver.cpp:266] Iteration 4500 (2.03763 iter/s, 24.5383s/50 iter), loss = 0.001072
I0130 22:54:36.942453 110538 solver.cpp:285]     Train net output #0: loss = 0.00107198 (* 1 = 0.00107198 loss)
I0130 22:54:36.944648 110538 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0130 22:55:01.196782 110538 solver.cpp:266] Iteration 4550 (2.06175 iter/s, 24.2512s/50 iter), loss = 0.00129667
I0130 22:55:01.196908 110538 solver.cpp:285]     Train net output #0: loss = 0.00129665 (* 1 = 0.00129665 loss)
I0130 22:55:01.199034 110538 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I0130 22:55:25.472079 110538 solver.cpp:266] Iteration 4600 (2.05997 iter/s, 24.2722s/50 iter), loss = 0.00385299
I0130 22:55:25.472112 110538 solver.cpp:285]     Train net output #0: loss = 0.00385298 (* 1 = 0.00385298 loss)
I0130 22:55:25.474334 110538 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0130 22:55:49.719192 110538 solver.cpp:266] Iteration 4650 (2.06237 iter/s, 24.244s/50 iter), loss = 0.00443259
I0130 22:55:49.719313 110538 solver.cpp:285]     Train net output #0: loss = 0.00443257 (* 1 = 0.00443257 loss)
I0130 22:55:49.719321 110538 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I0130 22:56:14.164885 110538 solver.cpp:266] Iteration 4700 (2.04544 iter/s, 24.4447s/50 iter), loss = 0.00303559
I0130 22:56:14.164916 110538 solver.cpp:285]     Train net output #0: loss = 0.00303558 (* 1 = 0.00303558 loss)
I0130 22:56:14.167148 110538 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0130 22:56:38.453294 110538 solver.cpp:266] Iteration 4750 (2.05886 iter/s, 24.2852s/50 iter), loss = 0.000990743
I0130 22:56:38.453423 110538 solver.cpp:285]     Train net output #0: loss = 0.00099073 (* 1 = 0.00099073 loss)
I0130 22:56:38.455539 110538 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I0130 22:57:02.638193 110538 solver.cpp:266] Iteration 4800 (2.06767 iter/s, 24.1818s/50 iter), loss = 0.0117352
I0130 22:57:02.638223 110538 solver.cpp:285]     Train net output #0: loss = 0.0117352 (* 1 = 0.0117352 loss)
I0130 22:57:02.640452 110538 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0130 22:57:26.977697 110538 solver.cpp:266] Iteration 4850 (2.05454 iter/s, 24.3363s/50 iter), loss = 0.00147997
I0130 22:57:26.977820 110538 solver.cpp:285]     Train net output #0: loss = 0.00147996 (* 1 = 0.00147996 loss)
I0130 22:57:26.979941 110538 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I0130 22:57:51.402123 110538 solver.cpp:266] Iteration 4900 (2.04739 iter/s, 24.4213s/50 iter), loss = 0.00116408
I0130 22:57:51.402156 110538 solver.cpp:285]     Train net output #0: loss = 0.00116406 (* 1 = 0.00116406 loss)
I0130 22:57:51.402163 110538 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0130 22:58:15.672025 110538 solver.cpp:266] Iteration 4950 (2.06024 iter/s, 24.269s/50 iter), loss = 0.00322794
I0130 22:58:15.672147 110538 solver.cpp:285]     Train net output #0: loss = 0.00322793 (* 1 = 0.00322793 loss)
I0130 22:58:15.674278 110538 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I0130 22:58:39.346489 110538 solver.cpp:418] Iteration 5000, Testing net (#0)
I0130 22:58:42.994827 110538 solver.cpp:517]     Test net output #0: loss = 0.165786 (* 1 = 0.165786 loss)
I0130 22:58:42.994843 110538 solver.cpp:517]     Test net output #1: top-1 = 0.948
I0130 22:58:43.487145 110538 solver.cpp:266] Iteration 5000 (1.7978 iter/s, 27.8118s/50 iter), loss = 0.018642
I0130 22:58:43.487175 110538 solver.cpp:285]     Train net output #0: loss = 0.018642 (* 1 = 0.018642 loss)
I0130 22:58:43.489397 110538 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0130 22:59:07.752511 110538 solver.cpp:266] Iteration 5050 (2.06082 iter/s, 24.2622s/50 iter), loss = 0.00773617
I0130 22:59:07.752658 110538 solver.cpp:285]     Train net output #0: loss = 0.00773617 (* 1 = 0.00773617 loss)
I0130 22:59:07.754762 110538 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I0130 22:59:31.978232 110538 solver.cpp:266] Iteration 5100 (2.06419 iter/s, 24.2226s/50 iter), loss = 0.000858467
I0130 22:59:31.978265 110538 solver.cpp:285]     Train net output #0: loss = 0.000858459 (* 1 = 0.000858459 loss)
I0130 22:59:31.980482 110538 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0130 22:59:56.291477 110538 solver.cpp:266] Iteration 5150 (2.05676 iter/s, 24.3101s/50 iter), loss = 0.00181987
I0130 22:59:56.291532 110538 solver.cpp:285]     Train net output #0: loss = 0.00181987 (* 1 = 0.00181987 loss)
I0130 22:59:56.291539 110538 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I0130 23:00:20.784060 110538 solver.cpp:266] Iteration 5200 (2.04151 iter/s, 24.4916s/50 iter), loss = 0.00175191
I0130 23:00:20.784091 110538 solver.cpp:285]     Train net output #0: loss = 0.00175191 (* 1 = 0.00175191 loss)
I0130 23:00:20.786329 110538 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0130 23:00:45.142077 110538 solver.cpp:266] Iteration 5250 (2.05298 iter/s, 24.3548s/50 iter), loss = 0.00403123
I0130 23:00:45.142184 110538 solver.cpp:285]     Train net output #0: loss = 0.00403123 (* 1 = 0.00403123 loss)
I0130 23:00:45.144335 110538 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I0130 23:01:09.394130 110538 solver.cpp:266] Iteration 5300 (2.06195 iter/s, 24.2489s/50 iter), loss = 0.016856
I0130 23:01:09.394160 110538 solver.cpp:285]     Train net output #0: loss = 0.016856 (* 1 = 0.016856 loss)
I0130 23:01:09.396390 110538 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0130 23:01:33.786415 110538 solver.cpp:266] Iteration 5350 (2.05009 iter/s, 24.3891s/50 iter), loss = 0.00230964
I0130 23:01:33.786551 110538 solver.cpp:285]     Train net output #0: loss = 0.00230963 (* 1 = 0.00230963 loss)
I0130 23:01:33.788624 110538 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I0130 23:01:58.132115 110538 solver.cpp:266] Iteration 5400 (2.05401 iter/s, 24.3426s/50 iter), loss = 0.000636285
I0130 23:01:58.132146 110538 solver.cpp:285]     Train net output #0: loss = 0.000636277 (* 1 = 0.000636277 loss)
I0130 23:01:58.132153 110538 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0130 23:02:22.606895 110538 solver.cpp:266] Iteration 5450 (2.043 iter/s, 24.4738s/50 iter), loss = 0.00180857
I0130 23:02:22.606967 110538 solver.cpp:285]     Train net output #0: loss = 0.00180856 (* 1 = 0.00180856 loss)
I0130 23:02:22.609153 110538 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I0130 23:02:47.010063 110538 solver.cpp:266] Iteration 5500 (2.04918 iter/s, 24.4s/50 iter), loss = 0.00388248
I0130 23:02:47.010092 110538 solver.cpp:285]     Train net output #0: loss = 0.00388247 (* 1 = 0.00388247 loss)
I0130 23:02:47.012310 110538 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0130 23:03:11.167805 110538 solver.cpp:266] Iteration 5550 (2.07 iter/s, 24.1546s/50 iter), loss = 0.0012297
I0130 23:03:11.167909 110538 solver.cpp:285]     Train net output #0: loss = 0.00122969 (* 1 = 0.00122969 loss)
I0130 23:03:11.170056 110538 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I0130 23:03:35.459615 110538 solver.cpp:266] Iteration 5600 (2.05857 iter/s, 24.2887s/50 iter), loss = 0.00290185
I0130 23:03:35.459656 110538 solver.cpp:285]     Train net output #0: loss = 0.00290184 (* 1 = 0.00290184 loss)
I0130 23:03:35.459664 110538 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0130 23:03:59.767086 110538 solver.cpp:266] Iteration 5650 (2.05706 iter/s, 24.3065s/50 iter), loss = 0.000584939
I0130 23:03:59.767211 110538 solver.cpp:285]     Train net output #0: loss = 0.000584934 (* 1 = 0.000584934 loss)
I0130 23:03:59.769328 110538 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I0130 23:04:24.170464 110538 solver.cpp:266] Iteration 5700 (2.04916 iter/s, 24.4002s/50 iter), loss = 0.00494931
I0130 23:04:24.170493 110538 solver.cpp:285]     Train net output #0: loss = 0.0049493 (* 1 = 0.0049493 loss)
I0130 23:04:24.172768 110538 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0130 23:04:48.443114 110538 solver.cpp:266] Iteration 5750 (2.0602 iter/s, 24.2694s/50 iter), loss = 0.0138222
I0130 23:04:48.443240 110538 solver.cpp:285]     Train net output #0: loss = 0.0138222 (* 1 = 0.0138222 loss)
I0130 23:04:48.445364 110538 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I0130 23:05:12.730881 110538 solver.cpp:266] Iteration 5800 (2.05892 iter/s, 24.2846s/50 iter), loss = 0.0111115
I0130 23:05:12.730922 110538 solver.cpp:285]     Train net output #0: loss = 0.0111115 (* 1 = 0.0111115 loss)
I0130 23:05:12.733080 110538 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0130 23:05:37.193423 110538 solver.cpp:266] Iteration 5850 (2.0442 iter/s, 24.4594s/50 iter), loss = 0.00152072
I0130 23:05:37.193554 110538 solver.cpp:285]     Train net output #0: loss = 0.00152071 (* 1 = 0.00152071 loss)
I0130 23:05:37.193562 110538 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I0130 23:06:01.558395 110538 solver.cpp:266] Iteration 5900 (2.05221 iter/s, 24.3639s/50 iter), loss = 0.0159568
I0130 23:06:01.558424 110538 solver.cpp:285]     Train net output #0: loss = 0.0159568 (* 1 = 0.0159568 loss)
I0130 23:06:01.560645 110538 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0130 23:06:25.846809 110538 solver.cpp:266] Iteration 5950 (2.05886 iter/s, 24.2853s/50 iter), loss = 0.00277159
I0130 23:06:25.846920 110538 solver.cpp:285]     Train net output #0: loss = 0.00277158 (* 1 = 0.00277158 loss)
I0130 23:06:25.849061 110538 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I0130 23:06:49.707008 110538 solver.cpp:418] Iteration 6000, Testing net (#0)
I0130 23:06:53.232784 110538 solver.cpp:517]     Test net output #0: loss = 0.183827 (* 1 = 0.183827 loss)
I0130 23:06:53.232802 110538 solver.cpp:517]     Test net output #1: top-1 = 0.954
I0130 23:06:53.786896 110538 solver.cpp:266] Iteration 6000 (1.78975 iter/s, 27.9368s/50 iter), loss = 0.000948976
I0130 23:06:53.786922 110538 solver.cpp:285]     Train net output #0: loss = 0.000948967 (* 1 = 0.000948967 loss)
I0130 23:06:53.789156 110538 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0130 23:07:18.080971 110538 solver.cpp:266] Iteration 6050 (2.05838 iter/s, 24.2909s/50 iter), loss = 0.00609734
I0130 23:07:18.081081 110538 solver.cpp:285]     Train net output #0: loss = 0.00609733 (* 1 = 0.00609733 loss)
I0130 23:07:18.083221 110538 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I0130 23:07:42.397106 110538 solver.cpp:266] Iteration 6100 (2.05651 iter/s, 24.313s/50 iter), loss = 0.0120467
I0130 23:07:42.397137 110538 solver.cpp:285]     Train net output #0: loss = 0.0120467 (* 1 = 0.0120467 loss)
I0130 23:07:42.399350 110538 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0130 23:08:06.844504 110538 solver.cpp:266] Iteration 6150 (2.04547 iter/s, 24.4443s/50 iter), loss = 0.00133217
I0130 23:08:06.844627 110538 solver.cpp:285]     Train net output #0: loss = 0.00133216 (* 1 = 0.00133216 loss)
I0130 23:08:06.844635 110538 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I0130 23:08:31.126451 110538 solver.cpp:266] Iteration 6200 (2.05923 iter/s, 24.2809s/50 iter), loss = 0.00212091
I0130 23:08:31.126482 110538 solver.cpp:285]     Train net output #0: loss = 0.0021209 (* 1 = 0.0021209 loss)
I0130 23:08:31.128700 110538 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0130 23:08:55.466543 110538 solver.cpp:266] Iteration 6250 (2.05449 iter/s, 24.3369s/50 iter), loss = 0.00353126
I0130 23:08:55.466603 110538 solver.cpp:285]     Train net output #0: loss = 0.00353125 (* 1 = 0.00353125 loss)
I0130 23:08:55.468799 110538 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I0130 23:09:19.604220 110538 solver.cpp:266] Iteration 6300 (2.07172 iter/s, 24.1345s/50 iter), loss = 0.000913855
I0130 23:09:19.604257 110538 solver.cpp:285]     Train net output #0: loss = 0.000913848 (* 1 = 0.000913848 loss)
I0130 23:09:19.606470 110538 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0130 23:09:43.973037 110538 solver.cpp:266] Iteration 6350 (2.05207 iter/s, 24.3657s/50 iter), loss = 0.00405226
I0130 23:09:43.973137 110538 solver.cpp:285]     Train net output #0: loss = 0.00405225 (* 1 = 0.00405225 loss)
I0130 23:09:43.975287 110538 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I0130 23:10:08.337101 110538 solver.cpp:266] Iteration 6400 (2.05247 iter/s, 24.3609s/50 iter), loss = 0.0261377
I0130 23:10:08.337129 110538 solver.cpp:285]     Train net output #0: loss = 0.0261377 (* 1 = 0.0261377 loss)
I0130 23:10:08.339342 110538 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0130 23:10:32.630403 110538 solver.cpp:266] Iteration 6450 (2.05845 iter/s, 24.2902s/50 iter), loss = 0.00518139
I0130 23:10:32.630457 110538 solver.cpp:285]     Train net output #0: loss = 0.00518139 (* 1 = 0.00518139 loss)
I0130 23:10:32.632675 110538 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I0130 23:10:57.073161 110538 solver.cpp:266] Iteration 6500 (2.04586 iter/s, 24.4396s/50 iter), loss = 0.00066706
I0130 23:10:57.073191 110538 solver.cpp:285]     Train net output #0: loss = 0.000667053 (* 1 = 0.000667053 loss)
I0130 23:10:57.075403 110538 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0130 23:11:21.475549 110538 solver.cpp:266] Iteration 6550 (2.04924 iter/s, 24.3992s/50 iter), loss = 0.00138701
I0130 23:11:21.475675 110538 solver.cpp:285]     Train net output #0: loss = 0.001387 (* 1 = 0.001387 loss)
I0130 23:11:21.477800 110538 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I0130 23:11:45.806499 110538 solver.cpp:266] Iteration 6600 (2.05526 iter/s, 24.3278s/50 iter), loss = 0.00648665
I0130 23:11:45.806527 110538 solver.cpp:285]     Train net output #0: loss = 0.00648664 (* 1 = 0.00648664 loss)
I0130 23:11:45.806596 110538 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0130 23:12:10.018817 110538 solver.cpp:266] Iteration 6650 (2.06515 iter/s, 24.2113s/50 iter), loss = 0.0010664
I0130 23:12:10.018935 110538 solver.cpp:285]     Train net output #0: loss = 0.0010664 (* 1 = 0.0010664 loss)
I0130 23:12:10.021068 110538 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I0130 23:12:34.307937 110538 solver.cpp:266] Iteration 6700 (2.0588 iter/s, 24.286s/50 iter), loss = 0.00354366
I0130 23:12:34.307970 110538 solver.cpp:285]     Train net output #0: loss = 0.00354366 (* 1 = 0.00354366 loss)
I0130 23:12:34.310145 110538 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0130 23:12:58.791538 110538 solver.cpp:266] Iteration 6750 (2.04244 iter/s, 24.4805s/50 iter), loss = 0.00970098
I0130 23:12:58.791592 110538 solver.cpp:285]     Train net output #0: loss = 0.00970098 (* 1 = 0.00970098 loss)
I0130 23:12:58.791632 110538 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I0130 23:13:23.135001 110538 solver.cpp:266] Iteration 6800 (2.05402 iter/s, 24.3425s/50 iter), loss = 0.00107812
I0130 23:13:23.135031 110538 solver.cpp:285]     Train net output #0: loss = 0.00107812 (* 1 = 0.00107812 loss)
I0130 23:13:23.137253 110538 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0130 23:13:47.424005 110538 solver.cpp:266] Iteration 6850 (2.05881 iter/s, 24.2859s/50 iter), loss = 0.00817227
I0130 23:13:47.424124 110538 solver.cpp:285]     Train net output #0: loss = 0.00817227 (* 1 = 0.00817227 loss)
I0130 23:13:47.426266 110538 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I0130 23:14:11.564716 110538 solver.cpp:266] Iteration 6900 (2.07146 iter/s, 24.1376s/50 iter), loss = 0.00368375
I0130 23:14:11.564752 110538 solver.cpp:285]     Train net output #0: loss = 0.00368374 (* 1 = 0.00368374 loss)
I0130 23:14:11.564759 110538 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0130 23:14:36.048748 110538 solver.cpp:266] Iteration 6950 (2.04223 iter/s, 24.4831s/50 iter), loss = 0.0178697
I0130 23:14:36.048869 110538 solver.cpp:285]     Train net output #0: loss = 0.0178697 (* 1 = 0.0178697 loss)
I0130 23:14:36.051007 110538 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I0130 23:14:59.866786 110538 solver.cpp:418] Iteration 7000, Testing net (#0)
I0130 23:15:03.435408 110538 solver.cpp:517]     Test net output #0: loss = 0.205146 (* 1 = 0.205146 loss)
I0130 23:15:03.435428 110538 solver.cpp:517]     Test net output #1: top-1 = 0.95125
I0130 23:15:03.844775 110538 solver.cpp:266] Iteration 7000 (1.79903 iter/s, 27.7927s/50 iter), loss = 0.00300997
I0130 23:15:03.844802 110538 solver.cpp:285]     Train net output #0: loss = 0.00300997 (* 1 = 0.00300997 loss)
I0130 23:15:03.847028 110538 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0130 23:15:28.177373 110538 solver.cpp:266] Iteration 7050 (2.05512 iter/s, 24.3294s/50 iter), loss = 0.00268787
I0130 23:15:28.177525 110538 solver.cpp:285]     Train net output #0: loss = 0.00268787 (* 1 = 0.00268787 loss)
I0130 23:15:28.177567 110538 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I0130 23:15:52.528105 110538 solver.cpp:266] Iteration 7100 (2.05342 iter/s, 24.3496s/50 iter), loss = 0.0194106
I0130 23:15:52.528139 110538 solver.cpp:285]     Train net output #0: loss = 0.0194106 (* 1 = 0.0194106 loss)
I0130 23:15:52.530359 110538 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0130 23:16:16.830742 110538 solver.cpp:266] Iteration 7150 (2.05766 iter/s, 24.2995s/50 iter), loss = 0.0127309
I0130 23:16:16.830881 110538 solver.cpp:285]     Train net output #0: loss = 0.0127309 (* 1 = 0.0127309 loss)
I0130 23:16:16.832996 110538 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I0130 23:16:41.118908 110538 solver.cpp:266] Iteration 7200 (2.05888 iter/s, 24.285s/50 iter), loss = 0.00280687
I0130 23:16:41.118937 110538 solver.cpp:285]     Train net output #0: loss = 0.00280687 (* 1 = 0.00280687 loss)
I0130 23:16:41.121147 110538 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0130 23:17:05.516713 110538 solver.cpp:266] Iteration 7250 (2.04963 iter/s, 24.3947s/50 iter), loss = 0.0015617
I0130 23:17:05.516850 110538 solver.cpp:285]     Train net output #0: loss = 0.0015617 (* 1 = 0.0015617 loss)
I0130 23:17:05.516858 110538 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I0130 23:17:29.802076 110538 solver.cpp:266] Iteration 7300 (2.05894 iter/s, 24.2843s/50 iter), loss = 0.000459204
I0130 23:17:29.802103 110538 solver.cpp:285]     Train net output #0: loss = 0.000459209 (* 1 = 0.000459209 loss)
I0130 23:17:29.804324 110538 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0130 23:17:54.088167 110538 solver.cpp:266] Iteration 7350 (2.05906 iter/s, 24.2829s/50 iter), loss = 0.000490192
I0130 23:17:54.088274 110538 solver.cpp:285]     Train net output #0: loss = 0.000490195 (* 1 = 0.000490195 loss)
I0130 23:17:54.090423 110538 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I0130 23:18:18.370446 110538 solver.cpp:266] Iteration 7400 (2.05938 iter/s, 24.2791s/50 iter), loss = 0.00456021
I0130 23:18:18.370477 110538 solver.cpp:285]     Train net output #0: loss = 0.00456022 (* 1 = 0.00456022 loss)
I0130 23:18:18.372689 110538 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0130 23:18:42.891381 110538 solver.cpp:266] Iteration 7450 (2.03934 iter/s, 24.5178s/50 iter), loss = 0.0120792
I0130 23:18:42.891495 110538 solver.cpp:285]     Train net output #0: loss = 0.0120792 (* 1 = 0.0120792 loss)
I0130 23:18:42.891502 110538 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I0130 23:19:07.182312 110538 solver.cpp:266] Iteration 7500 (2.05847 iter/s, 24.2899s/50 iter), loss = 0.000769137
I0130 23:19:07.182340 110538 solver.cpp:285]     Train net output #0: loss = 0.000769139 (* 1 = 0.000769139 loss)
I0130 23:19:07.184566 110538 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I0130 23:19:31.527956 110538 solver.cpp:266] Iteration 7550 (2.05402 iter/s, 24.3425s/50 iter), loss = 0.00299319
I0130 23:19:31.528054 110538 solver.cpp:285]     Train net output #0: loss = 0.00299319 (* 1 = 0.00299319 loss)
I0130 23:19:31.530210 110538 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I0130 23:19:55.747989 110538 solver.cpp:266] Iteration 7600 (2.06468 iter/s, 24.2169s/50 iter), loss = 0.00125449
I0130 23:19:55.748024 110538 solver.cpp:285]     Train net output #0: loss = 0.00125449 (* 1 = 0.00125449 loss)
I0130 23:19:55.750237 110538 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I0130 23:20:20.183502 110538 solver.cpp:266] Iteration 7650 (2.04647 iter/s, 24.4324s/50 iter), loss = 0.00142723
I0130 23:20:20.183642 110538 solver.cpp:285]     Train net output #0: loss = 0.00142723 (* 1 = 0.00142723 loss)
I0130 23:20:20.183666 110538 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I0130 23:20:44.512763 110538 solver.cpp:266] Iteration 7700 (2.05523 iter/s, 24.3282s/50 iter), loss = 0.00126211
I0130 23:20:44.512794 110538 solver.cpp:285]     Train net output #0: loss = 0.00126212 (* 1 = 0.00126212 loss)
I0130 23:20:44.515005 110538 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I0130 23:21:08.896541 110538 solver.cpp:266] Iteration 7750 (2.05081 iter/s, 24.3806s/50 iter), loss = 0.000403174
I0130 23:21:08.896596 110538 solver.cpp:285]     Train net output #0: loss = 0.000403176 (* 1 = 0.000403176 loss)
I0130 23:21:08.898790 110538 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I0130 23:21:33.039600 110538 solver.cpp:266] Iteration 7800 (2.07126 iter/s, 24.1399s/50 iter), loss = 0.0065951
I0130 23:21:33.039634 110538 solver.cpp:285]     Train net output #0: loss = 0.0065951 (* 1 = 0.0065951 loss)
I0130 23:21:33.041854 110538 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I0130 23:21:57.580533 110538 solver.cpp:266] Iteration 7850 (2.03767 iter/s, 24.5378s/50 iter), loss = 0.0160159
I0130 23:21:57.580667 110538 solver.cpp:285]     Train net output #0: loss = 0.0160159 (* 1 = 0.0160159 loss)
I0130 23:21:57.580675 110538 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I0130 23:22:21.915526 110538 solver.cpp:266] Iteration 7900 (2.05474 iter/s, 24.334s/50 iter), loss = 0.0056749
I0130 23:22:21.915563 110538 solver.cpp:285]     Train net output #0: loss = 0.0056749 (* 1 = 0.0056749 loss)
I0130 23:22:21.917784 110538 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I0130 23:22:46.272617 110538 solver.cpp:266] Iteration 7950 (2.05306 iter/s, 24.3539s/50 iter), loss = 0.00139198
I0130 23:22:46.272742 110538 solver.cpp:285]     Train net output #0: loss = 0.00139198 (* 1 = 0.00139198 loss)
I0130 23:22:46.274857 110538 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I0130 23:23:10.098484 110538 solver.cpp:418] Iteration 8000, Testing net (#0)
I0130 23:23:13.785394 110538 solver.cpp:517]     Test net output #0: loss = 0.217613 (* 1 = 0.217613 loss)
I0130 23:23:13.785413 110538 solver.cpp:517]     Test net output #1: top-1 = 0.95225
I0130 23:23:14.278157 110538 solver.cpp:266] Iteration 8000 (1.78557 iter/s, 28.0023s/50 iter), loss = 0.00433669
I0130 23:23:14.278192 110538 solver.cpp:285]     Train net output #0: loss = 0.00433669 (* 1 = 0.00433669 loss)
I0130 23:23:14.280417 110538 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I0130 23:23:38.544107 110538 solver.cpp:266] Iteration 8050 (2.06077 iter/s, 24.2628s/50 iter), loss = 0.00650102
I0130 23:23:38.544210 110538 solver.cpp:285]     Train net output #0: loss = 0.00650102 (* 1 = 0.00650102 loss)
I0130 23:23:38.544273 110538 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I0130 23:24:03.038352 110538 solver.cpp:266] Iteration 8100 (2.04139 iter/s, 24.4932s/50 iter), loss = 0.00714933
I0130 23:24:03.038383 110538 solver.cpp:285]     Train net output #0: loss = 0.00714932 (* 1 = 0.00714932 loss)
I0130 23:24:03.040601 110538 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I0130 23:24:27.316582 110538 solver.cpp:266] Iteration 8150 (2.05973 iter/s, 24.2751s/50 iter), loss = 0.00177371
I0130 23:24:27.316695 110538 solver.cpp:285]     Train net output #0: loss = 0.0017737 (* 1 = 0.0017737 loss)
I0130 23:24:27.318835 110538 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I0130 23:24:51.511842 110538 solver.cpp:266] Iteration 8200 (2.06679 iter/s, 24.1921s/50 iter), loss = 0.0025188
I0130 23:24:51.511875 110538 solver.cpp:285]     Train net output #0: loss = 0.00251879 (* 1 = 0.00251879 loss)
I0130 23:24:51.511881 110538 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I0130 23:25:16.027781 110538 solver.cpp:266] Iteration 8250 (2.03957 iter/s, 24.515s/50 iter), loss = 0.00464228
I0130 23:25:16.027848 110538 solver.cpp:285]     Train net output #0: loss = 0.00464227 (* 1 = 0.00464227 loss)
I0130 23:25:16.027855 110538 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I0130 23:25:40.526373 110538 solver.cpp:266] Iteration 8300 (2.04101 iter/s, 24.4976s/50 iter), loss = 0.000517927
I0130 23:25:40.526407 110538 solver.cpp:285]     Train net output #0: loss = 0.000517915 (* 1 = 0.000517915 loss)
I0130 23:25:40.528606 110538 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I0130 23:26:04.792866 110538 solver.cpp:266] Iteration 8350 (2.06072 iter/s, 24.2634s/50 iter), loss = 0.000911018
I0130 23:26:04.792986 110538 solver.cpp:285]     Train net output #0: loss = 0.000911004 (* 1 = 0.000911004 loss)
I0130 23:26:04.793030 110538 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I0130 23:26:29.112030 110538 solver.cpp:266] Iteration 8400 (2.05608 iter/s, 24.3181s/50 iter), loss = 0.00239337
I0130 23:26:29.112059 110538 solver.cpp:285]     Train net output #0: loss = 0.00239336 (* 1 = 0.00239336 loss)
I0130 23:26:29.114282 110538 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I0130 23:26:53.413084 110538 solver.cpp:266] Iteration 8450 (2.05779 iter/s, 24.2979s/50 iter), loss = 0.00399994
I0130 23:26:53.413215 110538 solver.cpp:285]     Train net output #0: loss = 0.00399993 (* 1 = 0.00399993 loss)
I0130 23:26:53.415331 110538 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I0130 23:27:17.758136 110538 solver.cpp:266] Iteration 8500 (2.05407 iter/s, 24.3419s/50 iter), loss = 0.00520751
I0130 23:27:17.758167 110538 solver.cpp:285]     Train net output #0: loss = 0.00520749 (* 1 = 0.00520749 loss)
I0130 23:27:17.758174 110538 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I0130 23:27:42.173153 110538 solver.cpp:266] Iteration 8550 (2.048 iter/s, 24.4141s/50 iter), loss = 0.00136709
I0130 23:27:42.173208 110538 solver.cpp:285]     Train net output #0: loss = 0.00136707 (* 1 = 0.00136707 loss)
I0130 23:27:42.175402 110538 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I0130 23:28:06.510763 110538 solver.cpp:266] Iteration 8600 (2.0547 iter/s, 24.3345s/50 iter), loss = 0.000537142
I0130 23:28:06.510793 110538 solver.cpp:285]     Train net output #0: loss = 0.000537126 (* 1 = 0.000537126 loss)
I0130 23:28:06.510840 110538 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I0130 23:28:30.859560 110538 solver.cpp:266] Iteration 8650 (2.05357 iter/s, 24.3478s/50 iter), loss = 0.000767136
I0130 23:28:30.859660 110538 solver.cpp:285]     Train net output #0: loss = 0.000767118 (* 1 = 0.000767118 loss)
I0130 23:28:30.861817 110538 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I0130 23:28:55.066870 110538 solver.cpp:266] Iteration 8700 (2.06576 iter/s, 24.2042s/50 iter), loss = 0.00284133
I0130 23:28:55.066900 110538 solver.cpp:285]     Train net output #0: loss = 0.00284131 (* 1 = 0.00284131 loss)
I0130 23:28:55.069111 110538 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I0130 23:29:19.517668 110538 solver.cpp:266] Iteration 8750 (2.04519 iter/s, 24.4477s/50 iter), loss = 0.0112439
I0130 23:29:19.517797 110538 solver.cpp:285]     Train net output #0: loss = 0.0112439 (* 1 = 0.0112439 loss)
I0130 23:29:19.517807 110538 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I0130 23:29:43.770936 110538 solver.cpp:266] Iteration 8800 (2.06166 iter/s, 24.2523s/50 iter), loss = 0.00563866
I0130 23:29:43.770964 110538 solver.cpp:285]     Train net output #0: loss = 0.00563864 (* 1 = 0.00563864 loss)
I0130 23:29:43.773178 110538 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I0130 23:30:08.124774 110538 solver.cpp:266] Iteration 8850 (2.05333 iter/s, 24.3507s/50 iter), loss = 0.00156981
I0130 23:30:08.124876 110538 solver.cpp:285]     Train net output #0: loss = 0.00156979 (* 1 = 0.00156979 loss)
I0130 23:30:08.127028 110538 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I0130 23:30:32.238849 110538 solver.cpp:266] Iteration 8900 (2.07375 iter/s, 24.1109s/50 iter), loss = 0.000366456
I0130 23:30:32.238879 110538 solver.cpp:285]     Train net output #0: loss = 0.000366439 (* 1 = 0.000366439 loss)
I0130 23:30:32.241093 110538 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I0130 23:30:56.643339 110538 solver.cpp:266] Iteration 8950 (2.04907 iter/s, 24.4013s/50 iter), loss = 0.00117007
I0130 23:30:56.643455 110538 solver.cpp:285]     Train net output #0: loss = 0.00117005 (* 1 = 0.00117005 loss)
I0130 23:30:56.643465 110538 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I0130 23:31:20.474179 110538 solver.cpp:418] Iteration 9000, Testing net (#0)
I0130 23:31:24.018851 110538 solver.cpp:517]     Test net output #0: loss = 0.225241 (* 1 = 0.225241 loss)
I0130 23:31:24.018869 110538 solver.cpp:517]     Test net output #1: top-1 = 0.9525
I0130 23:31:24.506800 110538 solver.cpp:266] Iteration 9000 (1.79454 iter/s, 27.8623s/50 iter), loss = 0.00041271
I0130 23:31:24.506832 110538 solver.cpp:285]     Train net output #0: loss = 0.000412692 (* 1 = 0.000412692 loss)
I0130 23:31:24.509063 110538 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I0130 23:31:48.767952 110538 solver.cpp:266] Iteration 9050 (2.06118 iter/s, 24.258s/50 iter), loss = 0.0273366
I0130 23:31:48.768064 110538 solver.cpp:285]     Train net output #0: loss = 0.0273366 (* 1 = 0.0273366 loss)
I0130 23:31:48.768102 110538 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I0130 23:32:13.293107 110538 solver.cpp:266] Iteration 9100 (2.03881 iter/s, 24.5241s/50 iter), loss = 0.00122473
I0130 23:32:13.293139 110538 solver.cpp:285]     Train net output #0: loss = 0.0012247 (* 1 = 0.0012247 loss)
I0130 23:32:13.293179 110538 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I0130 23:32:37.492403 110538 solver.cpp:266] Iteration 9150 (2.06626 iter/s, 24.1983s/50 iter), loss = 0.00269595
I0130 23:32:37.492502 110538 solver.cpp:285]     Train net output #0: loss = 0.00269593 (* 1 = 0.00269593 loss)
I0130 23:32:37.492557 110538 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I0130 23:33:01.755452 110538 solver.cpp:266] Iteration 9200 (2.06083 iter/s, 24.262s/50 iter), loss = 0.00220607
I0130 23:33:01.755483 110538 solver.cpp:285]     Train net output #0: loss = 0.00220605 (* 1 = 0.00220605 loss)
I0130 23:33:01.757709 110538 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I0130 23:33:26.120604 110538 solver.cpp:266] Iteration 9250 (2.05238 iter/s, 24.362s/50 iter), loss = 0.00612819
I0130 23:33:26.120656 110538 solver.cpp:285]     Train net output #0: loss = 0.00612817 (* 1 = 0.00612817 loss)
I0130 23:33:26.122853 110538 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I0130 23:33:50.639670 110538 solver.cpp:266] Iteration 9300 (2.03949 iter/s, 24.5159s/50 iter), loss = 0.00982402
I0130 23:33:50.639701 110538 solver.cpp:285]     Train net output #0: loss = 0.00982399 (* 1 = 0.00982399 loss)
I0130 23:33:50.641934 110538 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I0130 23:34:14.873682 110538 solver.cpp:266] Iteration 9350 (2.06348 iter/s, 24.2309s/50 iter), loss = 0.00505045
I0130 23:34:14.873783 110538 solver.cpp:285]     Train net output #0: loss = 0.00505043 (* 1 = 0.00505043 loss)
I0130 23:34:14.873837 110538 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I0130 23:34:39.301009 110538 solver.cpp:266] Iteration 9400 (2.04698 iter/s, 24.4263s/50 iter), loss = 0.00174591
I0130 23:34:39.301038 110538 solver.cpp:285]     Train net output #0: loss = 0.00174588 (* 1 = 0.00174588 loss)
I0130 23:34:39.303263 110538 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I0130 23:35:03.420677 110538 solver.cpp:266] Iteration 9450 (2.07327 iter/s, 24.1165s/50 iter), loss = 0.0109963
I0130 23:35:03.420806 110538 solver.cpp:285]     Train net output #0: loss = 0.0109963 (* 1 = 0.0109963 loss)
I0130 23:35:03.420846 110538 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I0130 23:35:27.782656 110538 solver.cpp:266] Iteration 9500 (2.05247 iter/s, 24.3609s/50 iter), loss = 0.00200146
I0130 23:35:27.782690 110538 solver.cpp:285]     Train net output #0: loss = 0.00200143 (* 1 = 0.00200143 loss)
I0130 23:35:27.784914 110538 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I0130 23:35:52.102955 110538 solver.cpp:266] Iteration 9550 (2.05616 iter/s, 24.3171s/50 iter), loss = 0.0016577
I0130 23:35:52.103055 110538 solver.cpp:285]     Train net output #0: loss = 0.00165767 (* 1 = 0.00165767 loss)
I0130 23:35:52.105196 110538 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I0130 23:36:16.356045 110538 solver.cpp:266] Iteration 9600 (2.06186 iter/s, 24.25s/50 iter), loss = 0.000847416
I0130 23:36:16.356075 110538 solver.cpp:285]     Train net output #0: loss = 0.000847388 (* 1 = 0.000847388 loss)
I0130 23:36:16.358300 110538 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I0130 23:36:40.494771 110538 solver.cpp:266] Iteration 9650 (2.07163 iter/s, 24.1356s/50 iter), loss = 0.00132332
I0130 23:36:40.494925 110538 solver.cpp:285]     Train net output #0: loss = 0.00132329 (* 1 = 0.00132329 loss)
I0130 23:36:40.495097 110538 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I0130 23:37:04.982120 110538 solver.cpp:266] Iteration 9700 (2.04197 iter/s, 24.4861s/50 iter), loss = 0.00938169
I0130 23:37:04.982151 110538 solver.cpp:285]     Train net output #0: loss = 0.00938166 (* 1 = 0.00938166 loss)
I0130 23:37:04.984349 110538 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I0130 23:37:29.257867 110538 solver.cpp:266] Iteration 9750 (2.05993 iter/s, 24.2726s/50 iter), loss = 0.00158677
I0130 23:37:29.257992 110538 solver.cpp:285]     Train net output #0: loss = 0.00158674 (* 1 = 0.00158674 loss)
I0130 23:37:29.260161 110538 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I0130 23:37:53.347388 110538 solver.cpp:266] Iteration 9800 (2.07587 iter/s, 24.0863s/50 iter), loss = 0.00157745
I0130 23:37:53.347419 110538 solver.cpp:285]     Train net output #0: loss = 0.00157742 (* 1 = 0.00157742 loss)
I0130 23:37:53.349633 110538 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I0130 23:38:17.772225 110538 solver.cpp:266] Iteration 9850 (2.04736 iter/s, 24.4217s/50 iter), loss = 0.00109433
I0130 23:38:17.772286 110538 solver.cpp:285]     Train net output #0: loss = 0.0010943 (* 1 = 0.0010943 loss)
I0130 23:38:17.772310 110538 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I0130 23:38:42.113204 110538 solver.cpp:266] Iteration 9900 (2.05423 iter/s, 24.34s/50 iter), loss = 0.00318769
I0130 23:38:42.113235 110538 solver.cpp:285]     Train net output #0: loss = 0.00318766 (* 1 = 0.00318766 loss)
I0130 23:38:42.115453 110538 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I0130 23:39:06.428803 110538 solver.cpp:266] Iteration 9950 (2.05656 iter/s, 24.3125s/50 iter), loss = 0.00367585
I0130 23:39:06.428930 110538 solver.cpp:285]     Train net output #0: loss = 0.00367582 (* 1 = 0.00367582 loss)
I0130 23:39:06.431066 110538 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I0130 23:39:30.245704 110538 solver.cpp:418] Iteration 10000, Testing net (#0)
I0130 23:39:33.960418 110538 solver.cpp:517]     Test net output #0: loss = 0.229336 (* 1 = 0.229336 loss)
I0130 23:39:33.960436 110538 solver.cpp:517]     Test net output #1: top-1 = 0.9525
I0130 23:39:34.449717 110538 solver.cpp:266] Iteration 10000 (1.78459 iter/s, 28.0176s/50 iter), loss = 0.00265081
I0130 23:39:34.449740 110538 solver.cpp:285]     Train net output #0: loss = 0.00265078 (* 1 = 0.00265078 loss)
I0130 23:39:34.451977 110538 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I0130 23:39:58.712146 110538 solver.cpp:266] Iteration 10050 (2.06107 iter/s, 24.2593s/50 iter), loss = 0.000974824
I0130 23:39:58.712208 110538 solver.cpp:285]     Train net output #0: loss = 0.000974795 (* 1 = 0.000974795 loss)
I0130 23:39:58.712271 110538 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I0130 23:40:23.088280 110538 solver.cpp:266] Iteration 10100 (2.05127 iter/s, 24.3751s/50 iter), loss = 0.00191103
I0130 23:40:23.088310 110538 solver.cpp:285]     Train net output #0: loss = 0.001911 (* 1 = 0.001911 loss)
I0130 23:40:23.090531 110538 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I0130 23:40:47.260144 110538 solver.cpp:266] Iteration 10150 (2.06879 iter/s, 24.1687s/50 iter), loss = 0.0189796
I0130 23:40:47.260257 110538 solver.cpp:285]     Train net output #0: loss = 0.0189795 (* 1 = 0.0189795 loss)
I0130 23:40:47.262399 110538 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I0130 23:41:11.614118 110538 solver.cpp:266] Iteration 10200 (2.05332 iter/s, 24.3508s/50 iter), loss = 0.00870482
I0130 23:41:11.614148 110538 solver.cpp:285]     Train net output #0: loss = 0.0087048 (* 1 = 0.0087048 loss)
I0130 23:41:11.614194 110538 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I0130 23:41:35.953738 110538 solver.cpp:266] Iteration 10250 (2.05435 iter/s, 24.3386s/50 iter), loss = 0.000780598
I0130 23:41:35.953886 110538 solver.cpp:285]     Train net output #0: loss = 0.000780573 (* 1 = 0.000780573 loss)
I0130 23:41:35.955991 110538 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I0130 23:42:00.288390 110538 solver.cpp:266] Iteration 10300 (2.05495 iter/s, 24.3315s/50 iter), loss = 0.00181204
I0130 23:42:00.288419 110538 solver.cpp:285]     Train net output #0: loss = 0.00181201 (* 1 = 0.00181201 loss)
I0130 23:42:00.290644 110538 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I0130 23:42:24.449219 110538 solver.cpp:266] Iteration 10350 (2.06974 iter/s, 24.1577s/50 iter), loss = 0.0151733
I0130 23:42:24.449352 110538 solver.cpp:285]     Train net output #0: loss = 0.0151733 (* 1 = 0.0151733 loss)
I0130 23:42:24.451470 110538 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I0130 23:42:48.905308 110538 solver.cpp:266] Iteration 10400 (2.04474 iter/s, 24.4529s/50 iter), loss = 0.00060662
I0130 23:42:48.905339 110538 solver.cpp:285]     Train net output #0: loss = 0.000606595 (* 1 = 0.000606595 loss)
I0130 23:42:48.907594 110538 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I0130 23:43:13.093075 110538 solver.cpp:266] Iteration 10450 (2.06743 iter/s, 24.1846s/50 iter), loss = 0.00243732
I0130 23:43:13.093186 110538 solver.cpp:285]     Train net output #0: loss = 0.0024373 (* 1 = 0.0024373 loss)
I0130 23:43:13.093227 110538 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I0130 23:43:37.498083 110538 solver.cpp:266] Iteration 10500 (2.04885 iter/s, 24.404s/50 iter), loss = 0.00460525
I0130 23:43:37.498112 110538 solver.cpp:285]     Train net output #0: loss = 0.00460523 (* 1 = 0.00460523 loss)
I0130 23:43:37.500334 110538 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I0130 23:44:01.752343 110538 solver.cpp:266] Iteration 10550 (2.06176 iter/s, 24.2511s/50 iter), loss = 0.000445444
I0130 23:44:01.752395 110538 solver.cpp:285]     Train net output #0: loss = 0.00044542 (* 1 = 0.00044542 loss)
I0130 23:44:01.754592 110538 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I0130 23:44:26.108163 110538 solver.cpp:266] Iteration 10600 (2.05316 iter/s, 24.3527s/50 iter), loss = 0.00482517
I0130 23:44:26.108193 110538 solver.cpp:285]     Train net output #0: loss = 0.00482514 (* 1 = 0.00482514 loss)
I0130 23:44:26.108536 110538 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I0130 23:44:50.384411 110538 solver.cpp:266] Iteration 10650 (2.05973 iter/s, 24.275s/50 iter), loss = 0.00697479
I0130 23:44:50.384516 110538 solver.cpp:285]     Train net output #0: loss = 0.00697477 (* 1 = 0.00697477 loss)
I0130 23:44:50.384558 110538 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I0130 23:45:14.666800 110538 solver.cpp:266] Iteration 10700 (2.05919 iter/s, 24.2813s/50 iter), loss = 0.00238018
I0130 23:45:14.666831 110538 solver.cpp:285]     Train net output #0: loss = 0.00238016 (* 1 = 0.00238016 loss)
I0130 23:45:14.669059 110538 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I0130 23:45:38.901070 110538 solver.cpp:266] Iteration 10750 (2.06346 iter/s, 24.2311s/50 iter), loss = 0.013709
I0130 23:45:38.901192 110538 solver.cpp:285]     Train net output #0: loss = 0.013709 (* 1 = 0.013709 loss)
I0130 23:45:38.903317 110538 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I0130 23:46:03.319679 110538 solver.cpp:266] Iteration 10800 (2.04788 iter/s, 24.4155s/50 iter), loss = 0.00332539
I0130 23:46:03.319715 110538 solver.cpp:285]     Train net output #0: loss = 0.00332537 (* 1 = 0.00332537 loss)
I0130 23:46:03.319721 110538 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I0130 23:46:27.840530 110538 solver.cpp:266] Iteration 10850 (2.03916 iter/s, 24.5199s/50 iter), loss = 0.000587637
I0130 23:46:27.840579 110538 solver.cpp:285]     Train net output #0: loss = 0.000587615 (* 1 = 0.000587615 loss)
I0130 23:46:27.840922 110538 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I0130 23:46:52.026604 110538 solver.cpp:266] Iteration 10900 (2.06742 iter/s, 24.1848s/50 iter), loss = 0.00759735
I0130 23:46:52.026634 110538 solver.cpp:285]     Train net output #0: loss = 0.00759732 (* 1 = 0.00759732 loss)
I0130 23:46:52.028849 110538 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I0130 23:47:16.171921 110538 solver.cpp:266] Iteration 10950 (2.07106 iter/s, 24.1422s/50 iter), loss = 0.00257885
I0130 23:47:16.172070 110538 solver.cpp:285]     Train net output #0: loss = 0.00257883 (* 1 = 0.00257883 loss)
I0130 23:47:16.174177 110538 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I0130 23:47:40.043009 110538 solver.cpp:418] Iteration 11000, Testing net (#0)
I0130 23:47:43.562604 110538 solver.cpp:517]     Test net output #0: loss = 0.231407 (* 1 = 0.231407 loss)
I0130 23:47:43.562625 110538 solver.cpp:517]     Test net output #1: top-1 = 0.95225
I0130 23:47:44.115471 110538 solver.cpp:266] Iteration 11000 (1.78953 iter/s, 27.9403s/50 iter), loss = 0.00094317
I0130 23:47:44.115501 110538 solver.cpp:285]     Train net output #0: loss = 0.000943149 (* 1 = 0.000943149 loss)
I0130 23:47:44.117719 110538 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I0130 23:48:08.483808 110538 solver.cpp:266] Iteration 11050 (2.05211 iter/s, 24.3652s/50 iter), loss = 0.00134169
I0130 23:48:08.483919 110538 solver.cpp:285]     Train net output #0: loss = 0.00134167 (* 1 = 0.00134167 loss)
I0130 23:48:08.486065 110538 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I0130 23:48:32.751029 110538 solver.cpp:266] Iteration 11100 (2.06066 iter/s, 24.2641s/50 iter), loss = 0.00284841
I0130 23:48:32.751085 110538 solver.cpp:285]     Train net output #0: loss = 0.00284838 (* 1 = 0.00284838 loss)
I0130 23:48:32.751091 110538 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I0130 23:48:57.126781 110538 solver.cpp:266] Iteration 11150 (2.0513 iter/s, 24.3748s/50 iter), loss = 0.00392104
I0130 23:48:57.126850 110538 solver.cpp:285]     Train net output #0: loss = 0.00392101 (* 1 = 0.00392101 loss)
I0130 23:48:57.129036 110538 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I0130 23:49:21.360626 110538 solver.cpp:266] Iteration 11200 (2.0635 iter/s, 24.2307s/50 iter), loss = 0.00116783
I0130 23:49:21.360656 110538 solver.cpp:285]     Train net output #0: loss = 0.00116781 (* 1 = 0.00116781 loss)
I0130 23:49:21.362874 110538 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I0130 23:49:45.527034 110538 solver.cpp:266] Iteration 11250 (2.06926 iter/s, 24.1633s/50 iter), loss = 0.00545294
I0130 23:49:45.527159 110538 solver.cpp:285]     Train net output #0: loss = 0.00545292 (* 1 = 0.00545292 loss)
I0130 23:49:45.529232 110538 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I0130 23:50:09.967036 110538 solver.cpp:266] Iteration 11300 (2.04609 iter/s, 24.4369s/50 iter), loss = 0.00907663
I0130 23:50:09.967067 110538 solver.cpp:285]     Train net output #0: loss = 0.00907661 (* 1 = 0.00907661 loss)
I0130 23:50:09.969288 110538 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I0130 23:50:34.242168 110538 solver.cpp:266] Iteration 11350 (2.05999 iter/s, 24.272s/50 iter), loss = 0.00399161
I0130 23:50:34.242280 110538 solver.cpp:285]     Train net output #0: loss = 0.00399158 (* 1 = 0.00399158 loss)
I0130 23:50:34.244418 110538 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I0130 23:50:58.485790 110538 solver.cpp:266] Iteration 11400 (2.06267 iter/s, 24.2405s/50 iter), loss = 0.000746415
I0130 23:50:58.485819 110538 solver.cpp:285]     Train net output #0: loss = 0.000746389 (* 1 = 0.000746389 loss)
I0130 23:50:58.488036 110538 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I0130 23:51:22.769618 110538 solver.cpp:266] Iteration 11450 (2.05925 iter/s, 24.2807s/50 iter), loss = 0.000760918
I0130 23:51:22.769747 110538 solver.cpp:285]     Train net output #0: loss = 0.000760891 (* 1 = 0.000760891 loss)
I0130 23:51:22.769754 110538 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I0130 23:51:47.236609 110538 solver.cpp:266] Iteration 11500 (2.04366 iter/s, 24.466s/50 iter), loss = 0.000367235
I0130 23:51:47.236641 110538 solver.cpp:285]     Train net output #0: loss = 0.000367209 (* 1 = 0.000367209 loss)
I0130 23:51:47.238873 110538 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I0130 23:52:11.508564 110538 solver.cpp:266] Iteration 11550 (2.06026 iter/s, 24.2688s/50 iter), loss = 0.000649024
I0130 23:52:11.508707 110538 solver.cpp:285]     Train net output #0: loss = 0.000648997 (* 1 = 0.000648997 loss)
I0130 23:52:11.510895 110538 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I0130 23:52:35.865438 110538 solver.cpp:266] Iteration 11600 (2.05308 iter/s, 24.3536s/50 iter), loss = 0.00612529
I0130 23:52:35.865468 110538 solver.cpp:285]     Train net output #0: loss = 0.00612526 (* 1 = 0.00612526 loss)
I0130 23:52:35.867691 110538 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I0130 23:53:00.039963 110538 solver.cpp:266] Iteration 11650 (2.06856 iter/s, 24.1714s/50 iter), loss = 0.00288803
I0130 23:53:00.040076 110538 solver.cpp:285]     Train net output #0: loss = 0.002888 (* 1 = 0.002888 loss)
I0130 23:53:00.040115 110538 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I0130 23:53:24.473947 110538 solver.cpp:266] Iteration 11700 (2.04642 iter/s, 24.4329s/50 iter), loss = 0.00477759
I0130 23:53:24.473978 110538 solver.cpp:285]     Train net output #0: loss = 0.00477756 (* 1 = 0.00477756 loss)
I0130 23:53:24.474020 110538 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I0130 23:53:48.819581 110538 solver.cpp:266] Iteration 11750 (2.05384 iter/s, 24.3447s/50 iter), loss = 0.0149781
I0130 23:53:48.819706 110538 solver.cpp:285]     Train net output #0: loss = 0.014978 (* 1 = 0.014978 loss)
I0130 23:53:48.821832 110538 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I0130 23:54:13.206408 110538 solver.cpp:266] Iteration 11800 (2.05055 iter/s, 24.3837s/50 iter), loss = 0.000931193
I0130 23:54:13.206437 110538 solver.cpp:285]     Train net output #0: loss = 0.000931164 (* 1 = 0.000931164 loss)
I0130 23:54:13.206488 110538 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I0130 23:54:37.491117 110538 solver.cpp:266] Iteration 11850 (2.05899 iter/s, 24.2837s/50 iter), loss = 0.00646609
I0130 23:54:37.491174 110538 solver.cpp:285]     Train net output #0: loss = 0.00646606 (* 1 = 0.00646606 loss)
I0130 23:54:37.493377 110538 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I0130 23:55:01.666296 110538 solver.cpp:266] Iteration 11900 (2.06851 iter/s, 24.172s/50 iter), loss = 0.0233169
I0130 23:55:01.666327 110538 solver.cpp:285]     Train net output #0: loss = 0.0233169 (* 1 = 0.0233169 loss)
I0130 23:55:01.668536 110538 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I0130 23:55:26.122707 110538 solver.cpp:266] Iteration 11950 (2.04472 iter/s, 24.4533s/50 iter), loss = 0.00264538
I0130 23:55:26.122829 110538 solver.cpp:285]     Train net output #0: loss = 0.00264535 (* 1 = 0.00264535 loss)
I0130 23:55:26.122836 110538 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I0130 23:55:49.894995 110538 solver.cpp:929] Snapshotting to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/snapshots/_iter_12000.caffemodel
I0130 23:55:52.395813 110538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.5/snapshots/_iter_12000.solverstate
I0130 23:55:53.099025 110538 solver.cpp:378] Iteration 12000, loss = 0.00245271
I0130 23:55:53.099050 110538 solver.cpp:418] Iteration 12000, Testing net (#0)
I0130 23:55:56.463121 110538 solver.cpp:517]     Test net output #0: loss = 0.2327 (* 1 = 0.2327 loss)
I0130 23:55:56.463173 110538 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I0130 23:55:56.463179 110538 solver.cpp:386] Optimization Done (2.06897 iter/s).
I0130 23:55:56.463182 110538 caffe_interface.cpp:530] Optimization Done.

## compression: 6-th run
$PRUNE_ROOT/deephi_compress compress -config ${WORK_DIR}/config6.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress6_alexnetBNnoLRN.txt
I0130 23:55:57.448175 110996 pruning_runner.cpp:190] Sens info found, use it.
I0130 23:55:58.712412 110996 pruning_runner.cpp:217] Start compressing, please wait...
I0130 23:56:05.720430 110996 pruning_runner.cpp:264] Compression complete 0.199943%
I0130 23:56:12.071804 110996 pruning_runner.cpp:264] Compression complete 0.399088%
I0130 23:56:18.222081 110996 pruning_runner.cpp:264] Compression complete 99.6109%
I0130 23:56:25.025650 110996 pruning_runner.cpp:264] Compression complete 99.8051%
I0130 23:56:32.262657 110996 pruning_runner.cpp:264] Compression complete 99.9025%
I0130 23:56:38.533955 110996 pruning_runner.cpp:264] Compression complete 99.9878%
I0130 23:56:44.952900 110996 pruning_runner.cpp:264] Compression complete 99.9939%
I0130 23:56:51.306162 110996 pruning_runner.cpp:264] Compression complete 99.9969%
I0130 23:56:57.468356 110996 pruning_runner.cpp:264] Compression complete 99.9998%
I0130 23:57:03.742259 110996 pruning_runner.cpp:264] Compression complete 99.9999%
I0130 23:57:09.935024 110996 pruning_runner.cpp:264] Compression complete 100%
I0130 23:57:16.175103 110996 pruning_runner.cpp:264] Compression complete 100%
I0130 23:57:22.312069 110996 caffe_interface.cpp:66] Use GPU with device ID 0
I0130 23:57:22.312376 110996 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0130 23:57:22.312719 110996 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0130 23:57:22.312880 110996 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0130 23:57:22.312983 110996 layer_factory.hpp:77] Creating layer data
I0130 23:57:22.313019 110996 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:22.313418 110996 net.cpp:94] Creating Layer data
I0130 23:57:22.313426 110996 net.cpp:409] data -> data
I0130 23:57:22.313433 110996 net.cpp:409] data -> label
I0130 23:57:22.314939 112207 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0130 23:57:22.314973 112207 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0130 23:57:22.315131 110996 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0130 23:57:22.315198 110996 data_layer.cpp:83] output data size: 50,3,227,227
I0130 23:57:22.407054 110996 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:22.407124 110996 net.cpp:144] Setting up data
I0130 23:57:22.407130 110996 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0130 23:57:22.407133 110996 net.cpp:151] Top shape: 50 (50)
I0130 23:57:22.407135 110996 net.cpp:159] Memory required for data: 30917600
I0130 23:57:22.407140 110996 layer_factory.hpp:77] Creating layer label_data_1_split
I0130 23:57:22.407150 110996 net.cpp:94] Creating Layer label_data_1_split
I0130 23:57:22.407152 110996 net.cpp:435] label_data_1_split <- label
I0130 23:57:22.407157 110996 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0130 23:57:22.407181 110996 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0130 23:57:22.407238 110996 net.cpp:144] Setting up label_data_1_split
I0130 23:57:22.407243 110996 net.cpp:151] Top shape: 50 (50)
I0130 23:57:22.407245 110996 net.cpp:151] Top shape: 50 (50)
I0130 23:57:22.407266 110996 net.cpp:159] Memory required for data: 30918000
I0130 23:57:22.407269 110996 layer_factory.hpp:77] Creating layer conv1
I0130 23:57:22.407279 110996 net.cpp:94] Creating Layer conv1
I0130 23:57:22.407281 110996 net.cpp:435] conv1 <- data
I0130 23:57:22.407286 110996 net.cpp:409] conv1 -> conv1
I0130 23:57:22.408942 110996 net.cpp:144] Setting up conv1
I0130 23:57:22.408953 110996 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:22.408957 110996 net.cpp:159] Memory required for data: 88998000
I0130 23:57:22.408982 110996 layer_factory.hpp:77] Creating layer bn1
I0130 23:57:22.408991 110996 net.cpp:94] Creating Layer bn1
I0130 23:57:22.408993 110996 net.cpp:435] bn1 <- conv1
I0130 23:57:22.408999 110996 net.cpp:409] bn1 -> scale1
I0130 23:57:22.409785 110996 net.cpp:144] Setting up bn1
I0130 23:57:22.409791 110996 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:22.409795 110996 net.cpp:159] Memory required for data: 147078000
I0130 23:57:22.409803 110996 layer_factory.hpp:77] Creating layer relu1
I0130 23:57:22.409809 110996 net.cpp:94] Creating Layer relu1
I0130 23:57:22.409812 110996 net.cpp:435] relu1 <- scale1
I0130 23:57:22.409816 110996 net.cpp:409] relu1 -> relu1
I0130 23:57:22.409843 110996 net.cpp:144] Setting up relu1
I0130 23:57:22.409848 110996 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:22.409850 110996 net.cpp:159] Memory required for data: 205158000
I0130 23:57:22.409853 110996 layer_factory.hpp:77] Creating layer pool1
I0130 23:57:22.409858 110996 net.cpp:94] Creating Layer pool1
I0130 23:57:22.409862 110996 net.cpp:435] pool1 <- relu1
I0130 23:57:22.409865 110996 net.cpp:409] pool1 -> pool1
I0130 23:57:22.409932 110996 net.cpp:144] Setting up pool1
I0130 23:57:22.409937 110996 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0130 23:57:22.409941 110996 net.cpp:159] Memory required for data: 219154800
I0130 23:57:22.409943 110996 layer_factory.hpp:77] Creating layer conv2
I0130 23:57:22.409950 110996 net.cpp:94] Creating Layer conv2
I0130 23:57:22.409952 110996 net.cpp:435] conv2 <- pool1
I0130 23:57:22.409956 110996 net.cpp:409] conv2 -> conv2
I0130 23:57:22.418179 110996 net.cpp:144] Setting up conv2
I0130 23:57:22.418205 110996 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:22.418207 110996 net.cpp:159] Memory required for data: 256479600
I0130 23:57:22.418220 110996 layer_factory.hpp:77] Creating layer bn2
I0130 23:57:22.418231 110996 net.cpp:94] Creating Layer bn2
I0130 23:57:22.418236 110996 net.cpp:435] bn2 <- conv2
I0130 23:57:22.418241 110996 net.cpp:409] bn2 -> scale2
I0130 23:57:22.418857 110996 net.cpp:144] Setting up bn2
I0130 23:57:22.418864 110996 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:22.418867 110996 net.cpp:159] Memory required for data: 293804400
I0130 23:57:22.418875 110996 layer_factory.hpp:77] Creating layer relu2
I0130 23:57:22.418882 110996 net.cpp:94] Creating Layer relu2
I0130 23:57:22.418885 110996 net.cpp:435] relu2 <- scale2
I0130 23:57:22.418889 110996 net.cpp:409] relu2 -> relu2
I0130 23:57:22.418907 110996 net.cpp:144] Setting up relu2
I0130 23:57:22.418911 110996 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:22.418915 110996 net.cpp:159] Memory required for data: 331129200
I0130 23:57:22.418917 110996 layer_factory.hpp:77] Creating layer pool2
I0130 23:57:22.418922 110996 net.cpp:94] Creating Layer pool2
I0130 23:57:22.418926 110996 net.cpp:435] pool2 <- relu2
I0130 23:57:22.418929 110996 net.cpp:409] pool2 -> pool2
I0130 23:57:22.418956 110996 net.cpp:144] Setting up pool2
I0130 23:57:22.418961 110996 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:22.418964 110996 net.cpp:159] Memory required for data: 339782000
I0130 23:57:22.418967 110996 layer_factory.hpp:77] Creating layer conv3
I0130 23:57:22.418974 110996 net.cpp:94] Creating Layer conv3
I0130 23:57:22.418977 110996 net.cpp:435] conv3 <- pool2
I0130 23:57:22.418983 110996 net.cpp:409] conv3 -> conv3
I0130 23:57:22.433590 110996 net.cpp:144] Setting up conv3
I0130 23:57:22.433640 110996 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:22.433645 110996 net.cpp:159] Memory required for data: 352761200
I0130 23:57:22.433657 110996 layer_factory.hpp:77] Creating layer relu3
I0130 23:57:22.433670 110996 net.cpp:94] Creating Layer relu3
I0130 23:57:22.433676 110996 net.cpp:435] relu3 <- conv3
I0130 23:57:22.433686 110996 net.cpp:409] relu3 -> relu3
I0130 23:57:22.433718 110996 net.cpp:144] Setting up relu3
I0130 23:57:22.433745 110996 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:22.433769 110996 net.cpp:159] Memory required for data: 365740400
I0130 23:57:22.433787 110996 layer_factory.hpp:77] Creating layer conv4
I0130 23:57:22.433812 110996 net.cpp:94] Creating Layer conv4
I0130 23:57:22.433827 110996 net.cpp:435] conv4 <- relu3
I0130 23:57:22.433845 110996 net.cpp:409] conv4 -> conv4
I0130 23:57:22.448984 110996 net.cpp:144] Setting up conv4
I0130 23:57:22.449007 110996 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:22.449009 110996 net.cpp:159] Memory required for data: 378719600
I0130 23:57:22.449035 110996 layer_factory.hpp:77] Creating layer relu4
I0130 23:57:22.449041 110996 net.cpp:94] Creating Layer relu4
I0130 23:57:22.449045 110996 net.cpp:435] relu4 <- conv4
I0130 23:57:22.449062 110996 net.cpp:409] relu4 -> relu4
I0130 23:57:22.449086 110996 net.cpp:144] Setting up relu4
I0130 23:57:22.449090 110996 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:22.449095 110996 net.cpp:159] Memory required for data: 391698800
I0130 23:57:22.449097 110996 layer_factory.hpp:77] Creating layer conv5
I0130 23:57:22.449108 110996 net.cpp:94] Creating Layer conv5
I0130 23:57:22.449116 110996 net.cpp:435] conv5 <- relu4
I0130 23:57:22.449122 110996 net.cpp:409] conv5 -> conv5
I0130 23:57:22.459805 110996 net.cpp:144] Setting up conv5
I0130 23:57:22.459826 110996 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:22.459828 110996 net.cpp:159] Memory required for data: 400351600
I0130 23:57:22.459836 110996 layer_factory.hpp:77] Creating layer relu5
I0130 23:57:22.459843 110996 net.cpp:94] Creating Layer relu5
I0130 23:57:22.459847 110996 net.cpp:435] relu5 <- conv5
I0130 23:57:22.459853 110996 net.cpp:409] relu5 -> relu5
I0130 23:57:22.459885 110996 net.cpp:144] Setting up relu5
I0130 23:57:22.459892 110996 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:22.459894 110996 net.cpp:159] Memory required for data: 409004400
I0130 23:57:22.459898 110996 layer_factory.hpp:77] Creating layer pool5
I0130 23:57:22.459908 110996 net.cpp:94] Creating Layer pool5
I0130 23:57:22.459911 110996 net.cpp:435] pool5 <- relu5
I0130 23:57:22.459916 110996 net.cpp:409] pool5 -> pool5
I0130 23:57:22.459947 110996 net.cpp:144] Setting up pool5
I0130 23:57:22.459954 110996 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0130 23:57:22.459956 110996 net.cpp:159] Memory required for data: 410847600
I0130 23:57:22.459959 110996 layer_factory.hpp:77] Creating layer fc6
I0130 23:57:22.459965 110996 net.cpp:94] Creating Layer fc6
I0130 23:57:22.459969 110996 net.cpp:435] fc6 <- pool5
I0130 23:57:22.459973 110996 net.cpp:409] fc6 -> fc6
I0130 23:57:22.785017 110996 net.cpp:144] Setting up fc6
I0130 23:57:22.785043 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.785045 110996 net.cpp:159] Memory required for data: 411666800
I0130 23:57:22.785051 110996 layer_factory.hpp:77] Creating layer relu6
I0130 23:57:22.785058 110996 net.cpp:94] Creating Layer relu6
I0130 23:57:22.785061 110996 net.cpp:435] relu6 <- fc6
I0130 23:57:22.785068 110996 net.cpp:409] relu6 -> relu6
I0130 23:57:22.785089 110996 net.cpp:144] Setting up relu6
I0130 23:57:22.785091 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.785094 110996 net.cpp:159] Memory required for data: 412486000
I0130 23:57:22.785095 110996 layer_factory.hpp:77] Creating layer drop6
I0130 23:57:22.785100 110996 net.cpp:94] Creating Layer drop6
I0130 23:57:22.785102 110996 net.cpp:435] drop6 <- relu6
I0130 23:57:22.785105 110996 net.cpp:409] drop6 -> drop6
I0130 23:57:22.785152 110996 net.cpp:144] Setting up drop6
I0130 23:57:22.785176 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.785178 110996 net.cpp:159] Memory required for data: 413305200
I0130 23:57:22.785181 110996 layer_factory.hpp:77] Creating layer fc7
I0130 23:57:22.785187 110996 net.cpp:94] Creating Layer fc7
I0130 23:57:22.785189 110996 net.cpp:435] fc7 <- drop6
I0130 23:57:22.785193 110996 net.cpp:409] fc7 -> fc7
I0130 23:57:22.927306 110996 net.cpp:144] Setting up fc7
I0130 23:57:22.927330 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.927332 110996 net.cpp:159] Memory required for data: 414124400
I0130 23:57:22.927356 110996 layer_factory.hpp:77] Creating layer bn7
I0130 23:57:22.927364 110996 net.cpp:94] Creating Layer bn7
I0130 23:57:22.927367 110996 net.cpp:435] bn7 <- fc7
I0130 23:57:22.927373 110996 net.cpp:409] bn7 -> scale7
I0130 23:57:22.927891 110996 net.cpp:144] Setting up bn7
I0130 23:57:22.927897 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.927901 110996 net.cpp:159] Memory required for data: 414943600
I0130 23:57:22.927907 110996 layer_factory.hpp:77] Creating layer relu7
I0130 23:57:22.927911 110996 net.cpp:94] Creating Layer relu7
I0130 23:57:22.927914 110996 net.cpp:435] relu7 <- scale7
I0130 23:57:22.927918 110996 net.cpp:409] relu7 -> relu7
I0130 23:57:22.927934 110996 net.cpp:144] Setting up relu7
I0130 23:57:22.927939 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.927942 110996 net.cpp:159] Memory required for data: 415762800
I0130 23:57:22.927944 110996 layer_factory.hpp:77] Creating layer drop7
I0130 23:57:22.927948 110996 net.cpp:94] Creating Layer drop7
I0130 23:57:22.927950 110996 net.cpp:435] drop7 <- relu7
I0130 23:57:22.927954 110996 net.cpp:409] drop7 -> drop7
I0130 23:57:22.927983 110996 net.cpp:144] Setting up drop7
I0130 23:57:22.927987 110996 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:22.927989 110996 net.cpp:159] Memory required for data: 416582000
I0130 23:57:22.927992 110996 layer_factory.hpp:77] Creating layer fc8
I0130 23:57:22.927999 110996 net.cpp:94] Creating Layer fc8
I0130 23:57:22.928002 110996 net.cpp:435] fc8 <- drop7
I0130 23:57:22.928009 110996 net.cpp:409] fc8 -> fc8
I0130 23:57:22.928892 110996 net.cpp:144] Setting up fc8
I0130 23:57:22.928905 110996 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:22.928906 110996 net.cpp:159] Memory required for data: 416582400
I0130 23:57:22.928913 110996 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0130 23:57:22.928920 110996 net.cpp:94] Creating Layer fc8_fc8_0_split
I0130 23:57:22.928925 110996 net.cpp:435] fc8_fc8_0_split <- fc8
I0130 23:57:22.928930 110996 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0130 23:57:22.928938 110996 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0130 23:57:22.928966 110996 net.cpp:144] Setting up fc8_fc8_0_split
I0130 23:57:22.928972 110996 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:22.928974 110996 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:22.928977 110996 net.cpp:159] Memory required for data: 416583200
I0130 23:57:22.928978 110996 layer_factory.hpp:77] Creating layer loss
I0130 23:57:22.928984 110996 net.cpp:94] Creating Layer loss
I0130 23:57:22.928987 110996 net.cpp:435] loss <- fc8_fc8_0_split_0
I0130 23:57:22.928989 110996 net.cpp:435] loss <- label_data_1_split_0
I0130 23:57:22.928994 110996 net.cpp:409] loss -> loss
I0130 23:57:22.929000 110996 layer_factory.hpp:77] Creating layer loss
I0130 23:57:22.929065 110996 net.cpp:144] Setting up loss
I0130 23:57:22.929070 110996 net.cpp:151] Top shape: (1)
I0130 23:57:22.929072 110996 net.cpp:154]     with loss weight 1
I0130 23:57:22.929081 110996 net.cpp:159] Memory required for data: 416583204
I0130 23:57:22.929083 110996 layer_factory.hpp:77] Creating layer accuracy-top1
I0130 23:57:22.929088 110996 net.cpp:94] Creating Layer accuracy-top1
I0130 23:57:22.929091 110996 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0130 23:57:22.929095 110996 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0130 23:57:22.929098 110996 net.cpp:409] accuracy-top1 -> top-1
I0130 23:57:22.929122 110996 net.cpp:144] Setting up accuracy-top1
I0130 23:57:22.929126 110996 net.cpp:151] Top shape: (1)
I0130 23:57:22.929128 110996 net.cpp:159] Memory required for data: 416583208
I0130 23:57:22.929131 110996 net.cpp:222] accuracy-top1 does not need backward computation.
I0130 23:57:22.929134 110996 net.cpp:220] loss needs backward computation.
I0130 23:57:22.929138 110996 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0130 23:57:22.929143 110996 net.cpp:220] fc8 needs backward computation.
I0130 23:57:22.929147 110996 net.cpp:220] drop7 needs backward computation.
I0130 23:57:22.929148 110996 net.cpp:220] relu7 needs backward computation.
I0130 23:57:22.929152 110996 net.cpp:220] bn7 needs backward computation.
I0130 23:57:22.929154 110996 net.cpp:220] fc7 needs backward computation.
I0130 23:57:22.929157 110996 net.cpp:220] drop6 needs backward computation.
I0130 23:57:22.929162 110996 net.cpp:220] relu6 needs backward computation.
I0130 23:57:22.929164 110996 net.cpp:220] fc6 needs backward computation.
I0130 23:57:22.929168 110996 net.cpp:220] pool5 needs backward computation.
I0130 23:57:22.929172 110996 net.cpp:220] relu5 needs backward computation.
I0130 23:57:22.929174 110996 net.cpp:220] conv5 needs backward computation.
I0130 23:57:22.929177 110996 net.cpp:220] relu4 needs backward computation.
I0130 23:57:22.929181 110996 net.cpp:220] conv4 needs backward computation.
I0130 23:57:22.929183 110996 net.cpp:220] relu3 needs backward computation.
I0130 23:57:22.929186 110996 net.cpp:220] conv3 needs backward computation.
I0130 23:57:22.929188 110996 net.cpp:220] pool2 needs backward computation.
I0130 23:57:22.929191 110996 net.cpp:220] relu2 needs backward computation.
I0130 23:57:22.929193 110996 net.cpp:220] bn2 needs backward computation.
I0130 23:57:22.929196 110996 net.cpp:220] conv2 needs backward computation.
I0130 23:57:22.929198 110996 net.cpp:220] pool1 needs backward computation.
I0130 23:57:22.929201 110996 net.cpp:220] relu1 needs backward computation.
I0130 23:57:22.929204 110996 net.cpp:220] bn1 needs backward computation.
I0130 23:57:22.929206 110996 net.cpp:220] conv1 needs backward computation.
I0130 23:57:22.929209 110996 net.cpp:222] label_data_1_split does not need backward computation.
I0130 23:57:22.929214 110996 net.cpp:222] data does not need backward computation.
I0130 23:57:22.929216 110996 net.cpp:264] This network produces output loss
I0130 23:57:22.929219 110996 net.cpp:264] This network produces output top-1
I0130 23:57:22.929241 110996 net.cpp:284] Network initialization done.
I0130 23:57:23.003149 110996 caffe_interface.cpp:363] Running for 80 iterations.
I0130 23:57:23.049679 110996 caffe_interface.cpp:125] Batch 0, loss = 0.145454
I0130 23:57:23.049702 110996 caffe_interface.cpp:125] Batch 0, top-1 = 0.96
I0130 23:57:23.070884 110996 caffe_interface.cpp:125] Batch 1, loss = 0.129016
I0130 23:57:23.070899 110996 caffe_interface.cpp:125] Batch 1, top-1 = 0.96
I0130 23:57:23.089785 110996 caffe_interface.cpp:125] Batch 2, loss = 0.4052
I0130 23:57:23.089802 110996 caffe_interface.cpp:125] Batch 2, top-1 = 0.94
I0130 23:57:23.136039 110996 caffe_interface.cpp:125] Batch 3, loss = 0.880324
I0130 23:57:23.136052 110996 caffe_interface.cpp:125] Batch 3, top-1 = 0.82
I0130 23:57:23.193369 110996 caffe_interface.cpp:125] Batch 4, loss = 0.214448
I0130 23:57:23.193380 110996 caffe_interface.cpp:125] Batch 4, top-1 = 0.96
I0130 23:57:23.250496 110996 caffe_interface.cpp:125] Batch 5, loss = 0.00920969
I0130 23:57:23.250507 110996 caffe_interface.cpp:125] Batch 5, top-1 = 1
I0130 23:57:23.306633 110996 caffe_interface.cpp:125] Batch 6, loss = 0.460612
I0130 23:57:23.306645 110996 caffe_interface.cpp:125] Batch 6, top-1 = 0.96
I0130 23:57:23.364682 110996 caffe_interface.cpp:125] Batch 7, loss = 0.0580731
I0130 23:57:23.364693 110996 caffe_interface.cpp:125] Batch 7, top-1 = 0.96
I0130 23:57:23.420667 110996 caffe_interface.cpp:125] Batch 8, loss = 0.0337304
I0130 23:57:23.420675 110996 caffe_interface.cpp:125] Batch 8, top-1 = 0.98
I0130 23:57:23.479195 110996 caffe_interface.cpp:125] Batch 9, loss = 0.50113
I0130 23:57:23.479223 110996 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I0130 23:57:23.537659 110996 caffe_interface.cpp:125] Batch 10, loss = 0.114543
I0130 23:57:23.537668 110996 caffe_interface.cpp:125] Batch 10, top-1 = 0.96
I0130 23:57:23.595979 110996 caffe_interface.cpp:125] Batch 11, loss = 0.652397
I0130 23:57:23.595988 110996 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I0130 23:57:23.652029 110996 caffe_interface.cpp:125] Batch 12, loss = 0.330864
I0130 23:57:23.652036 110996 caffe_interface.cpp:125] Batch 12, top-1 = 0.94
I0130 23:57:23.711191 110996 caffe_interface.cpp:125] Batch 13, loss = 0.169916
I0130 23:57:23.711199 110996 caffe_interface.cpp:125] Batch 13, top-1 = 0.96
I0130 23:57:23.767026 110996 caffe_interface.cpp:125] Batch 14, loss = 0.187547
I0130 23:57:23.767035 110996 caffe_interface.cpp:125] Batch 14, top-1 = 0.96
I0130 23:57:23.824609 110996 caffe_interface.cpp:125] Batch 15, loss = 0.127414
I0130 23:57:23.824616 110996 caffe_interface.cpp:125] Batch 15, top-1 = 0.98
I0130 23:57:23.882359 110996 caffe_interface.cpp:125] Batch 16, loss = 0.0846004
I0130 23:57:23.882366 110996 caffe_interface.cpp:125] Batch 16, top-1 = 0.98
I0130 23:57:23.911818 110996 caffe_interface.cpp:125] Batch 17, loss = 0.216589
I0130 23:57:23.911830 110996 caffe_interface.cpp:125] Batch 17, top-1 = 0.96
I0130 23:57:23.931838 110996 caffe_interface.cpp:125] Batch 18, loss = 0.289484
I0130 23:57:23.931846 110996 caffe_interface.cpp:125] Batch 18, top-1 = 0.94
I0130 23:57:23.978392 110996 caffe_interface.cpp:125] Batch 19, loss = 0.233026
I0130 23:57:23.978402 110996 caffe_interface.cpp:125] Batch 19, top-1 = 0.96
I0130 23:57:24.035928 110996 caffe_interface.cpp:125] Batch 20, loss = 0.200832
I0130 23:57:24.035954 110996 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I0130 23:57:24.093426 110996 caffe_interface.cpp:125] Batch 21, loss = 0.16754
I0130 23:57:24.093448 110996 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I0130 23:57:24.151454 110996 caffe_interface.cpp:125] Batch 22, loss = 0.139102
I0130 23:57:24.151474 110996 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I0130 23:57:24.210325 110996 caffe_interface.cpp:125] Batch 23, loss = 0.336164
I0130 23:57:24.210345 110996 caffe_interface.cpp:125] Batch 23, top-1 = 0.96
I0130 23:57:24.268246 110996 caffe_interface.cpp:125] Batch 24, loss = 0.103171
I0130 23:57:24.268265 110996 caffe_interface.cpp:125] Batch 24, top-1 = 0.98
I0130 23:57:24.316388 110996 caffe_interface.cpp:125] Batch 25, loss = 0.100075
I0130 23:57:24.316407 110996 caffe_interface.cpp:125] Batch 25, top-1 = 0.94
I0130 23:57:24.344986 110996 caffe_interface.cpp:125] Batch 26, loss = 0.186857
I0130 23:57:24.345005 110996 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I0130 23:57:24.365301 110996 caffe_interface.cpp:125] Batch 27, loss = 0.184181
I0130 23:57:24.365324 110996 caffe_interface.cpp:125] Batch 27, top-1 = 0.98
I0130 23:57:24.385282 110996 caffe_interface.cpp:125] Batch 28, loss = 0.387395
I0130 23:57:24.385301 110996 caffe_interface.cpp:125] Batch 28, top-1 = 0.9
I0130 23:57:24.404127 110996 caffe_interface.cpp:125] Batch 29, loss = 0.221361
I0130 23:57:24.404147 110996 caffe_interface.cpp:125] Batch 29, top-1 = 0.96
I0130 23:57:24.425456 110996 caffe_interface.cpp:125] Batch 30, loss = 0.0467385
I0130 23:57:24.425475 110996 caffe_interface.cpp:125] Batch 30, top-1 = 0.98
I0130 23:57:24.443964 110996 caffe_interface.cpp:125] Batch 31, loss = 0.00438698
I0130 23:57:24.443986 110996 caffe_interface.cpp:125] Batch 31, top-1 = 1
I0130 23:57:24.465071 110996 caffe_interface.cpp:125] Batch 32, loss = 0.161887
I0130 23:57:24.465093 110996 caffe_interface.cpp:125] Batch 32, top-1 = 0.96
I0130 23:57:24.495159 110996 caffe_interface.cpp:125] Batch 33, loss = 0.222105
I0130 23:57:24.495190 110996 caffe_interface.cpp:125] Batch 33, top-1 = 0.94
I0130 23:57:24.552840 110996 caffe_interface.cpp:125] Batch 34, loss = 0.388198
I0130 23:57:24.552850 110996 caffe_interface.cpp:125] Batch 34, top-1 = 0.92
I0130 23:57:24.607690 110996 caffe_interface.cpp:125] Batch 35, loss = 0.106859
I0130 23:57:24.607720 110996 caffe_interface.cpp:125] Batch 35, top-1 = 0.98
I0130 23:57:24.665010 110996 caffe_interface.cpp:125] Batch 36, loss = 0.56883
I0130 23:57:24.665019 110996 caffe_interface.cpp:125] Batch 36, top-1 = 0.9
I0130 23:57:24.720540 110996 caffe_interface.cpp:125] Batch 37, loss = 0.206352
I0130 23:57:24.720549 110996 caffe_interface.cpp:125] Batch 37, top-1 = 0.98
I0130 23:57:24.781829 110996 caffe_interface.cpp:125] Batch 38, loss = 0.328037
I0130 23:57:24.781837 110996 caffe_interface.cpp:125] Batch 38, top-1 = 0.9
I0130 23:57:24.841099 110996 caffe_interface.cpp:125] Batch 39, loss = 0.178166
I0130 23:57:24.841107 110996 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I0130 23:57:24.902799 110996 caffe_interface.cpp:125] Batch 40, loss = 0.128277
I0130 23:57:24.902807 110996 caffe_interface.cpp:125] Batch 40, top-1 = 0.98
I0130 23:57:24.958824 110996 caffe_interface.cpp:125] Batch 41, loss = 0.000101913
I0130 23:57:24.958833 110996 caffe_interface.cpp:125] Batch 41, top-1 = 1
I0130 23:57:25.020105 110996 caffe_interface.cpp:125] Batch 42, loss = 0.158371
I0130 23:57:25.020115 110996 caffe_interface.cpp:125] Batch 42, top-1 = 0.98
I0130 23:57:25.081744 110996 caffe_interface.cpp:125] Batch 43, loss = 0.081181
I0130 23:57:25.081753 110996 caffe_interface.cpp:125] Batch 43, top-1 = 0.96
I0130 23:57:25.139672 110996 caffe_interface.cpp:125] Batch 44, loss = 0.363334
I0130 23:57:25.139680 110996 caffe_interface.cpp:125] Batch 44, top-1 = 0.9
I0130 23:57:25.197443 110996 caffe_interface.cpp:125] Batch 45, loss = 0.706132
I0130 23:57:25.197451 110996 caffe_interface.cpp:125] Batch 45, top-1 = 0.84
I0130 23:57:25.255275 110996 caffe_interface.cpp:125] Batch 46, loss = 0.0683745
I0130 23:57:25.255282 110996 caffe_interface.cpp:125] Batch 46, top-1 = 0.98
I0130 23:57:25.296684 110996 caffe_interface.cpp:125] Batch 47, loss = 0.0201108
I0130 23:57:25.296694 110996 caffe_interface.cpp:125] Batch 47, top-1 = 0.98
I0130 23:57:25.317855 110996 caffe_interface.cpp:125] Batch 48, loss = 0.253487
I0130 23:57:25.317863 110996 caffe_interface.cpp:125] Batch 48, top-1 = 0.94
I0130 23:57:25.354776 110996 caffe_interface.cpp:125] Batch 49, loss = 0.109529
I0130 23:57:25.354784 110996 caffe_interface.cpp:125] Batch 49, top-1 = 0.98
I0130 23:57:25.412500 110996 caffe_interface.cpp:125] Batch 50, loss = 0.20622
I0130 23:57:25.412508 110996 caffe_interface.cpp:125] Batch 50, top-1 = 0.94
I0130 23:57:25.469733 110996 caffe_interface.cpp:125] Batch 51, loss = 0.302274
I0130 23:57:25.469745 110996 caffe_interface.cpp:125] Batch 51, top-1 = 0.92
I0130 23:57:25.527215 110996 caffe_interface.cpp:125] Batch 52, loss = 0.0104012
I0130 23:57:25.527223 110996 caffe_interface.cpp:125] Batch 52, top-1 = 1
I0130 23:57:25.585480 110996 caffe_interface.cpp:125] Batch 53, loss = 0.179671
I0130 23:57:25.585490 110996 caffe_interface.cpp:125] Batch 53, top-1 = 0.96
I0130 23:57:25.642637 110996 caffe_interface.cpp:125] Batch 54, loss = 0.0280758
I0130 23:57:25.642647 110996 caffe_interface.cpp:125] Batch 54, top-1 = 0.98
I0130 23:57:25.696409 110996 caffe_interface.cpp:125] Batch 55, loss = 0.447561
I0130 23:57:25.696420 110996 caffe_interface.cpp:125] Batch 55, top-1 = 0.94
I0130 23:57:25.728420 110996 caffe_interface.cpp:125] Batch 56, loss = 0.0304818
I0130 23:57:25.728431 110996 caffe_interface.cpp:125] Batch 56, top-1 = 0.98
I0130 23:57:25.750814 110996 caffe_interface.cpp:125] Batch 57, loss = 0.242874
I0130 23:57:25.750826 110996 caffe_interface.cpp:125] Batch 57, top-1 = 0.94
I0130 23:57:25.769917 110996 caffe_interface.cpp:125] Batch 58, loss = 0.405504
I0130 23:57:25.769930 110996 caffe_interface.cpp:125] Batch 58, top-1 = 0.92
I0130 23:57:25.788561 110996 caffe_interface.cpp:125] Batch 59, loss = 0.499948
I0130 23:57:25.788574 110996 caffe_interface.cpp:125] Batch 59, top-1 = 0.9
I0130 23:57:25.808360 110996 caffe_interface.cpp:125] Batch 60, loss = 0.339204
I0130 23:57:25.808370 110996 caffe_interface.cpp:125] Batch 60, top-1 = 0.96
I0130 23:57:25.828042 110996 caffe_interface.cpp:125] Batch 61, loss = 0.147611
I0130 23:57:25.828071 110996 caffe_interface.cpp:125] Batch 61, top-1 = 0.96
I0130 23:57:25.848388 110996 caffe_interface.cpp:125] Batch 62, loss = 0.0578328
I0130 23:57:25.848399 110996 caffe_interface.cpp:125] Batch 62, top-1 = 0.96
I0130 23:57:25.868094 110996 caffe_interface.cpp:125] Batch 63, loss = 0.375087
I0130 23:57:25.868106 110996 caffe_interface.cpp:125] Batch 63, top-1 = 0.94
I0130 23:57:25.907140 110996 caffe_interface.cpp:125] Batch 64, loss = 0.582341
I0130 23:57:25.907155 110996 caffe_interface.cpp:125] Batch 64, top-1 = 0.92
I0130 23:57:25.964534 110996 caffe_interface.cpp:125] Batch 65, loss = 0.261041
I0130 23:57:25.964543 110996 caffe_interface.cpp:125] Batch 65, top-1 = 0.96
I0130 23:57:26.021737 110996 caffe_interface.cpp:125] Batch 66, loss = 0.47687
I0130 23:57:26.021749 110996 caffe_interface.cpp:125] Batch 66, top-1 = 0.94
I0130 23:57:26.079952 110996 caffe_interface.cpp:125] Batch 67, loss = 0.449711
I0130 23:57:26.079969 110996 caffe_interface.cpp:125] Batch 67, top-1 = 0.92
I0130 23:57:26.141381 110996 caffe_interface.cpp:125] Batch 68, loss = 0.181248
I0130 23:57:26.141394 110996 caffe_interface.cpp:125] Batch 68, top-1 = 0.94
I0130 23:57:26.197947 110996 caffe_interface.cpp:125] Batch 69, loss = 0.552388
I0130 23:57:26.197958 110996 caffe_interface.cpp:125] Batch 69, top-1 = 0.9
I0130 23:57:26.261137 110996 caffe_interface.cpp:125] Batch 70, loss = 0.128039
I0130 23:57:26.261150 110996 caffe_interface.cpp:125] Batch 70, top-1 = 0.96
I0130 23:57:26.319650 110996 caffe_interface.cpp:125] Batch 71, loss = 0.10415
I0130 23:57:26.319659 110996 caffe_interface.cpp:125] Batch 71, top-1 = 0.96
I0130 23:57:26.377884 110996 caffe_interface.cpp:125] Batch 72, loss = 0.241912
I0130 23:57:26.377892 110996 caffe_interface.cpp:125] Batch 72, top-1 = 0.98
I0130 23:57:26.434121 110996 caffe_interface.cpp:125] Batch 73, loss = 0.0784438
I0130 23:57:26.434129 110996 caffe_interface.cpp:125] Batch 73, top-1 = 0.98
I0130 23:57:26.491696 110996 caffe_interface.cpp:125] Batch 74, loss = 0.0922228
I0130 23:57:26.491703 110996 caffe_interface.cpp:125] Batch 74, top-1 = 0.98
I0130 23:57:26.547348 110996 caffe_interface.cpp:125] Batch 75, loss = 0.0846019
I0130 23:57:26.547356 110996 caffe_interface.cpp:125] Batch 75, top-1 = 0.96
I0130 23:57:26.605067 110996 caffe_interface.cpp:125] Batch 76, loss = 0.374968
I0130 23:57:26.605074 110996 caffe_interface.cpp:125] Batch 76, top-1 = 0.9
I0130 23:57:26.660565 110996 caffe_interface.cpp:125] Batch 77, loss = 0.173075
I0130 23:57:26.660574 110996 caffe_interface.cpp:125] Batch 77, top-1 = 0.96
I0130 23:57:26.698400 110996 caffe_interface.cpp:125] Batch 78, loss = 0.127637
I0130 23:57:26.698410 110996 caffe_interface.cpp:125] Batch 78, top-1 = 0.96
I0130 23:57:26.718920 110996 caffe_interface.cpp:125] Batch 79, loss = 0.0638479
I0130 23:57:26.718930 110996 caffe_interface.cpp:125] Batch 79, top-1 = 0.98
I0130 23:57:26.718932 110996 caffe_interface.cpp:130] Loss: 0.2327
I0130 23:57:26.718940 110996 caffe_interface.cpp:142] loss = 0.2327 (* 1 = 0.2327 loss)
I0130 23:57:26.718943 110996 caffe_interface.cpp:142] top-1 = 0.95275
I0130 23:57:26.971721 110996 pruning_runner.cpp:306] pruning done, output model: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/sparse.caffemodel
I0130 23:57:26.971751 110996 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.6:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.949249864    | 0.952749848    | 0.00349998474  |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 1038707        | -72.4114685%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 957007842      | -55.5689812%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config /home/danieleb/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/config6.prototxt
## fine-tuning: 6-th run
$PRUNE_ROOT/deephi_compress finetune -config ${WORK_DIR}/config6.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune6_alexnetBNnoLRN.txt
I0130 23:57:27.239620 112238 deephi_compress.cpp:236] cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/net_finetune.prototxt
I0130 23:57:27.556625 112238 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0130 23:57:27.557050 112238 gpu_memory.cpp:55] Total memory: 25620447232, Free: 13496549376, dev_info[0]: total=25620447232 free=13496549376
I0130 23:57:27.557071 112238 caffe_interface.cpp:493] Using GPUs 0
I0130 23:57:27.557327 112238 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0130 23:57:28.466857 112238 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 12000
snapshot_prefix: "cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/net_finetune.prototxt"
type: "Adam"
I0130 23:57:28.466962 112238 solver.cpp:99] Creating training net from net file: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/net_finetune.prototxt
I0130 23:57:28.467181 112238 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0130 23:57:28.467195 112238 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0130 23:57:28.467342 112238 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0130 23:57:28.467420 112238 layer_factory.hpp:77] Creating layer data
I0130 23:57:28.467572 112238 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:28.468065 112238 net.cpp:94] Creating Layer data
I0130 23:57:28.468072 112238 net.cpp:409] data -> data
I0130 23:57:28.468081 112238 net.cpp:409] data -> label
I0130 23:57:28.470866 112275 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/train_lmdb
I0130 23:57:28.470911 112275 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0130 23:57:28.471187 112238 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I0130 23:57:28.471268 112238 data_layer.cpp:83] output data size: 256,3,227,227
I0130 23:57:28.896344 112238 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:28.896399 112238 net.cpp:144] Setting up data
I0130 23:57:28.896406 112238 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I0130 23:57:28.896410 112238 net.cpp:151] Top shape: 256 (256)
I0130 23:57:28.896428 112238 net.cpp:159] Memory required for data: 158298112
I0130 23:57:28.896433 112238 layer_factory.hpp:77] Creating layer conv1
I0130 23:57:28.896446 112238 net.cpp:94] Creating Layer conv1
I0130 23:57:28.896450 112238 net.cpp:435] conv1 <- data
I0130 23:57:28.896466 112238 net.cpp:409] conv1 -> conv1
I0130 23:57:28.898381 112238 net.cpp:144] Setting up conv1
I0130 23:57:28.898394 112238 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0130 23:57:28.898397 112238 net.cpp:159] Memory required for data: 455667712
I0130 23:57:28.898413 112238 layer_factory.hpp:77] Creating layer bn1
I0130 23:57:28.898422 112238 net.cpp:94] Creating Layer bn1
I0130 23:57:28.898425 112238 net.cpp:435] bn1 <- conv1
I0130 23:57:28.898432 112238 net.cpp:409] bn1 -> scale1
I0130 23:57:28.899714 112238 net.cpp:144] Setting up bn1
I0130 23:57:28.899721 112238 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0130 23:57:28.899724 112238 net.cpp:159] Memory required for data: 753037312
I0130 23:57:28.899734 112238 layer_factory.hpp:77] Creating layer relu1
I0130 23:57:28.899754 112238 net.cpp:94] Creating Layer relu1
I0130 23:57:28.899756 112238 net.cpp:435] relu1 <- scale1
I0130 23:57:28.899760 112238 net.cpp:409] relu1 -> relu1
I0130 23:57:28.899804 112238 net.cpp:144] Setting up relu1
I0130 23:57:28.899809 112238 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0130 23:57:28.899812 112238 net.cpp:159] Memory required for data: 1050406912
I0130 23:57:28.899814 112238 layer_factory.hpp:77] Creating layer pool1
I0130 23:57:28.899819 112238 net.cpp:94] Creating Layer pool1
I0130 23:57:28.899822 112238 net.cpp:435] pool1 <- relu1
I0130 23:57:28.899827 112238 net.cpp:409] pool1 -> pool1
I0130 23:57:28.899860 112238 net.cpp:144] Setting up pool1
I0130 23:57:28.899865 112238 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I0130 23:57:28.899868 112238 net.cpp:159] Memory required for data: 1122070528
I0130 23:57:28.899870 112238 layer_factory.hpp:77] Creating layer conv2
I0130 23:57:28.899876 112238 net.cpp:94] Creating Layer conv2
I0130 23:57:28.899879 112238 net.cpp:435] conv2 <- pool1
I0130 23:57:28.899883 112238 net.cpp:409] conv2 -> conv2
I0130 23:57:28.915469 112238 net.cpp:144] Setting up conv2
I0130 23:57:28.915486 112238 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0130 23:57:28.915489 112238 net.cpp:159] Memory required for data: 1313173504
I0130 23:57:28.915500 112238 layer_factory.hpp:77] Creating layer bn2
I0130 23:57:28.915511 112238 net.cpp:94] Creating Layer bn2
I0130 23:57:28.915515 112238 net.cpp:435] bn2 <- conv2
I0130 23:57:28.915521 112238 net.cpp:409] bn2 -> scale2
I0130 23:57:28.916085 112238 net.cpp:144] Setting up bn2
I0130 23:57:28.916095 112238 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0130 23:57:28.916100 112238 net.cpp:159] Memory required for data: 1504276480
I0130 23:57:28.916111 112238 layer_factory.hpp:77] Creating layer relu2
I0130 23:57:28.916119 112238 net.cpp:94] Creating Layer relu2
I0130 23:57:28.916124 112238 net.cpp:435] relu2 <- scale2
I0130 23:57:28.916131 112238 net.cpp:409] relu2 -> relu2
I0130 23:57:28.916155 112238 net.cpp:144] Setting up relu2
I0130 23:57:28.916162 112238 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0130 23:57:28.916167 112238 net.cpp:159] Memory required for data: 1695379456
I0130 23:57:28.916172 112238 layer_factory.hpp:77] Creating layer pool2
I0130 23:57:28.916179 112238 net.cpp:94] Creating Layer pool2
I0130 23:57:28.916183 112238 net.cpp:435] pool2 <- relu2
I0130 23:57:28.916204 112238 net.cpp:409] pool2 -> pool2
I0130 23:57:28.916239 112238 net.cpp:144] Setting up pool2
I0130 23:57:28.916245 112238 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0130 23:57:28.916249 112238 net.cpp:159] Memory required for data: 1739681792
I0130 23:57:28.916254 112238 layer_factory.hpp:77] Creating layer conv3
I0130 23:57:28.916265 112238 net.cpp:94] Creating Layer conv3
I0130 23:57:28.916270 112238 net.cpp:435] conv3 <- pool2
I0130 23:57:28.916276 112238 net.cpp:409] conv3 -> conv3
I0130 23:57:28.928320 112238 net.cpp:144] Setting up conv3
I0130 23:57:28.928359 112238 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0130 23:57:28.928364 112238 net.cpp:159] Memory required for data: 1806135296
I0130 23:57:28.928375 112238 layer_factory.hpp:77] Creating layer relu3
I0130 23:57:28.928386 112238 net.cpp:94] Creating Layer relu3
I0130 23:57:28.928391 112238 net.cpp:435] relu3 <- conv3
I0130 23:57:28.928400 112238 net.cpp:409] relu3 -> relu3
I0130 23:57:28.928436 112238 net.cpp:144] Setting up relu3
I0130 23:57:28.928447 112238 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0130 23:57:28.928452 112238 net.cpp:159] Memory required for data: 1872588800
I0130 23:57:28.928455 112238 layer_factory.hpp:77] Creating layer conv4
I0130 23:57:28.928472 112238 net.cpp:94] Creating Layer conv4
I0130 23:57:28.928477 112238 net.cpp:435] conv4 <- relu3
I0130 23:57:28.928483 112238 net.cpp:409] conv4 -> conv4
I0130 23:57:28.945353 112238 net.cpp:144] Setting up conv4
I0130 23:57:28.945372 112238 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0130 23:57:28.945376 112238 net.cpp:159] Memory required for data: 1939042304
I0130 23:57:28.945390 112238 layer_factory.hpp:77] Creating layer relu4
I0130 23:57:28.945415 112238 net.cpp:94] Creating Layer relu4
I0130 23:57:28.945420 112238 net.cpp:435] relu4 <- conv4
I0130 23:57:28.945428 112238 net.cpp:409] relu4 -> relu4
I0130 23:57:28.945452 112238 net.cpp:144] Setting up relu4
I0130 23:57:28.945456 112238 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0130 23:57:28.945459 112238 net.cpp:159] Memory required for data: 2005495808
I0130 23:57:28.945462 112238 layer_factory.hpp:77] Creating layer conv5
I0130 23:57:28.945472 112238 net.cpp:94] Creating Layer conv5
I0130 23:57:28.945475 112238 net.cpp:435] conv5 <- relu4
I0130 23:57:28.945482 112238 net.cpp:409] conv5 -> conv5
I0130 23:57:28.961834 112238 net.cpp:144] Setting up conv5
I0130 23:57:28.961858 112238 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0130 23:57:28.961863 112238 net.cpp:159] Memory required for data: 2049798144
I0130 23:57:28.961871 112238 layer_factory.hpp:77] Creating layer relu5
I0130 23:57:28.961882 112238 net.cpp:94] Creating Layer relu5
I0130 23:57:28.961889 112238 net.cpp:435] relu5 <- conv5
I0130 23:57:28.961906 112238 net.cpp:409] relu5 -> relu5
I0130 23:57:28.961942 112238 net.cpp:144] Setting up relu5
I0130 23:57:28.961954 112238 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0130 23:57:28.961959 112238 net.cpp:159] Memory required for data: 2094100480
I0130 23:57:28.961963 112238 layer_factory.hpp:77] Creating layer pool5
I0130 23:57:28.961972 112238 net.cpp:94] Creating Layer pool5
I0130 23:57:28.961977 112238 net.cpp:435] pool5 <- relu5
I0130 23:57:28.961984 112238 net.cpp:409] pool5 -> pool5
I0130 23:57:28.962038 112238 net.cpp:144] Setting up pool5
I0130 23:57:28.962062 112238 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I0130 23:57:28.962080 112238 net.cpp:159] Memory required for data: 2103537664
I0130 23:57:28.962095 112238 layer_factory.hpp:77] Creating layer fc6
I0130 23:57:28.962117 112238 net.cpp:94] Creating Layer fc6
I0130 23:57:28.962133 112238 net.cpp:435] fc6 <- pool5
I0130 23:57:28.962153 112238 net.cpp:409] fc6 -> fc6
I0130 23:57:29.354056 112238 net.cpp:144] Setting up fc6
I0130 23:57:29.354081 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.354084 112238 net.cpp:159] Memory required for data: 2107731968
I0130 23:57:29.354109 112238 layer_factory.hpp:77] Creating layer relu6
I0130 23:57:29.354125 112238 net.cpp:94] Creating Layer relu6
I0130 23:57:29.354151 112238 net.cpp:435] relu6 <- fc6
I0130 23:57:29.354168 112238 net.cpp:409] relu6 -> relu6
I0130 23:57:29.354193 112238 net.cpp:144] Setting up relu6
I0130 23:57:29.354197 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.354198 112238 net.cpp:159] Memory required for data: 2111926272
I0130 23:57:29.354202 112238 layer_factory.hpp:77] Creating layer drop6
I0130 23:57:29.354207 112238 net.cpp:94] Creating Layer drop6
I0130 23:57:29.354209 112238 net.cpp:435] drop6 <- relu6
I0130 23:57:29.354213 112238 net.cpp:409] drop6 -> drop6
I0130 23:57:29.354249 112238 net.cpp:144] Setting up drop6
I0130 23:57:29.354254 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.354255 112238 net.cpp:159] Memory required for data: 2116120576
I0130 23:57:29.354259 112238 layer_factory.hpp:77] Creating layer fc7
I0130 23:57:29.354265 112238 net.cpp:94] Creating Layer fc7
I0130 23:57:29.354269 112238 net.cpp:435] fc7 <- drop6
I0130 23:57:29.354274 112238 net.cpp:409] fc7 -> fc7
I0130 23:57:29.496258 112238 net.cpp:144] Setting up fc7
I0130 23:57:29.496279 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.496281 112238 net.cpp:159] Memory required for data: 2120314880
I0130 23:57:29.496289 112238 layer_factory.hpp:77] Creating layer bn7
I0130 23:57:29.496299 112238 net.cpp:94] Creating Layer bn7
I0130 23:57:29.496302 112238 net.cpp:435] bn7 <- fc7
I0130 23:57:29.496318 112238 net.cpp:409] bn7 -> scale7
I0130 23:57:29.496836 112238 net.cpp:144] Setting up bn7
I0130 23:57:29.496842 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.496845 112238 net.cpp:159] Memory required for data: 2124509184
I0130 23:57:29.496852 112238 layer_factory.hpp:77] Creating layer relu7
I0130 23:57:29.496856 112238 net.cpp:94] Creating Layer relu7
I0130 23:57:29.496860 112238 net.cpp:435] relu7 <- scale7
I0130 23:57:29.496863 112238 net.cpp:409] relu7 -> relu7
I0130 23:57:29.496883 112238 net.cpp:144] Setting up relu7
I0130 23:57:29.496888 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.496891 112238 net.cpp:159] Memory required for data: 2128703488
I0130 23:57:29.496896 112238 layer_factory.hpp:77] Creating layer drop7
I0130 23:57:29.496901 112238 net.cpp:94] Creating Layer drop7
I0130 23:57:29.496906 112238 net.cpp:435] drop7 <- relu7
I0130 23:57:29.496909 112238 net.cpp:409] drop7 -> drop7
I0130 23:57:29.496937 112238 net.cpp:144] Setting up drop7
I0130 23:57:29.496942 112238 net.cpp:151] Top shape: 256 4096 (1048576)
I0130 23:57:29.496944 112238 net.cpp:159] Memory required for data: 2132897792
I0130 23:57:29.496948 112238 layer_factory.hpp:77] Creating layer fc8
I0130 23:57:29.496953 112238 net.cpp:94] Creating Layer fc8
I0130 23:57:29.496956 112238 net.cpp:435] fc8 <- drop7
I0130 23:57:29.496961 112238 net.cpp:409] fc8 -> fc8
I0130 23:57:29.497833 112238 net.cpp:144] Setting up fc8
I0130 23:57:29.497843 112238 net.cpp:151] Top shape: 256 2 (512)
I0130 23:57:29.497845 112238 net.cpp:159] Memory required for data: 2132899840
I0130 23:57:29.497851 112238 layer_factory.hpp:77] Creating layer loss
I0130 23:57:29.497858 112238 net.cpp:94] Creating Layer loss
I0130 23:57:29.497860 112238 net.cpp:435] loss <- fc8
I0130 23:57:29.497864 112238 net.cpp:435] loss <- label
I0130 23:57:29.497871 112238 net.cpp:409] loss -> loss
I0130 23:57:29.497882 112238 layer_factory.hpp:77] Creating layer loss
I0130 23:57:29.497947 112238 net.cpp:144] Setting up loss
I0130 23:57:29.497953 112238 net.cpp:151] Top shape: (1)
I0130 23:57:29.497956 112238 net.cpp:154]     with loss weight 1
I0130 23:57:29.497968 112238 net.cpp:159] Memory required for data: 2132899844
I0130 23:57:29.497972 112238 net.cpp:220] loss needs backward computation.
I0130 23:57:29.497988 112238 net.cpp:220] fc8 needs backward computation.
I0130 23:57:29.497992 112238 net.cpp:220] drop7 needs backward computation.
I0130 23:57:29.497993 112238 net.cpp:220] relu7 needs backward computation.
I0130 23:57:29.497997 112238 net.cpp:220] bn7 needs backward computation.
I0130 23:57:29.497999 112238 net.cpp:220] fc7 needs backward computation.
I0130 23:57:29.498019 112238 net.cpp:220] drop6 needs backward computation.
I0130 23:57:29.498023 112238 net.cpp:220] relu6 needs backward computation.
I0130 23:57:29.498024 112238 net.cpp:220] fc6 needs backward computation.
I0130 23:57:29.498028 112238 net.cpp:220] pool5 needs backward computation.
I0130 23:57:29.498030 112238 net.cpp:220] relu5 needs backward computation.
I0130 23:57:29.498033 112238 net.cpp:220] conv5 needs backward computation.
I0130 23:57:29.498035 112238 net.cpp:220] relu4 needs backward computation.
I0130 23:57:29.498039 112238 net.cpp:220] conv4 needs backward computation.
I0130 23:57:29.498041 112238 net.cpp:220] relu3 needs backward computation.
I0130 23:57:29.498044 112238 net.cpp:220] conv3 needs backward computation.
I0130 23:57:29.498047 112238 net.cpp:220] pool2 needs backward computation.
I0130 23:57:29.498049 112238 net.cpp:220] relu2 needs backward computation.
I0130 23:57:29.498052 112238 net.cpp:220] bn2 needs backward computation.
I0130 23:57:29.498055 112238 net.cpp:220] conv2 needs backward computation.
I0130 23:57:29.498059 112238 net.cpp:220] pool1 needs backward computation.
I0130 23:57:29.498062 112238 net.cpp:220] relu1 needs backward computation.
I0130 23:57:29.498065 112238 net.cpp:220] bn1 needs backward computation.
I0130 23:57:29.498070 112238 net.cpp:220] conv1 needs backward computation.
I0130 23:57:29.498075 112238 net.cpp:222] data does not need backward computation.
I0130 23:57:29.498078 112238 net.cpp:264] This network produces output loss
I0130 23:57:29.498098 112238 net.cpp:284] Network initialization done.
I0130 23:57:29.498375 112238 solver.cpp:189] Creating test net (#0) specified by net file: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/net_finetune.prototxt
I0130 23:57:29.498409 112238 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0130 23:57:29.498574 112238 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0130 23:57:29.498677 112238 layer_factory.hpp:77] Creating layer data
I0130 23:57:29.498714 112238 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:29.499593 112238 net.cpp:94] Creating Layer data
I0130 23:57:29.499603 112238 net.cpp:409] data -> data
I0130 23:57:29.499611 112238 net.cpp:409] data -> label
I0130 23:57:29.501392 112305 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0130 23:57:29.501425 112305 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0130 23:57:29.501750 112238 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0130 23:57:29.501848 112238 data_layer.cpp:83] output data size: 50,3,227,227
I0130 23:57:29.596139 112238 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0130 23:57:29.596207 112238 net.cpp:144] Setting up data
I0130 23:57:29.596215 112238 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0130 23:57:29.596218 112238 net.cpp:151] Top shape: 50 (50)
I0130 23:57:29.596220 112238 net.cpp:159] Memory required for data: 30917600
I0130 23:57:29.596235 112238 layer_factory.hpp:77] Creating layer label_data_1_split
I0130 23:57:29.596247 112238 net.cpp:94] Creating Layer label_data_1_split
I0130 23:57:29.596252 112238 net.cpp:435] label_data_1_split <- label
I0130 23:57:29.596261 112238 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0130 23:57:29.596271 112238 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0130 23:57:29.596328 112238 net.cpp:144] Setting up label_data_1_split
I0130 23:57:29.596333 112238 net.cpp:151] Top shape: 50 (50)
I0130 23:57:29.596335 112238 net.cpp:151] Top shape: 50 (50)
I0130 23:57:29.596338 112238 net.cpp:159] Memory required for data: 30918000
I0130 23:57:29.596339 112238 layer_factory.hpp:77] Creating layer conv1
I0130 23:57:29.596350 112238 net.cpp:94] Creating Layer conv1
I0130 23:57:29.596354 112238 net.cpp:435] conv1 <- data
I0130 23:57:29.596357 112238 net.cpp:409] conv1 -> conv1
I0130 23:57:29.596899 112238 net.cpp:144] Setting up conv1
I0130 23:57:29.596905 112238 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:29.596909 112238 net.cpp:159] Memory required for data: 88998000
I0130 23:57:29.596917 112238 layer_factory.hpp:77] Creating layer bn1
I0130 23:57:29.596925 112238 net.cpp:94] Creating Layer bn1
I0130 23:57:29.596927 112238 net.cpp:435] bn1 <- conv1
I0130 23:57:29.596932 112238 net.cpp:409] bn1 -> scale1
I0130 23:57:29.599122 112238 net.cpp:144] Setting up bn1
I0130 23:57:29.599128 112238 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:29.599130 112238 net.cpp:159] Memory required for data: 147078000
I0130 23:57:29.599139 112238 layer_factory.hpp:77] Creating layer relu1
I0130 23:57:29.599162 112238 net.cpp:94] Creating Layer relu1
I0130 23:57:29.599165 112238 net.cpp:435] relu1 <- scale1
I0130 23:57:29.599169 112238 net.cpp:409] relu1 -> relu1
I0130 23:57:29.599208 112238 net.cpp:144] Setting up relu1
I0130 23:57:29.599213 112238 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0130 23:57:29.599216 112238 net.cpp:159] Memory required for data: 205158000
I0130 23:57:29.599218 112238 layer_factory.hpp:77] Creating layer pool1
I0130 23:57:29.599223 112238 net.cpp:94] Creating Layer pool1
I0130 23:57:29.599226 112238 net.cpp:435] pool1 <- relu1
I0130 23:57:29.599232 112238 net.cpp:409] pool1 -> pool1
I0130 23:57:29.599309 112238 net.cpp:144] Setting up pool1
I0130 23:57:29.599314 112238 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0130 23:57:29.599316 112238 net.cpp:159] Memory required for data: 219154800
I0130 23:57:29.599318 112238 layer_factory.hpp:77] Creating layer conv2
I0130 23:57:29.599326 112238 net.cpp:94] Creating Layer conv2
I0130 23:57:29.599342 112238 net.cpp:435] conv2 <- pool1
I0130 23:57:29.599349 112238 net.cpp:409] conv2 -> conv2
I0130 23:57:29.608158 112238 net.cpp:144] Setting up conv2
I0130 23:57:29.608194 112238 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:29.608201 112238 net.cpp:159] Memory required for data: 256479600
I0130 23:57:29.608222 112238 layer_factory.hpp:77] Creating layer bn2
I0130 23:57:29.608242 112238 net.cpp:94] Creating Layer bn2
I0130 23:57:29.608248 112238 net.cpp:435] bn2 <- conv2
I0130 23:57:29.608261 112238 net.cpp:409] bn2 -> scale2
I0130 23:57:29.609472 112238 net.cpp:144] Setting up bn2
I0130 23:57:29.609488 112238 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:29.609493 112238 net.cpp:159] Memory required for data: 293804400
I0130 23:57:29.609508 112238 layer_factory.hpp:77] Creating layer relu2
I0130 23:57:29.609524 112238 net.cpp:94] Creating Layer relu2
I0130 23:57:29.609532 112238 net.cpp:435] relu2 <- scale2
I0130 23:57:29.609541 112238 net.cpp:409] relu2 -> relu2
I0130 23:57:29.609580 112238 net.cpp:144] Setting up relu2
I0130 23:57:29.609589 112238 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0130 23:57:29.609596 112238 net.cpp:159] Memory required for data: 331129200
I0130 23:57:29.609601 112238 layer_factory.hpp:77] Creating layer pool2
I0130 23:57:29.609612 112238 net.cpp:94] Creating Layer pool2
I0130 23:57:29.609619 112238 net.cpp:435] pool2 <- relu2
I0130 23:57:29.609628 112238 net.cpp:409] pool2 -> pool2
I0130 23:57:29.609690 112238 net.cpp:144] Setting up pool2
I0130 23:57:29.609699 112238 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:29.609704 112238 net.cpp:159] Memory required for data: 339782000
I0130 23:57:29.609710 112238 layer_factory.hpp:77] Creating layer conv3
I0130 23:57:29.609727 112238 net.cpp:94] Creating Layer conv3
I0130 23:57:29.609735 112238 net.cpp:435] conv3 <- pool2
I0130 23:57:29.609745 112238 net.cpp:409] conv3 -> conv3
I0130 23:57:29.630928 112238 net.cpp:144] Setting up conv3
I0130 23:57:29.630950 112238 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:29.630952 112238 net.cpp:159] Memory required for data: 352761200
I0130 23:57:29.630971 112238 layer_factory.hpp:77] Creating layer relu3
I0130 23:57:29.630981 112238 net.cpp:94] Creating Layer relu3
I0130 23:57:29.630985 112238 net.cpp:435] relu3 <- conv3
I0130 23:57:29.630995 112238 net.cpp:409] relu3 -> relu3
I0130 23:57:29.631026 112238 net.cpp:144] Setting up relu3
I0130 23:57:29.631029 112238 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:29.631032 112238 net.cpp:159] Memory required for data: 365740400
I0130 23:57:29.631036 112238 layer_factory.hpp:77] Creating layer conv4
I0130 23:57:29.631047 112238 net.cpp:94] Creating Layer conv4
I0130 23:57:29.631050 112238 net.cpp:435] conv4 <- relu3
I0130 23:57:29.631057 112238 net.cpp:409] conv4 -> conv4
I0130 23:57:29.644726 112238 net.cpp:144] Setting up conv4
I0130 23:57:29.644757 112238 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:29.644762 112238 net.cpp:159] Memory required for data: 378719600
I0130 23:57:29.644775 112238 layer_factory.hpp:77] Creating layer relu4
I0130 23:57:29.644783 112238 net.cpp:94] Creating Layer relu4
I0130 23:57:29.644788 112238 net.cpp:435] relu4 <- conv4
I0130 23:57:29.644794 112238 net.cpp:409] relu4 -> relu4
I0130 23:57:29.644822 112238 net.cpp:144] Setting up relu4
I0130 23:57:29.644826 112238 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0130 23:57:29.644829 112238 net.cpp:159] Memory required for data: 391698800
I0130 23:57:29.644832 112238 layer_factory.hpp:77] Creating layer conv5
I0130 23:57:29.644845 112238 net.cpp:94] Creating Layer conv5
I0130 23:57:29.644847 112238 net.cpp:435] conv5 <- relu4
I0130 23:57:29.644852 112238 net.cpp:409] conv5 -> conv5
I0130 23:57:29.659417 112238 net.cpp:144] Setting up conv5
I0130 23:57:29.659443 112238 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:29.659448 112238 net.cpp:159] Memory required for data: 400351600
I0130 23:57:29.659457 112238 layer_factory.hpp:77] Creating layer relu5
I0130 23:57:29.659467 112238 net.cpp:94] Creating Layer relu5
I0130 23:57:29.659504 112238 net.cpp:435] relu5 <- conv5
I0130 23:57:29.659512 112238 net.cpp:409] relu5 -> relu5
I0130 23:57:29.659549 112238 net.cpp:144] Setting up relu5
I0130 23:57:29.659559 112238 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0130 23:57:29.659561 112238 net.cpp:159] Memory required for data: 409004400
I0130 23:57:29.659564 112238 layer_factory.hpp:77] Creating layer pool5
I0130 23:57:29.659574 112238 net.cpp:94] Creating Layer pool5
I0130 23:57:29.659577 112238 net.cpp:435] pool5 <- relu5
I0130 23:57:29.659581 112238 net.cpp:409] pool5 -> pool5
I0130 23:57:29.659617 112238 net.cpp:144] Setting up pool5
I0130 23:57:29.659623 112238 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0130 23:57:29.659626 112238 net.cpp:159] Memory required for data: 410847600
I0130 23:57:29.659629 112238 layer_factory.hpp:77] Creating layer fc6
I0130 23:57:29.659637 112238 net.cpp:94] Creating Layer fc6
I0130 23:57:29.659641 112238 net.cpp:435] fc6 <- pool5
I0130 23:57:29.659646 112238 net.cpp:409] fc6 -> fc6
I0130 23:57:30.001261 112238 net.cpp:144] Setting up fc6
I0130 23:57:30.001286 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.001288 112238 net.cpp:159] Memory required for data: 411666800
I0130 23:57:30.001297 112238 layer_factory.hpp:77] Creating layer relu6
I0130 23:57:30.001307 112238 net.cpp:94] Creating Layer relu6
I0130 23:57:30.001312 112238 net.cpp:435] relu6 <- fc6
I0130 23:57:30.001327 112238 net.cpp:409] relu6 -> relu6
I0130 23:57:30.001356 112238 net.cpp:144] Setting up relu6
I0130 23:57:30.001361 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.001364 112238 net.cpp:159] Memory required for data: 412486000
I0130 23:57:30.001368 112238 layer_factory.hpp:77] Creating layer drop6
I0130 23:57:30.001374 112238 net.cpp:94] Creating Layer drop6
I0130 23:57:30.001377 112238 net.cpp:435] drop6 <- relu6
I0130 23:57:30.001381 112238 net.cpp:409] drop6 -> drop6
I0130 23:57:30.001411 112238 net.cpp:144] Setting up drop6
I0130 23:57:30.001417 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.001420 112238 net.cpp:159] Memory required for data: 413305200
I0130 23:57:30.001422 112238 layer_factory.hpp:77] Creating layer fc7
I0130 23:57:30.001428 112238 net.cpp:94] Creating Layer fc7
I0130 23:57:30.001432 112238 net.cpp:435] fc7 <- drop6
I0130 23:57:30.001436 112238 net.cpp:409] fc7 -> fc7
I0130 23:57:30.143069 112238 net.cpp:144] Setting up fc7
I0130 23:57:30.143090 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.143093 112238 net.cpp:159] Memory required for data: 414124400
I0130 23:57:30.143101 112238 layer_factory.hpp:77] Creating layer bn7
I0130 23:57:30.143111 112238 net.cpp:94] Creating Layer bn7
I0130 23:57:30.143115 112238 net.cpp:435] bn7 <- fc7
I0130 23:57:30.143121 112238 net.cpp:409] bn7 -> scale7
I0130 23:57:30.143688 112238 net.cpp:144] Setting up bn7
I0130 23:57:30.143697 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.143700 112238 net.cpp:159] Memory required for data: 414943600
I0130 23:57:30.143707 112238 layer_factory.hpp:77] Creating layer relu7
I0130 23:57:30.143712 112238 net.cpp:94] Creating Layer relu7
I0130 23:57:30.143715 112238 net.cpp:435] relu7 <- scale7
I0130 23:57:30.143720 112238 net.cpp:409] relu7 -> relu7
I0130 23:57:30.143739 112238 net.cpp:144] Setting up relu7
I0130 23:57:30.143743 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.143746 112238 net.cpp:159] Memory required for data: 415762800
I0130 23:57:30.143748 112238 layer_factory.hpp:77] Creating layer drop7
I0130 23:57:30.143754 112238 net.cpp:94] Creating Layer drop7
I0130 23:57:30.143755 112238 net.cpp:435] drop7 <- relu7
I0130 23:57:30.143760 112238 net.cpp:409] drop7 -> drop7
I0130 23:57:30.143787 112238 net.cpp:144] Setting up drop7
I0130 23:57:30.143792 112238 net.cpp:151] Top shape: 50 4096 (204800)
I0130 23:57:30.143795 112238 net.cpp:159] Memory required for data: 416582000
I0130 23:57:30.143796 112238 layer_factory.hpp:77] Creating layer fc8
I0130 23:57:30.143801 112238 net.cpp:94] Creating Layer fc8
I0130 23:57:30.143823 112238 net.cpp:435] fc8 <- drop7
I0130 23:57:30.143827 112238 net.cpp:409] fc8 -> fc8
I0130 23:57:30.143996 112238 net.cpp:144] Setting up fc8
I0130 23:57:30.144001 112238 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:30.144002 112238 net.cpp:159] Memory required for data: 416582400
I0130 23:57:30.144007 112238 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0130 23:57:30.144011 112238 net.cpp:94] Creating Layer fc8_fc8_0_split
I0130 23:57:30.144013 112238 net.cpp:435] fc8_fc8_0_split <- fc8
I0130 23:57:30.144018 112238 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0130 23:57:30.144024 112238 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0130 23:57:30.144048 112238 net.cpp:144] Setting up fc8_fc8_0_split
I0130 23:57:30.144052 112238 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:30.144055 112238 net.cpp:151] Top shape: 50 2 (100)
I0130 23:57:30.144057 112238 net.cpp:159] Memory required for data: 416583200
I0130 23:57:30.144059 112238 layer_factory.hpp:77] Creating layer loss
I0130 23:57:30.144065 112238 net.cpp:94] Creating Layer loss
I0130 23:57:30.144068 112238 net.cpp:435] loss <- fc8_fc8_0_split_0
I0130 23:57:30.144071 112238 net.cpp:435] loss <- label_data_1_split_0
I0130 23:57:30.144075 112238 net.cpp:409] loss -> loss
I0130 23:57:30.144081 112238 layer_factory.hpp:77] Creating layer loss
I0130 23:57:30.144155 112238 net.cpp:144] Setting up loss
I0130 23:57:30.144160 112238 net.cpp:151] Top shape: (1)
I0130 23:57:30.144161 112238 net.cpp:154]     with loss weight 1
I0130 23:57:30.144170 112238 net.cpp:159] Memory required for data: 416583204
I0130 23:57:30.144172 112238 layer_factory.hpp:77] Creating layer accuracy-top1
I0130 23:57:30.144176 112238 net.cpp:94] Creating Layer accuracy-top1
I0130 23:57:30.144178 112238 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0130 23:57:30.144181 112238 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0130 23:57:30.144187 112238 net.cpp:409] accuracy-top1 -> top-1
I0130 23:57:30.144192 112238 net.cpp:144] Setting up accuracy-top1
I0130 23:57:30.144196 112238 net.cpp:151] Top shape: (1)
I0130 23:57:30.144197 112238 net.cpp:159] Memory required for data: 416583208
I0130 23:57:30.144201 112238 net.cpp:222] accuracy-top1 does not need backward computation.
I0130 23:57:30.144203 112238 net.cpp:220] loss needs backward computation.
I0130 23:57:30.144207 112238 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0130 23:57:30.144209 112238 net.cpp:220] fc8 needs backward computation.
I0130 23:57:30.144212 112238 net.cpp:220] drop7 needs backward computation.
I0130 23:57:30.144214 112238 net.cpp:220] relu7 needs backward computation.
I0130 23:57:30.144217 112238 net.cpp:220] bn7 needs backward computation.
I0130 23:57:30.144219 112238 net.cpp:220] fc7 needs backward computation.
I0130 23:57:30.144222 112238 net.cpp:220] drop6 needs backward computation.
I0130 23:57:30.144225 112238 net.cpp:220] relu6 needs backward computation.
I0130 23:57:30.144227 112238 net.cpp:220] fc6 needs backward computation.
I0130 23:57:30.144230 112238 net.cpp:220] pool5 needs backward computation.
I0130 23:57:30.144233 112238 net.cpp:220] relu5 needs backward computation.
I0130 23:57:30.144235 112238 net.cpp:220] conv5 needs backward computation.
I0130 23:57:30.144238 112238 net.cpp:220] relu4 needs backward computation.
I0130 23:57:30.144242 112238 net.cpp:220] conv4 needs backward computation.
I0130 23:57:30.144243 112238 net.cpp:220] relu3 needs backward computation.
I0130 23:57:30.144246 112238 net.cpp:220] conv3 needs backward computation.
I0130 23:57:30.144248 112238 net.cpp:220] pool2 needs backward computation.
I0130 23:57:30.144250 112238 net.cpp:220] relu2 needs backward computation.
I0130 23:57:30.144253 112238 net.cpp:220] bn2 needs backward computation.
I0130 23:57:30.144258 112238 net.cpp:220] conv2 needs backward computation.
I0130 23:57:30.144259 112238 net.cpp:220] pool1 needs backward computation.
I0130 23:57:30.144261 112238 net.cpp:220] relu1 needs backward computation.
I0130 23:57:30.144264 112238 net.cpp:220] bn1 needs backward computation.
I0130 23:57:30.144274 112238 net.cpp:220] conv1 needs backward computation.
I0130 23:57:30.144277 112238 net.cpp:222] label_data_1_split does not need backward computation.
I0130 23:57:30.144280 112238 net.cpp:222] data does not need backward computation.
I0130 23:57:30.144284 112238 net.cpp:264] This network produces output loss
I0130 23:57:30.144285 112238 net.cpp:264] This network produces output top-1
I0130 23:57:30.144304 112238 net.cpp:284] Network initialization done.
I0130 23:57:30.144402 112238 solver.cpp:63] Solver scaffolding done.
I0130 23:57:30.145593 112238 caffe_interface.cpp:93] Finetuning from cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/sparse.caffemodel
I0130 23:57:31.719219 112238 caffe_interface.cpp:527] Starting Optimization
I0130 23:57:31.719240 112238 solver.cpp:335] Solving 
I0130 23:57:31.719244 112238 solver.cpp:336] Learning Rate Policy: step
I0130 23:57:31.721160 112238 solver.cpp:418] Iteration 0, Testing net (#0)
I0130 23:57:35.369982 112238 solver.cpp:517]     Test net output #0: loss = 0.2327 (* 1 = 0.2327 loss)
I0130 23:57:35.370004 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I0130 23:57:35.921046 112238 solver.cpp:266] Iteration 0 (0 iter/s, 4.20161s/50 iter), loss = 0.00103844
I0130 23:57:35.921078 112238 solver.cpp:285]     Train net output #0: loss = 0.00103844 (* 1 = 0.00103844 loss)
I0130 23:57:35.923311 112238 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0130 23:58:00.118150 112238 solver.cpp:266] Iteration 50 (2.06663 iter/s, 24.194s/50 iter), loss = 0.0351823
I0130 23:58:00.118230 112238 solver.cpp:285]     Train net output #0: loss = 0.0351823 (* 1 = 0.0351823 loss)
I0130 23:58:00.118275 112238 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0130 23:58:24.161331 112238 solver.cpp:266] Iteration 100 (2.07968 iter/s, 24.0422s/50 iter), loss = 0.0302526
I0130 23:58:24.161370 112238 solver.cpp:285]     Train net output #0: loss = 0.0302526 (* 1 = 0.0302526 loss)
I0130 23:58:24.163594 112238 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0130 23:58:48.534276 112238 solver.cpp:266] Iteration 150 (2.05172 iter/s, 24.3698s/50 iter), loss = 0.0474056
I0130 23:58:48.534404 112238 solver.cpp:285]     Train net output #0: loss = 0.0474055 (* 1 = 0.0474055 loss)
I0130 23:58:48.534410 112238 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0130 23:59:13.038159 112238 solver.cpp:266] Iteration 200 (2.04058 iter/s, 24.5029s/50 iter), loss = 0.0504062
I0130 23:59:13.038194 112238 solver.cpp:285]     Train net output #0: loss = 0.0504062 (* 1 = 0.0504062 loss)
I0130 23:59:13.040416 112238 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0130 23:59:37.302912 112238 solver.cpp:266] Iteration 250 (2.06087 iter/s, 24.2616s/50 iter), loss = 0.0418967
I0130 23:59:37.303035 112238 solver.cpp:285]     Train net output #0: loss = 0.0418967 (* 1 = 0.0418967 loss)
I0130 23:59:37.305155 112238 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0131 00:00:01.789364 112238 solver.cpp:266] Iteration 300 (2.04221 iter/s, 24.4833s/50 iter), loss = 0.0596781
I0131 00:00:01.789389 112238 solver.cpp:285]     Train net output #0: loss = 0.0596781 (* 1 = 0.0596781 loss)
I0131 00:00:01.791625 112238 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0131 00:00:25.966858 112238 solver.cpp:266] Iteration 350 (2.06831 iter/s, 24.1743s/50 iter), loss = 0.0682851
I0131 00:00:25.966912 112238 solver.cpp:285]     Train net output #0: loss = 0.0682851 (* 1 = 0.0682851 loss)
I0131 00:00:25.969102 112238 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0131 00:00:50.493768 112238 solver.cpp:266] Iteration 400 (2.03884 iter/s, 24.5238s/50 iter), loss = 0.0472836
I0131 00:00:50.493799 112238 solver.cpp:285]     Train net output #0: loss = 0.0472836 (* 1 = 0.0472836 loss)
I0131 00:00:50.493842 112238 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0131 00:01:14.793423 112238 solver.cpp:266] Iteration 450 (2.05772 iter/s, 24.2987s/50 iter), loss = 0.0321595
I0131 00:01:14.793512 112238 solver.cpp:285]     Train net output #0: loss = 0.0321595 (* 1 = 0.0321595 loss)
I0131 00:01:14.795675 112238 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0131 00:01:39.175650 112238 solver.cpp:266] Iteration 500 (2.05094 iter/s, 24.3791s/50 iter), loss = 0.02306
I0131 00:01:39.175678 112238 solver.cpp:285]     Train net output #0: loss = 0.02306 (* 1 = 0.02306 loss)
I0131 00:01:39.177911 112238 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0131 00:02:03.448897 112238 solver.cpp:266] Iteration 550 (2.06015 iter/s, 24.2701s/50 iter), loss = 0.044703
I0131 00:02:03.449018 112238 solver.cpp:285]     Train net output #0: loss = 0.044703 (* 1 = 0.044703 loss)
I0131 00:02:03.451154 112238 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0131 00:02:27.854773 112238 solver.cpp:266] Iteration 600 (2.04895 iter/s, 24.4027s/50 iter), loss = 0.0712583
I0131 00:02:27.854812 112238 solver.cpp:285]     Train net output #0: loss = 0.0712583 (* 1 = 0.0712583 loss)
I0131 00:02:27.854820 112238 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0131 00:02:52.357012 112238 solver.cpp:266] Iteration 650 (2.04071 iter/s, 24.5013s/50 iter), loss = 0.035939
I0131 00:02:52.357118 112238 solver.cpp:285]     Train net output #0: loss = 0.035939 (* 1 = 0.035939 loss)
I0131 00:02:52.359254 112238 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0131 00:03:16.690294 112238 solver.cpp:266] Iteration 700 (2.05506 iter/s, 24.3301s/50 iter), loss = 0.0685908
I0131 00:03:16.690322 112238 solver.cpp:285]     Train net output #0: loss = 0.0685908 (* 1 = 0.0685908 loss)
I0131 00:03:16.690392 112238 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0131 00:03:41.136373 112238 solver.cpp:266] Iteration 750 (2.0454 iter/s, 24.4451s/50 iter), loss = 0.0346866
I0131 00:03:41.136476 112238 solver.cpp:285]     Train net output #0: loss = 0.0346865 (* 1 = 0.0346865 loss)
I0131 00:03:41.138633 112238 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0131 00:04:05.342555 112238 solver.cpp:266] Iteration 800 (2.06586 iter/s, 24.203s/50 iter), loss = 0.0368118
I0131 00:04:05.342593 112238 solver.cpp:285]     Train net output #0: loss = 0.0368118 (* 1 = 0.0368118 loss)
I0131 00:04:05.344799 112238 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0131 00:04:29.713898 112238 solver.cpp:266] Iteration 850 (2.05185 iter/s, 24.3682s/50 iter), loss = 0.0285958
I0131 00:04:29.714027 112238 solver.cpp:285]     Train net output #0: loss = 0.0285957 (* 1 = 0.0285957 loss)
I0131 00:04:29.714051 112238 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0131 00:04:54.163547 112238 solver.cpp:266] Iteration 900 (2.04511 iter/s, 24.4486s/50 iter), loss = 0.058883
I0131 00:04:54.163579 112238 solver.cpp:285]     Train net output #0: loss = 0.058883 (* 1 = 0.058883 loss)
I0131 00:04:54.165801 112238 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0131 00:05:18.584532 112238 solver.cpp:266] Iteration 950 (2.04768 iter/s, 24.4178s/50 iter), loss = 0.0391731
I0131 00:05:18.584633 112238 solver.cpp:285]     Train net output #0: loss = 0.0391731 (* 1 = 0.0391731 loss)
I0131 00:05:18.584679 112238 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0131 00:05:42.340793 112238 solver.cpp:418] Iteration 1000, Testing net (#0)
I0131 00:05:46.085733 112238 solver.cpp:517]     Test net output #0: loss = 0.29938 (* 1 = 0.29938 loss)
I0131 00:05:46.085752 112238 solver.cpp:517]     Test net output #1: top-1 = 0.91675
I0131 00:05:46.493994 112238 solver.cpp:266] Iteration 1000 (1.79158 iter/s, 27.9083s/50 iter), loss = 0.0421515
I0131 00:05:46.494021 112238 solver.cpp:285]     Train net output #0: loss = 0.0421515 (* 1 = 0.0421515 loss)
I0131 00:05:46.494027 112238 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0131 00:06:11.150089 112238 solver.cpp:266] Iteration 1050 (2.02797 iter/s, 24.6552s/50 iter), loss = 0.0629627
I0131 00:06:11.150156 112238 solver.cpp:285]     Train net output #0: loss = 0.0629626 (* 1 = 0.0629626 loss)
I0131 00:06:11.150430 112238 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0131 00:06:35.638980 112238 solver.cpp:266] Iteration 1100 (2.04185 iter/s, 24.4876s/50 iter), loss = 0.0255671
I0131 00:06:35.639010 112238 solver.cpp:285]     Train net output #0: loss = 0.025567 (* 1 = 0.025567 loss)
I0131 00:06:35.641240 112238 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0131 00:07:00.070057 112238 solver.cpp:266] Iteration 1150 (2.04684 iter/s, 24.4279s/50 iter), loss = 0.0900414
I0131 00:07:00.070173 112238 solver.cpp:285]     Train net output #0: loss = 0.0900414 (* 1 = 0.0900414 loss)
I0131 00:07:00.070230 112238 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0131 00:07:24.443255 112238 solver.cpp:266] Iteration 1200 (2.05152 iter/s, 24.3721s/50 iter), loss = 0.0289604
I0131 00:07:24.443284 112238 solver.cpp:285]     Train net output #0: loss = 0.0289603 (* 1 = 0.0289603 loss)
I0131 00:07:24.445514 112238 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0131 00:07:48.931668 112238 solver.cpp:266] Iteration 1250 (2.04205 iter/s, 24.4852s/50 iter), loss = 0.0325236
I0131 00:07:48.931797 112238 solver.cpp:285]     Train net output #0: loss = 0.0325236 (* 1 = 0.0325236 loss)
I0131 00:07:48.933930 112238 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0131 00:08:13.382956 112238 solver.cpp:266] Iteration 1300 (2.04515 iter/s, 24.4481s/50 iter), loss = 0.0591634
I0131 00:08:13.382988 112238 solver.cpp:285]     Train net output #0: loss = 0.0591634 (* 1 = 0.0591634 loss)
I0131 00:08:13.385205 112238 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0131 00:08:37.895596 112238 solver.cpp:266] Iteration 1350 (2.04003 iter/s, 24.5095s/50 iter), loss = 0.0356992
I0131 00:08:37.895722 112238 solver.cpp:285]     Train net output #0: loss = 0.0356992 (* 1 = 0.0356992 loss)
I0131 00:08:37.895730 112238 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0131 00:09:02.333672 112238 solver.cpp:266] Iteration 1400 (2.04607 iter/s, 24.437s/50 iter), loss = 0.0371594
I0131 00:09:02.333703 112238 solver.cpp:285]     Train net output #0: loss = 0.0371594 (* 1 = 0.0371594 loss)
I0131 00:09:02.335925 112238 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0131 00:09:26.703539 112238 solver.cpp:266] Iteration 1450 (2.05198 iter/s, 24.3667s/50 iter), loss = 0.0370292
I0131 00:09:26.703641 112238 solver.cpp:285]     Train net output #0: loss = 0.0370291 (* 1 = 0.0370291 loss)
I0131 00:09:26.705801 112238 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0131 00:09:50.980680 112238 solver.cpp:266] Iteration 1500 (2.05982 iter/s, 24.274s/50 iter), loss = 0.0268418
I0131 00:09:50.980710 112238 solver.cpp:285]     Train net output #0: loss = 0.0268417 (* 1 = 0.0268417 loss)
I0131 00:09:50.982923 112238 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0131 00:10:15.475711 112238 solver.cpp:266] Iteration 1550 (2.04149 iter/s, 24.4919s/50 iter), loss = 0.0198625
I0131 00:10:15.475848 112238 solver.cpp:285]     Train net output #0: loss = 0.0198624 (* 1 = 0.0198624 loss)
I0131 00:10:15.475857 112238 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0131 00:10:39.845881 112238 solver.cpp:266] Iteration 1600 (2.05178 iter/s, 24.3691s/50 iter), loss = 0.0874529
I0131 00:10:39.845914 112238 solver.cpp:285]     Train net output #0: loss = 0.0874528 (* 1 = 0.0874528 loss)
I0131 00:10:39.848140 112238 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0131 00:11:04.159778 112238 solver.cpp:266] Iteration 1650 (2.0567 iter/s, 24.3107s/50 iter), loss = 0.024782
I0131 00:11:04.159878 112238 solver.cpp:285]     Train net output #0: loss = 0.024782 (* 1 = 0.024782 loss)
I0131 00:11:04.162034 112238 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I0131 00:11:28.411507 112238 solver.cpp:266] Iteration 1700 (2.06198 iter/s, 24.2486s/50 iter), loss = 0.0552183
I0131 00:11:28.411538 112238 solver.cpp:285]     Train net output #0: loss = 0.0552183 (* 1 = 0.0552183 loss)
I0131 00:11:28.413750 112238 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I0131 00:11:52.938709 112238 solver.cpp:266] Iteration 1750 (2.03881 iter/s, 24.5241s/50 iter), loss = 0.0303966
I0131 00:11:52.938809 112238 solver.cpp:285]     Train net output #0: loss = 0.0303966 (* 1 = 0.0303966 loss)
I0131 00:11:52.940966 112238 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I0131 00:12:17.276844 112238 solver.cpp:266] Iteration 1800 (2.05466 iter/s, 24.335s/50 iter), loss = 0.0167175
I0131 00:12:17.276875 112238 solver.cpp:285]     Train net output #0: loss = 0.0167175 (* 1 = 0.0167175 loss)
I0131 00:12:17.276917 112238 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0131 00:12:41.668490 112238 solver.cpp:266] Iteration 1850 (2.04996 iter/s, 24.3907s/50 iter), loss = 0.0269166
I0131 00:12:41.668639 112238 solver.cpp:285]     Train net output #0: loss = 0.0269166 (* 1 = 0.0269166 loss)
I0131 00:12:41.670752 112238 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I0131 00:13:06.005336 112238 solver.cpp:266] Iteration 1900 (2.05476 iter/s, 24.3337s/50 iter), loss = 0.0398188
I0131 00:13:06.005367 112238 solver.cpp:285]     Train net output #0: loss = 0.0398187 (* 1 = 0.0398187 loss)
I0131 00:13:06.007586 112238 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I0131 00:13:30.476919 112238 solver.cpp:266] Iteration 1950 (2.04345 iter/s, 24.4684s/50 iter), loss = 0.0609118
I0131 00:13:30.477023 112238 solver.cpp:285]     Train net output #0: loss = 0.0609118 (* 1 = 0.0609118 loss)
I0131 00:13:30.478202 112238 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I0131 00:13:54.216028 112238 solver.cpp:418] Iteration 2000, Testing net (#0)
I0131 00:13:57.863629 112238 solver.cpp:517]     Test net output #0: loss = 0.199332 (* 1 = 0.199332 loss)
I0131 00:13:57.863647 112238 solver.cpp:517]     Test net output #1: top-1 = 0.93225
I0131 00:13:58.189507 112238 solver.cpp:266] Iteration 2000 (1.80438 iter/s, 27.7103s/50 iter), loss = 0.0493121
I0131 00:13:58.189539 112238 solver.cpp:285]     Train net output #0: loss = 0.0493121 (* 1 = 0.0493121 loss)
I0131 00:13:58.191759 112238 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0131 00:14:22.709554 112238 solver.cpp:266] Iteration 2050 (2.03941 iter/s, 24.5169s/50 iter), loss = 0.0565593
I0131 00:14:22.709699 112238 solver.cpp:285]     Train net output #0: loss = 0.0565593 (* 1 = 0.0565593 loss)
I0131 00:14:22.709707 112238 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I0131 00:14:47.028581 112238 solver.cpp:266] Iteration 2100 (2.05609 iter/s, 24.318s/50 iter), loss = 0.0627666
I0131 00:14:47.028616 112238 solver.cpp:285]     Train net output #0: loss = 0.0627666 (* 1 = 0.0627666 loss)
I0131 00:14:47.030819 112238 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I0131 00:15:11.445358 112238 solver.cpp:266] Iteration 2150 (2.04804 iter/s, 24.4136s/50 iter), loss = 0.0541709
I0131 00:15:11.445461 112238 solver.cpp:285]     Train net output #0: loss = 0.0541708 (* 1 = 0.0541708 loss)
I0131 00:15:11.447608 112238 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I0131 00:15:35.760195 112238 solver.cpp:266] Iteration 2200 (2.05662 iter/s, 24.3117s/50 iter), loss = 0.0472545
I0131 00:15:35.760224 112238 solver.cpp:285]     Train net output #0: loss = 0.0472544 (* 1 = 0.0472544 loss)
I0131 00:15:35.762428 112238 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0131 00:16:00.236636 112238 solver.cpp:266] Iteration 2250 (2.04304 iter/s, 24.4733s/50 iter), loss = 0.0745231
I0131 00:16:00.236735 112238 solver.cpp:285]     Train net output #0: loss = 0.074523 (* 1 = 0.074523 loss)
I0131 00:16:00.238857 112238 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I0131 00:16:24.704401 112238 solver.cpp:266] Iteration 2300 (2.04377 iter/s, 24.4646s/50 iter), loss = 0.0239931
I0131 00:16:24.704435 112238 solver.cpp:285]     Train net output #0: loss = 0.0239931 (* 1 = 0.0239931 loss)
I0131 00:16:24.704442 112238 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I0131 00:16:49.064438 112238 solver.cpp:266] Iteration 2350 (2.05262 iter/s, 24.3591s/50 iter), loss = 0.0497696
I0131 00:16:49.064559 112238 solver.cpp:285]     Train net output #0: loss = 0.0497696 (* 1 = 0.0497696 loss)
I0131 00:16:49.066699 112238 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I0131 00:17:13.492084 112238 solver.cpp:266] Iteration 2400 (2.04713 iter/s, 24.4245s/50 iter), loss = 0.074543
I0131 00:17:13.492112 112238 solver.cpp:285]     Train net output #0: loss = 0.074543 (* 1 = 0.074543 loss)
I0131 00:17:13.494335 112238 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0131 00:17:37.756542 112238 solver.cpp:266] Iteration 2450 (2.06089 iter/s, 24.2613s/50 iter), loss = 0.0247622
I0131 00:17:37.756669 112238 solver.cpp:285]     Train net output #0: loss = 0.0247622 (* 1 = 0.0247622 loss)
I0131 00:17:37.758786 112238 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I0131 00:18:02.173902 112238 solver.cpp:266] Iteration 2500 (2.04799 iter/s, 24.4142s/50 iter), loss = 0.0405185
I0131 00:18:02.173933 112238 solver.cpp:285]     Train net output #0: loss = 0.0405185 (* 1 = 0.0405185 loss)
I0131 00:18:02.173938 112238 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0131 00:18:26.626771 112238 solver.cpp:266] Iteration 2550 (2.04483 iter/s, 24.4519s/50 iter), loss = 0.0361229
I0131 00:18:26.626897 112238 solver.cpp:285]     Train net output #0: loss = 0.0361229 (* 1 = 0.0361229 loss)
I0131 00:18:26.629024 112238 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I0131 00:18:51.080184 112238 solver.cpp:266] Iteration 2600 (2.04497 iter/s, 24.4503s/50 iter), loss = 0.01996
I0131 00:18:51.080214 112238 solver.cpp:285]     Train net output #0: loss = 0.01996 (* 1 = 0.01996 loss)
I0131 00:18:51.082437 112238 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0131 00:19:15.417341 112238 solver.cpp:266] Iteration 2650 (2.05474 iter/s, 24.334s/50 iter), loss = 0.0102819
I0131 00:19:15.417389 112238 solver.cpp:285]     Train net output #0: loss = 0.0102819 (* 1 = 0.0102819 loss)
I0131 00:19:15.419600 112238 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I0131 00:19:39.717093 112238 solver.cpp:266] Iteration 2700 (2.0579 iter/s, 24.2966s/50 iter), loss = 0.0119236
I0131 00:19:39.717123 112238 solver.cpp:285]     Train net output #0: loss = 0.0119236 (* 1 = 0.0119236 loss)
I0131 00:19:39.719310 112238 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0131 00:20:04.260368 112238 solver.cpp:266] Iteration 2750 (2.03748 iter/s, 24.5402s/50 iter), loss = 0.0165196
I0131 00:20:04.260700 112238 solver.cpp:285]     Train net output #0: loss = 0.0165196 (* 1 = 0.0165196 loss)
I0131 00:20:04.260732 112238 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I0131 00:20:28.646576 112238 solver.cpp:266] Iteration 2800 (2.05044 iter/s, 24.385s/50 iter), loss = 0.0138661
I0131 00:20:28.646606 112238 solver.cpp:285]     Train net output #0: loss = 0.0138661 (* 1 = 0.0138661 loss)
I0131 00:20:28.648825 112238 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0131 00:20:52.979856 112238 solver.cpp:266] Iteration 2850 (2.05506 iter/s, 24.3301s/50 iter), loss = 0.022382
I0131 00:20:52.979979 112238 solver.cpp:285]     Train net output #0: loss = 0.022382 (* 1 = 0.022382 loss)
I0131 00:20:52.982120 112238 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I0131 00:21:17.402751 112238 solver.cpp:266] Iteration 2900 (2.04752 iter/s, 24.4197s/50 iter), loss = 0.0106096
I0131 00:21:17.402781 112238 solver.cpp:285]     Train net output #0: loss = 0.0106096 (* 1 = 0.0106096 loss)
I0131 00:21:17.405001 112238 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0131 00:21:41.734329 112238 solver.cpp:266] Iteration 2950 (2.05521 iter/s, 24.3284s/50 iter), loss = 0.00800368
I0131 00:21:41.734397 112238 solver.cpp:285]     Train net output #0: loss = 0.00800368 (* 1 = 0.00800368 loss)
I0131 00:21:41.736573 112238 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I0131 00:22:05.739969 112238 solver.cpp:418] Iteration 3000, Testing net (#0)
I0131 00:22:09.198015 112238 solver.cpp:517]     Test net output #0: loss = 0.156372 (* 1 = 0.156372 loss)
I0131 00:22:09.198032 112238 solver.cpp:517]     Test net output #1: top-1 = 0.946
I0131 00:22:09.755336 112238 solver.cpp:266] Iteration 3000 (1.78458 iter/s, 28.0177s/50 iter), loss = 0.0304175
I0131 00:22:09.755359 112238 solver.cpp:285]     Train net output #0: loss = 0.0304175 (* 1 = 0.0304175 loss)
I0131 00:22:09.757586 112238 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0131 00:22:34.069761 112238 solver.cpp:266] Iteration 3050 (2.05666 iter/s, 24.3113s/50 iter), loss = 0.0123895
I0131 00:22:34.069926 112238 solver.cpp:285]     Train net output #0: loss = 0.0123895 (* 1 = 0.0123895 loss)
I0131 00:22:34.072019 112238 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I0131 00:22:58.592711 112238 solver.cpp:266] Iteration 3100 (2.03917 iter/s, 24.5198s/50 iter), loss = 0.013946
I0131 00:22:58.592743 112238 solver.cpp:285]     Train net output #0: loss = 0.013946 (* 1 = 0.013946 loss)
I0131 00:22:58.594939 112238 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0131 00:23:23.095502 112238 solver.cpp:266] Iteration 3150 (2.04084 iter/s, 24.4997s/50 iter), loss = 0.0115746
I0131 00:23:23.095556 112238 solver.cpp:285]     Train net output #0: loss = 0.0115746 (* 1 = 0.0115746 loss)
I0131 00:23:23.095562 112238 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I0131 00:23:47.655236 112238 solver.cpp:266] Iteration 3200 (2.03593 iter/s, 24.5588s/50 iter), loss = 0.020968
I0131 00:23:47.655274 112238 solver.cpp:285]     Train net output #0: loss = 0.020968 (* 1 = 0.020968 loss)
I0131 00:23:47.655611 112238 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0131 00:24:11.971398 112238 solver.cpp:266] Iteration 3250 (2.05635 iter/s, 24.3149s/50 iter), loss = 0.0130873
I0131 00:24:11.971496 112238 solver.cpp:285]     Train net output #0: loss = 0.0130873 (* 1 = 0.0130873 loss)
I0131 00:24:11.973706 112238 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I0131 00:24:36.215212 112238 solver.cpp:266] Iteration 3300 (2.06265 iter/s, 24.2406s/50 iter), loss = 0.0054388
I0131 00:24:36.215241 112238 solver.cpp:285]     Train net output #0: loss = 0.00543878 (* 1 = 0.00543878 loss)
I0131 00:24:36.217468 112238 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0131 00:25:00.478966 112238 solver.cpp:266] Iteration 3350 (2.06095 iter/s, 24.2606s/50 iter), loss = 0.0156232
I0131 00:25:00.479094 112238 solver.cpp:285]     Train net output #0: loss = 0.0156232 (* 1 = 0.0156232 loss)
I0131 00:25:00.479100 112238 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I0131 00:25:24.923426 112238 solver.cpp:266] Iteration 3400 (2.04554 iter/s, 24.4434s/50 iter), loss = 0.0131467
I0131 00:25:24.923458 112238 solver.cpp:285]     Train net output #0: loss = 0.0131467 (* 1 = 0.0131467 loss)
I0131 00:25:24.925688 112238 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0131 00:25:49.368662 112238 solver.cpp:266] Iteration 3450 (2.04565 iter/s, 24.4421s/50 iter), loss = 0.00262709
I0131 00:25:49.368716 112238 solver.cpp:285]     Train net output #0: loss = 0.00262708 (* 1 = 0.00262708 loss)
I0131 00:25:49.368768 112238 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I0131 00:26:13.633304 112238 solver.cpp:266] Iteration 3500 (2.0607 iter/s, 24.2636s/50 iter), loss = 0.00689511
I0131 00:26:13.633347 112238 solver.cpp:285]     Train net output #0: loss = 0.0068951 (* 1 = 0.0068951 loss)
I0131 00:26:13.635563 112238 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0131 00:26:37.947973 112238 solver.cpp:266] Iteration 3550 (2.05664 iter/s, 24.3115s/50 iter), loss = 0.00960523
I0131 00:26:37.948078 112238 solver.cpp:285]     Train net output #0: loss = 0.00960522 (* 1 = 0.00960522 loss)
I0131 00:26:37.950233 112238 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I0131 00:27:02.427196 112238 solver.cpp:266] Iteration 3600 (2.04281 iter/s, 24.4761s/50 iter), loss = 0.00849811
I0131 00:27:02.427232 112238 solver.cpp:285]     Train net output #0: loss = 0.00849811 (* 1 = 0.00849811 loss)
I0131 00:27:02.427238 112238 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0131 00:27:26.748104 112238 solver.cpp:266] Iteration 3650 (2.05592 iter/s, 24.32s/50 iter), loss = 0.00909852
I0131 00:27:26.748229 112238 solver.cpp:285]     Train net output #0: loss = 0.00909851 (* 1 = 0.00909851 loss)
I0131 00:27:26.750370 112238 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I0131 00:27:51.051053 112238 solver.cpp:266] Iteration 3700 (2.05763 iter/s, 24.2998s/50 iter), loss = 0.0231343
I0131 00:27:51.051082 112238 solver.cpp:285]     Train net output #0: loss = 0.0231343 (* 1 = 0.0231343 loss)
I0131 00:27:51.053314 112238 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0131 00:28:15.314990 112238 solver.cpp:266] Iteration 3750 (2.06094 iter/s, 24.2608s/50 iter), loss = 0.00440594
I0131 00:28:15.315079 112238 solver.cpp:285]     Train net output #0: loss = 0.00440594 (* 1 = 0.00440594 loss)
I0131 00:28:15.317237 112238 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I0131 00:28:39.570650 112238 solver.cpp:266] Iteration 3800 (2.06164 iter/s, 24.2525s/50 iter), loss = 0.00445907
I0131 00:28:39.570688 112238 solver.cpp:285]     Train net output #0: loss = 0.00445906 (* 1 = 0.00445906 loss)
I0131 00:28:39.570708 112238 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0131 00:29:03.924109 112238 solver.cpp:266] Iteration 3850 (2.05318 iter/s, 24.3525s/50 iter), loss = 0.00297889
I0131 00:29:03.924260 112238 solver.cpp:285]     Train net output #0: loss = 0.00297888 (* 1 = 0.00297888 loss)
I0131 00:29:03.926362 112238 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I0131 00:29:28.221248 112238 solver.cpp:266] Iteration 3900 (2.05812 iter/s, 24.294s/50 iter), loss = 0.00433181
I0131 00:29:28.221276 112238 solver.cpp:285]     Train net output #0: loss = 0.0043318 (* 1 = 0.0043318 loss)
I0131 00:29:28.223500 112238 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0131 00:29:52.366778 112238 solver.cpp:266] Iteration 3950 (2.07105 iter/s, 24.1424s/50 iter), loss = 0.00523835
I0131 00:29:52.366881 112238 solver.cpp:285]     Train net output #0: loss = 0.00523834 (* 1 = 0.00523834 loss)
I0131 00:29:52.366888 112238 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I0131 00:30:16.384807 112238 solver.cpp:418] Iteration 4000, Testing net (#0)
I0131 00:30:19.892122 112238 solver.cpp:517]     Test net output #0: loss = 0.154396 (* 1 = 0.154396 loss)
I0131 00:30:19.892139 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95125
I0131 00:30:20.446846 112238 solver.cpp:266] Iteration 4000 (1.78069 iter/s, 28.0789s/50 iter), loss = 0.002185
I0131 00:30:20.446874 112238 solver.cpp:285]     Train net output #0: loss = 0.00218499 (* 1 = 0.00218499 loss)
I0131 00:30:20.449097 112238 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0131 00:30:44.791915 112238 solver.cpp:266] Iteration 4050 (2.05407 iter/s, 24.3419s/50 iter), loss = 0.0149271
I0131 00:30:44.792019 112238 solver.cpp:285]     Train net output #0: loss = 0.0149271 (* 1 = 0.0149271 loss)
I0131 00:30:44.794165 112238 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I0131 00:31:09.139771 112238 solver.cpp:266] Iteration 4100 (2.05383 iter/s, 24.3447s/50 iter), loss = 0.0245907
I0131 00:31:09.139802 112238 solver.cpp:285]     Train net output #0: loss = 0.0245907 (* 1 = 0.0245907 loss)
I0131 00:31:09.139808 112238 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0131 00:31:33.634949 112238 solver.cpp:266] Iteration 4150 (2.0413 iter/s, 24.4942s/50 iter), loss = 0.00384972
I0131 00:31:33.635022 112238 solver.cpp:285]     Train net output #0: loss = 0.00384971 (* 1 = 0.00384971 loss)
I0131 00:31:33.637255 112238 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I0131 00:31:58.003787 112238 solver.cpp:266] Iteration 4200 (2.05207 iter/s, 24.3656s/50 iter), loss = 0.00102994
I0131 00:31:58.003815 112238 solver.cpp:285]     Train net output #0: loss = 0.00102993 (* 1 = 0.00102993 loss)
I0131 00:31:58.006036 112238 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0131 00:32:22.283253 112238 solver.cpp:266] Iteration 4250 (2.05962 iter/s, 24.2763s/50 iter), loss = 0.00293285
I0131 00:32:22.283366 112238 solver.cpp:285]     Train net output #0: loss = 0.00293284 (* 1 = 0.00293284 loss)
I0131 00:32:22.285499 112238 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I0131 00:32:46.541473 112238 solver.cpp:266] Iteration 4300 (2.06142 iter/s, 24.2551s/50 iter), loss = 0.00297052
I0131 00:32:46.541498 112238 solver.cpp:285]     Train net output #0: loss = 0.00297051 (* 1 = 0.00297051 loss)
I0131 00:32:46.543716 112238 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0131 00:33:10.840725 112238 solver.cpp:266] Iteration 4350 (2.05794 iter/s, 24.2961s/50 iter), loss = 0.00333715
I0131 00:33:10.840848 112238 solver.cpp:285]     Train net output #0: loss = 0.00333713 (* 1 = 0.00333713 loss)
I0131 00:33:10.840855 112238 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I0131 00:33:35.245059 112238 solver.cpp:266] Iteration 4400 (2.0489 iter/s, 24.4033s/50 iter), loss = 0.0234935
I0131 00:33:35.245092 112238 solver.cpp:285]     Train net output #0: loss = 0.0234935 (* 1 = 0.0234935 loss)
I0131 00:33:35.247310 112238 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0131 00:33:59.558884 112238 solver.cpp:266] Iteration 4450 (2.05671 iter/s, 24.3107s/50 iter), loss = 0.0368847
I0131 00:33:59.558959 112238 solver.cpp:285]     Train net output #0: loss = 0.0368847 (* 1 = 0.0368847 loss)
I0131 00:33:59.561216 112238 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I0131 00:34:23.744531 112238 solver.cpp:266] Iteration 4500 (2.06762 iter/s, 24.1824s/50 iter), loss = 0.00141574
I0131 00:34:23.744562 112238 solver.cpp:285]     Train net output #0: loss = 0.00141573 (* 1 = 0.00141573 loss)
I0131 00:34:23.746783 112238 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0131 00:34:48.006057 112238 solver.cpp:266] Iteration 4550 (2.06114 iter/s, 24.2584s/50 iter), loss = 0.000938269
I0131 00:34:48.006184 112238 solver.cpp:285]     Train net output #0: loss = 0.000938255 (* 1 = 0.000938255 loss)
I0131 00:34:48.008303 112238 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I0131 00:35:12.445822 112238 solver.cpp:266] Iteration 4600 (2.04611 iter/s, 24.4366s/50 iter), loss = 0.000447612
I0131 00:35:12.445853 112238 solver.cpp:285]     Train net output #0: loss = 0.000447597 (* 1 = 0.000447597 loss)
I0131 00:35:12.446725 112238 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0131 00:35:36.754582 112238 solver.cpp:266] Iteration 4650 (2.05702 iter/s, 24.307s/50 iter), loss = 0.0089637
I0131 00:35:36.754704 112238 solver.cpp:285]     Train net output #0: loss = 0.00896369 (* 1 = 0.00896369 loss)
I0131 00:35:36.756836 112238 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I0131 00:36:00.998395 112238 solver.cpp:266] Iteration 4700 (2.06265 iter/s, 24.2407s/50 iter), loss = 0.00150157
I0131 00:36:00.998426 112238 solver.cpp:285]     Train net output #0: loss = 0.00150155 (* 1 = 0.00150155 loss)
I0131 00:36:01.000659 112238 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0131 00:36:25.167657 112238 solver.cpp:266] Iteration 4750 (2.06901 iter/s, 24.1661s/50 iter), loss = 0.00832696
I0131 00:36:25.167788 112238 solver.cpp:285]     Train net output #0: loss = 0.00832694 (* 1 = 0.00832694 loss)
I0131 00:36:25.169903 112238 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I0131 00:36:49.547104 112238 solver.cpp:266] Iteration 4800 (2.05117 iter/s, 24.3763s/50 iter), loss = 0.00802783
I0131 00:36:49.547137 112238 solver.cpp:285]     Train net output #0: loss = 0.00802782 (* 1 = 0.00802782 loss)
I0131 00:36:49.547143 112238 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0131 00:37:13.873790 112238 solver.cpp:266] Iteration 4850 (2.05543 iter/s, 24.3258s/50 iter), loss = 0.00173567
I0131 00:37:13.873914 112238 solver.cpp:285]     Train net output #0: loss = 0.00173566 (* 1 = 0.00173566 loss)
I0131 00:37:13.876035 112238 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I0131 00:37:38.215157 112238 solver.cpp:266] Iteration 4900 (2.05438 iter/s, 24.3382s/50 iter), loss = 0.00833755
I0131 00:37:38.215188 112238 solver.cpp:285]     Train net output #0: loss = 0.00833754 (* 1 = 0.00833754 loss)
I0131 00:37:38.217404 112238 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0131 00:38:02.455761 112238 solver.cpp:266] Iteration 4950 (2.06292 iter/s, 24.2375s/50 iter), loss = 0.00250769
I0131 00:38:02.455878 112238 solver.cpp:285]     Train net output #0: loss = 0.00250768 (* 1 = 0.00250768 loss)
I0131 00:38:02.458016 112238 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I0131 00:38:26.375639 112238 solver.cpp:418] Iteration 5000, Testing net (#0)
I0131 00:38:29.741683 112238 solver.cpp:517]     Test net output #0: loss = 0.174704 (* 1 = 0.174704 loss)
I0131 00:38:29.741699 112238 solver.cpp:517]     Test net output #1: top-1 = 0.9515
I0131 00:38:30.292744 112238 solver.cpp:266] Iteration 5000 (1.79638 iter/s, 27.8337s/50 iter), loss = 0.02238
I0131 00:38:30.292769 112238 solver.cpp:285]     Train net output #0: loss = 0.02238 (* 1 = 0.02238 loss)
I0131 00:38:30.294998 112238 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0131 00:38:54.560981 112238 solver.cpp:266] Iteration 5050 (2.06057 iter/s, 24.2651s/50 iter), loss = 0.00488485
I0131 00:38:54.561112 112238 solver.cpp:285]     Train net output #0: loss = 0.00488484 (* 1 = 0.00488484 loss)
I0131 00:38:54.563228 112238 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I0131 00:39:18.931252 112238 solver.cpp:266] Iteration 5100 (2.05194 iter/s, 24.3671s/50 iter), loss = 0.00403037
I0131 00:39:18.931284 112238 solver.cpp:285]     Train net output #0: loss = 0.00403036 (* 1 = 0.00403036 loss)
I0131 00:39:18.932725 112238 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0131 00:39:43.230664 112238 solver.cpp:266] Iteration 5150 (2.05786 iter/s, 24.297s/50 iter), loss = 0.00833026
I0131 00:39:43.230772 112238 solver.cpp:285]     Train net output #0: loss = 0.00833025 (* 1 = 0.00833025 loss)
I0131 00:39:43.232921 112238 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I0131 00:40:07.445022 112238 solver.cpp:266] Iteration 5200 (2.06516 iter/s, 24.2112s/50 iter), loss = 0.000748042
I0131 00:40:07.445051 112238 solver.cpp:285]     Train net output #0: loss = 0.00074803 (* 1 = 0.00074803 loss)
I0131 00:40:07.447276 112238 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0131 00:40:31.600158 112238 solver.cpp:266] Iteration 5250 (2.07022 iter/s, 24.152s/50 iter), loss = 0.00057746
I0131 00:40:31.600281 112238 solver.cpp:285]     Train net output #0: loss = 0.000577451 (* 1 = 0.000577451 loss)
I0131 00:40:31.602399 112238 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I0131 00:40:56.096920 112238 solver.cpp:266] Iteration 5300 (2.04135 iter/s, 24.4936s/50 iter), loss = 0.00677777
I0131 00:40:56.096952 112238 solver.cpp:285]     Train net output #0: loss = 0.00677776 (* 1 = 0.00677776 loss)
I0131 00:40:56.097283 112238 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0131 00:41:20.337157 112238 solver.cpp:266] Iteration 5350 (2.06279 iter/s, 24.239s/50 iter), loss = 0.0047303
I0131 00:41:20.337296 112238 solver.cpp:285]     Train net output #0: loss = 0.00473029 (* 1 = 0.00473029 loss)
I0131 00:41:20.339375 112238 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I0131 00:41:44.463543 112238 solver.cpp:266] Iteration 5400 (2.07269 iter/s, 24.1233s/50 iter), loss = 0.00349848
I0131 00:41:44.463573 112238 solver.cpp:285]     Train net output #0: loss = 0.00349847 (* 1 = 0.00349847 loss)
I0131 00:41:44.465800 112238 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0131 00:42:08.724712 112238 solver.cpp:266] Iteration 5450 (2.06117 iter/s, 24.258s/50 iter), loss = 0.00139416
I0131 00:42:08.724838 112238 solver.cpp:285]     Train net output #0: loss = 0.00139415 (* 1 = 0.00139415 loss)
I0131 00:42:08.724844 112238 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I0131 00:42:33.178637 112238 solver.cpp:266] Iteration 5500 (2.04475 iter/s, 24.4529s/50 iter), loss = 0.00238813
I0131 00:42:33.178676 112238 solver.cpp:285]     Train net output #0: loss = 0.00238812 (* 1 = 0.00238812 loss)
I0131 00:42:33.180878 112238 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0131 00:42:57.401239 112238 solver.cpp:266] Iteration 5550 (2.06445 iter/s, 24.2195s/50 iter), loss = 0.00129468
I0131 00:42:57.401337 112238 solver.cpp:285]     Train net output #0: loss = 0.00129466 (* 1 = 0.00129466 loss)
I0131 00:42:57.403496 112238 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I0131 00:43:21.519410 112238 solver.cpp:266] Iteration 5600 (2.0734 iter/s, 24.115s/50 iter), loss = 0.00122623
I0131 00:43:21.519449 112238 solver.cpp:285]     Train net output #0: loss = 0.00122621 (* 1 = 0.00122621 loss)
I0131 00:43:21.521662 112238 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0131 00:43:45.968642 112238 solver.cpp:266] Iteration 5650 (2.04532 iter/s, 24.4461s/50 iter), loss = 0.00228329
I0131 00:43:45.968767 112238 solver.cpp:285]     Train net output #0: loss = 0.00228328 (* 1 = 0.00228328 loss)
I0131 00:43:45.968775 112238 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I0131 00:44:10.324832 112238 solver.cpp:266] Iteration 5700 (2.05295 iter/s, 24.3552s/50 iter), loss = 0.0015259
I0131 00:44:10.324860 112238 solver.cpp:285]     Train net output #0: loss = 0.00152589 (* 1 = 0.00152589 loss)
I0131 00:44:10.327082 112238 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0131 00:44:34.623982 112238 solver.cpp:266] Iteration 5750 (2.05795 iter/s, 24.296s/50 iter), loss = 0.00116802
I0131 00:44:34.624110 112238 solver.cpp:285]     Train net output #0: loss = 0.00116802 (* 1 = 0.00116802 loss)
I0131 00:44:34.626230 112238 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I0131 00:44:58.815426 112238 solver.cpp:266] Iteration 5800 (2.06711 iter/s, 24.1883s/50 iter), loss = 0.00403768
I0131 00:44:58.815456 112238 solver.cpp:285]     Train net output #0: loss = 0.00403768 (* 1 = 0.00403768 loss)
I0131 00:44:58.817673 112238 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0131 00:45:23.122190 112238 solver.cpp:266] Iteration 5850 (2.05731 iter/s, 24.3036s/50 iter), loss = 0.00425961
I0131 00:45:23.122314 112238 solver.cpp:285]     Train net output #0: loss = 0.00425961 (* 1 = 0.00425961 loss)
I0131 00:45:23.122321 112238 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I0131 00:45:47.632539 112238 solver.cpp:266] Iteration 5900 (2.04004 iter/s, 24.5093s/50 iter), loss = 0.00310501
I0131 00:45:47.632570 112238 solver.cpp:285]     Train net output #0: loss = 0.003105 (* 1 = 0.003105 loss)
I0131 00:45:47.634791 112238 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0131 00:46:11.893800 112238 solver.cpp:266] Iteration 5950 (2.06117 iter/s, 24.2581s/50 iter), loss = 0.00332987
I0131 00:46:11.893920 112238 solver.cpp:285]     Train net output #0: loss = 0.00332986 (* 1 = 0.00332986 loss)
I0131 00:46:11.896131 112238 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I0131 00:46:35.543435 112238 solver.cpp:418] Iteration 6000, Testing net (#0)
I0131 00:46:39.238906 112238 solver.cpp:517]     Test net output #0: loss = 0.192291 (* 1 = 0.192291 loss)
I0131 00:46:39.238924 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95175
I0131 00:46:39.719935 112238 solver.cpp:266] Iteration 6000 (1.79709 iter/s, 27.8228s/50 iter), loss = 0.0017542
I0131 00:46:39.719964 112238 solver.cpp:285]     Train net output #0: loss = 0.0017542 (* 1 = 0.0017542 loss)
I0131 00:46:39.720011 112238 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0131 00:47:04.020931 112238 solver.cpp:266] Iteration 6050 (2.05761 iter/s, 24.3s/50 iter), loss = 0.00183055
I0131 00:47:04.021035 112238 solver.cpp:285]     Train net output #0: loss = 0.00183054 (* 1 = 0.00183054 loss)
I0131 00:47:04.023187 112238 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I0131 00:47:28.299700 112238 solver.cpp:266] Iteration 6100 (2.05968 iter/s, 24.2756s/50 iter), loss = 0.000434005
I0131 00:47:28.299727 112238 solver.cpp:285]     Train net output #0: loss = 0.000433996 (* 1 = 0.000433996 loss)
I0131 00:47:28.301954 112238 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0131 00:47:52.405032 112238 solver.cpp:266] Iteration 6150 (2.0745 iter/s, 24.1022s/50 iter), loss = 0.00474342
I0131 00:47:52.405171 112238 solver.cpp:285]     Train net output #0: loss = 0.00474342 (* 1 = 0.00474342 loss)
I0131 00:47:52.407279 112238 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I0131 00:48:16.821728 112238 solver.cpp:266] Iteration 6200 (2.04804 iter/s, 24.4135s/50 iter), loss = 0.0015952
I0131 00:48:16.821759 112238 solver.cpp:285]     Train net output #0: loss = 0.00159519 (* 1 = 0.00159519 loss)
I0131 00:48:16.823969 112238 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0131 00:48:41.023891 112238 solver.cpp:266] Iteration 6250 (2.0662 iter/s, 24.199s/50 iter), loss = 0.00891026
I0131 00:48:41.023941 112238 solver.cpp:285]     Train net output #0: loss = 0.00891025 (* 1 = 0.00891025 loss)
I0131 00:48:41.023984 112238 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I0131 00:49:05.233783 112238 solver.cpp:266] Iteration 6300 (2.06536 iter/s, 24.2089s/50 iter), loss = 0.00307746
I0131 00:49:05.233822 112238 solver.cpp:285]     Train net output #0: loss = 0.00307745 (* 1 = 0.00307745 loss)
I0131 00:49:05.236042 112238 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0131 00:49:29.473397 112238 solver.cpp:266] Iteration 6350 (2.06301 iter/s, 24.2365s/50 iter), loss = 0.00441212
I0131 00:49:29.473529 112238 solver.cpp:285]     Train net output #0: loss = 0.00441211 (* 1 = 0.00441211 loss)
I0131 00:49:29.475641 112238 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I0131 00:49:53.987412 112238 solver.cpp:266] Iteration 6400 (2.03991 iter/s, 24.5109s/50 iter), loss = 0.000992378
I0131 00:49:53.987455 112238 solver.cpp:285]     Train net output #0: loss = 0.000992366 (* 1 = 0.000992366 loss)
I0131 00:49:53.990078 112238 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0131 00:50:18.259160 112238 solver.cpp:266] Iteration 6450 (2.06031 iter/s, 24.2682s/50 iter), loss = 0.00127007
I0131 00:50:18.259290 112238 solver.cpp:285]     Train net output #0: loss = 0.00127006 (* 1 = 0.00127006 loss)
I0131 00:50:18.261420 112238 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I0131 00:50:42.700788 112238 solver.cpp:266] Iteration 6500 (2.04595 iter/s, 24.4385s/50 iter), loss = 0.00300961
I0131 00:50:42.700815 112238 solver.cpp:285]     Train net output #0: loss = 0.0030096 (* 1 = 0.0030096 loss)
I0131 00:50:42.703042 112238 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0131 00:51:06.893494 112238 solver.cpp:266] Iteration 6550 (2.06701 iter/s, 24.1896s/50 iter), loss = 0.00146957
I0131 00:51:06.893613 112238 solver.cpp:285]     Train net output #0: loss = 0.00146956 (* 1 = 0.00146956 loss)
I0131 00:51:06.895745 112238 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I0131 00:51:31.222558 112238 solver.cpp:266] Iteration 6600 (2.05542 iter/s, 24.3259s/50 iter), loss = 0.00150695
I0131 00:51:31.222591 112238 solver.cpp:285]     Train net output #0: loss = 0.00150694 (* 1 = 0.00150694 loss)
I0131 00:51:31.222599 112238 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0131 00:51:55.654279 112238 solver.cpp:266] Iteration 6650 (2.0466 iter/s, 24.4308s/50 iter), loss = 0.00449108
I0131 00:51:55.654409 112238 solver.cpp:285]     Train net output #0: loss = 0.00449107 (* 1 = 0.00449107 loss)
I0131 00:51:55.656535 112238 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I0131 00:52:19.917861 112238 solver.cpp:266] Iteration 6700 (2.06097 iter/s, 24.2604s/50 iter), loss = 0.00131769
I0131 00:52:19.917891 112238 solver.cpp:285]     Train net output #0: loss = 0.00131768 (* 1 = 0.00131768 loss)
I0131 00:52:19.920115 112238 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0131 00:52:44.109258 112238 solver.cpp:266] Iteration 6750 (2.06712 iter/s, 24.1882s/50 iter), loss = 0.00123415
I0131 00:52:44.109364 112238 solver.cpp:285]     Train net output #0: loss = 0.00123413 (* 1 = 0.00123413 loss)
I0131 00:52:44.111510 112238 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I0131 00:53:08.380568 112238 solver.cpp:266] Iteration 6800 (2.06031 iter/s, 24.2682s/50 iter), loss = 0.000469901
I0131 00:53:08.380600 112238 solver.cpp:285]     Train net output #0: loss = 0.00046989 (* 1 = 0.00046989 loss)
I0131 00:53:08.380605 112238 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0131 00:53:32.826488 112238 solver.cpp:266] Iteration 6850 (2.04541 iter/s, 24.445s/50 iter), loss = 0.00224108
I0131 00:53:32.826592 112238 solver.cpp:285]     Train net output #0: loss = 0.00224107 (* 1 = 0.00224107 loss)
I0131 00:53:32.827872 112238 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I0131 00:53:57.094686 112238 solver.cpp:266] Iteration 6900 (2.0605 iter/s, 24.2659s/50 iter), loss = 0.00204311
I0131 00:53:57.094724 112238 solver.cpp:285]     Train net output #0: loss = 0.0020431 (* 1 = 0.0020431 loss)
I0131 00:53:57.096992 112238 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0131 00:54:21.252384 112238 solver.cpp:266] Iteration 6950 (2.07001 iter/s, 24.1545s/50 iter), loss = 0.0127626
I0131 00:54:21.252506 112238 solver.cpp:285]     Train net output #0: loss = 0.0127626 (* 1 = 0.0127626 loss)
I0131 00:54:21.254633 112238 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I0131 00:54:45.147889 112238 solver.cpp:418] Iteration 7000, Testing net (#0)
I0131 00:54:48.612951 112238 solver.cpp:517]     Test net output #0: loss = 0.207612 (* 1 = 0.207612 loss)
I0131 00:54:48.612967 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95375
I0131 00:54:49.126874 112238 solver.cpp:266] Iteration 7000 (1.79397 iter/s, 27.8712s/50 iter), loss = 0.000949947
I0131 00:54:49.126899 112238 solver.cpp:285]     Train net output #0: loss = 0.000949935 (* 1 = 0.000949935 loss)
I0131 00:54:49.126966 112238 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0131 00:55:13.456076 112238 solver.cpp:266] Iteration 7050 (2.05523 iter/s, 24.3282s/50 iter), loss = 0.00233365
I0131 00:55:13.456226 112238 solver.cpp:285]     Train net output #0: loss = 0.00233364 (* 1 = 0.00233364 loss)
I0131 00:55:13.458320 112238 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I0131 00:55:37.623358 112238 solver.cpp:266] Iteration 7100 (2.06918 iter/s, 24.1641s/50 iter), loss = 0.0138691
I0131 00:55:37.623387 112238 solver.cpp:285]     Train net output #0: loss = 0.0138691 (* 1 = 0.0138691 loss)
I0131 00:55:37.625602 112238 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0131 00:56:02.046254 112238 solver.cpp:266] Iteration 7150 (2.04752 iter/s, 24.4198s/50 iter), loss = 0.0206364
I0131 00:56:02.046362 112238 solver.cpp:285]     Train net output #0: loss = 0.0206364 (* 1 = 0.0206364 loss)
I0131 00:56:02.046386 112238 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I0131 00:56:26.416925 112238 solver.cpp:266] Iteration 7200 (2.05173 iter/s, 24.3697s/50 iter), loss = 0.00190237
I0131 00:56:26.416959 112238 solver.cpp:285]     Train net output #0: loss = 0.00190236 (* 1 = 0.00190236 loss)
I0131 00:56:26.419178 112238 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0131 00:56:50.697044 112238 solver.cpp:266] Iteration 7250 (2.05956 iter/s, 24.277s/50 iter), loss = 0.0030054
I0131 00:56:50.697161 112238 solver.cpp:285]     Train net output #0: loss = 0.00300539 (* 1 = 0.00300539 loss)
I0131 00:56:50.699383 112238 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I0131 00:57:14.910148 112238 solver.cpp:266] Iteration 7300 (2.06527 iter/s, 24.2099s/50 iter), loss = 0.00156354
I0131 00:57:14.910178 112238 solver.cpp:285]     Train net output #0: loss = 0.00156353 (* 1 = 0.00156353 loss)
I0131 00:57:14.912358 112238 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0131 00:57:39.232411 112238 solver.cpp:266] Iteration 7350 (2.05599 iter/s, 24.3192s/50 iter), loss = 0.00029977
I0131 00:57:39.232509 112238 solver.cpp:285]     Train net output #0: loss = 0.000299761 (* 1 = 0.000299761 loss)
I0131 00:57:39.232532 112238 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I0131 00:58:03.616981 112238 solver.cpp:266] Iteration 7400 (2.05056 iter/s, 24.3836s/50 iter), loss = 0.00894045
I0131 00:58:03.617012 112238 solver.cpp:285]     Train net output #0: loss = 0.00894044 (* 1 = 0.00894044 loss)
I0131 00:58:03.619230 112238 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0131 00:58:27.912341 112238 solver.cpp:266] Iteration 7450 (2.05827 iter/s, 24.2922s/50 iter), loss = 0.00221972
I0131 00:58:27.912444 112238 solver.cpp:285]     Train net output #0: loss = 0.00221971 (* 1 = 0.00221971 loss)
I0131 00:58:27.914605 112238 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I0131 00:58:52.100543 112238 solver.cpp:266] Iteration 7500 (2.06739 iter/s, 24.185s/50 iter), loss = 0.00575999
I0131 00:58:52.100574 112238 solver.cpp:285]     Train net output #0: loss = 0.00575998 (* 1 = 0.00575998 loss)
I0131 00:58:52.102788 112238 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I0131 00:59:16.532862 112238 solver.cpp:266] Iteration 7550 (2.04673 iter/s, 24.4292s/50 iter), loss = 0.00168471
I0131 00:59:16.532968 112238 solver.cpp:285]     Train net output #0: loss = 0.00168471 (* 1 = 0.00168471 loss)
I0131 00:59:16.533229 112238 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I0131 00:59:40.857823 112238 solver.cpp:266] Iteration 7600 (2.05561 iter/s, 24.3237s/50 iter), loss = 0.000800584
I0131 00:59:40.857856 112238 solver.cpp:285]     Train net output #0: loss = 0.000800577 (* 1 = 0.000800577 loss)
I0131 00:59:40.860074 112238 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I0131 01:00:05.164811 112238 solver.cpp:266] Iteration 7650 (2.05729 iter/s, 24.3038s/50 iter), loss = 0.00292285
I0131 01:00:05.164897 112238 solver.cpp:285]     Train net output #0: loss = 0.00292285 (* 1 = 0.00292285 loss)
I0131 01:00:05.167063 112238 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I0131 01:00:29.464831 112238 solver.cpp:266] Iteration 7700 (2.05788 iter/s, 24.2969s/50 iter), loss = 0.00269123
I0131 01:00:29.464860 112238 solver.cpp:285]     Train net output #0: loss = 0.00269122 (* 1 = 0.00269122 loss)
I0131 01:00:29.467088 112238 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I0131 01:00:53.751480 112238 solver.cpp:266] Iteration 7750 (2.05901 iter/s, 24.2835s/50 iter), loss = 0.000696984
I0131 01:00:53.751605 112238 solver.cpp:285]     Train net output #0: loss = 0.000696977 (* 1 = 0.000696977 loss)
I0131 01:00:53.753736 112238 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I0131 01:01:18.196691 112238 solver.cpp:266] Iteration 7800 (2.04565 iter/s, 24.4421s/50 iter), loss = 0.00149855
I0131 01:01:18.196722 112238 solver.cpp:285]     Train net output #0: loss = 0.00149855 (* 1 = 0.00149855 loss)
I0131 01:01:18.196807 112238 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I0131 01:01:42.472467 112238 solver.cpp:266] Iteration 7850 (2.05975 iter/s, 24.2748s/50 iter), loss = 0.00115584
I0131 01:01:42.472524 112238 solver.cpp:285]     Train net output #0: loss = 0.00115584 (* 1 = 0.00115584 loss)
I0131 01:01:42.474710 112238 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I0131 01:02:06.675442 112238 solver.cpp:266] Iteration 7900 (2.06613 iter/s, 24.1998s/50 iter), loss = 0.00101993
I0131 01:02:06.675472 112238 solver.cpp:285]     Train net output #0: loss = 0.00101992 (* 1 = 0.00101992 loss)
I0131 01:02:06.677687 112238 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I0131 01:02:30.891181 112238 solver.cpp:266] Iteration 7950 (2.06504 iter/s, 24.2126s/50 iter), loss = 0.000840525
I0131 01:02:30.891290 112238 solver.cpp:285]     Train net output #0: loss = 0.000840518 (* 1 = 0.000840518 loss)
I0131 01:02:30.893419 112238 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I0131 01:02:54.774899 112238 solver.cpp:418] Iteration 8000, Testing net (#0)
I0131 01:02:58.162107 112238 solver.cpp:517]     Test net output #0: loss = 0.223495 (* 1 = 0.223495 loss)
I0131 01:02:58.162127 112238 solver.cpp:517]     Test net output #1: top-1 = 0.9535
I0131 01:02:58.669039 112238 solver.cpp:266] Iteration 8000 (1.80021 iter/s, 27.7746s/50 iter), loss = 0.000383377
I0131 01:02:58.669070 112238 solver.cpp:285]     Train net output #0: loss = 0.000383372 (* 1 = 0.000383372 loss)
I0131 01:02:58.671293 112238 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I0131 01:03:22.969398 112238 solver.cpp:266] Iteration 8050 (2.05785 iter/s, 24.2972s/50 iter), loss = 0.000599213
I0131 01:03:22.969506 112238 solver.cpp:285]     Train net output #0: loss = 0.000599206 (* 1 = 0.000599206 loss)
I0131 01:03:22.969530 112238 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I0131 01:03:47.314787 112238 solver.cpp:266] Iteration 8100 (2.05386 iter/s, 24.3444s/50 iter), loss = 0.00477864
I0131 01:03:47.314818 112238 solver.cpp:285]     Train net output #0: loss = 0.00477863 (* 1 = 0.00477863 loss)
I0131 01:03:47.317039 112238 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I0131 01:04:11.545507 112238 solver.cpp:266] Iteration 8150 (2.06376 iter/s, 24.2276s/50 iter), loss = 0.003305
I0131 01:04:11.545624 112238 solver.cpp:285]     Train net output #0: loss = 0.00330499 (* 1 = 0.00330499 loss)
I0131 01:04:11.547773 112238 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I0131 01:04:35.930743 112238 solver.cpp:266] Iteration 8200 (2.05069 iter/s, 24.3821s/50 iter), loss = 0.0273724
I0131 01:04:35.930773 112238 solver.cpp:285]     Train net output #0: loss = 0.0273724 (* 1 = 0.0273724 loss)
I0131 01:04:35.933002 112238 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I0131 01:05:00.098235 112238 solver.cpp:266] Iteration 8250 (2.06916 iter/s, 24.1643s/50 iter), loss = 0.000668799
I0131 01:05:00.098389 112238 solver.cpp:285]     Train net output #0: loss = 0.000668792 (* 1 = 0.000668792 loss)
I0131 01:05:00.098398 112238 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I0131 01:05:24.568245 112238 solver.cpp:266] Iteration 8300 (2.04341 iter/s, 24.4689s/50 iter), loss = 0.000373757
I0131 01:05:24.568274 112238 solver.cpp:285]     Train net output #0: loss = 0.00037375 (* 1 = 0.00037375 loss)
I0131 01:05:24.568615 112238 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I0131 01:05:48.823627 112238 solver.cpp:266] Iteration 8350 (2.06151 iter/s, 24.2541s/50 iter), loss = 0.000441428
I0131 01:05:48.823680 112238 solver.cpp:285]     Train net output #0: loss = 0.000441419 (* 1 = 0.000441419 loss)
I0131 01:05:48.825933 112238 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I0131 01:06:12.954090 112238 solver.cpp:266] Iteration 8400 (2.07234 iter/s, 24.1273s/50 iter), loss = 0.00108797
I0131 01:06:12.954120 112238 solver.cpp:285]     Train net output #0: loss = 0.00108796 (* 1 = 0.00108796 loss)
I0131 01:06:12.956341 112238 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I0131 01:06:37.343991 112238 solver.cpp:266] Iteration 8450 (2.05029 iter/s, 24.3867s/50 iter), loss = 0.00129897
I0131 01:06:37.344069 112238 solver.cpp:285]     Train net output #0: loss = 0.00129896 (* 1 = 0.00129896 loss)
I0131 01:06:37.346228 112238 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I0131 01:07:01.724664 112238 solver.cpp:266] Iteration 8500 (2.05107 iter/s, 24.3775s/50 iter), loss = 0.00263745
I0131 01:07:01.724694 112238 solver.cpp:285]     Train net output #0: loss = 0.00263743 (* 1 = 0.00263743 loss)
I0131 01:07:01.726927 112238 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I0131 01:07:26.063218 112238 solver.cpp:266] Iteration 8550 (2.05462 iter/s, 24.3354s/50 iter), loss = 0.000649389
I0131 01:07:26.063333 112238 solver.cpp:285]     Train net output #0: loss = 0.000649377 (* 1 = 0.000649377 loss)
I0131 01:07:26.065549 112238 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I0131 01:07:50.242560 112238 solver.cpp:266] Iteration 8600 (2.06816 iter/s, 24.1761s/50 iter), loss = 0.00030468
I0131 01:07:50.242592 112238 solver.cpp:285]     Train net output #0: loss = 0.000304667 (* 1 = 0.000304667 loss)
I0131 01:07:50.244838 112238 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I0131 01:08:14.514362 112238 solver.cpp:266] Iteration 8650 (2.06027 iter/s, 24.2686s/50 iter), loss = 0.0027029
I0131 01:08:14.514469 112238 solver.cpp:285]     Train net output #0: loss = 0.00270289 (* 1 = 0.00270289 loss)
I0131 01:08:14.514479 112238 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I0131 01:08:38.967092 112238 solver.cpp:266] Iteration 8700 (2.04485 iter/s, 24.4517s/50 iter), loss = 0.0015419
I0131 01:08:38.967124 112238 solver.cpp:285]     Train net output #0: loss = 0.00154189 (* 1 = 0.00154189 loss)
I0131 01:08:38.969372 112238 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I0131 01:09:03.293881 112238 solver.cpp:266] Iteration 8750 (2.05562 iter/s, 24.3236s/50 iter), loss = 0.00855378
I0131 01:09:03.294013 112238 solver.cpp:285]     Train net output #0: loss = 0.00855377 (* 1 = 0.00855377 loss)
I0131 01:09:03.294052 112238 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I0131 01:09:27.607697 112238 solver.cpp:266] Iteration 8800 (2.05653 iter/s, 24.3127s/50 iter), loss = 0.0104983
I0131 01:09:27.607728 112238 solver.cpp:285]     Train net output #0: loss = 0.0104982 (* 1 = 0.0104982 loss)
I0131 01:09:27.609961 112238 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I0131 01:09:51.863380 112238 solver.cpp:266] Iteration 8850 (2.06164 iter/s, 24.2525s/50 iter), loss = 0.00645312
I0131 01:09:51.863433 112238 solver.cpp:285]     Train net output #0: loss = 0.0064531 (* 1 = 0.0064531 loss)
I0131 01:09:51.865628 112238 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I0131 01:10:16.320945 112238 solver.cpp:266] Iteration 8900 (2.04462 iter/s, 24.4544s/50 iter), loss = 0.00010352
I0131 01:10:16.320982 112238 solver.cpp:285]     Train net output #0: loss = 0.000103501 (* 1 = 0.000103501 loss)
I0131 01:10:16.320991 112238 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I0131 01:10:40.627372 112238 solver.cpp:266] Iteration 8950 (2.05715 iter/s, 24.3055s/50 iter), loss = 0.00371543
I0131 01:10:40.627509 112238 solver.cpp:285]     Train net output #0: loss = 0.00371541 (* 1 = 0.00371541 loss)
I0131 01:10:40.630030 112238 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I0131 01:11:04.370120 112238 solver.cpp:418] Iteration 9000, Testing net (#0)
I0131 01:11:08.156394 112238 solver.cpp:517]     Test net output #0: loss = 0.232235 (* 1 = 0.232235 loss)
I0131 01:11:08.156414 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I0131 01:11:08.460674 112238 solver.cpp:266] Iteration 9000 (1.79665 iter/s, 27.8296s/50 iter), loss = 0.000324458
I0131 01:11:08.460701 112238 solver.cpp:285]     Train net output #0: loss = 0.000324434 (* 1 = 0.000324434 loss)
I0131 01:11:08.462914 112238 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I0131 01:11:32.902424 112238 solver.cpp:266] Iteration 9050 (2.04594 iter/s, 24.4386s/50 iter), loss = 0.0146247
I0131 01:11:32.902565 112238 solver.cpp:285]     Train net output #0: loss = 0.0146247 (* 1 = 0.0146247 loss)
I0131 01:11:32.904686 112238 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I0131 01:11:57.150174 112238 solver.cpp:266] Iteration 9100 (2.06232 iter/s, 24.2446s/50 iter), loss = 0.00136575
I0131 01:11:57.150205 112238 solver.cpp:285]     Train net output #0: loss = 0.00136572 (* 1 = 0.00136572 loss)
I0131 01:11:57.152434 112238 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I0131 01:12:21.421746 112238 solver.cpp:266] Iteration 9150 (2.06029 iter/s, 24.2684s/50 iter), loss = 0.0011467
I0131 01:12:21.421854 112238 solver.cpp:285]     Train net output #0: loss = 0.00114668 (* 1 = 0.00114668 loss)
I0131 01:12:21.423993 112238 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I0131 01:12:45.772739 112238 solver.cpp:266] Iteration 9200 (2.05357 iter/s, 24.3478s/50 iter), loss = 0.000853922
I0131 01:12:45.772775 112238 solver.cpp:285]     Train net output #0: loss = 0.000853899 (* 1 = 0.000853899 loss)
I0131 01:12:45.772784 112238 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I0131 01:13:10.174180 112238 solver.cpp:266] Iteration 9250 (2.04914 iter/s, 24.4005s/50 iter), loss = 0.000608372
I0131 01:13:10.174302 112238 solver.cpp:285]     Train net output #0: loss = 0.000608347 (* 1 = 0.000608347 loss)
I0131 01:13:10.176427 112238 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I0131 01:13:34.549300 112238 solver.cpp:266] Iteration 9300 (2.05154 iter/s, 24.372s/50 iter), loss = 0.00260629
I0131 01:13:34.549329 112238 solver.cpp:285]     Train net output #0: loss = 0.00260627 (* 1 = 0.00260627 loss)
I0131 01:13:34.549510 112238 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I0131 01:13:58.707252 112238 solver.cpp:266] Iteration 9350 (2.06981 iter/s, 24.1569s/50 iter), loss = 0.00214712
I0131 01:13:58.707307 112238 solver.cpp:285]     Train net output #0: loss = 0.00214709 (* 1 = 0.00214709 loss)
I0131 01:13:58.709502 112238 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I0131 01:14:22.958457 112238 solver.cpp:266] Iteration 9400 (2.06202 iter/s, 24.2481s/50 iter), loss = 0.00540575
I0131 01:14:22.958488 112238 solver.cpp:285]     Train net output #0: loss = 0.00540573 (* 1 = 0.00540573 loss)
I0131 01:14:22.958510 112238 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I0131 01:14:47.496424 112238 solver.cpp:266] Iteration 9450 (2.03774 iter/s, 24.537s/50 iter), loss = 0.010174
I0131 01:14:47.496541 112238 solver.cpp:285]     Train net output #0: loss = 0.010174 (* 1 = 0.010174 loss)
I0131 01:14:47.498673 112238 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I0131 01:15:11.886740 112238 solver.cpp:266] Iteration 9500 (2.05026 iter/s, 24.3872s/50 iter), loss = 0.000854698
I0131 01:15:11.886767 112238 solver.cpp:285]     Train net output #0: loss = 0.000854674 (* 1 = 0.000854674 loss)
I0131 01:15:11.887687 112238 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I0131 01:15:36.076090 112238 solver.cpp:266] Iteration 9550 (2.06718 iter/s, 24.1875s/50 iter), loss = 0.0111395
I0131 01:15:36.076210 112238 solver.cpp:285]     Train net output #0: loss = 0.0111395 (* 1 = 0.0111395 loss)
I0131 01:15:36.078363 112238 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I0131 01:16:00.414489 112238 solver.cpp:266] Iteration 9600 (2.05463 iter/s, 24.3352s/50 iter), loss = 0.000630743
I0131 01:16:00.414520 112238 solver.cpp:285]     Train net output #0: loss = 0.00063072 (* 1 = 0.00063072 loss)
I0131 01:16:00.416740 112238 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I0131 01:16:24.912371 112238 solver.cpp:266] Iteration 9650 (2.04126 iter/s, 24.4947s/50 iter), loss = 0.00506555
I0131 01:16:24.912451 112238 solver.cpp:285]     Train net output #0: loss = 0.00506553 (* 1 = 0.00506553 loss)
I0131 01:16:24.912696 112238 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I0131 01:16:49.319445 112238 solver.cpp:266] Iteration 9700 (2.04869 iter/s, 24.4058s/50 iter), loss = 0.000446963
I0131 01:16:49.319478 112238 solver.cpp:285]     Train net output #0: loss = 0.00044694 (* 1 = 0.00044694 loss)
I0131 01:16:49.321696 112238 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I0131 01:17:13.630594 112238 solver.cpp:266] Iteration 9750 (2.05694 iter/s, 24.308s/50 iter), loss = 0.000994014
I0131 01:17:13.630723 112238 solver.cpp:285]     Train net output #0: loss = 0.000993991 (* 1 = 0.000993991 loss)
I0131 01:17:13.632853 112238 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I0131 01:17:37.827580 112238 solver.cpp:266] Iteration 9800 (2.06664 iter/s, 24.1938s/50 iter), loss = 0.00318444
I0131 01:17:37.827610 112238 solver.cpp:285]     Train net output #0: loss = 0.00318442 (* 1 = 0.00318442 loss)
I0131 01:17:37.829826 112238 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I0131 01:18:02.221207 112238 solver.cpp:266] Iteration 9850 (2.04998 iter/s, 24.3905s/50 iter), loss = 0.00143276
I0131 01:18:02.221312 112238 solver.cpp:285]     Train net output #0: loss = 0.00143274 (* 1 = 0.00143274 loss)
I0131 01:18:02.221355 112238 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I0131 01:18:26.182145 112238 solver.cpp:266] Iteration 9900 (2.08682 iter/s, 23.9599s/50 iter), loss = 0.00395063
I0131 01:18:26.182176 112238 solver.cpp:285]     Train net output #0: loss = 0.0039506 (* 1 = 0.0039506 loss)
I0131 01:18:26.184404 112238 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I0131 01:18:50.451339 112238 solver.cpp:266] Iteration 9950 (2.06049 iter/s, 24.266s/50 iter), loss = 0.00436052
I0131 01:18:50.451442 112238 solver.cpp:285]     Train net output #0: loss = 0.0043605 (* 1 = 0.0043605 loss)
I0131 01:18:50.453593 112238 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I0131 01:19:14.370587 112238 solver.cpp:418] Iteration 10000, Testing net (#0)
I0131 01:19:17.773159 112238 solver.cpp:517]     Test net output #0: loss = 0.235029 (* 1 = 0.235029 loss)
I0131 01:19:17.773180 112238 solver.cpp:517]     Test net output #1: top-1 = 0.9525
I0131 01:19:18.290029 112238 solver.cpp:266] Iteration 10000 (1.79627 iter/s, 27.8354s/50 iter), loss = 0.00431481
I0131 01:19:18.290052 112238 solver.cpp:285]     Train net output #0: loss = 0.00431479 (* 1 = 0.00431479 loss)
I0131 01:19:18.292280 112238 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I0131 01:19:42.630424 112238 solver.cpp:266] Iteration 10050 (2.05446 iter/s, 24.3372s/50 iter), loss = 0.000802662
I0131 01:19:42.630520 112238 solver.cpp:285]     Train net output #0: loss = 0.00080264 (* 1 = 0.00080264 loss)
I0131 01:19:42.632673 112238 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I0131 01:20:07.088073 112238 solver.cpp:266] Iteration 10100 (2.04461 iter/s, 24.4545s/50 iter), loss = 0.00104817
I0131 01:20:07.088105 112238 solver.cpp:285]     Train net output #0: loss = 0.00104815 (* 1 = 0.00104815 loss)
I0131 01:20:07.088112 112238 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I0131 01:20:31.472712 112238 solver.cpp:266] Iteration 10150 (2.05055 iter/s, 24.3837s/50 iter), loss = 0.010139
I0131 01:20:31.472815 112238 solver.cpp:285]     Train net output #0: loss = 0.0101389 (* 1 = 0.0101389 loss)
I0131 01:20:31.474979 112238 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I0131 01:20:55.806329 112238 solver.cpp:266] Iteration 10200 (2.05504 iter/s, 24.3305s/50 iter), loss = 0.000606782
I0131 01:20:55.806358 112238 solver.cpp:285]     Train net output #0: loss = 0.000606762 (* 1 = 0.000606762 loss)
I0131 01:20:55.806586 112238 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I0131 01:21:20.242347 112238 solver.cpp:266] Iteration 10250 (2.04626 iter/s, 24.4349s/50 iter), loss = 0.000401238
I0131 01:21:20.242501 112238 solver.cpp:285]     Train net output #0: loss = 0.000401218 (* 1 = 0.000401218 loss)
I0131 01:21:20.244578 112238 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I0131 01:21:44.632668 112238 solver.cpp:266] Iteration 10300 (2.05026 iter/s, 24.3872s/50 iter), loss = 0.00078107
I0131 01:21:44.632697 112238 solver.cpp:285]     Train net output #0: loss = 0.000781049 (* 1 = 0.000781049 loss)
I0131 01:21:44.634915 112238 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I0131 01:22:08.962093 112238 solver.cpp:266] Iteration 10350 (2.05539 iter/s, 24.3263s/50 iter), loss = 0.00136173
I0131 01:22:08.962780 112238 solver.cpp:285]     Train net output #0: loss = 0.00136171 (* 1 = 0.00136171 loss)
I0131 01:22:08.962788 112238 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I0131 01:22:33.245967 112238 solver.cpp:266] Iteration 10400 (2.05911 iter/s, 24.2823s/50 iter), loss = 0.00133344
I0131 01:22:33.246001 112238 solver.cpp:285]     Train net output #0: loss = 0.00133342 (* 1 = 0.00133342 loss)
I0131 01:22:33.248220 112238 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I0131 01:22:57.637209 112238 solver.cpp:266] Iteration 10450 (2.05018 iter/s, 24.3881s/50 iter), loss = 0.00129238
I0131 01:22:57.637326 112238 solver.cpp:285]     Train net output #0: loss = 0.00129236 (* 1 = 0.00129236 loss)
I0131 01:22:57.639472 112238 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I0131 01:23:21.975525 112238 solver.cpp:266] Iteration 10500 (2.05464 iter/s, 24.3352s/50 iter), loss = 0.00383647
I0131 01:23:21.975553 112238 solver.cpp:285]     Train net output #0: loss = 0.00383645 (* 1 = 0.00383645 loss)
I0131 01:23:21.977774 112238 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I0131 01:23:46.337182 112238 solver.cpp:266] Iteration 10550 (2.05267 iter/s, 24.3585s/50 iter), loss = 0.00110003
I0131 01:23:46.337304 112238 solver.cpp:285]     Train net output #0: loss = 0.0011 (* 1 = 0.0011 loss)
I0131 01:23:46.337311 112238 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I0131 01:24:10.826148 112238 solver.cpp:266] Iteration 10600 (2.04182 iter/s, 24.4879s/50 iter), loss = 0.00320898
I0131 01:24:10.826181 112238 solver.cpp:285]     Train net output #0: loss = 0.00320896 (* 1 = 0.00320896 loss)
I0131 01:24:10.828411 112238 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I0131 01:24:35.148073 112238 solver.cpp:266] Iteration 10650 (2.05603 iter/s, 24.3188s/50 iter), loss = 0.00410811
I0131 01:24:35.148172 112238 solver.cpp:285]     Train net output #0: loss = 0.00410809 (* 1 = 0.00410809 loss)
I0131 01:24:35.148216 112238 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I0131 01:24:59.514899 112238 solver.cpp:266] Iteration 10700 (2.05206 iter/s, 24.3658s/50 iter), loss = 0.00349205
I0131 01:24:59.514928 112238 solver.cpp:285]     Train net output #0: loss = 0.00349203 (* 1 = 0.00349203 loss)
I0131 01:24:59.517149 112238 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I0131 01:25:23.874796 112238 solver.cpp:266] Iteration 10750 (2.05282 iter/s, 24.3567s/50 iter), loss = 0.0189847
I0131 01:25:23.874935 112238 solver.cpp:285]     Train net output #0: loss = 0.0189847 (* 1 = 0.0189847 loss)
I0131 01:25:23.877049 112238 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I0131 01:25:48.187387 112238 solver.cpp:266] Iteration 10800 (2.05681 iter/s, 24.3094s/50 iter), loss = 0.0136922
I0131 01:25:48.187419 112238 solver.cpp:285]     Train net output #0: loss = 0.0136922 (* 1 = 0.0136922 loss)
I0131 01:25:48.187425 112238 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I0131 01:26:12.724531 112238 solver.cpp:266] Iteration 10850 (2.0378 iter/s, 24.5362s/50 iter), loss = 0.011864
I0131 01:26:12.724640 112238 solver.cpp:285]     Train net output #0: loss = 0.0118639 (* 1 = 0.0118639 loss)
I0131 01:26:12.726791 112238 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I0131 01:26:37.077266 112238 solver.cpp:266] Iteration 10900 (2.05342 iter/s, 24.3496s/50 iter), loss = 0.00317762
I0131 01:26:37.077306 112238 solver.cpp:285]     Train net output #0: loss = 0.00317759 (* 1 = 0.00317759 loss)
I0131 01:26:37.077347 112238 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I0131 01:27:01.460225 112238 solver.cpp:266] Iteration 10950 (2.0507 iter/s, 24.382s/50 iter), loss = 0.000545296
I0131 01:27:01.460353 112238 solver.cpp:285]     Train net output #0: loss = 0.000545269 (* 1 = 0.000545269 loss)
I0131 01:27:01.462472 112238 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I0131 01:27:25.371596 112238 solver.cpp:418] Iteration 11000, Testing net (#0)
I0131 01:27:28.886606 112238 solver.cpp:517]     Test net output #0: loss = 0.236795 (* 1 = 0.236795 loss)
I0131 01:27:28.886623 112238 solver.cpp:517]     Test net output #1: top-1 = 0.95275
I0131 01:27:29.348973 112238 solver.cpp:266] Iteration 11000 (1.79305 iter/s, 27.8855s/50 iter), loss = 0.00140329
I0131 01:27:29.348999 112238 solver.cpp:285]     Train net output #0: loss = 0.00140326 (* 1 = 0.00140326 loss)
I0131 01:27:29.351224 112238 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I0131 01:27:53.790765 112238 solver.cpp:266] Iteration 11050 (2.04594 iter/s, 24.4386s/50 iter), loss = 0.00081971
I0131 01:27:53.790891 112238 solver.cpp:285]     Train net output #0: loss = 0.000819683 (* 1 = 0.000819683 loss)
I0131 01:27:53.793001 112238 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I0131 01:28:18.072789 112238 solver.cpp:266] Iteration 11100 (2.0594 iter/s, 24.2789s/50 iter), loss = 0.00166057
I0131 01:28:18.072820 112238 solver.cpp:285]     Train net output #0: loss = 0.00166055 (* 1 = 0.00166055 loss)
I0131 01:28:18.075044 112238 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I0131 01:28:42.260371 112238 solver.cpp:266] Iteration 11150 (2.06745 iter/s, 24.1844s/50 iter), loss = 0.0057657
I0131 01:28:42.260426 112238 solver.cpp:285]     Train net output #0: loss = 0.00576567 (* 1 = 0.00576567 loss)
I0131 01:28:42.260471 112238 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I0131 01:29:06.790323 112238 solver.cpp:266] Iteration 11200 (2.03841 iter/s, 24.529s/50 iter), loss = 0.00365311
I0131 01:29:06.790350 112238 solver.cpp:285]     Train net output #0: loss = 0.00365308 (* 1 = 0.00365308 loss)
I0131 01:29:06.792572 112238 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I0131 01:29:31.021987 112238 solver.cpp:266] Iteration 11250 (2.06368 iter/s, 24.2285s/50 iter), loss = 0.00556591
I0131 01:29:31.022089 112238 solver.cpp:285]     Train net output #0: loss = 0.00556588 (* 1 = 0.00556588 loss)
I0131 01:29:31.024240 112238 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I0131 01:29:55.179833 112238 solver.cpp:266] Iteration 11300 (2.06999 iter/s, 24.1547s/50 iter), loss = 0.0248686
I0131 01:29:55.179863 112238 solver.cpp:285]     Train net output #0: loss = 0.0248686 (* 1 = 0.0248686 loss)
I0131 01:29:55.182081 112238 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I0131 01:30:19.596542 112238 solver.cpp:266] Iteration 11350 (2.04804 iter/s, 24.4136s/50 iter), loss = 0.00133462
I0131 01:30:19.596630 112238 solver.cpp:285]     Train net output #0: loss = 0.00133459 (* 1 = 0.00133459 loss)
I0131 01:30:19.596638 112238 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I0131 01:30:44.005268 112238 solver.cpp:266] Iteration 11400 (2.04853 iter/s, 24.4077s/50 iter), loss = 0.0003567
I0131 01:30:44.005298 112238 solver.cpp:285]     Train net output #0: loss = 0.000356664 (* 1 = 0.000356664 loss)
I0131 01:30:44.007520 112238 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I0131 01:31:08.493206 112238 solver.cpp:266] Iteration 11450 (2.04208 iter/s, 24.4848s/50 iter), loss = 0.000573669
I0131 01:31:08.493324 112238 solver.cpp:285]     Train net output #0: loss = 0.000573631 (* 1 = 0.000573631 loss)
I0131 01:31:08.493369 112238 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I0131 01:31:32.733006 112238 solver.cpp:266] Iteration 11500 (2.06281 iter/s, 24.2387s/50 iter), loss = 0.00151753
I0131 01:31:32.733033 112238 solver.cpp:285]     Train net output #0: loss = 0.00151749 (* 1 = 0.00151749 loss)
I0131 01:31:32.735252 112238 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I0131 01:31:56.984406 112238 solver.cpp:266] Iteration 11550 (2.062 iter/s, 24.2483s/50 iter), loss = 0.00129634
I0131 01:31:56.984525 112238 solver.cpp:285]     Train net output #0: loss = 0.0012963 (* 1 = 0.0012963 loss)
I0131 01:31:56.984534 112238 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I0131 01:32:21.362481 112238 solver.cpp:266] Iteration 11600 (2.05111 iter/s, 24.3771s/50 iter), loss = 0.00990216
I0131 01:32:21.362514 112238 solver.cpp:285]     Train net output #0: loss = 0.00990213 (* 1 = 0.00990213 loss)
I0131 01:32:21.364727 112238 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I0131 01:32:45.701540 112238 solver.cpp:266] Iteration 11650 (2.05458 iter/s, 24.3359s/50 iter), loss = 0.000913115
I0131 01:32:45.701603 112238 solver.cpp:285]     Train net output #0: loss = 0.000913076 (* 1 = 0.000913076 loss)
I0131 01:32:45.703791 112238 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I0131 01:33:10.080729 112238 solver.cpp:266] Iteration 11700 (2.05119 iter/s, 24.376s/50 iter), loss = 0.0121816
I0131 01:33:10.080759 112238 solver.cpp:285]     Train net output #0: loss = 0.0121815 (* 1 = 0.0121815 loss)
I0131 01:33:10.082976 112238 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I0131 01:33:34.460677 112238 solver.cpp:266] Iteration 11750 (2.05113 iter/s, 24.3768s/50 iter), loss = 0.00624015
I0131 01:33:34.460748 112238 solver.cpp:285]     Train net output #0: loss = 0.00624011 (* 1 = 0.00624011 loss)
I0131 01:33:34.460788 112238 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I0131 01:33:58.949292 112238 solver.cpp:266] Iteration 11800 (2.04185 iter/s, 24.4876s/50 iter), loss = 0.00459029
I0131 01:33:58.949335 112238 solver.cpp:285]     Train net output #0: loss = 0.00459025 (* 1 = 0.00459025 loss)
I0131 01:33:58.949379 112238 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I0131 01:34:23.305058 112238 solver.cpp:266] Iteration 11850 (2.05298 iter/s, 24.3548s/50 iter), loss = 0.000331041
I0131 01:34:23.305191 112238 solver.cpp:285]     Train net output #0: loss = 0.000331001 (* 1 = 0.000331001 loss)
I0131 01:34:23.307313 112238 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I0131 01:34:47.697240 112238 solver.cpp:266] Iteration 11900 (2.0501 iter/s, 24.389s/50 iter), loss = 0.0246259
I0131 01:34:47.697270 112238 solver.cpp:285]     Train net output #0: loss = 0.0246258 (* 1 = 0.0246258 loss)
I0131 01:34:47.699493 112238 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I0131 01:35:11.917055 112238 solver.cpp:266] Iteration 11950 (2.06469 iter/s, 24.2167s/50 iter), loss = 0.00615569
I0131 01:35:11.917165 112238 solver.cpp:285]     Train net output #0: loss = 0.00615565 (* 1 = 0.00615565 loss)
I0131 01:35:11.919301 112238 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I0131 01:35:35.798816 112238 solver.cpp:929] Snapshotting to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/snapshots/_iter_12000.caffemodel
I0131 01:35:38.262837 112238 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.6/snapshots/_iter_12000.solverstate
I0131 01:35:38.897477 112238 solver.cpp:378] Iteration 12000, loss = 0.00100188
I0131 01:35:38.897502 112238 solver.cpp:418] Iteration 12000, Testing net (#0)
I0131 01:35:42.568462 112238 solver.cpp:517]     Test net output #0: loss = 0.237602 (* 1 = 0.237602 loss)
I0131 01:35:42.568562 112238 solver.cpp:517]     Test net output #1: top-1 = 0.953
I0131 01:35:42.568567 112238 solver.cpp:386] Optimization Done (2.04793 iter/s).
I0131 01:35:42.568570 112238 caffe_interface.cpp:530] Optimization Done.

## compression: 7-th run
$PRUNE_ROOT/deephi_compress compress -config ${WORK_DIR}/config7.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress7_alexnetBNnoLRN.txt
I0131 01:35:43.645570 112883 pruning_runner.cpp:190] Sens info found, use it.
I0131 01:35:44.899552 112883 pruning_runner.cpp:217] Start compressing, please wait...
I0131 01:35:51.964174 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:35:58.753762 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:05.259712 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:11.837415 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:18.813369 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:25.089494 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:31.659665 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:38.116503 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:44.394047 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:50.644428 112883 pruning_runner.cpp:264] Compression complete 0%
I0131 01:36:57.268581 112883 pruning_runner.cpp:264] Compression complete 50%
I0131 01:37:03.570475 112883 pruning_runner.cpp:264] Compression complete 96.9697%
I0131 01:37:09.850488 112883 pruning_runner.cpp:264] Compression complete 99.2424%
I0131 01:37:16.338413 112883 pruning_runner.cpp:264] Compression complete 99.9047%
I0131 01:37:22.652756 112883 pruning_runner.cpp:264] Compression complete 99.9523%
I0131 01:37:29.220836 112883 pruning_runner.cpp:264] Compression complete 99.9998%
I0131 01:37:35.516646 112883 pruning_runner.cpp:264] Compression complete 99.9999%
I0131 01:37:42.355255 112883 pruning_runner.cpp:264] Compression complete 100%
I0131 01:37:48.691385 112883 caffe_interface.cpp:66] Use GPU with device ID 0
I0131 01:37:48.691679 112883 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0131 01:37:48.692016 112883 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0131 01:37:48.692185 112883 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0131 01:37:48.692308 112883 layer_factory.hpp:77] Creating layer data
I0131 01:37:48.692354 112883 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:48.692761 112883 net.cpp:94] Creating Layer data
I0131 01:37:48.692770 112883 net.cpp:409] data -> data
I0131 01:37:48.692782 112883 net.cpp:409] data -> label
I0131 01:37:48.694336 114635 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0131 01:37:48.694370 114635 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0131 01:37:48.694548 112883 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0131 01:37:48.694639 112883 data_layer.cpp:83] output data size: 50,3,227,227
I0131 01:37:48.779057 112883 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:48.779134 112883 net.cpp:144] Setting up data
I0131 01:37:48.779142 112883 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0131 01:37:48.779146 112883 net.cpp:151] Top shape: 50 (50)
I0131 01:37:48.779150 112883 net.cpp:159] Memory required for data: 30917600
I0131 01:37:48.779155 112883 layer_factory.hpp:77] Creating layer label_data_1_split
I0131 01:37:48.779165 112883 net.cpp:94] Creating Layer label_data_1_split
I0131 01:37:48.779170 112883 net.cpp:435] label_data_1_split <- label
I0131 01:37:48.779177 112883 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0131 01:37:48.779187 112883 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0131 01:37:48.779284 112883 net.cpp:144] Setting up label_data_1_split
I0131 01:37:48.779307 112883 net.cpp:151] Top shape: 50 (50)
I0131 01:37:48.779311 112883 net.cpp:151] Top shape: 50 (50)
I0131 01:37:48.779314 112883 net.cpp:159] Memory required for data: 30918000
I0131 01:37:48.779315 112883 layer_factory.hpp:77] Creating layer conv1
I0131 01:37:48.779330 112883 net.cpp:94] Creating Layer conv1
I0131 01:37:48.779335 112883 net.cpp:435] conv1 <- data
I0131 01:37:48.779340 112883 net.cpp:409] conv1 -> conv1
I0131 01:37:48.781018 112883 net.cpp:144] Setting up conv1
I0131 01:37:48.781033 112883 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:48.781035 112883 net.cpp:159] Memory required for data: 88998000
I0131 01:37:48.781061 112883 layer_factory.hpp:77] Creating layer bn1
I0131 01:37:48.781069 112883 net.cpp:94] Creating Layer bn1
I0131 01:37:48.781074 112883 net.cpp:435] bn1 <- conv1
I0131 01:37:48.781078 112883 net.cpp:409] bn1 -> scale1
I0131 01:37:48.781744 112883 net.cpp:144] Setting up bn1
I0131 01:37:48.781750 112883 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:48.781754 112883 net.cpp:159] Memory required for data: 147078000
I0131 01:37:48.781764 112883 layer_factory.hpp:77] Creating layer relu1
I0131 01:37:48.781769 112883 net.cpp:94] Creating Layer relu1
I0131 01:37:48.781774 112883 net.cpp:435] relu1 <- scale1
I0131 01:37:48.781777 112883 net.cpp:409] relu1 -> relu1
I0131 01:37:48.781807 112883 net.cpp:144] Setting up relu1
I0131 01:37:48.781812 112883 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:48.781814 112883 net.cpp:159] Memory required for data: 205158000
I0131 01:37:48.781817 112883 layer_factory.hpp:77] Creating layer pool1
I0131 01:37:48.781822 112883 net.cpp:94] Creating Layer pool1
I0131 01:37:48.781826 112883 net.cpp:435] pool1 <- relu1
I0131 01:37:48.781828 112883 net.cpp:409] pool1 -> pool1
I0131 01:37:48.781884 112883 net.cpp:144] Setting up pool1
I0131 01:37:48.781888 112883 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0131 01:37:48.781891 112883 net.cpp:159] Memory required for data: 219154800
I0131 01:37:48.781893 112883 layer_factory.hpp:77] Creating layer conv2
I0131 01:37:48.781901 112883 net.cpp:94] Creating Layer conv2
I0131 01:37:48.781903 112883 net.cpp:435] conv2 <- pool1
I0131 01:37:48.781908 112883 net.cpp:409] conv2 -> conv2
I0131 01:37:48.790184 112883 net.cpp:144] Setting up conv2
I0131 01:37:48.790213 112883 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:48.790216 112883 net.cpp:159] Memory required for data: 256479600
I0131 01:37:48.790242 112883 layer_factory.hpp:77] Creating layer bn2
I0131 01:37:48.790253 112883 net.cpp:94] Creating Layer bn2
I0131 01:37:48.790258 112883 net.cpp:435] bn2 <- conv2
I0131 01:37:48.790266 112883 net.cpp:409] bn2 -> scale2
I0131 01:37:48.793185 112883 net.cpp:144] Setting up bn2
I0131 01:37:48.793192 112883 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:48.793195 112883 net.cpp:159] Memory required for data: 293804400
I0131 01:37:48.793201 112883 layer_factory.hpp:77] Creating layer relu2
I0131 01:37:48.793206 112883 net.cpp:94] Creating Layer relu2
I0131 01:37:48.793208 112883 net.cpp:435] relu2 <- scale2
I0131 01:37:48.793212 112883 net.cpp:409] relu2 -> relu2
I0131 01:37:48.793226 112883 net.cpp:144] Setting up relu2
I0131 01:37:48.793231 112883 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:48.793233 112883 net.cpp:159] Memory required for data: 331129200
I0131 01:37:48.793237 112883 layer_factory.hpp:77] Creating layer pool2
I0131 01:37:48.793242 112883 net.cpp:94] Creating Layer pool2
I0131 01:37:48.793246 112883 net.cpp:435] pool2 <- relu2
I0131 01:37:48.793265 112883 net.cpp:409] pool2 -> pool2
I0131 01:37:48.795667 112883 net.cpp:144] Setting up pool2
I0131 01:37:48.795675 112883 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:48.795676 112883 net.cpp:159] Memory required for data: 339782000
I0131 01:37:48.795678 112883 layer_factory.hpp:77] Creating layer conv3
I0131 01:37:48.795686 112883 net.cpp:94] Creating Layer conv3
I0131 01:37:48.795689 112883 net.cpp:435] conv3 <- pool2
I0131 01:37:48.795704 112883 net.cpp:409] conv3 -> conv3
I0131 01:37:48.808054 112883 net.cpp:144] Setting up conv3
I0131 01:37:48.808079 112883 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:48.808081 112883 net.cpp:159] Memory required for data: 352761200
I0131 01:37:48.808089 112883 layer_factory.hpp:77] Creating layer relu3
I0131 01:37:48.808097 112883 net.cpp:94] Creating Layer relu3
I0131 01:37:48.808101 112883 net.cpp:435] relu3 <- conv3
I0131 01:37:48.808118 112883 net.cpp:409] relu3 -> relu3
I0131 01:37:48.808149 112883 net.cpp:144] Setting up relu3
I0131 01:37:48.808156 112883 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:48.808158 112883 net.cpp:159] Memory required for data: 365740400
I0131 01:37:48.808161 112883 layer_factory.hpp:77] Creating layer conv4
I0131 01:37:48.808173 112883 net.cpp:94] Creating Layer conv4
I0131 01:37:48.808182 112883 net.cpp:435] conv4 <- relu3
I0131 01:37:48.808188 112883 net.cpp:409] conv4 -> conv4
I0131 01:37:48.824987 112883 net.cpp:144] Setting up conv4
I0131 01:37:48.825011 112883 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:48.825013 112883 net.cpp:159] Memory required for data: 378719600
I0131 01:37:48.825029 112883 layer_factory.hpp:77] Creating layer relu4
I0131 01:37:48.825042 112883 net.cpp:94] Creating Layer relu4
I0131 01:37:48.825047 112883 net.cpp:435] relu4 <- conv4
I0131 01:37:48.825057 112883 net.cpp:409] relu4 -> relu4
I0131 01:37:48.825109 112883 net.cpp:144] Setting up relu4
I0131 01:37:48.825114 112883 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:48.825117 112883 net.cpp:159] Memory required for data: 391698800
I0131 01:37:48.825120 112883 layer_factory.hpp:77] Creating layer conv5
I0131 01:37:48.825134 112883 net.cpp:94] Creating Layer conv5
I0131 01:37:48.825137 112883 net.cpp:435] conv5 <- relu4
I0131 01:37:48.825145 112883 net.cpp:409] conv5 -> conv5
I0131 01:37:48.843428 112883 net.cpp:144] Setting up conv5
I0131 01:37:48.843454 112883 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:48.843457 112883 net.cpp:159] Memory required for data: 400351600
I0131 01:37:48.843466 112883 layer_factory.hpp:77] Creating layer relu5
I0131 01:37:48.843474 112883 net.cpp:94] Creating Layer relu5
I0131 01:37:48.843479 112883 net.cpp:435] relu5 <- conv5
I0131 01:37:48.843485 112883 net.cpp:409] relu5 -> relu5
I0131 01:37:48.843513 112883 net.cpp:144] Setting up relu5
I0131 01:37:48.843519 112883 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:48.843523 112883 net.cpp:159] Memory required for data: 409004400
I0131 01:37:48.843525 112883 layer_factory.hpp:77] Creating layer pool5
I0131 01:37:48.843533 112883 net.cpp:94] Creating Layer pool5
I0131 01:37:48.843535 112883 net.cpp:435] pool5 <- relu5
I0131 01:37:48.843541 112883 net.cpp:409] pool5 -> pool5
I0131 01:37:48.843569 112883 net.cpp:144] Setting up pool5
I0131 01:37:48.843574 112883 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0131 01:37:48.843576 112883 net.cpp:159] Memory required for data: 410847600
I0131 01:37:48.843580 112883 layer_factory.hpp:77] Creating layer fc6
I0131 01:37:48.843587 112883 net.cpp:94] Creating Layer fc6
I0131 01:37:48.843590 112883 net.cpp:435] fc6 <- pool5
I0131 01:37:48.843595 112883 net.cpp:409] fc6 -> fc6
I0131 01:37:49.176190 112883 net.cpp:144] Setting up fc6
I0131 01:37:49.176214 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.176218 112883 net.cpp:159] Memory required for data: 411666800
I0131 01:37:49.176224 112883 layer_factory.hpp:77] Creating layer relu6
I0131 01:37:49.176232 112883 net.cpp:94] Creating Layer relu6
I0131 01:37:49.176236 112883 net.cpp:435] relu6 <- fc6
I0131 01:37:49.176242 112883 net.cpp:409] relu6 -> relu6
I0131 01:37:49.176275 112883 net.cpp:144] Setting up relu6
I0131 01:37:49.176280 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.176281 112883 net.cpp:159] Memory required for data: 412486000
I0131 01:37:49.176283 112883 layer_factory.hpp:77] Creating layer drop6
I0131 01:37:49.176288 112883 net.cpp:94] Creating Layer drop6
I0131 01:37:49.176291 112883 net.cpp:435] drop6 <- relu6
I0131 01:37:49.176316 112883 net.cpp:409] drop6 -> drop6
I0131 01:37:49.176342 112883 net.cpp:144] Setting up drop6
I0131 01:37:49.176347 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.176350 112883 net.cpp:159] Memory required for data: 413305200
I0131 01:37:49.176352 112883 layer_factory.hpp:77] Creating layer fc7
I0131 01:37:49.176359 112883 net.cpp:94] Creating Layer fc7
I0131 01:37:49.176362 112883 net.cpp:435] fc7 <- drop6
I0131 01:37:49.176368 112883 net.cpp:409] fc7 -> fc7
I0131 01:37:49.318352 112883 net.cpp:144] Setting up fc7
I0131 01:37:49.318375 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.318378 112883 net.cpp:159] Memory required for data: 414124400
I0131 01:37:49.318387 112883 layer_factory.hpp:77] Creating layer bn7
I0131 01:37:49.318397 112883 net.cpp:94] Creating Layer bn7
I0131 01:37:49.318399 112883 net.cpp:435] bn7 <- fc7
I0131 01:37:49.318405 112883 net.cpp:409] bn7 -> scale7
I0131 01:37:49.318928 112883 net.cpp:144] Setting up bn7
I0131 01:37:49.318934 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.318938 112883 net.cpp:159] Memory required for data: 414943600
I0131 01:37:49.318944 112883 layer_factory.hpp:77] Creating layer relu7
I0131 01:37:49.318951 112883 net.cpp:94] Creating Layer relu7
I0131 01:37:49.318954 112883 net.cpp:435] relu7 <- scale7
I0131 01:37:49.318960 112883 net.cpp:409] relu7 -> relu7
I0131 01:37:49.318980 112883 net.cpp:144] Setting up relu7
I0131 01:37:49.318985 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.318989 112883 net.cpp:159] Memory required for data: 415762800
I0131 01:37:49.318990 112883 layer_factory.hpp:77] Creating layer drop7
I0131 01:37:49.318996 112883 net.cpp:94] Creating Layer drop7
I0131 01:37:49.319000 112883 net.cpp:435] drop7 <- relu7
I0131 01:37:49.319005 112883 net.cpp:409] drop7 -> drop7
I0131 01:37:49.319031 112883 net.cpp:144] Setting up drop7
I0131 01:37:49.319036 112883 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:49.319038 112883 net.cpp:159] Memory required for data: 416582000
I0131 01:37:49.319041 112883 layer_factory.hpp:77] Creating layer fc8
I0131 01:37:49.319047 112883 net.cpp:94] Creating Layer fc8
I0131 01:37:49.319051 112883 net.cpp:435] fc8 <- drop7
I0131 01:37:49.319057 112883 net.cpp:409] fc8 -> fc8
I0131 01:37:49.319936 112883 net.cpp:144] Setting up fc8
I0131 01:37:49.319948 112883 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:49.319950 112883 net.cpp:159] Memory required for data: 416582400
I0131 01:37:49.319957 112883 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0131 01:37:49.319962 112883 net.cpp:94] Creating Layer fc8_fc8_0_split
I0131 01:37:49.319965 112883 net.cpp:435] fc8_fc8_0_split <- fc8
I0131 01:37:49.319969 112883 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0131 01:37:49.319977 112883 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0131 01:37:49.320006 112883 net.cpp:144] Setting up fc8_fc8_0_split
I0131 01:37:49.320011 112883 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:49.320014 112883 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:49.320016 112883 net.cpp:159] Memory required for data: 416583200
I0131 01:37:49.320019 112883 layer_factory.hpp:77] Creating layer loss
I0131 01:37:49.320024 112883 net.cpp:94] Creating Layer loss
I0131 01:37:49.320027 112883 net.cpp:435] loss <- fc8_fc8_0_split_0
I0131 01:37:49.320034 112883 net.cpp:435] loss <- label_data_1_split_0
I0131 01:37:49.320039 112883 net.cpp:409] loss -> loss
I0131 01:37:49.320047 112883 layer_factory.hpp:77] Creating layer loss
I0131 01:37:49.320116 112883 net.cpp:144] Setting up loss
I0131 01:37:49.320120 112883 net.cpp:151] Top shape: (1)
I0131 01:37:49.320122 112883 net.cpp:154]     with loss weight 1
I0131 01:37:49.320135 112883 net.cpp:159] Memory required for data: 416583204
I0131 01:37:49.320138 112883 layer_factory.hpp:77] Creating layer accuracy-top1
I0131 01:37:49.320144 112883 net.cpp:94] Creating Layer accuracy-top1
I0131 01:37:49.320148 112883 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0131 01:37:49.320152 112883 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0131 01:37:49.320175 112883 net.cpp:409] accuracy-top1 -> top-1
I0131 01:37:49.320183 112883 net.cpp:144] Setting up accuracy-top1
I0131 01:37:49.320188 112883 net.cpp:151] Top shape: (1)
I0131 01:37:49.320191 112883 net.cpp:159] Memory required for data: 416583208
I0131 01:37:49.320194 112883 net.cpp:222] accuracy-top1 does not need backward computation.
I0131 01:37:49.320199 112883 net.cpp:220] loss needs backward computation.
I0131 01:37:49.320204 112883 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0131 01:37:49.320206 112883 net.cpp:220] fc8 needs backward computation.
I0131 01:37:49.320210 112883 net.cpp:220] drop7 needs backward computation.
I0131 01:37:49.320214 112883 net.cpp:220] relu7 needs backward computation.
I0131 01:37:49.320217 112883 net.cpp:220] bn7 needs backward computation.
I0131 01:37:49.320220 112883 net.cpp:220] fc7 needs backward computation.
I0131 01:37:49.320225 112883 net.cpp:220] drop6 needs backward computation.
I0131 01:37:49.320228 112883 net.cpp:220] relu6 needs backward computation.
I0131 01:37:49.320232 112883 net.cpp:220] fc6 needs backward computation.
I0131 01:37:49.320235 112883 net.cpp:220] pool5 needs backward computation.
I0131 01:37:49.320238 112883 net.cpp:220] relu5 needs backward computation.
I0131 01:37:49.320242 112883 net.cpp:220] conv5 needs backward computation.
I0131 01:37:49.320246 112883 net.cpp:220] relu4 needs backward computation.
I0131 01:37:49.320250 112883 net.cpp:220] conv4 needs backward computation.
I0131 01:37:49.320252 112883 net.cpp:220] relu3 needs backward computation.
I0131 01:37:49.320257 112883 net.cpp:220] conv3 needs backward computation.
I0131 01:37:49.320261 112883 net.cpp:220] pool2 needs backward computation.
I0131 01:37:49.320264 112883 net.cpp:220] relu2 needs backward computation.
I0131 01:37:49.320268 112883 net.cpp:220] bn2 needs backward computation.
I0131 01:37:49.320271 112883 net.cpp:220] conv2 needs backward computation.
I0131 01:37:49.320276 112883 net.cpp:220] pool1 needs backward computation.
I0131 01:37:49.320277 112883 net.cpp:220] relu1 needs backward computation.
I0131 01:37:49.320281 112883 net.cpp:220] bn1 needs backward computation.
I0131 01:37:49.320283 112883 net.cpp:220] conv1 needs backward computation.
I0131 01:37:49.320286 112883 net.cpp:222] label_data_1_split does not need backward computation.
I0131 01:37:49.320291 112883 net.cpp:222] data does not need backward computation.
I0131 01:37:49.320291 112883 net.cpp:264] This network produces output loss
I0131 01:37:49.320296 112883 net.cpp:264] This network produces output top-1
I0131 01:37:49.320317 112883 net.cpp:284] Network initialization done.
I0131 01:37:49.395927 112883 caffe_interface.cpp:363] Running for 80 iterations.
I0131 01:37:49.490108 112883 caffe_interface.cpp:125] Batch 0, loss = 0.128616
I0131 01:37:49.490134 112883 caffe_interface.cpp:125] Batch 0, top-1 = 0.94
I0131 01:37:49.547260 112883 caffe_interface.cpp:125] Batch 1, loss = 0.161075
I0131 01:37:49.547322 112883 caffe_interface.cpp:125] Batch 1, top-1 = 0.98
I0131 01:37:49.604825 112883 caffe_interface.cpp:125] Batch 2, loss = 0.416555
I0131 01:37:49.604848 112883 caffe_interface.cpp:125] Batch 2, top-1 = 0.94
I0131 01:37:49.663331 112883 caffe_interface.cpp:125] Batch 3, loss = 0.792399
I0131 01:37:49.663354 112883 caffe_interface.cpp:125] Batch 3, top-1 = 0.84
I0131 01:37:49.718938 112883 caffe_interface.cpp:125] Batch 4, loss = 0.193807
I0131 01:37:49.718961 112883 caffe_interface.cpp:125] Batch 4, top-1 = 0.96
I0131 01:37:49.768764 112883 caffe_interface.cpp:125] Batch 5, loss = 0.109238
I0131 01:37:49.768784 112883 caffe_interface.cpp:125] Batch 5, top-1 = 0.96
I0131 01:37:49.796152 112883 caffe_interface.cpp:125] Batch 6, loss = 0.479157
I0131 01:37:49.796180 112883 caffe_interface.cpp:125] Batch 6, top-1 = 0.96
I0131 01:37:49.816283 112883 caffe_interface.cpp:125] Batch 7, loss = 0.101415
I0131 01:37:49.816306 112883 caffe_interface.cpp:125] Batch 7, top-1 = 0.96
I0131 01:37:49.835286 112883 caffe_interface.cpp:125] Batch 8, loss = 0.0484611
I0131 01:37:49.835307 112883 caffe_interface.cpp:125] Batch 8, top-1 = 0.98
I0131 01:37:49.853816 112883 caffe_interface.cpp:125] Batch 9, loss = 0.490976
I0131 01:37:49.853835 112883 caffe_interface.cpp:125] Batch 9, top-1 = 0.94
I0131 01:37:49.872993 112883 caffe_interface.cpp:125] Batch 10, loss = 0.113073
I0131 01:37:49.873013 112883 caffe_interface.cpp:125] Batch 10, top-1 = 0.98
I0131 01:37:49.893054 112883 caffe_interface.cpp:125] Batch 11, loss = 0.640107
I0131 01:37:49.893079 112883 caffe_interface.cpp:125] Batch 11, top-1 = 0.9
I0131 01:37:49.913120 112883 caffe_interface.cpp:125] Batch 12, loss = 0.275604
I0131 01:37:49.913141 112883 caffe_interface.cpp:125] Batch 12, top-1 = 0.96
I0131 01:37:49.932102 112883 caffe_interface.cpp:125] Batch 13, loss = 0.116206
I0131 01:37:49.932126 112883 caffe_interface.cpp:125] Batch 13, top-1 = 0.96
I0131 01:37:49.962357 112883 caffe_interface.cpp:125] Batch 14, loss = 0.197206
I0131 01:37:49.962381 112883 caffe_interface.cpp:125] Batch 14, top-1 = 0.96
I0131 01:37:50.019758 112883 caffe_interface.cpp:125] Batch 15, loss = 0.147607
I0131 01:37:50.019780 112883 caffe_interface.cpp:125] Batch 15, top-1 = 0.96
I0131 01:37:50.082070 112883 caffe_interface.cpp:125] Batch 16, loss = 0.127784
I0131 01:37:50.082101 112883 caffe_interface.cpp:125] Batch 16, top-1 = 0.98
I0131 01:37:50.139638 112883 caffe_interface.cpp:125] Batch 17, loss = 0.273585
I0131 01:37:50.139670 112883 caffe_interface.cpp:125] Batch 17, top-1 = 0.96
I0131 01:37:50.197232 112883 caffe_interface.cpp:125] Batch 18, loss = 0.215765
I0131 01:37:50.197263 112883 caffe_interface.cpp:125] Batch 18, top-1 = 0.94
I0131 01:37:50.255061 112883 caffe_interface.cpp:125] Batch 19, loss = 0.11456
I0131 01:37:50.255081 112883 caffe_interface.cpp:125] Batch 19, top-1 = 0.96
I0131 01:37:50.313640 112883 caffe_interface.cpp:125] Batch 20, loss = 0.157399
I0131 01:37:50.313659 112883 caffe_interface.cpp:125] Batch 20, top-1 = 0.96
I0131 01:37:50.369475 112883 caffe_interface.cpp:125] Batch 21, loss = 0.156517
I0131 01:37:50.369494 112883 caffe_interface.cpp:125] Batch 21, top-1 = 0.98
I0131 01:37:50.427248 112883 caffe_interface.cpp:125] Batch 22, loss = 0.123432
I0131 01:37:50.427266 112883 caffe_interface.cpp:125] Batch 22, top-1 = 0.98
I0131 01:37:50.483136 112883 caffe_interface.cpp:125] Batch 23, loss = 0.288895
I0131 01:37:50.483155 112883 caffe_interface.cpp:125] Batch 23, top-1 = 0.96
I0131 01:37:50.540963 112883 caffe_interface.cpp:125] Batch 24, loss = 0.154466
I0131 01:37:50.540982 112883 caffe_interface.cpp:125] Batch 24, top-1 = 0.96
I0131 01:37:50.596789 112883 caffe_interface.cpp:125] Batch 25, loss = 0.0575462
I0131 01:37:50.596807 112883 caffe_interface.cpp:125] Batch 25, top-1 = 0.94
I0131 01:37:50.654224 112883 caffe_interface.cpp:125] Batch 26, loss = 0.227215
I0131 01:37:50.654242 112883 caffe_interface.cpp:125] Batch 26, top-1 = 0.96
I0131 01:37:50.709789 112883 caffe_interface.cpp:125] Batch 27, loss = 0.224
I0131 01:37:50.709805 112883 caffe_interface.cpp:125] Batch 27, top-1 = 0.98
I0131 01:37:50.756629 112883 caffe_interface.cpp:125] Batch 28, loss = 0.513104
I0131 01:37:50.756657 112883 caffe_interface.cpp:125] Batch 28, top-1 = 0.88
I0131 01:37:50.778213 112883 caffe_interface.cpp:125] Batch 29, loss = 0.206814
I0131 01:37:50.778231 112883 caffe_interface.cpp:125] Batch 29, top-1 = 0.96
I0131 01:37:50.820948 112883 caffe_interface.cpp:125] Batch 30, loss = 0.061006
I0131 01:37:50.820967 112883 caffe_interface.cpp:125] Batch 30, top-1 = 0.98
I0131 01:37:50.879149 112883 caffe_interface.cpp:125] Batch 31, loss = 0.00484538
I0131 01:37:50.879209 112883 caffe_interface.cpp:125] Batch 31, top-1 = 1
I0131 01:37:50.936386 112883 caffe_interface.cpp:125] Batch 32, loss = 0.128862
I0131 01:37:50.936403 112883 caffe_interface.cpp:125] Batch 32, top-1 = 0.98
I0131 01:37:50.993881 112883 caffe_interface.cpp:125] Batch 33, loss = 0.253964
I0131 01:37:50.993892 112883 caffe_interface.cpp:125] Batch 33, top-1 = 0.94
I0131 01:37:51.049638 112883 caffe_interface.cpp:125] Batch 34, loss = 0.519612
I0131 01:37:51.049655 112883 caffe_interface.cpp:125] Batch 34, top-1 = 0.9
I0131 01:37:51.106068 112883 caffe_interface.cpp:125] Batch 35, loss = 0.006363
I0131 01:37:51.106089 112883 caffe_interface.cpp:125] Batch 35, top-1 = 1
I0131 01:37:51.159843 112883 caffe_interface.cpp:125] Batch 36, loss = 0.462075
I0131 01:37:51.159867 112883 caffe_interface.cpp:125] Batch 36, top-1 = 0.92
I0131 01:37:51.186502 112883 caffe_interface.cpp:125] Batch 37, loss = 0.200939
I0131 01:37:51.186523 112883 caffe_interface.cpp:125] Batch 37, top-1 = 0.96
I0131 01:37:51.207491 112883 caffe_interface.cpp:125] Batch 38, loss = 0.409492
I0131 01:37:51.207514 112883 caffe_interface.cpp:125] Batch 38, top-1 = 0.9
I0131 01:37:51.225611 112883 caffe_interface.cpp:125] Batch 39, loss = 0.176898
I0131 01:37:51.225646 112883 caffe_interface.cpp:125] Batch 39, top-1 = 0.98
I0131 01:37:51.245690 112883 caffe_interface.cpp:125] Batch 40, loss = 0.128142
I0131 01:37:51.245702 112883 caffe_interface.cpp:125] Batch 40, top-1 = 0.98
I0131 01:37:51.263942 112883 caffe_interface.cpp:125] Batch 41, loss = 4.20939e-05
I0131 01:37:51.263954 112883 caffe_interface.cpp:125] Batch 41, top-1 = 1
I0131 01:37:51.285362 112883 caffe_interface.cpp:125] Batch 42, loss = 0.0971913
I0131 01:37:51.285396 112883 caffe_interface.cpp:125] Batch 42, top-1 = 0.98
I0131 01:37:51.303256 112883 caffe_interface.cpp:125] Batch 43, loss = 0.0593918
I0131 01:37:51.303267 112883 caffe_interface.cpp:125] Batch 43, top-1 = 0.98
I0131 01:37:51.323154 112883 caffe_interface.cpp:125] Batch 44, loss = 0.305443
I0131 01:37:51.323164 112883 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I0131 01:37:51.355347 112883 caffe_interface.cpp:125] Batch 45, loss = 0.603505
I0131 01:37:51.355357 112883 caffe_interface.cpp:125] Batch 45, top-1 = 0.88
I0131 01:37:51.412463 112883 caffe_interface.cpp:125] Batch 46, loss = 0.0352718
I0131 01:37:51.412472 112883 caffe_interface.cpp:125] Batch 46, top-1 = 0.98
I0131 01:37:51.467547 112883 caffe_interface.cpp:125] Batch 47, loss = 0.0985387
I0131 01:37:51.467556 112883 caffe_interface.cpp:125] Batch 47, top-1 = 0.98
I0131 01:37:51.525003 112883 caffe_interface.cpp:125] Batch 48, loss = 0.347186
I0131 01:37:51.525022 112883 caffe_interface.cpp:125] Batch 48, top-1 = 0.92
I0131 01:37:51.582684 112883 caffe_interface.cpp:125] Batch 49, loss = 0.208632
I0131 01:37:51.582695 112883 caffe_interface.cpp:125] Batch 49, top-1 = 0.94
I0131 01:37:51.641505 112883 caffe_interface.cpp:125] Batch 50, loss = 0.435023
I0131 01:37:51.641513 112883 caffe_interface.cpp:125] Batch 50, top-1 = 0.92
I0131 01:37:51.698034 112883 caffe_interface.cpp:125] Batch 51, loss = 0.218984
I0131 01:37:51.698042 112883 caffe_interface.cpp:125] Batch 51, top-1 = 0.96
I0131 01:37:51.755992 112883 caffe_interface.cpp:125] Batch 52, loss = 0.00443783
I0131 01:37:51.756001 112883 caffe_interface.cpp:125] Batch 52, top-1 = 1
I0131 01:37:51.811776 112883 caffe_interface.cpp:125] Batch 53, loss = 0.169303
I0131 01:37:51.811784 112883 caffe_interface.cpp:125] Batch 53, top-1 = 0.98
I0131 01:37:51.869777 112883 caffe_interface.cpp:125] Batch 54, loss = 0.10614
I0131 01:37:51.869784 112883 caffe_interface.cpp:125] Batch 54, top-1 = 0.96
I0131 01:37:51.927803 112883 caffe_interface.cpp:125] Batch 55, loss = 0.470167
I0131 01:37:51.927811 112883 caffe_interface.cpp:125] Batch 55, top-1 = 0.94
I0131 01:37:51.985781 112883 caffe_interface.cpp:125] Batch 56, loss = 0.074434
I0131 01:37:51.985790 112883 caffe_interface.cpp:125] Batch 56, top-1 = 0.96
I0131 01:37:52.043889 112883 caffe_interface.cpp:125] Batch 57, loss = 0.152577
I0131 01:37:52.043897 112883 caffe_interface.cpp:125] Batch 57, top-1 = 0.98
I0131 01:37:52.101320 112883 caffe_interface.cpp:125] Batch 58, loss = 0.295556
I0131 01:37:52.101326 112883 caffe_interface.cpp:125] Batch 58, top-1 = 0.94
I0131 01:37:52.145283 112883 caffe_interface.cpp:125] Batch 59, loss = 0.385724
I0131 01:37:52.145292 112883 caffe_interface.cpp:125] Batch 59, top-1 = 0.92
I0131 01:37:52.165488 112883 caffe_interface.cpp:125] Batch 60, loss = 0.288004
I0131 01:37:52.165496 112883 caffe_interface.cpp:125] Batch 60, top-1 = 0.96
I0131 01:37:52.208640 112883 caffe_interface.cpp:125] Batch 61, loss = 0.283143
I0131 01:37:52.208648 112883 caffe_interface.cpp:125] Batch 61, top-1 = 0.92
I0131 01:37:52.266263 112883 caffe_interface.cpp:125] Batch 62, loss = 0.0145623
I0131 01:37:52.266273 112883 caffe_interface.cpp:125] Batch 62, top-1 = 1
I0131 01:37:52.323979 112883 caffe_interface.cpp:125] Batch 63, loss = 0.359857
I0131 01:37:52.323989 112883 caffe_interface.cpp:125] Batch 63, top-1 = 0.96
I0131 01:37:52.381608 112883 caffe_interface.cpp:125] Batch 64, loss = 0.709841
I0131 01:37:52.381618 112883 caffe_interface.cpp:125] Batch 64, top-1 = 0.9
I0131 01:37:52.440577 112883 caffe_interface.cpp:125] Batch 65, loss = 0.307853
I0131 01:37:52.440587 112883 caffe_interface.cpp:125] Batch 65, top-1 = 0.94
I0131 01:37:52.497910 112883 caffe_interface.cpp:125] Batch 66, loss = 0.595851
I0131 01:37:52.497920 112883 caffe_interface.cpp:125] Batch 66, top-1 = 0.92
I0131 01:37:52.550943 112883 caffe_interface.cpp:125] Batch 67, loss = 0.37533
I0131 01:37:52.550952 112883 caffe_interface.cpp:125] Batch 67, top-1 = 0.9
I0131 01:37:52.576653 112883 caffe_interface.cpp:125] Batch 68, loss = 0.271292
I0131 01:37:52.576664 112883 caffe_interface.cpp:125] Batch 68, top-1 = 0.96
I0131 01:37:52.597694 112883 caffe_interface.cpp:125] Batch 69, loss = 0.408023
I0131 01:37:52.597703 112883 caffe_interface.cpp:125] Batch 69, top-1 = 0.88
I0131 01:37:52.616186 112883 caffe_interface.cpp:125] Batch 70, loss = 0.115412
I0131 01:37:52.616196 112883 caffe_interface.cpp:125] Batch 70, top-1 = 0.96
I0131 01:37:52.635613 112883 caffe_interface.cpp:125] Batch 71, loss = 0.0420846
I0131 01:37:52.635623 112883 caffe_interface.cpp:125] Batch 71, top-1 = 0.98
I0131 01:37:52.654121 112883 caffe_interface.cpp:125] Batch 72, loss = 0.264158
I0131 01:37:52.654290 112883 caffe_interface.cpp:125] Batch 72, top-1 = 0.96
I0131 01:37:52.674834 112883 caffe_interface.cpp:125] Batch 73, loss = 0.164651
I0131 01:37:52.674847 112883 caffe_interface.cpp:125] Batch 73, top-1 = 0.98
I0131 01:37:52.693598 112883 caffe_interface.cpp:125] Batch 74, loss = 0.101232
I0131 01:37:52.693609 112883 caffe_interface.cpp:125] Batch 74, top-1 = 0.98
I0131 01:37:52.713452 112883 caffe_interface.cpp:125] Batch 75, loss = 0.192875
I0131 01:37:52.713461 112883 caffe_interface.cpp:125] Batch 75, top-1 = 0.94
I0131 01:37:52.749459 112883 caffe_interface.cpp:125] Batch 76, loss = 0.399706
I0131 01:37:52.749469 112883 caffe_interface.cpp:125] Batch 76, top-1 = 0.92
I0131 01:37:52.809026 112883 caffe_interface.cpp:125] Batch 77, loss = 0.256691
I0131 01:37:52.809032 112883 caffe_interface.cpp:125] Batch 77, top-1 = 0.96
I0131 01:37:52.866498 112883 caffe_interface.cpp:125] Batch 78, loss = 0.182341
I0131 01:37:52.866508 112883 caffe_interface.cpp:125] Batch 78, top-1 = 0.96
I0131 01:37:52.924010 112883 caffe_interface.cpp:125] Batch 79, loss = 0.00289693
I0131 01:37:52.924019 112883 caffe_interface.cpp:125] Batch 79, top-1 = 1
I0131 01:37:52.924022 112883 caffe_interface.cpp:130] Loss: 0.237602
I0131 01:37:52.924029 112883 caffe_interface.cpp:142] loss = 0.237602 (* 1 = 0.237602 loss)
I0131 01:37:52.924036 112883 caffe_interface.cpp:142] top-1 = 0.953
I0131 01:37:53.166990 112883 pruning_runner.cpp:306] pruning done, output model: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/sparse.caffemodel
I0131 01:37:53.167018 112883 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.7:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.949249864    | 0.952999711    | 0.00374984741  |
+-------------------------------------------------------------------+
| Weights        | 3764995        | 530877         | -85.8996658%   |
+-------------------------------------------------------------------+
| Operations     | 2153918368     | 728647580      | -66.1710739%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config /home/danieleb/ML/cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/config7.prototxt
## fine-tuning: 7-th run
$PRUNE_ROOT/deephi_compress finetune -config ${WORK_DIR}/config7.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune7_alexnetBNnoLRN.txt
I0131 01:37:53.432911 114666 deephi_compress.cpp:236] cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/net_finetune.prototxt
I0131 01:37:53.624701 114666 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0131 01:37:53.625151 114666 gpu_memory.cpp:55] Total memory: 25620447232, Free: 13496549376, dev_info[0]: total=25620447232 free=13496549376
I0131 01:37:53.625164 114666 caffe_interface.cpp:493] Using GPUs 0
I0131 01:37:53.625414 114666 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0131 01:37:54.682373 114666 solver.cpp:51] Initializing solver from parameters: 
test_iter: 80
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 12000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 2500
snapshot: 12000
snapshot_prefix: "cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/net_finetune.prototxt"
type: "Adam"
I0131 01:37:54.682494 114666 solver.cpp:99] Creating training net from net file: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/net_finetune.prototxt
I0131 01:37:54.682714 114666 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0131 01:37:54.682727 114666 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0131 01:37:54.682871 114666 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0131 01:37:54.682936 114666 layer_factory.hpp:77] Creating layer data
I0131 01:37:54.683092 114666 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:54.683538 114666 net.cpp:94] Creating Layer data
I0131 01:37:54.683548 114666 net.cpp:409] data -> data
I0131 01:37:54.683558 114666 net.cpp:409] data -> label
I0131 01:37:54.685871   322 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/train_lmdb
I0131 01:37:54.685914   322 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0131 01:37:54.686250 114666 data_layer.cpp:78] ReshapePrefetch 256, 3, 227, 227
I0131 01:37:54.686374 114666 data_layer.cpp:83] output data size: 256,3,227,227
I0131 01:37:55.099743 114666 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:55.099813 114666 net.cpp:144] Setting up data
I0131 01:37:55.099822 114666 net.cpp:151] Top shape: 256 3 227 227 (39574272)
I0131 01:37:55.099825 114666 net.cpp:151] Top shape: 256 (256)
I0131 01:37:55.099828 114666 net.cpp:159] Memory required for data: 158298112
I0131 01:37:55.099833 114666 layer_factory.hpp:77] Creating layer conv1
I0131 01:37:55.099846 114666 net.cpp:94] Creating Layer conv1
I0131 01:37:55.099849 114666 net.cpp:435] conv1 <- data
I0131 01:37:55.099865 114666 net.cpp:409] conv1 -> conv1
I0131 01:37:55.101753 114666 net.cpp:144] Setting up conv1
I0131 01:37:55.101766 114666 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0131 01:37:55.101769 114666 net.cpp:159] Memory required for data: 455667712
I0131 01:37:55.101783 114666 layer_factory.hpp:77] Creating layer bn1
I0131 01:37:55.101792 114666 net.cpp:94] Creating Layer bn1
I0131 01:37:55.101795 114666 net.cpp:435] bn1 <- conv1
I0131 01:37:55.101800 114666 net.cpp:409] bn1 -> scale1
I0131 01:37:55.103041 114666 net.cpp:144] Setting up bn1
I0131 01:37:55.103049 114666 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0131 01:37:55.103052 114666 net.cpp:159] Memory required for data: 753037312
I0131 01:37:55.103061 114666 layer_factory.hpp:77] Creating layer relu1
I0131 01:37:55.103066 114666 net.cpp:94] Creating Layer relu1
I0131 01:37:55.103070 114666 net.cpp:435] relu1 <- scale1
I0131 01:37:55.103073 114666 net.cpp:409] relu1 -> relu1
I0131 01:37:55.103098 114666 net.cpp:144] Setting up relu1
I0131 01:37:55.103103 114666 net.cpp:151] Top shape: 256 96 55 55 (74342400)
I0131 01:37:55.103106 114666 net.cpp:159] Memory required for data: 1050406912
I0131 01:37:55.103108 114666 layer_factory.hpp:77] Creating layer pool1
I0131 01:37:55.103114 114666 net.cpp:94] Creating Layer pool1
I0131 01:37:55.103116 114666 net.cpp:435] pool1 <- relu1
I0131 01:37:55.103121 114666 net.cpp:409] pool1 -> pool1
I0131 01:37:55.103193 114666 net.cpp:144] Setting up pool1
I0131 01:37:55.103199 114666 net.cpp:151] Top shape: 256 96 27 27 (17915904)
I0131 01:37:55.103202 114666 net.cpp:159] Memory required for data: 1122070528
I0131 01:37:55.103205 114666 layer_factory.hpp:77] Creating layer conv2
I0131 01:37:55.103214 114666 net.cpp:94] Creating Layer conv2
I0131 01:37:55.103216 114666 net.cpp:435] conv2 <- pool1
I0131 01:37:55.103220 114666 net.cpp:409] conv2 -> conv2
I0131 01:37:55.120187 114666 net.cpp:144] Setting up conv2
I0131 01:37:55.120203 114666 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0131 01:37:55.120208 114666 net.cpp:159] Memory required for data: 1313173504
I0131 01:37:55.120219 114666 layer_factory.hpp:77] Creating layer bn2
I0131 01:37:55.120229 114666 net.cpp:94] Creating Layer bn2
I0131 01:37:55.120234 114666 net.cpp:435] bn2 <- conv2
I0131 01:37:55.120249 114666 net.cpp:409] bn2 -> scale2
I0131 01:37:55.120815 114666 net.cpp:144] Setting up bn2
I0131 01:37:55.120823 114666 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0131 01:37:55.120826 114666 net.cpp:159] Memory required for data: 1504276480
I0131 01:37:55.120834 114666 layer_factory.hpp:77] Creating layer relu2
I0131 01:37:55.120841 114666 net.cpp:94] Creating Layer relu2
I0131 01:37:55.120844 114666 net.cpp:435] relu2 <- scale2
I0131 01:37:55.120848 114666 net.cpp:409] relu2 -> relu2
I0131 01:37:55.120867 114666 net.cpp:144] Setting up relu2
I0131 01:37:55.120872 114666 net.cpp:151] Top shape: 256 256 27 27 (47775744)
I0131 01:37:55.120874 114666 net.cpp:159] Memory required for data: 1695379456
I0131 01:37:55.120877 114666 layer_factory.hpp:77] Creating layer pool2
I0131 01:37:55.120884 114666 net.cpp:94] Creating Layer pool2
I0131 01:37:55.120887 114666 net.cpp:435] pool2 <- relu2
I0131 01:37:55.120908 114666 net.cpp:409] pool2 -> pool2
I0131 01:37:55.120936 114666 net.cpp:144] Setting up pool2
I0131 01:37:55.120940 114666 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0131 01:37:55.120944 114666 net.cpp:159] Memory required for data: 1739681792
I0131 01:37:55.120946 114666 layer_factory.hpp:77] Creating layer conv3
I0131 01:37:55.120955 114666 net.cpp:94] Creating Layer conv3
I0131 01:37:55.120959 114666 net.cpp:435] conv3 <- pool2
I0131 01:37:55.120963 114666 net.cpp:409] conv3 -> conv3
I0131 01:37:55.134222 114666 net.cpp:144] Setting up conv3
I0131 01:37:55.134248 114666 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0131 01:37:55.134253 114666 net.cpp:159] Memory required for data: 1806135296
I0131 01:37:55.134263 114666 layer_factory.hpp:77] Creating layer relu3
I0131 01:37:55.134274 114666 net.cpp:94] Creating Layer relu3
I0131 01:37:55.134279 114666 net.cpp:435] relu3 <- conv3
I0131 01:37:55.134295 114666 net.cpp:409] relu3 -> relu3
I0131 01:37:55.134331 114666 net.cpp:144] Setting up relu3
I0131 01:37:55.134359 114666 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0131 01:37:55.134374 114666 net.cpp:159] Memory required for data: 1872588800
I0131 01:37:55.134389 114666 layer_factory.hpp:77] Creating layer conv4
I0131 01:37:55.134413 114666 net.cpp:94] Creating Layer conv4
I0131 01:37:55.134429 114666 net.cpp:435] conv4 <- relu3
I0131 01:37:55.134449 114666 net.cpp:409] conv4 -> conv4
I0131 01:37:55.150058 114666 net.cpp:144] Setting up conv4
I0131 01:37:55.150080 114666 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0131 01:37:55.150084 114666 net.cpp:159] Memory required for data: 1939042304
I0131 01:37:55.150111 114666 layer_factory.hpp:77] Creating layer relu4
I0131 01:37:55.150130 114666 net.cpp:94] Creating Layer relu4
I0131 01:37:55.150135 114666 net.cpp:435] relu4 <- conv4
I0131 01:37:55.150142 114666 net.cpp:409] relu4 -> relu4
I0131 01:37:55.150172 114666 net.cpp:144] Setting up relu4
I0131 01:37:55.150177 114666 net.cpp:151] Top shape: 256 384 13 13 (16613376)
I0131 01:37:55.150182 114666 net.cpp:159] Memory required for data: 2005495808
I0131 01:37:55.150192 114666 layer_factory.hpp:77] Creating layer conv5
I0131 01:37:55.150203 114666 net.cpp:94] Creating Layer conv5
I0131 01:37:55.150211 114666 net.cpp:435] conv5 <- relu4
I0131 01:37:55.150218 114666 net.cpp:409] conv5 -> conv5
I0131 01:37:55.166579 114666 net.cpp:144] Setting up conv5
I0131 01:37:55.166647 114666 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0131 01:37:55.166664 114666 net.cpp:159] Memory required for data: 2049798144
I0131 01:37:55.166688 114666 layer_factory.hpp:77] Creating layer relu5
I0131 01:37:55.166712 114666 net.cpp:94] Creating Layer relu5
I0131 01:37:55.166729 114666 net.cpp:435] relu5 <- conv5
I0131 01:37:55.166751 114666 net.cpp:409] relu5 -> relu5
I0131 01:37:55.166811 114666 net.cpp:144] Setting up relu5
I0131 01:37:55.166831 114666 net.cpp:151] Top shape: 256 256 13 13 (11075584)
I0131 01:37:55.166846 114666 net.cpp:159] Memory required for data: 2094100480
I0131 01:37:55.166860 114666 layer_factory.hpp:77] Creating layer pool5
I0131 01:37:55.166880 114666 net.cpp:94] Creating Layer pool5
I0131 01:37:55.166898 114666 net.cpp:435] pool5 <- relu5
I0131 01:37:55.166915 114666 net.cpp:409] pool5 -> pool5
I0131 01:37:55.166975 114666 net.cpp:144] Setting up pool5
I0131 01:37:55.166995 114666 net.cpp:151] Top shape: 256 256 6 6 (2359296)
I0131 01:37:55.167011 114666 net.cpp:159] Memory required for data: 2103537664
I0131 01:37:55.167026 114666 layer_factory.hpp:77] Creating layer fc6
I0131 01:37:55.167049 114666 net.cpp:94] Creating Layer fc6
I0131 01:37:55.167065 114666 net.cpp:435] fc6 <- pool5
I0131 01:37:55.167085 114666 net.cpp:409] fc6 -> fc6
I0131 01:37:55.553709 114666 net.cpp:144] Setting up fc6
I0131 01:37:55.553736 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.553738 114666 net.cpp:159] Memory required for data: 2107731968
I0131 01:37:55.553762 114666 layer_factory.hpp:77] Creating layer relu6
I0131 01:37:55.553771 114666 net.cpp:94] Creating Layer relu6
I0131 01:37:55.553794 114666 net.cpp:435] relu6 <- fc6
I0131 01:37:55.553802 114666 net.cpp:409] relu6 -> relu6
I0131 01:37:55.553822 114666 net.cpp:144] Setting up relu6
I0131 01:37:55.553825 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.553828 114666 net.cpp:159] Memory required for data: 2111926272
I0131 01:37:55.553829 114666 layer_factory.hpp:77] Creating layer drop6
I0131 01:37:55.553835 114666 net.cpp:94] Creating Layer drop6
I0131 01:37:55.553838 114666 net.cpp:435] drop6 <- relu6
I0131 01:37:55.553841 114666 net.cpp:409] drop6 -> drop6
I0131 01:37:55.553865 114666 net.cpp:144] Setting up drop6
I0131 01:37:55.553871 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.553872 114666 net.cpp:159] Memory required for data: 2116120576
I0131 01:37:55.553875 114666 layer_factory.hpp:77] Creating layer fc7
I0131 01:37:55.553880 114666 net.cpp:94] Creating Layer fc7
I0131 01:37:55.553884 114666 net.cpp:435] fc7 <- drop6
I0131 01:37:55.553887 114666 net.cpp:409] fc7 -> fc7
I0131 01:37:55.695583 114666 net.cpp:144] Setting up fc7
I0131 01:37:55.695611 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.695613 114666 net.cpp:159] Memory required for data: 2120314880
I0131 01:37:55.695621 114666 layer_factory.hpp:77] Creating layer bn7
I0131 01:37:55.695629 114666 net.cpp:94] Creating Layer bn7
I0131 01:37:55.695632 114666 net.cpp:435] bn7 <- fc7
I0131 01:37:55.695639 114666 net.cpp:409] bn7 -> scale7
I0131 01:37:55.696146 114666 net.cpp:144] Setting up bn7
I0131 01:37:55.696153 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.696156 114666 net.cpp:159] Memory required for data: 2124509184
I0131 01:37:55.696163 114666 layer_factory.hpp:77] Creating layer relu7
I0131 01:37:55.696168 114666 net.cpp:94] Creating Layer relu7
I0131 01:37:55.696171 114666 net.cpp:435] relu7 <- scale7
I0131 01:37:55.696174 114666 net.cpp:409] relu7 -> relu7
I0131 01:37:55.696192 114666 net.cpp:144] Setting up relu7
I0131 01:37:55.696195 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.696198 114666 net.cpp:159] Memory required for data: 2128703488
I0131 01:37:55.696202 114666 layer_factory.hpp:77] Creating layer drop7
I0131 01:37:55.696205 114666 net.cpp:94] Creating Layer drop7
I0131 01:37:55.696208 114666 net.cpp:435] drop7 <- relu7
I0131 01:37:55.696213 114666 net.cpp:409] drop7 -> drop7
I0131 01:37:55.696233 114666 net.cpp:144] Setting up drop7
I0131 01:37:55.696238 114666 net.cpp:151] Top shape: 256 4096 (1048576)
I0131 01:37:55.696240 114666 net.cpp:159] Memory required for data: 2132897792
I0131 01:37:55.696243 114666 layer_factory.hpp:77] Creating layer fc8
I0131 01:37:55.696249 114666 net.cpp:94] Creating Layer fc8
I0131 01:37:55.696250 114666 net.cpp:435] fc8 <- drop7
I0131 01:37:55.696254 114666 net.cpp:409] fc8 -> fc8
I0131 01:37:55.697141 114666 net.cpp:144] Setting up fc8
I0131 01:37:55.697152 114666 net.cpp:151] Top shape: 256 2 (512)
I0131 01:37:55.697155 114666 net.cpp:159] Memory required for data: 2132899840
I0131 01:37:55.697162 114666 layer_factory.hpp:77] Creating layer loss
I0131 01:37:55.697170 114666 net.cpp:94] Creating Layer loss
I0131 01:37:55.697175 114666 net.cpp:435] loss <- fc8
I0131 01:37:55.697180 114666 net.cpp:435] loss <- label
I0131 01:37:55.697183 114666 net.cpp:409] loss -> loss
I0131 01:37:55.697190 114666 layer_factory.hpp:77] Creating layer loss
I0131 01:37:55.697252 114666 net.cpp:144] Setting up loss
I0131 01:37:55.697257 114666 net.cpp:151] Top shape: (1)
I0131 01:37:55.697258 114666 net.cpp:154]     with loss weight 1
I0131 01:37:55.697268 114666 net.cpp:159] Memory required for data: 2132899844
I0131 01:37:55.697270 114666 net.cpp:220] loss needs backward computation.
I0131 01:37:55.697283 114666 net.cpp:220] fc8 needs backward computation.
I0131 01:37:55.697285 114666 net.cpp:220] drop7 needs backward computation.
I0131 01:37:55.697289 114666 net.cpp:220] relu7 needs backward computation.
I0131 01:37:55.697293 114666 net.cpp:220] bn7 needs backward computation.
I0131 01:37:55.697296 114666 net.cpp:220] fc7 needs backward computation.
I0131 01:37:55.697319 114666 net.cpp:220] drop6 needs backward computation.
I0131 01:37:55.697322 114666 net.cpp:220] relu6 needs backward computation.
I0131 01:37:55.697325 114666 net.cpp:220] fc6 needs backward computation.
I0131 01:37:55.697329 114666 net.cpp:220] pool5 needs backward computation.
I0131 01:37:55.697333 114666 net.cpp:220] relu5 needs backward computation.
I0131 01:37:55.697337 114666 net.cpp:220] conv5 needs backward computation.
I0131 01:37:55.697340 114666 net.cpp:220] relu4 needs backward computation.
I0131 01:37:55.697343 114666 net.cpp:220] conv4 needs backward computation.
I0131 01:37:55.697346 114666 net.cpp:220] relu3 needs backward computation.
I0131 01:37:55.697348 114666 net.cpp:220] conv3 needs backward computation.
I0131 01:37:55.697351 114666 net.cpp:220] pool2 needs backward computation.
I0131 01:37:55.697355 114666 net.cpp:220] relu2 needs backward computation.
I0131 01:37:55.697356 114666 net.cpp:220] bn2 needs backward computation.
I0131 01:37:55.697360 114666 net.cpp:220] conv2 needs backward computation.
I0131 01:37:55.697362 114666 net.cpp:220] pool1 needs backward computation.
I0131 01:37:55.697365 114666 net.cpp:220] relu1 needs backward computation.
I0131 01:37:55.697368 114666 net.cpp:220] bn1 needs backward computation.
I0131 01:37:55.697371 114666 net.cpp:220] conv1 needs backward computation.
I0131 01:37:55.697373 114666 net.cpp:222] data does not need backward computation.
I0131 01:37:55.697377 114666 net.cpp:264] This network produces output loss
I0131 01:37:55.697393 114666 net.cpp:284] Network initialization done.
I0131 01:37:55.697667 114666 solver.cpp:189] Creating test net (#0) specified by net file: cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/net_finetune.prototxt
I0131 01:37:55.697695 114666 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0131 01:37:55.697857 114666 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0131 01:37:55.697962 114666 layer_factory.hpp:77] Creating layer data
I0131 01:37:55.698006 114666 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:55.698913 114666 net.cpp:94] Creating Layer data
I0131 01:37:55.698925 114666 net.cpp:409] data -> data
I0131 01:37:55.698935 114666 net.cpp:409] data -> label
I0131 01:37:55.700183   352 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0131 01:37:55.700218   352 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0131 01:37:55.700481 114666 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0131 01:37:55.700553 114666 data_layer.cpp:83] output data size: 50,3,227,227
I0131 01:37:55.797091 114666 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 01:37:55.797147 114666 net.cpp:144] Setting up data
I0131 01:37:55.797155 114666 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0131 01:37:55.797158 114666 net.cpp:151] Top shape: 50 (50)
I0131 01:37:55.797176 114666 net.cpp:159] Memory required for data: 30917600
I0131 01:37:55.797180 114666 layer_factory.hpp:77] Creating layer label_data_1_split
I0131 01:37:55.797189 114666 net.cpp:94] Creating Layer label_data_1_split
I0131 01:37:55.797192 114666 net.cpp:435] label_data_1_split <- label
I0131 01:37:55.797199 114666 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0131 01:37:55.797206 114666 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0131 01:37:55.797276 114666 net.cpp:144] Setting up label_data_1_split
I0131 01:37:55.797281 114666 net.cpp:151] Top shape: 50 (50)
I0131 01:37:55.797282 114666 net.cpp:151] Top shape: 50 (50)
I0131 01:37:55.797284 114666 net.cpp:159] Memory required for data: 30918000
I0131 01:37:55.797287 114666 layer_factory.hpp:77] Creating layer conv1
I0131 01:37:55.797297 114666 net.cpp:94] Creating Layer conv1
I0131 01:37:55.797299 114666 net.cpp:435] conv1 <- data
I0131 01:37:55.797303 114666 net.cpp:409] conv1 -> conv1
I0131 01:37:55.797909 114666 net.cpp:144] Setting up conv1
I0131 01:37:55.797915 114666 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:55.797917 114666 net.cpp:159] Memory required for data: 88998000
I0131 01:37:55.797926 114666 layer_factory.hpp:77] Creating layer bn1
I0131 01:37:55.797933 114666 net.cpp:94] Creating Layer bn1
I0131 01:37:55.797935 114666 net.cpp:435] bn1 <- conv1
I0131 01:37:55.797940 114666 net.cpp:409] bn1 -> scale1
I0131 01:37:55.799818 114666 net.cpp:144] Setting up bn1
I0131 01:37:55.799824 114666 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:55.799826 114666 net.cpp:159] Memory required for data: 147078000
I0131 01:37:55.799835 114666 layer_factory.hpp:77] Creating layer relu1
I0131 01:37:55.799855 114666 net.cpp:94] Creating Layer relu1
I0131 01:37:55.799859 114666 net.cpp:435] relu1 <- scale1
I0131 01:37:55.799862 114666 net.cpp:409] relu1 -> relu1
I0131 01:37:55.800117 114666 net.cpp:144] Setting up relu1
I0131 01:37:55.800122 114666 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 01:37:55.800125 114666 net.cpp:159] Memory required for data: 205158000
I0131 01:37:55.800127 114666 layer_factory.hpp:77] Creating layer pool1
I0131 01:37:55.800132 114666 net.cpp:94] Creating Layer pool1
I0131 01:37:55.800134 114666 net.cpp:435] pool1 <- relu1
I0131 01:37:55.800138 114666 net.cpp:409] pool1 -> pool1
I0131 01:37:55.800179 114666 net.cpp:144] Setting up pool1
I0131 01:37:55.800182 114666 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0131 01:37:55.800185 114666 net.cpp:159] Memory required for data: 219154800
I0131 01:37:55.800187 114666 layer_factory.hpp:77] Creating layer conv2
I0131 01:37:55.800194 114666 net.cpp:94] Creating Layer conv2
I0131 01:37:55.800196 114666 net.cpp:435] conv2 <- pool1
I0131 01:37:55.800218 114666 net.cpp:409] conv2 -> conv2
I0131 01:37:55.806849 114666 net.cpp:144] Setting up conv2
I0131 01:37:55.806869 114666 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:55.806872 114666 net.cpp:159] Memory required for data: 256479600
I0131 01:37:55.806883 114666 layer_factory.hpp:77] Creating layer bn2
I0131 01:37:55.806895 114666 net.cpp:94] Creating Layer bn2
I0131 01:37:55.806898 114666 net.cpp:435] bn2 <- conv2
I0131 01:37:55.806905 114666 net.cpp:409] bn2 -> scale2
I0131 01:37:55.807539 114666 net.cpp:144] Setting up bn2
I0131 01:37:55.807549 114666 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:55.807554 114666 net.cpp:159] Memory required for data: 293804400
I0131 01:37:55.807565 114666 layer_factory.hpp:77] Creating layer relu2
I0131 01:37:55.807575 114666 net.cpp:94] Creating Layer relu2
I0131 01:37:55.807580 114666 net.cpp:435] relu2 <- scale2
I0131 01:37:55.807586 114666 net.cpp:409] relu2 -> relu2
I0131 01:37:55.807615 114666 net.cpp:144] Setting up relu2
I0131 01:37:55.807621 114666 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 01:37:55.807626 114666 net.cpp:159] Memory required for data: 331129200
I0131 01:37:55.807631 114666 layer_factory.hpp:77] Creating layer pool2
I0131 01:37:55.807639 114666 net.cpp:94] Creating Layer pool2
I0131 01:37:55.807643 114666 net.cpp:435] pool2 <- relu2
I0131 01:37:55.807651 114666 net.cpp:409] pool2 -> pool2
I0131 01:37:55.807693 114666 net.cpp:144] Setting up pool2
I0131 01:37:55.807699 114666 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:55.807703 114666 net.cpp:159] Memory required for data: 339782000
I0131 01:37:55.807708 114666 layer_factory.hpp:77] Creating layer conv3
I0131 01:37:55.807721 114666 net.cpp:94] Creating Layer conv3
I0131 01:37:55.807726 114666 net.cpp:435] conv3 <- pool2
I0131 01:37:55.807734 114666 net.cpp:409] conv3 -> conv3
I0131 01:37:55.832695 114666 net.cpp:144] Setting up conv3
I0131 01:37:55.832726 114666 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:55.832731 114666 net.cpp:159] Memory required for data: 352761200
I0131 01:37:55.832744 114666 layer_factory.hpp:77] Creating layer relu3
I0131 01:37:55.832756 114666 net.cpp:94] Creating Layer relu3
I0131 01:37:55.832764 114666 net.cpp:435] relu3 <- conv3
I0131 01:37:55.832773 114666 net.cpp:409] relu3 -> relu3
I0131 01:37:55.832813 114666 net.cpp:144] Setting up relu3
I0131 01:37:55.832823 114666 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:55.832828 114666 net.cpp:159] Memory required for data: 365740400
I0131 01:37:55.832831 114666 layer_factory.hpp:77] Creating layer conv4
I0131 01:37:55.832845 114666 net.cpp:94] Creating Layer conv4
I0131 01:37:55.832851 114666 net.cpp:435] conv4 <- relu3
I0131 01:37:55.832859 114666 net.cpp:409] conv4 -> conv4
I0131 01:37:55.850958 114666 net.cpp:144] Setting up conv4
I0131 01:37:55.850989 114666 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:55.850996 114666 net.cpp:159] Memory required for data: 378719600
I0131 01:37:55.851012 114666 layer_factory.hpp:77] Creating layer relu4
I0131 01:37:55.851023 114666 net.cpp:94] Creating Layer relu4
I0131 01:37:55.851030 114666 net.cpp:435] relu4 <- conv4
I0131 01:37:55.851039 114666 net.cpp:409] relu4 -> relu4
I0131 01:37:55.851080 114666 net.cpp:144] Setting up relu4
I0131 01:37:55.851089 114666 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 01:37:55.851092 114666 net.cpp:159] Memory required for data: 391698800
I0131 01:37:55.851097 114666 layer_factory.hpp:77] Creating layer conv5
I0131 01:37:55.851110 114666 net.cpp:94] Creating Layer conv5
I0131 01:37:55.851117 114666 net.cpp:435] conv5 <- relu4
I0131 01:37:55.851125 114666 net.cpp:409] conv5 -> conv5
I0131 01:37:55.863730 114666 net.cpp:144] Setting up conv5
I0131 01:37:55.863754 114666 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:55.863756 114666 net.cpp:159] Memory required for data: 400351600
I0131 01:37:55.863764 114666 layer_factory.hpp:77] Creating layer relu5
I0131 01:37:55.863771 114666 net.cpp:94] Creating Layer relu5
I0131 01:37:55.863797 114666 net.cpp:435] relu5 <- conv5
I0131 01:37:55.863803 114666 net.cpp:409] relu5 -> relu5
I0131 01:37:55.863829 114666 net.cpp:144] Setting up relu5
I0131 01:37:55.863833 114666 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 01:37:55.863835 114666 net.cpp:159] Memory required for data: 409004400
I0131 01:37:55.863838 114666 layer_factory.hpp:77] Creating layer pool5
I0131 01:37:55.863844 114666 net.cpp:94] Creating Layer pool5
I0131 01:37:55.863847 114666 net.cpp:435] pool5 <- relu5
I0131 01:37:55.863852 114666 net.cpp:409] pool5 -> pool5
I0131 01:37:55.863878 114666 net.cpp:144] Setting up pool5
I0131 01:37:55.863883 114666 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0131 01:37:55.863885 114666 net.cpp:159] Memory required for data: 410847600
I0131 01:37:55.863888 114666 layer_factory.hpp:77] Creating layer fc6
I0131 01:37:55.863893 114666 net.cpp:94] Creating Layer fc6
I0131 01:37:55.863895 114666 net.cpp:435] fc6 <- pool5
I0131 01:37:55.863900 114666 net.cpp:409] fc6 -> fc6
I0131 01:37:56.189721 114666 net.cpp:144] Setting up fc6
I0131 01:37:56.189748 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.189750 114666 net.cpp:159] Memory required for data: 411666800
I0131 01:37:56.189759 114666 layer_factory.hpp:77] Creating layer relu6
I0131 01:37:56.189765 114666 net.cpp:94] Creating Layer relu6
I0131 01:37:56.189769 114666 net.cpp:435] relu6 <- fc6
I0131 01:37:56.189774 114666 net.cpp:409] relu6 -> relu6
I0131 01:37:56.189818 114666 net.cpp:144] Setting up relu6
I0131 01:37:56.189822 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.189824 114666 net.cpp:159] Memory required for data: 412486000
I0131 01:37:56.189827 114666 layer_factory.hpp:77] Creating layer drop6
I0131 01:37:56.189834 114666 net.cpp:94] Creating Layer drop6
I0131 01:37:56.189836 114666 net.cpp:435] drop6 <- relu6
I0131 01:37:56.189841 114666 net.cpp:409] drop6 -> drop6
I0131 01:37:56.189872 114666 net.cpp:144] Setting up drop6
I0131 01:37:56.189877 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.189880 114666 net.cpp:159] Memory required for data: 413305200
I0131 01:37:56.189882 114666 layer_factory.hpp:77] Creating layer fc7
I0131 01:37:56.189888 114666 net.cpp:94] Creating Layer fc7
I0131 01:37:56.189893 114666 net.cpp:435] fc7 <- drop6
I0131 01:37:56.189898 114666 net.cpp:409] fc7 -> fc7
I0131 01:37:56.345914 114666 net.cpp:144] Setting up fc7
I0131 01:37:56.345947 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.345953 114666 net.cpp:159] Memory required for data: 414124400
I0131 01:37:56.345966 114666 layer_factory.hpp:77] Creating layer bn7
I0131 01:37:56.345981 114666 net.cpp:94] Creating Layer bn7
I0131 01:37:56.345988 114666 net.cpp:435] bn7 <- fc7
I0131 01:37:56.346000 114666 net.cpp:409] bn7 -> scale7
I0131 01:37:56.346863 114666 net.cpp:144] Setting up bn7
I0131 01:37:56.346873 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.346875 114666 net.cpp:159] Memory required for data: 414943600
I0131 01:37:56.346889 114666 layer_factory.hpp:77] Creating layer relu7
I0131 01:37:56.346894 114666 net.cpp:94] Creating Layer relu7
I0131 01:37:56.346899 114666 net.cpp:435] relu7 <- scale7
I0131 01:37:56.346904 114666 net.cpp:409] relu7 -> relu7
I0131 01:37:56.346931 114666 net.cpp:144] Setting up relu7
I0131 01:37:56.346938 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.346941 114666 net.cpp:159] Memory required for data: 415762800
I0131 01:37:56.346945 114666 layer_factory.hpp:77] Creating layer drop7
I0131 01:37:56.346951 114666 net.cpp:94] Creating Layer drop7
I0131 01:37:56.346956 114666 net.cpp:435] drop7 <- relu7
I0131 01:37:56.346961 114666 net.cpp:409] drop7 -> drop7
I0131 01:37:56.346998 114666 net.cpp:144] Setting up drop7
I0131 01:37:56.347005 114666 net.cpp:151] Top shape: 50 4096 (204800)
I0131 01:37:56.347008 114666 net.cpp:159] Memory required for data: 416582000
I0131 01:37:56.347012 114666 layer_factory.hpp:77] Creating layer fc8
I0131 01:37:56.347020 114666 net.cpp:94] Creating Layer fc8
I0131 01:37:56.347048 114666 net.cpp:435] fc8 <- drop7
I0131 01:37:56.347055 114666 net.cpp:409] fc8 -> fc8
I0131 01:37:56.347296 114666 net.cpp:144] Setting up fc8
I0131 01:37:56.347303 114666 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:56.347307 114666 net.cpp:159] Memory required for data: 416582400
I0131 01:37:56.347313 114666 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0131 01:37:56.347319 114666 net.cpp:94] Creating Layer fc8_fc8_0_split
I0131 01:37:56.347323 114666 net.cpp:435] fc8_fc8_0_split <- fc8
I0131 01:37:56.347331 114666 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0131 01:37:56.347337 114666 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0131 01:37:56.347374 114666 net.cpp:144] Setting up fc8_fc8_0_split
I0131 01:37:56.347380 114666 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:56.347385 114666 net.cpp:151] Top shape: 50 2 (100)
I0131 01:37:56.347388 114666 net.cpp:159] Memory required for data: 416583200
I0131 01:37:56.347391 114666 layer_factory.hpp:77] Creating layer loss
I0131 01:37:56.347398 114666 net.cpp:94] Creating Layer loss
I0131 01:37:56.347402 114666 net.cpp:435] loss <- fc8_fc8_0_split_0
I0131 01:37:56.347407 114666 net.cpp:435] loss <- label_data_1_split_0
I0131 01:37:56.347414 114666 net.cpp:409] loss -> loss
I0131 01:37:56.347421 114666 layer_factory.hpp:77] Creating layer loss
I0131 01:37:56.347528 114666 net.cpp:144] Setting up loss
I0131 01:37:56.347535 114666 net.cpp:151] Top shape: (1)
I0131 01:37:56.347538 114666 net.cpp:154]     with loss weight 1
I0131 01:37:56.347549 114666 net.cpp:159] Memory required for data: 416583204
I0131 01:37:56.347554 114666 layer_factory.hpp:77] Creating layer accuracy-top1
I0131 01:37:56.347561 114666 net.cpp:94] Creating Layer accuracy-top1
I0131 01:37:56.347564 114666 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0131 01:37:56.347569 114666 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0131 01:37:56.347575 114666 net.cpp:409] accuracy-top1 -> top-1
I0131 01:37:56.347584 114666 net.cpp:144] Setting up accuracy-top1
I0131 01:37:56.347589 114666 net.cpp:151] Top shape: (1)
I0131 01:37:56.347591 114666 net.cpp:159] Memory required for data: 416583208
I0131 01:37:56.347596 114666 net.cpp:222] accuracy-top1 does not need backward computation.
I0131 01:37:56.347600 114666 net.cpp:220] loss needs backward computation.
I0131 01:37:56.347605 114666 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0131 01:37:56.347609 114666 net.cpp:220] fc8 needs backward computation.
I0131 01:37:56.347615 114666 net.cpp:220] drop7 needs backward computation.
I0131 01:37:56.347617 114666 net.cpp:220] relu7 needs backward computation.
I0131 01:37:56.347621 114666 net.cpp:220] bn7 needs backward computation.
I0131 01:37:56.347625 114666 net.cpp:220] fc7 needs backward computation.
I0131 01:37:56.347630 114666 net.cpp:220] drop6 needs backward computation.
I0131 01:37:56.347635 114666 net.cpp:220] relu6 needs backward computation.
I0131 01:37:56.347638 114666 net.cpp:220] fc6 needs backward computation.
I0131 01:37:56.347642 114666 net.cpp:220] pool5 needs backward computation.
I0131 01:37:56.347646 114666 net.cpp:220] relu5 needs backward computation.
I0131 01:37:56.347651 114666 net.cpp:220] conv5 needs backward computation.
I0131 01:37:56.347654 114666 net.cpp:220] relu4 needs backward computation.
I0131 01:37:56.347657 114666 net.cpp:220] conv4 needs backward computation.
I0131 01:37:56.347662 114666 net.cpp:220] relu3 needs backward computation.
I0131 01:37:56.347666 114666 net.cpp:220] conv3 needs backward computation.
I0131 01:37:56.347669 114666 net.cpp:220] pool2 needs backward computation.
I0131 01:37:56.347672 114666 net.cpp:220] relu2 needs backward computation.
I0131 01:37:56.347677 114666 net.cpp:220] bn2 needs backward computation.
I0131 01:37:56.347681 114666 net.cpp:220] conv2 needs backward computation.
I0131 01:37:56.347684 114666 net.cpp:220] pool1 needs backward computation.
I0131 01:37:56.347688 114666 net.cpp:220] relu1 needs backward computation.
I0131 01:37:56.347693 114666 net.cpp:220] bn1 needs backward computation.
I0131 01:37:56.347705 114666 net.cpp:220] conv1 needs backward computation.
I0131 01:37:56.347710 114666 net.cpp:222] label_data_1_split does not need backward computation.
I0131 01:37:56.347715 114666 net.cpp:222] data does not need backward computation.
I0131 01:37:56.347719 114666 net.cpp:264] This network produces output loss
I0131 01:37:56.347723 114666 net.cpp:264] This network produces output top-1
I0131 01:37:56.347750 114666 net.cpp:284] Network initialization done.
I0131 01:37:56.347877 114666 solver.cpp:63] Solver scaffolding done.
I0131 01:37:56.349692 114666 caffe_interface.cpp:93] Finetuning from cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/sparse.caffemodel
I0131 01:37:57.985929 114666 caffe_interface.cpp:527] Starting Optimization
I0131 01:37:57.985951 114666 solver.cpp:335] Solving 
I0131 01:37:57.985954 114666 solver.cpp:336] Learning Rate Policy: step
I0131 01:37:57.987820 114666 solver.cpp:418] Iteration 0, Testing net (#0)
I0131 01:38:01.362579 114666 solver.cpp:517]     Test net output #0: loss = 0.237602 (* 1 = 0.237602 loss)
I0131 01:38:01.362613 114666 solver.cpp:517]     Test net output #1: top-1 = 0.953
I0131 01:38:01.847947 114666 solver.cpp:266] Iteration 0 (0 iter/s, 3.8618s/50 iter), loss = 0.000402244
I0131 01:38:01.847980 114666 solver.cpp:285]     Train net output #0: loss = 0.000402244 (* 1 = 0.000402244 loss)
I0131 01:38:01.850214 114666 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0131 01:38:25.961721 114666 solver.cpp:266] Iteration 50 (2.07377 iter/s, 24.1106s/50 iter), loss = 0.0666117
I0131 01:38:25.961805 114666 solver.cpp:285]     Train net output #0: loss = 0.0666117 (* 1 = 0.0666117 loss)
I0131 01:38:25.963966 114666 sgd_solver.cpp:106] Iteration 50, lr = 0.001
I0131 01:38:50.207978 114666 solver.cpp:266] Iteration 100 (2.06244 iter/s, 24.2431s/50 iter), loss = 0.0248328
I0131 01:38:50.208010 114666 solver.cpp:285]     Train net output #0: loss = 0.0248328 (* 1 = 0.0248328 loss)
I0131 01:38:50.210225 114666 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0131 01:39:14.810794 114666 solver.cpp:266] Iteration 150 (2.03255 iter/s, 24.5997s/50 iter), loss = 0.0463975
I0131 01:39:14.810931 114666 solver.cpp:285]     Train net output #0: loss = 0.0463975 (* 1 = 0.0463975 loss)
I0131 01:39:14.813024 114666 sgd_solver.cpp:106] Iteration 150, lr = 0.001
I0131 01:39:39.060858 114666 solver.cpp:266] Iteration 200 (2.06212 iter/s, 24.2469s/50 iter), loss = 0.0312108
I0131 01:39:39.060889 114666 solver.cpp:285]     Train net output #0: loss = 0.0312108 (* 1 = 0.0312108 loss)
I0131 01:39:39.063117 114666 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0131 01:40:03.458653 114666 solver.cpp:266] Iteration 250 (2.04963 iter/s, 24.3946s/50 iter), loss = 0.0218942
I0131 01:40:03.458768 114666 solver.cpp:285]     Train net output #0: loss = 0.0218942 (* 1 = 0.0218942 loss)
I0131 01:40:03.458776 114666 sgd_solver.cpp:106] Iteration 250, lr = 0.001
I0131 01:40:27.900779 114666 solver.cpp:266] Iteration 300 (2.04573 iter/s, 24.4411s/50 iter), loss = 0.0552214
I0131 01:40:27.900812 114666 solver.cpp:285]     Train net output #0: loss = 0.0552214 (* 1 = 0.0552214 loss)
I0131 01:40:27.903033 114666 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0131 01:40:52.256458 114666 solver.cpp:266] Iteration 350 (2.05317 iter/s, 24.3525s/50 iter), loss = 0.0535929
I0131 01:40:52.256562 114666 solver.cpp:285]     Train net output #0: loss = 0.053593 (* 1 = 0.053593 loss)
I0131 01:40:52.256608 114666 sgd_solver.cpp:106] Iteration 350, lr = 0.001
I0131 01:41:16.572127 114666 solver.cpp:266] Iteration 400 (2.05638 iter/s, 24.3146s/50 iter), loss = 0.0283147
I0131 01:41:16.572171 114666 solver.cpp:285]     Train net output #0: loss = 0.0283147 (* 1 = 0.0283147 loss)
I0131 01:41:16.574379 114666 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0131 01:41:40.702543 114666 solver.cpp:266] Iteration 450 (2.07234 iter/s, 24.1273s/50 iter), loss = 0.0300556
I0131 01:41:40.702680 114666 solver.cpp:285]     Train net output #0: loss = 0.0300556 (* 1 = 0.0300556 loss)
I0131 01:41:40.702720 114666 sgd_solver.cpp:106] Iteration 450, lr = 0.001
I0131 01:42:05.113342 114666 solver.cpp:266] Iteration 500 (2.04837 iter/s, 24.4097s/50 iter), loss = 0.050669
I0131 01:42:05.113374 114666 solver.cpp:285]     Train net output #0: loss = 0.0506691 (* 1 = 0.0506691 loss)
I0131 01:42:05.115584 114666 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0131 01:42:29.386981 114666 solver.cpp:266] Iteration 550 (2.06011 iter/s, 24.2705s/50 iter), loss = 0.0589351
I0131 01:42:29.387109 114666 solver.cpp:285]     Train net output #0: loss = 0.0589351 (* 1 = 0.0589351 loss)
I0131 01:42:29.389238 114666 sgd_solver.cpp:106] Iteration 550, lr = 0.001
I0131 01:42:53.640012 114666 solver.cpp:266] Iteration 600 (2.06187 iter/s, 24.2499s/50 iter), loss = 0.0818154
I0131 01:42:53.640043 114666 solver.cpp:285]     Train net output #0: loss = 0.0818154 (* 1 = 0.0818154 loss)
I0131 01:42:53.642259 114666 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0131 01:43:18.043751 114666 solver.cpp:266] Iteration 650 (2.04913 iter/s, 24.4006s/50 iter), loss = 0.0378612
I0131 01:43:18.043872 114666 solver.cpp:285]     Train net output #0: loss = 0.0378613 (* 1 = 0.0378613 loss)
I0131 01:43:18.045987 114666 sgd_solver.cpp:106] Iteration 650, lr = 0.001
I0131 01:43:42.485210 114666 solver.cpp:266] Iteration 700 (2.04597 iter/s, 24.4383s/50 iter), loss = 0.0166086
I0131 01:43:42.485242 114666 solver.cpp:285]     Train net output #0: loss = 0.0166086 (* 1 = 0.0166086 loss)
I0131 01:43:42.485288 114666 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0131 01:44:06.722545 114666 solver.cpp:266] Iteration 750 (2.06302 iter/s, 24.2364s/50 iter), loss = 0.0200214
I0131 01:44:06.722654 114666 solver.cpp:285]     Train net output #0: loss = 0.0200214 (* 1 = 0.0200214 loss)
I0131 01:44:06.722697 114666 sgd_solver.cpp:106] Iteration 750, lr = 0.001
I0131 01:44:31.035672 114666 solver.cpp:266] Iteration 800 (2.05659 iter/s, 24.3121s/50 iter), loss = 0.0445993
I0131 01:44:31.035703 114666 solver.cpp:285]     Train net output #0: loss = 0.0445994 (* 1 = 0.0445994 loss)
I0131 01:44:31.037931 114666 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0131 01:44:55.354784 114666 solver.cpp:266] Iteration 850 (2.05626 iter/s, 24.316s/50 iter), loss = 0.0943108
I0131 01:44:55.354895 114666 solver.cpp:285]     Train net output #0: loss = 0.0943108 (* 1 = 0.0943108 loss)
I0131 01:44:55.357028 114666 sgd_solver.cpp:106] Iteration 850, lr = 0.001
I0131 01:45:19.805233 114666 solver.cpp:266] Iteration 900 (2.04522 iter/s, 24.4473s/50 iter), loss = 0.0350591
I0131 01:45:19.805267 114666 solver.cpp:285]     Train net output #0: loss = 0.0350591 (* 1 = 0.0350591 loss)
I0131 01:45:19.807480 114666 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0131 01:45:44.055443 114666 solver.cpp:266] Iteration 950 (2.0621 iter/s, 24.2471s/50 iter), loss = 0.028493
I0131 01:45:44.055558 114666 solver.cpp:285]     Train net output #0: loss = 0.028493 (* 1 = 0.028493 loss)
I0131 01:45:44.057691 114666 sgd_solver.cpp:106] Iteration 950, lr = 0.001
I0131 01:46:07.892647 114666 solver.cpp:418] Iteration 1000, Testing net (#0)
I0131 01:46:11.463922 114666 solver.cpp:517]     Test net output #0: loss = 0.281277 (* 1 = 0.281277 loss)
I0131 01:46:11.463937 114666 solver.cpp:517]     Test net output #1: top-1 = 0.9315
I0131 01:46:11.933338 114666 solver.cpp:266] Iteration 1000 (1.79375 iter/s, 27.8746s/50 iter), loss = 0.0368653
I0131 01:46:11.933368 114666 solver.cpp:285]     Train net output #0: loss = 0.0368653 (* 1 = 0.0368653 loss)
I0131 01:46:11.935590 114666 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0131 01:46:36.269784 114666 solver.cpp:266] Iteration 1050 (2.0548 iter/s, 24.3333s/50 iter), loss = 0.0200536
I0131 01:46:36.269883 114666 solver.cpp:285]     Train net output #0: loss = 0.0200536 (* 1 = 0.0200536 loss)
I0131 01:46:36.272032 114666 sgd_solver.cpp:106] Iteration 1050, lr = 0.001
I0131 01:47:00.546958 114666 solver.cpp:266] Iteration 1100 (2.05981 iter/s, 24.274s/50 iter), loss = 0.028836
I0131 01:47:00.546991 114666 solver.cpp:285]     Train net output #0: loss = 0.028836 (* 1 = 0.028836 loss)
I0131 01:47:00.549206 114666 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I0131 01:47:25.032819 114666 solver.cpp:266] Iteration 1150 (2.04226 iter/s, 24.4827s/50 iter), loss = 0.066898
I0131 01:47:25.032968 114666 solver.cpp:285]     Train net output #0: loss = 0.066898 (* 1 = 0.066898 loss)
I0131 01:47:25.033161 114666 sgd_solver.cpp:106] Iteration 1150, lr = 0.001
I0131 01:47:49.375684 114666 solver.cpp:266] Iteration 1200 (2.05409 iter/s, 24.3416s/50 iter), loss = 0.0542473
I0131 01:47:49.375715 114666 solver.cpp:285]     Train net output #0: loss = 0.0542473 (* 1 = 0.0542473 loss)
I0131 01:47:49.377938 114666 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0131 01:48:13.713235 114666 solver.cpp:266] Iteration 1250 (2.0547 iter/s, 24.3344s/50 iter), loss = 0.0520384
I0131 01:48:13.713362 114666 solver.cpp:285]     Train net output #0: loss = 0.0520384 (* 1 = 0.0520384 loss)
I0131 01:48:13.715492 114666 sgd_solver.cpp:106] Iteration 1250, lr = 0.001
I0131 01:48:38.118804 114666 solver.cpp:266] Iteration 1300 (2.04898 iter/s, 24.4024s/50 iter), loss = 0.0697425
I0131 01:48:38.118839 114666 solver.cpp:285]     Train net output #0: loss = 0.0697425 (* 1 = 0.0697425 loss)
I0131 01:48:38.121050 114666 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I0131 01:49:02.518205 114666 solver.cpp:266] Iteration 1350 (2.04949 iter/s, 24.3963s/50 iter), loss = 0.0362462
I0131 01:49:02.518330 114666 solver.cpp:285]     Train net output #0: loss = 0.0362462 (* 1 = 0.0362462 loss)
I0131 01:49:02.518373 114666 sgd_solver.cpp:106] Iteration 1350, lr = 0.001
I0131 01:49:26.787425 114666 solver.cpp:266] Iteration 1400 (2.06031 iter/s, 24.2682s/50 iter), loss = 0.0421716
I0131 01:49:26.787457 114666 solver.cpp:285]     Train net output #0: loss = 0.0421716 (* 1 = 0.0421716 loss)
I0131 01:49:26.789671 114666 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0131 01:49:51.141229 114666 solver.cpp:266] Iteration 1450 (2.05333 iter/s, 24.3507s/50 iter), loss = 0.0422603
I0131 01:49:51.141332 114666 solver.cpp:285]     Train net output #0: loss = 0.0422603 (* 1 = 0.0422603 loss)
I0131 01:49:51.143488 114666 sgd_solver.cpp:106] Iteration 1450, lr = 0.001
I0131 01:50:15.546649 114666 solver.cpp:266] Iteration 1500 (2.04899 iter/s, 24.4023s/50 iter), loss = 0.0445018
I0131 01:50:15.546679 114666 solver.cpp:285]     Train net output #0: loss = 0.0445018 (* 1 = 0.0445018 loss)
I0131 01:50:15.548897 114666 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I0131 01:50:39.750872 114666 solver.cpp:266] Iteration 1550 (2.06602 iter/s, 24.2011s/50 iter), loss = 0.0380576
I0131 01:50:39.750938 114666 solver.cpp:285]     Train net output #0: loss = 0.0380576 (* 1 = 0.0380576 loss)
I0131 01:50:39.750962 114666 sgd_solver.cpp:106] Iteration 1550, lr = 0.001
I0131 01:51:04.168568 114666 solver.cpp:266] Iteration 1600 (2.04778 iter/s, 24.4167s/50 iter), loss = 0.0644749
I0131 01:51:04.168602 114666 solver.cpp:285]     Train net output #0: loss = 0.0644749 (* 1 = 0.0644749 loss)
I0131 01:51:04.170816 114666 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0131 01:51:28.452740 114666 solver.cpp:266] Iteration 1650 (2.05922 iter/s, 24.281s/50 iter), loss = 0.0168524
I0131 01:51:28.452859 114666 solver.cpp:285]     Train net output #0: loss = 0.0168524 (* 1 = 0.0168524 loss)
I0131 01:51:28.454994 114666 sgd_solver.cpp:106] Iteration 1650, lr = 0.001
I0131 01:51:52.771240 114666 solver.cpp:266] Iteration 1700 (2.05631 iter/s, 24.3153s/50 iter), loss = 0.064689
I0131 01:51:52.771270 114666 solver.cpp:285]     Train net output #0: loss = 0.0646891 (* 1 = 0.0646891 loss)
I0131 01:51:52.773489 114666 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I0131 01:52:17.139829 114666 solver.cpp:266] Iteration 1750 (2.05209 iter/s, 24.3654s/50 iter), loss = 0.0292647
I0131 01:52:17.139961 114666 solver.cpp:285]     Train net output #0: loss = 0.0292647 (* 1 = 0.0292647 loss)
I0131 01:52:17.142072 114666 sgd_solver.cpp:106] Iteration 1750, lr = 0.001
I0131 01:52:41.517478 114666 solver.cpp:266] Iteration 1800 (2.05132 iter/s, 24.3745s/50 iter), loss = 0.0525932
I0131 01:52:41.517508 114666 solver.cpp:285]     Train net output #0: loss = 0.0525933 (* 1 = 0.0525933 loss)
I0131 01:52:41.519726 114666 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0131 01:53:05.839720 114666 solver.cpp:266] Iteration 1850 (2.056 iter/s, 24.3191s/50 iter), loss = 0.017983
I0131 01:53:05.839812 114666 solver.cpp:285]     Train net output #0: loss = 0.017983 (* 1 = 0.017983 loss)
I0131 01:53:05.842108 114666 sgd_solver.cpp:106] Iteration 1850, lr = 0.001
I0131 01:53:30.292235 114666 solver.cpp:266] Iteration 1900 (2.04505 iter/s, 24.4492s/50 iter), loss = 0.0348412
I0131 01:53:30.292273 114666 solver.cpp:285]     Train net output #0: loss = 0.0348412 (* 1 = 0.0348412 loss)
I0131 01:53:30.294497 114666 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I0131 01:53:54.481711 114666 solver.cpp:266] Iteration 1950 (2.06728 iter/s, 24.1863s/50 iter), loss = 0.0788139
I0131 01:53:54.481842 114666 solver.cpp:285]     Train net output #0: loss = 0.0788139 (* 1 = 0.0788139 loss)
I0131 01:53:54.483952 114666 sgd_solver.cpp:106] Iteration 1950, lr = 0.001
I0131 01:54:18.347573 114666 solver.cpp:418] Iteration 2000, Testing net (#0)
I0131 01:54:21.785959 114666 solver.cpp:517]     Test net output #0: loss = 0.228309 (* 1 = 0.228309 loss)
I0131 01:54:21.785976 114666 solver.cpp:517]     Test net output #1: top-1 = 0.934
I0131 01:54:22.338356 114666 solver.cpp:266] Iteration 2000 (1.79511 iter/s, 27.8534s/50 iter), loss = 0.067947
I0131 01:54:22.338379 114666 solver.cpp:285]     Train net output #0: loss = 0.067947 (* 1 = 0.067947 loss)
I0131 01:54:22.340620 114666 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0131 01:54:46.519892 114666 solver.cpp:266] Iteration 2050 (2.06796 iter/s, 24.1784s/50 iter), loss = 0.0640751
I0131 01:54:46.519958 114666 solver.cpp:285]     Train net output #0: loss = 0.0640751 (* 1 = 0.0640751 loss)
I0131 01:54:46.519963 114666 sgd_solver.cpp:106] Iteration 2050, lr = 0.001
I0131 01:55:10.934434 114666 solver.cpp:266] Iteration 2100 (2.04804 iter/s, 24.4136s/50 iter), loss = 0.0486188
I0131 01:55:10.934468 114666 solver.cpp:285]     Train net output #0: loss = 0.0486189 (* 1 = 0.0486189 loss)
I0131 01:55:10.936691 114666 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I0131 01:55:35.226264 114666 solver.cpp:266] Iteration 2150 (2.05857 iter/s, 24.2887s/50 iter), loss = 0.0666668
I0131 01:55:35.226388 114666 solver.cpp:285]     Train net output #0: loss = 0.0666668 (* 1 = 0.0666668 loss)
I0131 01:55:35.228497 114666 sgd_solver.cpp:106] Iteration 2150, lr = 0.001
I0131 01:55:59.445650 114666 solver.cpp:266] Iteration 2200 (2.06473 iter/s, 24.2163s/50 iter), loss = 0.027477
I0131 01:55:59.445680 114666 solver.cpp:285]     Train net output #0: loss = 0.0274771 (* 1 = 0.0274771 loss)
I0131 01:55:59.447888 114666 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0131 01:56:23.821841 114666 solver.cpp:266] Iteration 2250 (2.05145 iter/s, 24.3731s/50 iter), loss = 0.0421005
I0131 01:56:23.821985 114666 solver.cpp:285]     Train net output #0: loss = 0.0421005 (* 1 = 0.0421005 loss)
I0131 01:56:23.821992 114666 sgd_solver.cpp:106] Iteration 2250, lr = 0.001
I0131 01:56:48.172538 114666 solver.cpp:266] Iteration 2300 (2.05342 iter/s, 24.3497s/50 iter), loss = 0.0275647
I0131 01:56:48.172569 114666 solver.cpp:285]     Train net output #0: loss = 0.0275648 (* 1 = 0.0275648 loss)
I0131 01:56:48.174785 114666 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I0131 01:57:12.531664 114666 solver.cpp:266] Iteration 2350 (2.05288 iter/s, 24.356s/50 iter), loss = 0.0401643
I0131 01:57:12.531711 114666 solver.cpp:285]     Train net output #0: loss = 0.0401643 (* 1 = 0.0401643 loss)
I0131 01:57:12.533921 114666 sgd_solver.cpp:106] Iteration 2350, lr = 0.001
I0131 01:57:36.719091 114666 solver.cpp:266] Iteration 2400 (2.06746 iter/s, 24.1843s/50 iter), loss = 0.0592768
I0131 01:57:36.719125 114666 solver.cpp:285]     Train net output #0: loss = 0.0592769 (* 1 = 0.0592769 loss)
I0131 01:57:36.721338 114666 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0131 01:58:01.124421 114666 solver.cpp:266] Iteration 2450 (2.049 iter/s, 24.4022s/50 iter), loss = 0.017316
I0131 01:58:01.124563 114666 solver.cpp:285]     Train net output #0: loss = 0.017316 (* 1 = 0.017316 loss)
I0131 01:58:01.124858 114666 sgd_solver.cpp:106] Iteration 2450, lr = 0.001
I0131 01:58:25.368110 114666 solver.cpp:266] Iteration 2500 (2.06251 iter/s, 24.2424s/50 iter), loss = 0.0413201
I0131 01:58:25.368140 114666 solver.cpp:285]     Train net output #0: loss = 0.0413201 (* 1 = 0.0413201 loss)
I0131 01:58:25.370378 114666 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0131 01:58:49.604346 114666 solver.cpp:266] Iteration 2550 (2.0633 iter/s, 24.2331s/50 iter), loss = 0.0157704
I0131 01:58:49.604473 114666 solver.cpp:285]     Train net output #0: loss = 0.0157705 (* 1 = 0.0157705 loss)
I0131 01:58:49.606585 114666 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I0131 01:59:14.007611 114666 solver.cpp:266] Iteration 2600 (2.04917 iter/s, 24.4001s/50 iter), loss = 0.0201411
I0131 01:59:14.007643 114666 solver.cpp:285]     Train net output #0: loss = 0.0201412 (* 1 = 0.0201412 loss)
I0131 01:59:14.007649 114666 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0131 01:59:38.399443 114666 solver.cpp:266] Iteration 2650 (2.04994 iter/s, 24.3909s/50 iter), loss = 0.00950308
I0131 01:59:38.399498 114666 solver.cpp:285]     Train net output #0: loss = 0.00950312 (* 1 = 0.00950312 loss)
I0131 01:59:38.401680 114666 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I0131 02:00:02.711349 114666 solver.cpp:266] Iteration 2700 (2.05687 iter/s, 24.3088s/50 iter), loss = 0.012323
I0131 02:00:02.711378 114666 solver.cpp:285]     Train net output #0: loss = 0.0123231 (* 1 = 0.0123231 loss)
I0131 02:00:02.713587 114666 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0131 02:00:26.930481 114666 solver.cpp:266] Iteration 2750 (2.06475 iter/s, 24.216s/50 iter), loss = 0.013394
I0131 02:00:26.930624 114666 solver.cpp:285]     Train net output #0: loss = 0.013394 (* 1 = 0.013394 loss)
I0131 02:00:26.932729 114666 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I0131 02:00:51.313688 114666 solver.cpp:266] Iteration 2800 (2.05086 iter/s, 24.3801s/50 iter), loss = 0.00731135
I0131 02:00:51.313721 114666 solver.cpp:285]     Train net output #0: loss = 0.00731139 (* 1 = 0.00731139 loss)
I0131 02:00:51.313729 114666 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0131 02:01:15.736099 114666 solver.cpp:266] Iteration 2850 (2.04738 iter/s, 24.4215s/50 iter), loss = 0.027937
I0131 02:01:15.736150 114666 solver.cpp:285]     Train net output #0: loss = 0.0279371 (* 1 = 0.0279371 loss)
I0131 02:01:15.738350 114666 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I0131 02:01:40.062459 114666 solver.cpp:266] Iteration 2900 (2.05565 iter/s, 24.3232s/50 iter), loss = 0.00978929
I0131 02:01:40.062487 114666 solver.cpp:285]     Train net output #0: loss = 0.00978933 (* 1 = 0.00978933 loss)
I0131 02:01:40.064710 114666 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0131 02:02:04.339370 114666 solver.cpp:266] Iteration 2950 (2.05984 iter/s, 24.2738s/50 iter), loss = 0.0187076
I0131 02:02:04.339483 114666 solver.cpp:285]     Train net output #0: loss = 0.0187076 (* 1 = 0.0187076 loss)
I0131 02:02:04.341617 114666 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I0131 02:02:28.236878 114666 solver.cpp:418] Iteration 3000, Testing net (#0)
I0131 02:02:31.737998 114666 solver.cpp:517]     Test net output #0: loss = 0.142955 (* 1 = 0.142955 loss)
I0131 02:02:31.738015 114666 solver.cpp:517]     Test net output #1: top-1 = 0.95125
I0131 02:02:32.296787 114666 solver.cpp:266] Iteration 3000 (1.78864 iter/s, 27.9541s/50 iter), loss = 0.00863573
I0131 02:02:32.296810 114666 solver.cpp:285]     Train net output #0: loss = 0.00863578 (* 1 = 0.00863578 loss)
I0131 02:02:32.296869 114666 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0131 02:02:56.477725 114666 solver.cpp:266] Iteration 3050 (2.06783 iter/s, 24.18s/50 iter), loss = 0.00684374
I0131 02:02:56.477880 114666 solver.cpp:285]     Train net output #0: loss = 0.00684379 (* 1 = 0.00684379 loss)
I0131 02:02:56.479975 114666 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I0131 02:03:20.861002 114666 solver.cpp:266] Iteration 3100 (2.05085 iter/s, 24.3801s/50 iter), loss = 0.0114962
I0131 02:03:20.861033 114666 solver.cpp:285]     Train net output #0: loss = 0.0114962 (* 1 = 0.0114962 loss)
I0131 02:03:20.863250 114666 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0131 02:03:45.296013 114666 solver.cpp:266] Iteration 3150 (2.04651 iter/s, 24.4319s/50 iter), loss = 0.0110504
I0131 02:03:45.296093 114666 solver.cpp:285]     Train net output #0: loss = 0.0110505 (* 1 = 0.0110505 loss)
I0131 02:03:45.296134 114666 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I0131 02:04:09.574199 114666 solver.cpp:266] Iteration 3200 (2.05955 iter/s, 24.2772s/50 iter), loss = 0.0182479
I0131 02:04:09.574229 114666 solver.cpp:285]     Train net output #0: loss = 0.018248 (* 1 = 0.018248 loss)
I0131 02:04:09.574278 114666 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0131 02:04:33.822883 114666 solver.cpp:266] Iteration 3250 (2.06205 iter/s, 24.2477s/50 iter), loss = 0.0127873
I0131 02:04:33.822990 114666 solver.cpp:285]     Train net output #0: loss = 0.0127873 (* 1 = 0.0127873 loss)
I0131 02:04:33.825137 114666 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I0131 02:04:58.024724 114666 solver.cpp:266] Iteration 3300 (2.06623 iter/s, 24.1987s/50 iter), loss = 0.0115774
I0131 02:04:58.024755 114666 solver.cpp:285]     Train net output #0: loss = 0.0115775 (* 1 = 0.0115775 loss)
I0131 02:04:58.026974 114666 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0131 02:05:22.458308 114666 solver.cpp:266] Iteration 3350 (2.04663 iter/s, 24.4304s/50 iter), loss = 0.00390154
I0131 02:05:22.458411 114666 solver.cpp:285]     Train net output #0: loss = 0.00390159 (* 1 = 0.00390159 loss)
I0131 02:05:22.458420 114666 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I0131 02:05:46.830739 114666 solver.cpp:266] Iteration 3400 (2.05158 iter/s, 24.3714s/50 iter), loss = 0.00998241
I0131 02:05:46.830771 114666 solver.cpp:285]     Train net output #0: loss = 0.00998246 (* 1 = 0.00998246 loss)
I0131 02:05:46.832989 114666 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0131 02:06:11.193766 114666 solver.cpp:266] Iteration 3450 (2.05256 iter/s, 24.3599s/50 iter), loss = 0.00290121
I0131 02:06:11.193817 114666 solver.cpp:285]     Train net output #0: loss = 0.00290126 (* 1 = 0.00290126 loss)
I0131 02:06:11.196018 114666 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I0131 02:06:35.333972 114666 solver.cpp:266] Iteration 3500 (2.0715 iter/s, 24.1371s/50 iter), loss = 0.00333522
I0131 02:06:35.334003 114666 solver.cpp:285]     Train net output #0: loss = 0.00333527 (* 1 = 0.00333527 loss)
I0131 02:06:35.336200 114666 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0131 02:06:59.717609 114666 solver.cpp:266] Iteration 3550 (2.05082 iter/s, 24.3805s/50 iter), loss = 0.00769114
I0131 02:06:59.717679 114666 solver.cpp:285]     Train net output #0: loss = 0.0076912 (* 1 = 0.0076912 loss)
I0131 02:06:59.717686 114666 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I0131 02:07:24.248739 114666 solver.cpp:266] Iteration 3600 (2.03831 iter/s, 24.5302s/50 iter), loss = 0.00509351
I0131 02:07:24.248786 114666 solver.cpp:285]     Train net output #0: loss = 0.00509356 (* 1 = 0.00509356 loss)
I0131 02:07:24.249588 114666 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0131 02:07:48.582772 114666 solver.cpp:266] Iteration 3650 (2.05488 iter/s, 24.3323s/50 iter), loss = 0.00750591
I0131 02:07:48.582844 114666 solver.cpp:285]     Train net output #0: loss = 0.00750596 (* 1 = 0.00750596 loss)
I0131 02:07:48.585028 114666 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I0131 02:08:12.816465 114666 solver.cpp:266] Iteration 3700 (2.06351 iter/s, 24.2305s/50 iter), loss = 0.0134376
I0131 02:08:12.816500 114666 solver.cpp:285]     Train net output #0: loss = 0.0134376 (* 1 = 0.0134376 loss)
I0131 02:08:12.818717 114666 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0131 02:08:37.089905 114666 solver.cpp:266] Iteration 3750 (2.06013 iter/s, 24.2703s/50 iter), loss = 0.00986602
I0131 02:08:37.090065 114666 solver.cpp:285]     Train net output #0: loss = 0.00986606 (* 1 = 0.00986606 loss)
I0131 02:08:37.092152 114666 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I0131 02:09:01.463505 114666 solver.cpp:266] Iteration 3800 (2.05166 iter/s, 24.3705s/50 iter), loss = 0.00285601
I0131 02:09:01.463538 114666 solver.cpp:285]     Train net output #0: loss = 0.00285606 (* 1 = 0.00285606 loss)
I0131 02:09:01.463560 114666 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0131 02:09:25.691526 114666 solver.cpp:266] Iteration 3850 (2.06381 iter/s, 24.2271s/50 iter), loss = 0.00723495
I0131 02:09:25.691670 114666 solver.cpp:285]     Train net output #0: loss = 0.00723499 (* 1 = 0.00723499 loss)
I0131 02:09:25.693778 114666 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I0131 02:09:49.942821 114666 solver.cpp:266] Iteration 3900 (2.06201 iter/s, 24.2481s/50 iter), loss = 0.00237134
I0131 02:09:49.942849 114666 solver.cpp:285]     Train net output #0: loss = 0.00237138 (* 1 = 0.00237138 loss)
I0131 02:09:49.945071 114666 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0131 02:10:14.073328 114666 solver.cpp:266] Iteration 3950 (2.07234 iter/s, 24.1274s/50 iter), loss = 0.00434467
I0131 02:10:14.073431 114666 solver.cpp:285]     Train net output #0: loss = 0.00434471 (* 1 = 0.00434471 loss)
I0131 02:10:14.075570 114666 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I0131 02:10:37.958657 114666 solver.cpp:418] Iteration 4000, Testing net (#0)
I0131 02:10:41.353206 114666 solver.cpp:517]     Test net output #0: loss = 0.154121 (* 1 = 0.154121 loss)
I0131 02:10:41.353224 114666 solver.cpp:517]     Test net output #1: top-1 = 0.94975
I0131 02:10:41.906119 114666 solver.cpp:266] Iteration 4000 (1.79665 iter/s, 27.8295s/50 iter), loss = 0.00338418
I0131 02:10:41.906143 114666 solver.cpp:285]     Train net output #0: loss = 0.00338423 (* 1 = 0.00338423 loss)
I0131 02:10:41.908381 114666 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0131 02:11:06.129447 114666 solver.cpp:266] Iteration 4050 (2.06439 iter/s, 24.2202s/50 iter), loss = 0.00325812
I0131 02:11:06.129549 114666 solver.cpp:285]     Train net output #0: loss = 0.00325817 (* 1 = 0.00325817 loss)
I0131 02:11:06.129556 114666 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I0131 02:11:30.440966 114666 solver.cpp:266] Iteration 4100 (2.05672 iter/s, 24.3105s/50 iter), loss = 0.0116466
I0131 02:11:30.440999 114666 solver.cpp:285]     Train net output #0: loss = 0.0116467 (* 1 = 0.0116467 loss)
I0131 02:11:30.443222 114666 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0131 02:11:54.812469 114666 solver.cpp:266] Iteration 4150 (2.05184 iter/s, 24.3683s/50 iter), loss = 0.00430184
I0131 02:11:54.812568 114666 solver.cpp:285]     Train net output #0: loss = 0.00430189 (* 1 = 0.00430189 loss)
I0131 02:11:54.814781 114666 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I0131 02:12:18.926206 114666 solver.cpp:266] Iteration 4200 (2.07378 iter/s, 24.1105s/50 iter), loss = 0.00168814
I0131 02:12:18.926239 114666 solver.cpp:285]     Train net output #0: loss = 0.00168818 (* 1 = 0.00168818 loss)
I0131 02:12:18.928454 114666 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0131 02:12:43.221272 114666 solver.cpp:266] Iteration 4250 (2.0583 iter/s, 24.2919s/50 iter), loss = 0.010539
I0131 02:12:43.221398 114666 solver.cpp:285]     Train net output #0: loss = 0.0105391 (* 1 = 0.0105391 loss)
I0131 02:12:43.221406 114666 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I0131 02:13:07.531919 114666 solver.cpp:266] Iteration 4300 (2.0568 iter/s, 24.3096s/50 iter), loss = 0.000430742
I0131 02:13:07.531950 114666 solver.cpp:285]     Train net output #0: loss = 0.000430793 (* 1 = 0.000430793 loss)
I0131 02:13:07.534180 114666 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0131 02:13:31.801146 114666 solver.cpp:266] Iteration 4350 (2.06049 iter/s, 24.2661s/50 iter), loss = 0.0087911
I0131 02:13:31.801265 114666 solver.cpp:285]     Train net output #0: loss = 0.00879115 (* 1 = 0.00879115 loss)
I0131 02:13:31.803392 114666 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I0131 02:13:55.949368 114666 solver.cpp:266] Iteration 4400 (2.07082 iter/s, 24.1451s/50 iter), loss = 0.0352061
I0131 02:13:55.949400 114666 solver.cpp:285]     Train net output #0: loss = 0.0352061 (* 1 = 0.0352061 loss)
I0131 02:13:55.951619 114666 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0131 02:14:20.395328 114666 solver.cpp:266] Iteration 4450 (2.04559 iter/s, 24.4428s/50 iter), loss = 0.0133363
I0131 02:14:20.395480 114666 solver.cpp:285]     Train net output #0: loss = 0.0133364 (* 1 = 0.0133364 loss)
I0131 02:14:20.397547 114666 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I0131 02:14:44.576247 114666 solver.cpp:266] Iteration 4500 (2.06801 iter/s, 24.1778s/50 iter), loss = 0.00121711
I0131 02:14:44.576275 114666 solver.cpp:285]     Train net output #0: loss = 0.00121717 (* 1 = 0.00121717 loss)
I0131 02:14:44.576324 114666 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0131 02:15:08.720360 114666 solver.cpp:266] Iteration 4550 (2.07098 iter/s, 24.1431s/50 iter), loss = 0.00499611
I0131 02:15:08.720475 114666 solver.cpp:285]     Train net output #0: loss = 0.00499617 (* 1 = 0.00499617 loss)
I0131 02:15:08.722605 114666 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I0131 02:15:32.894672 114666 solver.cpp:266] Iteration 4600 (2.06858 iter/s, 24.1712s/50 iter), loss = 0.000852339
I0131 02:15:32.894704 114666 solver.cpp:285]     Train net output #0: loss = 0.000852398 (* 1 = 0.000852398 loss)
I0131 02:15:32.894726 114666 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0131 02:15:57.349565 114666 solver.cpp:266] Iteration 4650 (2.04466 iter/s, 24.454s/50 iter), loss = 0.00690235
I0131 02:15:57.349619 114666 solver.cpp:285]     Train net output #0: loss = 0.00690241 (* 1 = 0.00690241 loss)
I0131 02:15:57.351816 114666 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I0131 02:16:21.566967 114666 solver.cpp:266] Iteration 4700 (2.0649 iter/s, 24.2143s/50 iter), loss = 0.00367095
I0131 02:16:21.566996 114666 solver.cpp:285]     Train net output #0: loss = 0.00367101 (* 1 = 0.00367101 loss)
I0131 02:16:21.569214 114666 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0131 02:16:45.770889 114666 solver.cpp:266] Iteration 4750 (2.06605 iter/s, 24.2008s/50 iter), loss = 0.00199304
I0131 02:16:45.771005 114666 solver.cpp:285]     Train net output #0: loss = 0.00199311 (* 1 = 0.00199311 loss)
I0131 02:16:45.773141 114666 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I0131 02:17:09.923812 114666 solver.cpp:266] Iteration 4800 (2.07041 iter/s, 24.1498s/50 iter), loss = 0.00879868
I0131 02:17:09.923846 114666 solver.cpp:285]     Train net output #0: loss = 0.00879875 (* 1 = 0.00879875 loss)
I0131 02:17:09.926055 114666 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0131 02:17:34.317296 114666 solver.cpp:266] Iteration 4850 (2.04999 iter/s, 24.3903s/50 iter), loss = 0.00139619
I0131 02:17:34.317410 114666 solver.cpp:285]     Train net output #0: loss = 0.00139625 (* 1 = 0.00139625 loss)
I0131 02:17:34.317736 114666 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I0131 02:17:58.446691 114666 solver.cpp:266] Iteration 4900 (2.07228 iter/s, 24.1281s/50 iter), loss = 0.000839386
I0131 02:17:58.446717 114666 solver.cpp:285]     Train net output #0: loss = 0.000839449 (* 1 = 0.000839449 loss)
I0131 02:17:58.448946 114666 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0131 02:18:22.548904 114666 solver.cpp:266] Iteration 4950 (2.07477 iter/s, 24.0991s/50 iter), loss = 0.0100717
I0131 02:18:22.548969 114666 solver.cpp:285]     Train net output #0: loss = 0.0100717 (* 1 = 0.0100717 loss)
I0131 02:18:22.551149 114666 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I0131 02:18:46.382776 114666 solver.cpp:418] Iteration 5000, Testing net (#0)
I0131 02:18:49.839294 114666 solver.cpp:517]     Test net output #0: loss = 0.187299 (* 1 = 0.187299 loss)
I0131 02:18:49.839311 114666 solver.cpp:517]     Test net output #1: top-1 = 0.9505
I0131 02:18:50.384261 114666 solver.cpp:266] Iteration 5000 (1.79649 iter/s, 27.8321s/50 iter), loss = 0.00155952
I0131 02:18:50.384290 114666 solver.cpp:285]     Train net output #0: loss = 0.00155958 (* 1 = 0.00155958 loss)
I0131 02:18:50.386509 114666 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0131 02:19:14.482656 114666 solver.cpp:266] Iteration 5050 (2.0751 iter/s, 24.0953s/50 iter), loss = 0.00208337
I0131 02:19:14.482789 114666 solver.cpp:285]     Train net output #0: loss = 0.00208344 (* 1 = 0.00208344 loss)
I0131 02:19:14.484910 114666 sgd_solver.cpp:106] Iteration 5050, lr = 1e-05
I0131 02:19:38.766412 114666 solver.cpp:266] Iteration 5100 (2.05926 iter/s, 24.2806s/50 iter), loss = 0.000802653
I0131 02:19:38.766443 114666 solver.cpp:285]     Train net output #0: loss = 0.000802716 (* 1 = 0.000802716 loss)
I0131 02:19:38.766486 114666 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0131 02:20:03.096596 114666 solver.cpp:266] Iteration 5150 (2.05514 iter/s, 24.3292s/50 iter), loss = 0.00219057
I0131 02:20:03.096657 114666 solver.cpp:285]     Train net output #0: loss = 0.00219062 (* 1 = 0.00219062 loss)
I0131 02:20:03.098839 114666 sgd_solver.cpp:106] Iteration 5150, lr = 1e-05
I0131 02:20:27.335247 114666 solver.cpp:266] Iteration 5200 (2.06309 iter/s, 24.2355s/50 iter), loss = 0.0012375
I0131 02:20:27.335273 114666 solver.cpp:285]     Train net output #0: loss = 0.00123755 (* 1 = 0.00123755 loss)
I0131 02:20:27.337494 114666 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0131 02:20:51.631942 114666 solver.cpp:266] Iteration 5250 (2.05816 iter/s, 24.2936s/50 iter), loss = 0.00357047
I0131 02:20:51.632051 114666 solver.cpp:285]     Train net output #0: loss = 0.00357053 (* 1 = 0.00357053 loss)
I0131 02:20:51.634187 114666 sgd_solver.cpp:106] Iteration 5250, lr = 1e-05
I0131 02:21:15.774960 114666 solver.cpp:266] Iteration 5300 (2.07126 iter/s, 24.1399s/50 iter), loss = 0.00501535
I0131 02:21:15.774991 114666 solver.cpp:285]     Train net output #0: loss = 0.00501541 (* 1 = 0.00501541 loss)
I0131 02:21:15.774997 114666 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0131 02:21:40.191330 114666 solver.cpp:266] Iteration 5350 (2.04788 iter/s, 24.4154s/50 iter), loss = 0.00254198
I0131 02:21:40.191457 114666 solver.cpp:285]     Train net output #0: loss = 0.00254204 (* 1 = 0.00254204 loss)
I0131 02:21:40.191790 114666 sgd_solver.cpp:106] Iteration 5350, lr = 1e-05
I0131 02:22:04.446960 114666 solver.cpp:266] Iteration 5400 (2.06149 iter/s, 24.2543s/50 iter), loss = 0.00079293
I0131 02:22:04.446992 114666 solver.cpp:285]     Train net output #0: loss = 0.000792991 (* 1 = 0.000792991 loss)
I0131 02:22:04.449211 114666 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0131 02:22:28.695616 114666 solver.cpp:266] Iteration 5450 (2.06224 iter/s, 24.2455s/50 iter), loss = 0.00470339
I0131 02:22:28.695680 114666 solver.cpp:285]     Train net output #0: loss = 0.00470345 (* 1 = 0.00470345 loss)
I0131 02:22:28.697862 114666 sgd_solver.cpp:106] Iteration 5450, lr = 1e-05
I0131 02:22:52.917536 114666 solver.cpp:266] Iteration 5500 (2.06451 iter/s, 24.2188s/50 iter), loss = 0.000750526
I0131 02:22:52.917565 114666 solver.cpp:285]     Train net output #0: loss = 0.000750588 (* 1 = 0.000750588 loss)
I0131 02:22:52.919780 114666 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0131 02:23:17.263368 114666 solver.cpp:266] Iteration 5550 (2.054 iter/s, 24.3427s/50 iter), loss = 0.00094298
I0131 02:23:17.263516 114666 solver.cpp:285]     Train net output #0: loss = 0.000943042 (* 1 = 0.000943042 loss)
I0131 02:23:17.263523 114666 sgd_solver.cpp:106] Iteration 5550, lr = 1e-05
I0131 02:23:41.524406 114666 solver.cpp:266] Iteration 5600 (2.06101 iter/s, 24.26s/50 iter), loss = 0.00107729
I0131 02:23:41.524435 114666 solver.cpp:285]     Train net output #0: loss = 0.00107735 (* 1 = 0.00107735 loss)
I0131 02:23:41.526662 114666 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0131 02:24:05.735891 114666 solver.cpp:266] Iteration 5650 (2.0654 iter/s, 24.2083s/50 iter), loss = 0.00128113
I0131 02:24:05.736009 114666 solver.cpp:285]     Train net output #0: loss = 0.0012812 (* 1 = 0.0012812 loss)
I0131 02:24:05.738140 114666 sgd_solver.cpp:106] Iteration 5650, lr = 1e-05
I0131 02:24:29.931066 114666 solver.cpp:266] Iteration 5700 (2.0668 iter/s, 24.192s/50 iter), loss = 0.00697747
I0131 02:24:29.931094 114666 solver.cpp:285]     Train net output #0: loss = 0.00697753 (* 1 = 0.00697753 loss)
I0131 02:24:29.933322 114666 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0131 02:24:54.193187 114666 solver.cpp:266] Iteration 5750 (2.06109 iter/s, 24.259s/50 iter), loss = 0.00277543
I0131 02:24:54.193341 114666 solver.cpp:285]     Train net output #0: loss = 0.0027755 (* 1 = 0.0027755 loss)
I0131 02:24:54.193348 114666 sgd_solver.cpp:106] Iteration 5750, lr = 1e-05
I0131 02:25:18.360110 114666 solver.cpp:266] Iteration 5800 (2.06903 iter/s, 24.1659s/50 iter), loss = 0.00110006
I0131 02:25:18.360141 114666 solver.cpp:285]     Train net output #0: loss = 0.00110013 (* 1 = 0.00110013 loss)
I0131 02:25:18.360189 114666 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0131 02:25:42.489141 114666 solver.cpp:266] Iteration 5850 (2.07228 iter/s, 24.1281s/50 iter), loss = 0.000511512
I0131 02:25:42.489261 114666 solver.cpp:285]     Train net output #0: loss = 0.000511582 (* 1 = 0.000511582 loss)
I0131 02:25:42.491390 114666 sgd_solver.cpp:106] Iteration 5850, lr = 1e-05
I0131 02:26:06.706451 114666 solver.cpp:266] Iteration 5900 (2.06491 iter/s, 24.2142s/50 iter), loss = 0.00350362
I0131 02:26:06.706483 114666 solver.cpp:285]     Train net output #0: loss = 0.00350369 (* 1 = 0.00350369 loss)
I0131 02:26:06.706490 114666 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0131 02:26:30.982972 114666 solver.cpp:266] Iteration 5950 (2.05968 iter/s, 24.2756s/50 iter), loss = 0.010599
I0131 02:26:30.983036 114666 solver.cpp:285]     Train net output #0: loss = 0.0105991 (* 1 = 0.0105991 loss)
I0131 02:26:30.985211 114666 sgd_solver.cpp:106] Iteration 5950, lr = 1e-05
I0131 02:26:54.692756 114666 solver.cpp:418] Iteration 6000, Testing net (#0)
I0131 02:26:58.370115 114666 solver.cpp:517]     Test net output #0: loss = 0.188639 (* 1 = 0.188639 loss)
I0131 02:26:58.370129 114666 solver.cpp:517]     Test net output #1: top-1 = 0.9555
I0131 02:26:58.731911 114666 solver.cpp:266] Iteration 6000 (1.80208 iter/s, 27.7457s/50 iter), loss = 0.00171252
I0131 02:26:58.731941 114666 solver.cpp:285]     Train net output #0: loss = 0.00171259 (* 1 = 0.00171259 loss)
I0131 02:26:58.731989 114666 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0131 02:27:22.966368 114666 solver.cpp:266] Iteration 6050 (2.06326 iter/s, 24.2335s/50 iter), loss = 0.00351641
I0131 02:27:22.966490 114666 solver.cpp:285]     Train net output #0: loss = 0.00351648 (* 1 = 0.00351648 loss)
I0131 02:27:22.968621 114666 sgd_solver.cpp:106] Iteration 6050, lr = 1e-05
I0131 02:27:47.109470 114666 solver.cpp:266] Iteration 6100 (2.07125 iter/s, 24.14s/50 iter), loss = 0.000332918
I0131 02:27:47.109501 114666 solver.cpp:285]     Train net output #0: loss = 0.000332987 (* 1 = 0.000332987 loss)
I0131 02:27:47.111726 114666 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0131 02:28:11.198868 114666 solver.cpp:266] Iteration 6150 (2.07587 iter/s, 24.0863s/50 iter), loss = 0.00570629
I0131 02:28:11.199002 114666 solver.cpp:285]     Train net output #0: loss = 0.00570635 (* 1 = 0.00570635 loss)
I0131 02:28:11.199041 114666 sgd_solver.cpp:106] Iteration 6150, lr = 1e-05
I0131 02:28:35.617522 114666 solver.cpp:266] Iteration 6200 (2.0477 iter/s, 24.4176s/50 iter), loss = 0.000725332
I0131 02:28:35.617555 114666 solver.cpp:285]     Train net output #0: loss = 0.000725399 (* 1 = 0.000725399 loss)
I0131 02:28:35.619772 114666 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0131 02:28:59.851130 114666 solver.cpp:266] Iteration 6250 (2.06352 iter/s, 24.2305s/50 iter), loss = 0.00392936
I0131 02:28:59.851235 114666 solver.cpp:285]     Train net output #0: loss = 0.00392942 (* 1 = 0.00392942 loss)
I0131 02:28:59.853384 114666 sgd_solver.cpp:106] Iteration 6250, lr = 1e-05
I0131 02:29:23.892462 114666 solver.cpp:266] Iteration 6300 (2.08002 iter/s, 24.0382s/50 iter), loss = 0.00201094
I0131 02:29:23.892499 114666 solver.cpp:285]     Train net output #0: loss = 0.00201101 (* 1 = 0.00201101 loss)
I0131 02:29:23.894714 114666 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0131 02:29:48.170464 114666 solver.cpp:266] Iteration 6350 (2.05974 iter/s, 24.2749s/50 iter), loss = 0.00235127
I0131 02:29:48.170629 114666 solver.cpp:285]     Train net output #0: loss = 0.00235133 (* 1 = 0.00235133 loss)
I0131 02:29:48.170667 114666 sgd_solver.cpp:106] Iteration 6350, lr = 1e-05
I0131 02:30:12.478116 114666 solver.cpp:266] Iteration 6400 (2.05706 iter/s, 24.3066s/50 iter), loss = 0.00699655
I0131 02:30:12.478145 114666 solver.cpp:285]     Train net output #0: loss = 0.00699662 (* 1 = 0.00699662 loss)
I0131 02:30:12.480360 114666 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0131 02:30:36.636641 114666 solver.cpp:266] Iteration 6450 (2.06993 iter/s, 24.1554s/50 iter), loss = 0.000922608
I0131 02:30:36.636780 114666 solver.cpp:285]     Train net output #0: loss = 0.000922676 (* 1 = 0.000922676 loss)
I0131 02:30:36.638896 114666 sgd_solver.cpp:106] Iteration 6450, lr = 1e-05
I0131 02:31:00.833537 114666 solver.cpp:266] Iteration 6500 (2.06665 iter/s, 24.1937s/50 iter), loss = 0.000855872
I0131 02:31:00.833568 114666 solver.cpp:285]     Train net output #0: loss = 0.000855939 (* 1 = 0.000855939 loss)
I0131 02:31:00.835786 114666 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0131 02:31:25.212641 114666 solver.cpp:266] Iteration 6550 (2.0512 iter/s, 24.376s/50 iter), loss = 0.000812853
I0131 02:31:25.212746 114666 solver.cpp:285]     Train net output #0: loss = 0.00081292 (* 1 = 0.00081292 loss)
I0131 02:31:25.213070 114666 sgd_solver.cpp:106] Iteration 6550, lr = 1e-05
I0131 02:31:49.412328 114666 solver.cpp:266] Iteration 6600 (2.06626 iter/s, 24.1984s/50 iter), loss = 0.00326857
I0131 02:31:49.412355 114666 solver.cpp:285]     Train net output #0: loss = 0.00326863 (* 1 = 0.00326863 loss)
I0131 02:31:49.412403 114666 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0131 02:32:13.592420 114666 solver.cpp:266] Iteration 6650 (2.0679 iter/s, 24.1791s/50 iter), loss = 0.000521699
I0131 02:32:13.592548 114666 solver.cpp:285]     Train net output #0: loss = 0.000521766 (* 1 = 0.000521766 loss)
I0131 02:32:13.594678 114666 sgd_solver.cpp:106] Iteration 6650, lr = 1e-05
I0131 02:32:37.693024 114666 solver.cpp:266] Iteration 6700 (2.07491 iter/s, 24.0975s/50 iter), loss = 0.00141724
I0131 02:32:37.693058 114666 solver.cpp:285]     Train net output #0: loss = 0.00141731 (* 1 = 0.00141731 loss)
I0131 02:32:37.693064 114666 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0131 02:33:01.947114 114666 solver.cpp:266] Iteration 6750 (2.06159 iter/s, 24.2532s/50 iter), loss = 0.000716387
I0131 02:33:01.947196 114666 solver.cpp:285]     Train net output #0: loss = 0.000716455 (* 1 = 0.000716455 loss)
I0131 02:33:01.949368 114666 sgd_solver.cpp:106] Iteration 6750, lr = 1e-05
I0131 02:33:26.208629 114666 solver.cpp:266] Iteration 6800 (2.06114 iter/s, 24.2584s/50 iter), loss = 0.000448614
I0131 02:33:26.208658 114666 solver.cpp:285]     Train net output #0: loss = 0.000448683 (* 1 = 0.000448683 loss)
I0131 02:33:26.210880 114666 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0131 02:33:50.328497 114666 solver.cpp:266] Iteration 6850 (2.07325 iter/s, 24.1167s/50 iter), loss = 0.0207981
I0131 02:33:50.328629 114666 solver.cpp:285]     Train net output #0: loss = 0.0207982 (* 1 = 0.0207982 loss)
I0131 02:33:50.330677 114666 sgd_solver.cpp:106] Iteration 6850, lr = 1e-05
I0131 02:34:14.693858 114666 solver.cpp:266] Iteration 6900 (2.05235 iter/s, 24.3623s/50 iter), loss = 0.00155388
I0131 02:34:14.693893 114666 solver.cpp:285]     Train net output #0: loss = 0.00155395 (* 1 = 0.00155395 loss)
I0131 02:34:14.696120 114666 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0131 02:34:38.890270 114666 solver.cpp:266] Iteration 6950 (2.06669 iter/s, 24.1933s/50 iter), loss = 0.0162401
I0131 02:34:38.890388 114666 solver.cpp:285]     Train net output #0: loss = 0.0162402 (* 1 = 0.0162402 loss)
I0131 02:34:38.892483 114666 sgd_solver.cpp:106] Iteration 6950, lr = 1e-05
I0131 02:35:02.558972 114666 solver.cpp:418] Iteration 7000, Testing net (#0)
I0131 02:35:06.283968 114666 solver.cpp:517]     Test net output #0: loss = 0.207364 (* 1 = 0.207364 loss)
I0131 02:35:06.283987 114666 solver.cpp:517]     Test net output #1: top-1 = 0.95525
I0131 02:35:06.636135 114666 solver.cpp:266] Iteration 7000 (1.80228 iter/s, 27.7426s/50 iter), loss = 0.00138963
I0131 02:35:06.636169 114666 solver.cpp:285]     Train net output #0: loss = 0.0013897 (* 1 = 0.0013897 loss)
I0131 02:35:06.636176 114666 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0131 02:35:30.989887 114666 solver.cpp:266] Iteration 7050 (2.05315 iter/s, 24.3528s/50 iter), loss = 0.00255933
I0131 02:35:30.990042 114666 solver.cpp:285]     Train net output #0: loss = 0.0025594 (* 1 = 0.0025594 loss)
I0131 02:35:30.990265 114666 sgd_solver.cpp:106] Iteration 7050, lr = 1e-05
I0131 02:35:55.177378 114666 solver.cpp:266] Iteration 7100 (2.06729 iter/s, 24.1862s/50 iter), loss = 0.00197262
I0131 02:35:55.177408 114666 solver.cpp:285]     Train net output #0: loss = 0.00197268 (* 1 = 0.00197268 loss)
I0131 02:35:55.177458 114666 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0131 02:36:19.329381 114666 solver.cpp:266] Iteration 7150 (2.0703 iter/s, 24.151s/50 iter), loss = 0.0269352
I0131 02:36:19.329512 114666 solver.cpp:285]     Train net output #0: loss = 0.0269352 (* 1 = 0.0269352 loss)
I0131 02:36:19.331650 114666 sgd_solver.cpp:106] Iteration 7150, lr = 1e-05
I0131 02:36:43.606297 114666 solver.cpp:266] Iteration 7200 (2.05984 iter/s, 24.2738s/50 iter), loss = 0.00582299
I0131 02:36:43.606325 114666 solver.cpp:285]     Train net output #0: loss = 0.00582306 (* 1 = 0.00582306 loss)
I0131 02:36:43.608520 114666 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0131 02:37:07.966387 114666 solver.cpp:266] Iteration 7250 (2.0528 iter/s, 24.357s/50 iter), loss = 0.00314593
I0131 02:37:07.966462 114666 solver.cpp:285]     Train net output #0: loss = 0.003146 (* 1 = 0.003146 loss)
I0131 02:37:07.966487 114666 sgd_solver.cpp:106] Iteration 7250, lr = 1e-05
I0131 02:37:32.167701 114666 solver.cpp:266] Iteration 7300 (2.06609 iter/s, 24.2003s/50 iter), loss = 0.000952773
I0131 02:37:32.167735 114666 solver.cpp:285]     Train net output #0: loss = 0.000952847 (* 1 = 0.000952847 loss)
I0131 02:37:32.169946 114666 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0131 02:37:56.510246 114666 solver.cpp:266] Iteration 7350 (2.05428 iter/s, 24.3394s/50 iter), loss = 0.000231355
I0131 02:37:56.510360 114666 solver.cpp:285]     Train net output #0: loss = 0.000231428 (* 1 = 0.000231428 loss)
I0131 02:37:56.512507 114666 sgd_solver.cpp:106] Iteration 7350, lr = 1e-05
I0131 02:38:20.675583 114666 solver.cpp:266] Iteration 7400 (2.06935 iter/s, 24.1622s/50 iter), loss = 0.0100511
I0131 02:38:20.675616 114666 solver.cpp:285]     Train net output #0: loss = 0.0100512 (* 1 = 0.0100512 loss)
I0131 02:38:20.677827 114666 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0131 02:38:45.026474 114666 solver.cpp:266] Iteration 7450 (2.05358 iter/s, 24.3477s/50 iter), loss = 0.00317531
I0131 02:38:45.026612 114666 solver.cpp:285]     Train net output #0: loss = 0.00317538 (* 1 = 0.00317538 loss)
I0131 02:38:45.027001 114666 sgd_solver.cpp:106] Iteration 7450, lr = 1e-05
I0131 02:39:09.339751 114666 solver.cpp:266] Iteration 7500 (2.05661 iter/s, 24.3119s/50 iter), loss = 0.00181671
I0131 02:39:09.339785 114666 solver.cpp:285]     Train net output #0: loss = 0.00181678 (* 1 = 0.00181678 loss)
I0131 02:39:09.342010 114666 sgd_solver.cpp:106] Iteration 7500, lr = 1e-06
I0131 02:39:33.467754 114666 solver.cpp:266] Iteration 7550 (2.07255 iter/s, 24.1249s/50 iter), loss = 0.00140124
I0131 02:39:33.467857 114666 solver.cpp:285]     Train net output #0: loss = 0.00140131 (* 1 = 0.00140131 loss)
I0131 02:39:33.470008 114666 sgd_solver.cpp:106] Iteration 7550, lr = 1e-06
I0131 02:39:57.637142 114666 solver.cpp:266] Iteration 7600 (2.069 iter/s, 24.1662s/50 iter), loss = 0.0032941
I0131 02:39:57.637169 114666 solver.cpp:285]     Train net output #0: loss = 0.00329417 (* 1 = 0.00329417 loss)
I0131 02:39:57.639390 114666 sgd_solver.cpp:106] Iteration 7600, lr = 1e-06
I0131 02:40:21.920677 114666 solver.cpp:266] Iteration 7650 (2.05927 iter/s, 24.2804s/50 iter), loss = 0.000414558
I0131 02:40:21.920828 114666 solver.cpp:285]     Train net output #0: loss = 0.000414631 (* 1 = 0.000414631 loss)
I0131 02:40:21.922924 114666 sgd_solver.cpp:106] Iteration 7650, lr = 1e-06
I0131 02:40:46.079231 114666 solver.cpp:266] Iteration 7700 (2.06993 iter/s, 24.1554s/50 iter), loss = 0.00115229
I0131 02:40:46.079260 114666 solver.cpp:285]     Train net output #0: loss = 0.00115237 (* 1 = 0.00115237 loss)
I0131 02:40:46.079320 114666 sgd_solver.cpp:106] Iteration 7700, lr = 1e-06
I0131 02:41:10.129513 114666 solver.cpp:266] Iteration 7750 (2.07906 iter/s, 24.0493s/50 iter), loss = 0.000281204
I0131 02:41:10.129642 114666 solver.cpp:285]     Train net output #0: loss = 0.000281279 (* 1 = 0.000281279 loss)
I0131 02:41:10.131752 114666 sgd_solver.cpp:106] Iteration 7750, lr = 1e-06
I0131 02:41:34.454629 114666 solver.cpp:266] Iteration 7800 (2.05575 iter/s, 24.322s/50 iter), loss = 0.000603186
I0131 02:41:34.454663 114666 solver.cpp:285]     Train net output #0: loss = 0.000603262 (* 1 = 0.000603262 loss)
I0131 02:41:34.454671 114666 sgd_solver.cpp:106] Iteration 7800, lr = 1e-06
I0131 02:41:58.767007 114666 solver.cpp:266] Iteration 7850 (2.05664 iter/s, 24.3114s/50 iter), loss = 0.000919937
I0131 02:41:58.767120 114666 solver.cpp:285]     Train net output #0: loss = 0.000920014 (* 1 = 0.000920014 loss)
I0131 02:41:58.769258 114666 sgd_solver.cpp:106] Iteration 7850, lr = 1e-06
I0131 02:42:23.049183 114666 solver.cpp:266] Iteration 7900 (2.05939 iter/s, 24.279s/50 iter), loss = 0.00105131
I0131 02:42:23.049213 114666 solver.cpp:285]     Train net output #0: loss = 0.00105138 (* 1 = 0.00105138 loss)
I0131 02:42:23.051439 114666 sgd_solver.cpp:106] Iteration 7900, lr = 1e-06
I0131 02:42:47.110273 114666 solver.cpp:266] Iteration 7950 (2.07832 iter/s, 24.0579s/50 iter), loss = 0.00156823
I0131 02:42:47.110399 114666 solver.cpp:285]     Train net output #0: loss = 0.0015683 (* 1 = 0.0015683 loss)
I0131 02:42:47.112478 114666 sgd_solver.cpp:106] Iteration 7950, lr = 1e-06
I0131 02:43:10.905329 114666 solver.cpp:418] Iteration 8000, Testing net (#0)
I0131 02:43:14.354975 114666 solver.cpp:517]     Test net output #0: loss = 0.220039 (* 1 = 0.220039 loss)
I0131 02:43:14.354996 114666 solver.cpp:517]     Test net output #1: top-1 = 0.9565
I0131 02:43:14.830199 114666 solver.cpp:266] Iteration 8000 (1.80397 iter/s, 27.7167s/50 iter), loss = 0.00130132
I0131 02:43:14.830229 114666 solver.cpp:285]     Train net output #0: loss = 0.00130139 (* 1 = 0.00130139 loss)
I0131 02:43:14.832460 114666 sgd_solver.cpp:106] Iteration 8000, lr = 1e-06
I0131 02:43:39.070472 114666 solver.cpp:266] Iteration 8050 (2.06295 iter/s, 24.2371s/50 iter), loss = 0.00100321
I0131 02:43:39.070600 114666 solver.cpp:285]     Train net output #0: loss = 0.00100328 (* 1 = 0.00100328 loss)
I0131 02:43:39.070608 114666 sgd_solver.cpp:106] Iteration 8050, lr = 1e-06
I0131 02:44:03.348701 114666 solver.cpp:266] Iteration 8100 (2.05955 iter/s, 24.2772s/50 iter), loss = 0.0100733
I0131 02:44:03.348731 114666 solver.cpp:285]     Train net output #0: loss = 0.0100734 (* 1 = 0.0100734 loss)
I0131 02:44:03.350944 114666 sgd_solver.cpp:106] Iteration 8100, lr = 1e-06
I0131 02:44:27.545419 114666 solver.cpp:266] Iteration 8150 (2.06666 iter/s, 24.1936s/50 iter), loss = 0.00389965
I0131 02:44:27.545536 114666 solver.cpp:285]     Train net output #0: loss = 0.00389972 (* 1 = 0.00389972 loss)
I0131 02:44:27.547677 114666 sgd_solver.cpp:106] Iteration 8150, lr = 1e-06
I0131 02:44:51.761860 114666 solver.cpp:266] Iteration 8200 (2.06498 iter/s, 24.2133s/50 iter), loss = 0.00115245
I0131 02:44:51.761891 114666 solver.cpp:285]     Train net output #0: loss = 0.00115252 (* 1 = 0.00115252 loss)
I0131 02:44:51.764123 114666 sgd_solver.cpp:106] Iteration 8200, lr = 1e-06
I0131 02:45:16.004422 114666 solver.cpp:266] Iteration 8250 (2.06276 iter/s, 24.2394s/50 iter), loss = 0.00127447
I0131 02:45:16.004498 114666 solver.cpp:285]     Train net output #0: loss = 0.00127454 (* 1 = 0.00127454 loss)
I0131 02:45:16.004506 114666 sgd_solver.cpp:106] Iteration 8250, lr = 1e-06
I0131 02:45:40.245642 114666 solver.cpp:266] Iteration 8300 (2.06269 iter/s, 24.2402s/50 iter), loss = 0.000394728
I0131 02:45:40.245674 114666 solver.cpp:285]     Train net output #0: loss = 0.000394796 (* 1 = 0.000394796 loss)
I0131 02:45:40.247889 114666 sgd_solver.cpp:106] Iteration 8300, lr = 1e-06
I0131 02:46:04.456387 114666 solver.cpp:266] Iteration 8350 (2.06547 iter/s, 24.2076s/50 iter), loss = 0.000505664
I0131 02:46:04.456507 114666 solver.cpp:285]     Train net output #0: loss = 0.000505731 (* 1 = 0.000505731 loss)
I0131 02:46:04.458640 114666 sgd_solver.cpp:106] Iteration 8350, lr = 1e-06
I0131 02:46:28.626456 114666 solver.cpp:266] Iteration 8400 (2.06894 iter/s, 24.1669s/50 iter), loss = 0.00861853
I0131 02:46:28.626494 114666 solver.cpp:285]     Train net output #0: loss = 0.0086186 (* 1 = 0.0086186 loss)
I0131 02:46:28.628700 114666 sgd_solver.cpp:106] Iteration 8400, lr = 1e-06
I0131 02:46:53.065443 114666 solver.cpp:266] Iteration 8450 (2.04617 iter/s, 24.4358s/50 iter), loss = 0.000568027
I0131 02:46:53.065562 114666 solver.cpp:285]     Train net output #0: loss = 0.000568096 (* 1 = 0.000568096 loss)
I0131 02:46:53.065979 114666 sgd_solver.cpp:106] Iteration 8450, lr = 1e-06
I0131 02:47:17.308833 114666 solver.cpp:266] Iteration 8500 (2.06254 iter/s, 24.242s/50 iter), loss = 0.00224647
I0131 02:47:17.308862 114666 solver.cpp:285]     Train net output #0: loss = 0.00224654 (* 1 = 0.00224654 loss)
I0131 02:47:17.311097 114666 sgd_solver.cpp:106] Iteration 8500, lr = 1e-06
I0131 02:47:41.467595 114666 solver.cpp:266] Iteration 8550 (2.06991 iter/s, 24.1556s/50 iter), loss = 0.00167619
I0131 02:47:41.467703 114666 solver.cpp:285]     Train net output #0: loss = 0.00167626 (* 1 = 0.00167626 loss)
I0131 02:47:41.469849 114666 sgd_solver.cpp:106] Iteration 8550, lr = 1e-06
I0131 02:48:05.705294 114666 solver.cpp:266] Iteration 8600 (2.06317 iter/s, 24.2346s/50 iter), loss = 0.000248745
I0131 02:48:05.705328 114666 solver.cpp:285]     Train net output #0: loss = 0.000248812 (* 1 = 0.000248812 loss)
I0131 02:48:05.705368 114666 sgd_solver.cpp:106] Iteration 8600, lr = 1e-06
I0131 02:48:29.937556 114666 solver.cpp:266] Iteration 8650 (2.06345 iter/s, 24.2313s/50 iter), loss = 0.0108647
I0131 02:48:29.937678 114666 solver.cpp:285]     Train net output #0: loss = 0.0108648 (* 1 = 0.0108648 loss)
I0131 02:48:29.939780 114666 sgd_solver.cpp:106] Iteration 8650, lr = 1e-06
I0131 02:48:54.157588 114666 solver.cpp:266] Iteration 8700 (2.06467 iter/s, 24.2169s/50 iter), loss = 0.00380501
I0131 02:48:54.157631 114666 solver.cpp:285]     Train net output #0: loss = 0.00380508 (* 1 = 0.00380508 loss)
I0131 02:48:54.159840 114666 sgd_solver.cpp:106] Iteration 8700, lr = 1e-06
I0131 02:49:18.363659 114666 solver.cpp:266] Iteration 8750 (2.06587 iter/s, 24.2029s/50 iter), loss = 0.00957725
I0131 02:49:18.363785 114666 solver.cpp:285]     Train net output #0: loss = 0.00957732 (* 1 = 0.00957732 loss)
I0131 02:49:18.365908 114666 sgd_solver.cpp:106] Iteration 8750, lr = 1e-06
I0131 02:49:42.778120 114666 solver.cpp:266] Iteration 8800 (2.04823 iter/s, 24.4113s/50 iter), loss = 0.00169723
I0131 02:49:42.778164 114666 solver.cpp:285]     Train net output #0: loss = 0.0016973 (* 1 = 0.0016973 loss)
I0131 02:49:42.780378 114666 sgd_solver.cpp:106] Iteration 8800, lr = 1e-06
I0131 02:50:06.946776 114666 solver.cpp:266] Iteration 8850 (2.06906 iter/s, 24.1655s/50 iter), loss = 0.00130434
I0131 02:50:06.946916 114666 solver.cpp:285]     Train net output #0: loss = 0.00130441 (* 1 = 0.00130441 loss)
I0131 02:50:06.949018 114666 sgd_solver.cpp:106] Iteration 8850, lr = 1e-06
I0131 02:50:31.065587 114666 solver.cpp:266] Iteration 8900 (2.07334 iter/s, 24.1157s/50 iter), loss = 0.000107834
I0131 02:50:31.065619 114666 solver.cpp:285]     Train net output #0: loss = 0.000107903 (* 1 = 0.000107903 loss)
I0131 02:50:31.067812 114666 sgd_solver.cpp:106] Iteration 8900, lr = 1e-06
I0131 02:50:55.422582 114666 solver.cpp:266] Iteration 8950 (2.05306 iter/s, 24.3539s/50 iter), loss = 0.000446986
I0131 02:50:55.422715 114666 solver.cpp:285]     Train net output #0: loss = 0.000447055 (* 1 = 0.000447055 loss)
I0131 02:50:55.422727 114666 sgd_solver.cpp:106] Iteration 8950, lr = 1e-06
I0131 02:51:19.138244 114666 solver.cpp:418] Iteration 9000, Testing net (#0)
I0131 02:51:22.700842 114666 solver.cpp:517]     Test net output #0: loss = 0.228875 (* 1 = 0.228875 loss)
I0131 02:51:22.700862 114666 solver.cpp:517]     Test net output #1: top-1 = 0.95625
I0131 02:51:23.164140 114666 solver.cpp:266] Iteration 9000 (1.80243 iter/s, 27.7404s/50 iter), loss = 0.000364435
I0131 02:51:23.164175 114666 solver.cpp:285]     Train net output #0: loss = 0.000364504 (* 1 = 0.000364504 loss)
I0131 02:51:23.166395 114666 sgd_solver.cpp:106] Iteration 9000, lr = 1e-06
I0131 02:51:47.460110 114666 solver.cpp:266] Iteration 9050 (2.05822 iter/s, 24.2928s/50 iter), loss = 0.00741696
I0131 02:51:47.460170 114666 solver.cpp:285]     Train net output #0: loss = 0.00741703 (* 1 = 0.00741703 loss)
I0131 02:51:47.460189 114666 sgd_solver.cpp:106] Iteration 9050, lr = 1e-06
I0131 02:52:11.711966 114666 solver.cpp:266] Iteration 9100 (2.06178 iter/s, 24.2509s/50 iter), loss = 0.00124215
I0131 02:52:11.712007 114666 solver.cpp:285]     Train net output #0: loss = 0.00124222 (* 1 = 0.00124222 loss)
I0131 02:52:11.714221 114666 sgd_solver.cpp:106] Iteration 9100, lr = 1e-06
I0131 02:52:36.212065 114666 solver.cpp:266] Iteration 9150 (2.04107 iter/s, 24.4969s/50 iter), loss = 0.00144349
I0131 02:52:36.212191 114666 solver.cpp:285]     Train net output #0: loss = 0.00144355 (* 1 = 0.00144355 loss)
I0131 02:52:36.214459 114666 sgd_solver.cpp:106] Iteration 9150, lr = 1e-06
I0131 02:53:00.447084 114666 solver.cpp:266] Iteration 9200 (2.06341 iter/s, 24.2317s/50 iter), loss = 0.00147939
I0131 02:53:00.447116 114666 solver.cpp:285]     Train net output #0: loss = 0.00147946 (* 1 = 0.00147946 loss)
I0131 02:53:00.449331 114666 sgd_solver.cpp:106] Iteration 9200, lr = 1e-06
I0131 02:53:24.678403 114666 solver.cpp:266] Iteration 9250 (2.06371 iter/s, 24.2282s/50 iter), loss = 0.00096205
I0131 02:53:24.678474 114666 solver.cpp:285]     Train net output #0: loss = 0.000962115 (* 1 = 0.000962115 loss)
I0131 02:53:24.680652 114666 sgd_solver.cpp:106] Iteration 9250, lr = 1e-06
I0131 02:53:48.844519 114666 solver.cpp:266] Iteration 9300 (2.06928 iter/s, 24.163s/50 iter), loss = 0.00497157
I0131 02:53:48.844555 114666 solver.cpp:285]     Train net output #0: loss = 0.00497164 (* 1 = 0.00497164 loss)
I0131 02:53:48.846778 114666 sgd_solver.cpp:106] Iteration 9300, lr = 1e-06
I0131 02:54:13.234100 114666 solver.cpp:266] Iteration 9350 (2.05032 iter/s, 24.3864s/50 iter), loss = 0.000439025
I0131 02:54:13.234160 114666 solver.cpp:285]     Train net output #0: loss = 0.000439093 (* 1 = 0.000439093 loss)
I0131 02:54:13.234184 114666 sgd_solver.cpp:106] Iteration 9350, lr = 1e-06
I0131 02:54:37.574201 114666 solver.cpp:266] Iteration 9400 (2.0543 iter/s, 24.3391s/50 iter), loss = 0.00502876
I0131 02:54:37.574239 114666 solver.cpp:285]     Train net output #0: loss = 0.00502883 (* 1 = 0.00502883 loss)
I0131 02:54:37.576447 114666 sgd_solver.cpp:106] Iteration 9400, lr = 1e-06
I0131 02:55:01.799718 114666 solver.cpp:266] Iteration 9450 (2.06421 iter/s, 24.2224s/50 iter), loss = 0.0109726
I0131 02:55:01.799836 114666 solver.cpp:285]     Train net output #0: loss = 0.0109726 (* 1 = 0.0109726 loss)
I0131 02:55:01.801970 114666 sgd_solver.cpp:106] Iteration 9450, lr = 1e-06
I0131 02:55:25.821904 114666 solver.cpp:266] Iteration 9500 (2.08168 iter/s, 24.0191s/50 iter), loss = 0.00144568
I0131 02:55:25.821938 114666 solver.cpp:285]     Train net output #0: loss = 0.00144575 (* 1 = 0.00144575 loss)
I0131 02:55:25.824149 114666 sgd_solver.cpp:106] Iteration 9500, lr = 1e-06
I0131 02:55:50.187350 114666 solver.cpp:266] Iteration 9550 (2.05235 iter/s, 24.3623s/50 iter), loss = 0.00414106
I0131 02:55:50.187505 114666 solver.cpp:285]     Train net output #0: loss = 0.00414112 (* 1 = 0.00414112 loss)
I0131 02:55:50.189642 114666 sgd_solver.cpp:106] Iteration 9550, lr = 1e-06
I0131 02:56:14.377132 114666 solver.cpp:266] Iteration 9600 (2.06726 iter/s, 24.1866s/50 iter), loss = 0.00157478
I0131 02:56:14.377163 114666 solver.cpp:285]     Train net output #0: loss = 0.00157485 (* 1 = 0.00157485 loss)
I0131 02:56:14.377230 114666 sgd_solver.cpp:106] Iteration 9600, lr = 1e-06
I0131 02:56:38.518286 114666 solver.cpp:266] Iteration 9650 (2.07124 iter/s, 24.1402s/50 iter), loss = 0.00206547
I0131 02:56:38.518401 114666 solver.cpp:285]     Train net output #0: loss = 0.00206554 (* 1 = 0.00206554 loss)
I0131 02:56:38.520530 114666 sgd_solver.cpp:106] Iteration 9650, lr = 1e-06
I0131 02:57:02.713516 114666 solver.cpp:266] Iteration 9700 (2.06679 iter/s, 24.1921s/50 iter), loss = 0.00066468
I0131 02:57:02.713548 114666 solver.cpp:285]     Train net output #0: loss = 0.000664751 (* 1 = 0.000664751 loss)
I0131 02:57:02.713593 114666 sgd_solver.cpp:106] Iteration 9700, lr = 1e-06
I0131 02:57:26.991498 114666 solver.cpp:266] Iteration 9750 (2.05956 iter/s, 24.277s/50 iter), loss = 0.000695326
I0131 02:57:26.991626 114666 solver.cpp:285]     Train net output #0: loss = 0.000695397 (* 1 = 0.000695397 loss)
I0131 02:57:26.993747 114666 sgd_solver.cpp:106] Iteration 9750, lr = 1e-06
I0131 02:57:51.173763 114666 solver.cpp:266] Iteration 9800 (2.0679 iter/s, 24.1791s/50 iter), loss = 0.00144934
I0131 02:57:51.173792 114666 solver.cpp:285]     Train net output #0: loss = 0.00144941 (* 1 = 0.00144941 loss)
I0131 02:57:51.176012 114666 sgd_solver.cpp:106] Iteration 9800, lr = 1e-06
I0131 02:58:15.225986 114666 solver.cpp:266] Iteration 9850 (2.07908 iter/s, 24.0491s/50 iter), loss = 0.000482757
I0131 02:58:15.226094 114666 solver.cpp:285]     Train net output #0: loss = 0.000482826 (* 1 = 0.000482826 loss)
I0131 02:58:15.228229 114666 sgd_solver.cpp:106] Iteration 9850, lr = 1e-06
I0131 02:58:39.659005 114666 solver.cpp:266] Iteration 9900 (2.04667 iter/s, 24.4299s/50 iter), loss = 0.00109478
I0131 02:58:39.659046 114666 solver.cpp:285]     Train net output #0: loss = 0.00109485 (* 1 = 0.00109485 loss)
I0131 02:58:39.659358 114666 sgd_solver.cpp:106] Iteration 9900, lr = 1e-06
I0131 02:59:03.881832 114666 solver.cpp:266] Iteration 9950 (2.06427 iter/s, 24.2216s/50 iter), loss = 0.00662618
I0131 02:59:03.881948 114666 solver.cpp:285]     Train net output #0: loss = 0.00662625 (* 1 = 0.00662625 loss)
I0131 02:59:03.881991 114666 sgd_solver.cpp:106] Iteration 9950, lr = 1e-06
I0131 02:59:27.513988 114666 solver.cpp:418] Iteration 10000, Testing net (#0)
I0131 02:59:31.138072 114666 solver.cpp:517]     Test net output #0: loss = 0.234258 (* 1 = 0.234258 loss)
I0131 02:59:31.138087 114666 solver.cpp:517]     Test net output #1: top-1 = 0.95625
I0131 02:59:31.620507 114666 solver.cpp:266] Iteration 10000 (1.80261 iter/s, 27.7375s/50 iter), loss = 0.00193802
I0131 02:59:31.620533 114666 solver.cpp:285]     Train net output #0: loss = 0.00193809 (* 1 = 0.00193809 loss)
I0131 02:59:31.622787 114666 sgd_solver.cpp:106] Iteration 10000, lr = 1e-07
I0131 02:59:55.736227 114666 solver.cpp:266] Iteration 10050 (2.07361 iter/s, 24.1126s/50 iter), loss = 0.000986938
I0131 02:59:55.736284 114666 solver.cpp:285]     Train net output #0: loss = 0.000987011 (* 1 = 0.000987011 loss)
I0131 02:59:55.738482 114666 sgd_solver.cpp:106] Iteration 10050, lr = 1e-07
I0131 03:00:19.965093 114666 solver.cpp:266] Iteration 10100 (2.06392 iter/s, 24.2257s/50 iter), loss = 0.00147416
I0131 03:00:19.965126 114666 solver.cpp:285]     Train net output #0: loss = 0.00147424 (* 1 = 0.00147424 loss)
I0131 03:00:19.967344 114666 sgd_solver.cpp:106] Iteration 10100, lr = 1e-07
I0131 03:00:44.162269 114666 solver.cpp:266] Iteration 10150 (2.06663 iter/s, 24.194s/50 iter), loss = 0.00964034
I0131 03:00:44.162376 114666 solver.cpp:285]     Train net output #0: loss = 0.00964042 (* 1 = 0.00964042 loss)
I0131 03:00:44.162384 114666 sgd_solver.cpp:106] Iteration 10150, lr = 1e-07
I0131 03:01:08.412511 114666 solver.cpp:266] Iteration 10200 (2.06192 iter/s, 24.2492s/50 iter), loss = 0.00100501
I0131 03:01:08.412540 114666 solver.cpp:285]     Train net output #0: loss = 0.00100509 (* 1 = 0.00100509 loss)
I0131 03:01:08.414763 114666 sgd_solver.cpp:106] Iteration 10200, lr = 1e-07
I0131 03:01:32.708312 114666 solver.cpp:266] Iteration 10250 (2.05824 iter/s, 24.2927s/50 iter), loss = 0.000774029
I0131 03:01:32.708454 114666 solver.cpp:285]     Train net output #0: loss = 0.000774109 (* 1 = 0.000774109 loss)
I0131 03:01:32.710557 114666 sgd_solver.cpp:106] Iteration 10250, lr = 1e-07
I0131 03:01:56.775413 114666 solver.cpp:266] Iteration 10300 (2.0778 iter/s, 24.064s/50 iter), loss = 0.00177202
I0131 03:01:56.775454 114666 solver.cpp:285]     Train net output #0: loss = 0.0017721 (* 1 = 0.0017721 loss)
I0131 03:01:56.777591 114666 sgd_solver.cpp:106] Iteration 10300, lr = 1e-07
I0131 03:02:21.093137 114666 solver.cpp:266] Iteration 10350 (2.05637 iter/s, 24.3146s/50 iter), loss = 0.0110996
I0131 03:02:21.093269 114666 solver.cpp:285]     Train net output #0: loss = 0.0110997 (* 1 = 0.0110997 loss)
I0131 03:02:21.093277 114666 sgd_solver.cpp:106] Iteration 10350, lr = 1e-07
I0131 03:02:45.303658 114666 solver.cpp:266] Iteration 10400 (2.06531 iter/s, 24.2095s/50 iter), loss = 0.000365596
I0131 03:02:45.303687 114666 solver.cpp:285]     Train net output #0: loss = 0.000365678 (* 1 = 0.000365678 loss)
I0131 03:02:45.303740 114666 sgd_solver.cpp:106] Iteration 10400, lr = 1e-07
I0131 03:03:09.325150 114666 solver.cpp:266] Iteration 10450 (2.08155 iter/s, 24.0205s/50 iter), loss = 0.00465729
I0131 03:03:09.325249 114666 solver.cpp:285]     Train net output #0: loss = 0.00465738 (* 1 = 0.00465738 loss)
I0131 03:03:09.327404 114666 sgd_solver.cpp:106] Iteration 10450, lr = 1e-07
I0131 03:03:33.617084 114666 solver.cpp:266] Iteration 10500 (2.05856 iter/s, 24.2888s/50 iter), loss = 0.00137071
I0131 03:03:33.617117 114666 solver.cpp:285]     Train net output #0: loss = 0.00137079 (* 1 = 0.00137079 loss)
I0131 03:03:33.619347 114666 sgd_solver.cpp:106] Iteration 10500, lr = 1e-07
I0131 03:03:57.881232 114666 solver.cpp:266] Iteration 10550 (2.06092 iter/s, 24.261s/50 iter), loss = 0.000674274
I0131 03:03:57.881306 114666 solver.cpp:285]     Train net output #0: loss = 0.000674359 (* 1 = 0.000674359 loss)
I0131 03:03:57.883481 114666 sgd_solver.cpp:106] Iteration 10550, lr = 1e-07
I0131 03:04:22.128518 114666 solver.cpp:266] Iteration 10600 (2.06235 iter/s, 24.2441s/50 iter), loss = 0.0060184
I0131 03:04:22.128547 114666 solver.cpp:285]     Train net output #0: loss = 0.00601849 (* 1 = 0.00601849 loss)
I0131 03:04:22.130772 114666 sgd_solver.cpp:106] Iteration 10600, lr = 1e-07
I0131 03:04:46.293792 114666 solver.cpp:266] Iteration 10650 (2.06935 iter/s, 24.1621s/50 iter), loss = 0.00405681
I0131 03:04:46.293910 114666 solver.cpp:285]     Train net output #0: loss = 0.0040569 (* 1 = 0.0040569 loss)
I0131 03:04:46.296041 114666 sgd_solver.cpp:106] Iteration 10650, lr = 1e-07
I0131 03:05:10.549986 114666 solver.cpp:266] Iteration 10700 (2.0616 iter/s, 24.253s/50 iter), loss = 0.008282
I0131 03:05:10.550019 114666 solver.cpp:285]     Train net output #0: loss = 0.00828209 (* 1 = 0.00828209 loss)
I0131 03:05:10.550025 114666 sgd_solver.cpp:106] Iteration 10700, lr = 1e-07
I0131 03:05:34.810534 114666 solver.cpp:266] Iteration 10750 (2.06104 iter/s, 24.2596s/50 iter), loss = 0.0177414
I0131 03:05:34.810742 114666 solver.cpp:285]     Train net output #0: loss = 0.0177415 (* 1 = 0.0177415 loss)
I0131 03:05:34.812796 114666 sgd_solver.cpp:106] Iteration 10750, lr = 1e-07
I0131 03:05:59.115038 114666 solver.cpp:266] Iteration 10800 (2.0575 iter/s, 24.3014s/50 iter), loss = 0.0033658
I0131 03:05:59.115079 114666 solver.cpp:285]     Train net output #0: loss = 0.00336589 (* 1 = 0.00336589 loss)
I0131 03:05:59.117297 114666 sgd_solver.cpp:106] Iteration 10800, lr = 1e-07
I0131 03:06:23.291335 114666 solver.cpp:266] Iteration 10850 (2.06841 iter/s, 24.1731s/50 iter), loss = 0.000566665
I0131 03:06:23.291383 114666 solver.cpp:285]     Train net output #0: loss = 0.00056675 (* 1 = 0.00056675 loss)
I0131 03:06:23.293584 114666 sgd_solver.cpp:106] Iteration 10850, lr = 1e-07
I0131 03:06:47.562997 114666 solver.cpp:266] Iteration 10900 (2.06028 iter/s, 24.2685s/50 iter), loss = 0.00341309
I0131 03:06:47.563030 114666 solver.cpp:285]     Train net output #0: loss = 0.00341317 (* 1 = 0.00341317 loss)
I0131 03:06:47.563035 114666 sgd_solver.cpp:106] Iteration 10900, lr = 1e-07
I0131 03:07:12.028789 114666 solver.cpp:266] Iteration 10950 (2.04375 iter/s, 24.4649s/50 iter), loss = 0.000350978
I0131 03:07:12.028945 114666 solver.cpp:285]     Train net output #0: loss = 0.00035106 (* 1 = 0.00035106 loss)
I0131 03:07:12.031055 114666 sgd_solver.cpp:106] Iteration 10950, lr = 1e-07
I0131 03:07:35.722108 114666 solver.cpp:418] Iteration 11000, Testing net (#0)
I0131 03:07:39.375270 114666 solver.cpp:517]     Test net output #0: loss = 0.235749 (* 1 = 0.235749 loss)
I0131 03:07:39.375288 114666 solver.cpp:517]     Test net output #1: top-1 = 0.956
I0131 03:07:39.726029 114666 solver.cpp:266] Iteration 11000 (1.80545 iter/s, 27.694s/50 iter), loss = 0.0168641
I0131 03:07:39.726068 114666 solver.cpp:285]     Train net output #0: loss = 0.0168642 (* 1 = 0.0168642 loss)
I0131 03:07:39.728273 114666 sgd_solver.cpp:106] Iteration 11000, lr = 1e-07
I0131 03:08:04.109001 114666 solver.cpp:266] Iteration 11050 (2.05088 iter/s, 24.3798s/50 iter), loss = 0.00211001
I0131 03:08:04.109127 114666 solver.cpp:285]     Train net output #0: loss = 0.00211009 (* 1 = 0.00211009 loss)
I0131 03:08:04.109134 114666 sgd_solver.cpp:106] Iteration 11050, lr = 1e-07
I0131 03:08:28.410658 114666 solver.cpp:266] Iteration 11100 (2.05756 iter/s, 24.3006s/50 iter), loss = 0.0012156
I0131 03:08:28.410691 114666 solver.cpp:285]     Train net output #0: loss = 0.00121568 (* 1 = 0.00121568 loss)
I0131 03:08:28.412909 114666 sgd_solver.cpp:106] Iteration 11100, lr = 1e-07
I0131 03:08:52.678339 114666 solver.cpp:266] Iteration 11150 (2.06062 iter/s, 24.2645s/50 iter), loss = 0.00128019
I0131 03:08:52.678444 114666 solver.cpp:285]     Train net output #0: loss = 0.00128027 (* 1 = 0.00128027 loss)
I0131 03:08:52.680593 114666 sgd_solver.cpp:106] Iteration 11150, lr = 1e-07
I0131 03:09:16.809799 114666 solver.cpp:266] Iteration 11200 (2.07225 iter/s, 24.1283s/50 iter), loss = 0.00214404
I0131 03:09:16.809830 114666 solver.cpp:285]     Train net output #0: loss = 0.00214412 (* 1 = 0.00214412 loss)
I0131 03:09:16.812052 114666 sgd_solver.cpp:106] Iteration 11200, lr = 1e-07
I0131 03:09:41.205819 114666 solver.cpp:266] Iteration 11250 (2.04978 iter/s, 24.3929s/50 iter), loss = 0.0282158
I0131 03:09:41.205935 114666 solver.cpp:285]     Train net output #0: loss = 0.0282159 (* 1 = 0.0282159 loss)
I0131 03:09:41.205992 114666 sgd_solver.cpp:106] Iteration 11250, lr = 1e-07
I0131 03:10:05.512317 114666 solver.cpp:266] Iteration 11300 (2.05715 iter/s, 24.3054s/50 iter), loss = 0.00265797
I0131 03:10:05.512348 114666 solver.cpp:285]     Train net output #0: loss = 0.00265805 (* 1 = 0.00265805 loss)
I0131 03:10:05.514564 114666 sgd_solver.cpp:106] Iteration 11300, lr = 1e-07
I0131 03:10:29.731974 114666 solver.cpp:266] Iteration 11350 (2.06471 iter/s, 24.2165s/50 iter), loss = 0.00171266
I0131 03:10:29.732034 114666 solver.cpp:285]     Train net output #0: loss = 0.00171274 (* 1 = 0.00171274 loss)
I0131 03:10:29.734226 114666 sgd_solver.cpp:106] Iteration 11350, lr = 1e-07
I0131 03:10:53.927290 114666 solver.cpp:266] Iteration 11400 (2.06678 iter/s, 24.1922s/50 iter), loss = 0.000277424
I0131 03:10:53.927322 114666 solver.cpp:285]     Train net output #0: loss = 0.000277503 (* 1 = 0.000277503 loss)
I0131 03:10:53.929538 114666 sgd_solver.cpp:106] Iteration 11400, lr = 1e-07
I0131 03:11:18.217448 114666 solver.cpp:266] Iteration 11450 (2.05871 iter/s, 24.287s/50 iter), loss = 0.000477168
I0131 03:11:18.217561 114666 solver.cpp:285]     Train net output #0: loss = 0.000477248 (* 1 = 0.000477248 loss)
I0131 03:11:18.217567 114666 sgd_solver.cpp:106] Iteration 11450, lr = 1e-07
I0131 03:11:42.479809 114666 solver.cpp:266] Iteration 11500 (2.06089 iter/s, 24.2614s/50 iter), loss = 0.00105812
I0131 03:11:42.479836 114666 solver.cpp:285]     Train net output #0: loss = 0.00105821 (* 1 = 0.00105821 loss)
I0131 03:11:42.482064 114666 sgd_solver.cpp:106] Iteration 11500, lr = 1e-07
I0131 03:12:06.629459 114666 solver.cpp:266] Iteration 11550 (2.07069 iter/s, 24.1465s/50 iter), loss = 0.000479249
I0131 03:12:06.629617 114666 solver.cpp:285]     Train net output #0: loss = 0.000479333 (* 1 = 0.000479333 loss)
I0131 03:12:06.631713 114666 sgd_solver.cpp:106] Iteration 11550, lr = 1e-07
I0131 03:12:30.755070 114666 solver.cpp:266] Iteration 11600 (2.07276 iter/s, 24.1225s/50 iter), loss = 0.0127487
I0131 03:12:30.755102 114666 solver.cpp:285]     Train net output #0: loss = 0.0127488 (* 1 = 0.0127488 loss)
I0131 03:12:30.757314 114666 sgd_solver.cpp:106] Iteration 11600, lr = 1e-07
I0131 03:12:54.976585 114666 solver.cpp:266] Iteration 11650 (2.06455 iter/s, 24.2184s/50 iter), loss = 0.000996628
I0131 03:12:54.976697 114666 solver.cpp:285]     Train net output #0: loss = 0.000996711 (* 1 = 0.000996711 loss)
I0131 03:12:54.978828 114666 sgd_solver.cpp:106] Iteration 11650, lr = 1e-07
I0131 03:13:19.291532 114666 solver.cpp:266] Iteration 11700 (2.05661 iter/s, 24.3118s/50 iter), loss = 0.0104816
I0131 03:13:19.291563 114666 solver.cpp:285]     Train net output #0: loss = 0.0104817 (* 1 = 0.0104817 loss)
I0131 03:13:19.293781 114666 sgd_solver.cpp:106] Iteration 11700, lr = 1e-07
I0131 03:13:43.266116 114666 solver.cpp:266] Iteration 11750 (2.08581 iter/s, 23.9715s/50 iter), loss = 0.0134414
I0131 03:13:43.266232 114666 solver.cpp:285]     Train net output #0: loss = 0.0134414 (* 1 = 0.0134414 loss)
I0131 03:13:43.269050 114666 sgd_solver.cpp:106] Iteration 11750, lr = 1e-07
I0131 03:14:07.727809 114666 solver.cpp:266] Iteration 11800 (2.04433 iter/s, 24.4579s/50 iter), loss = 0.00176849
I0131 03:14:07.727839 114666 solver.cpp:285]     Train net output #0: loss = 0.00176857 (* 1 = 0.00176857 loss)
I0131 03:14:07.730058 114666 sgd_solver.cpp:106] Iteration 11800, lr = 1e-07
I0131 03:14:31.982089 114666 solver.cpp:266] Iteration 11850 (2.06176 iter/s, 24.2511s/50 iter), loss = 0.000576255
I0131 03:14:31.982219 114666 solver.cpp:285]     Train net output #0: loss = 0.000576336 (* 1 = 0.000576336 loss)
I0131 03:14:31.984344 114666 sgd_solver.cpp:106] Iteration 11850, lr = 1e-07
I0131 03:14:56.108319 114666 solver.cpp:266] Iteration 11900 (2.0727 iter/s, 24.1231s/50 iter), loss = 0.0157266
I0131 03:14:56.108361 114666 solver.cpp:285]     Train net output #0: loss = 0.0157267 (* 1 = 0.0157267 loss)
I0131 03:14:56.110569 114666 sgd_solver.cpp:106] Iteration 11900, lr = 1e-07
I0131 03:15:20.414403 114666 solver.cpp:266] Iteration 11950 (2.05736 iter/s, 24.3029s/50 iter), loss = 0.00191112
I0131 03:15:20.414458 114666 solver.cpp:285]     Train net output #0: loss = 0.0019112 (* 1 = 0.0019112 loss)
I0131 03:15:20.414497 114666 sgd_solver.cpp:106] Iteration 11950, lr = 1e-07
I0131 03:15:44.276013 114666 solver.cpp:929] Snapshotting to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/snapshots/_iter_12000.caffemodel
I0131 03:15:46.769455 114666 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/regular_rate_0.7/snapshots/_iter_12000.solverstate
I0131 03:15:47.382912 114666 solver.cpp:378] Iteration 12000, loss = 0.000926132
I0131 03:15:47.383410 114666 solver.cpp:418] Iteration 12000, Testing net (#0)
I0131 03:15:50.948686 114666 solver.cpp:517]     Test net output #0: loss = 0.236468 (* 1 = 0.236468 loss)
I0131 03:15:50.948798 114666 solver.cpp:517]     Test net output #1: top-1 = 0.956
I0131 03:15:50.948804 114666 solver.cpp:386] Optimization Done (2.05416 iter/s).
I0131 03:15:50.948808 114666 caffe_interface.cpp:530] Optimization Done.

## last step: get the final output model
## note that it does not work if you used the "final.prototxt" as wrongly described by transform help
$PRUNE_ROOT/deephi_compress transform -model ${WORK_DIR}/train_val.prototxt -weights ${WORK_DIR}/regular_rate_0.7/sparse.caffemodel 2>&1 | tee ${WORK_DIR}/rpt/logfile_transform_alexnetBNnoLRN.txt
I0131 03:15:53.096451   790 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0131 03:15:53.100423   790 gpu_memory.cpp:55] Total memory: 25620447232, Free: 13496549376, dev_info[0]: total=25620447232 free=13496549376
I0131 03:15:53.100436   790 caffe_interface.cpp:66] Use GPU with device ID 0
I0131 03:15:53.100833   790 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0131 03:15:54.130859   790 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0131 03:15:54.131062   790 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "scale7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0131 03:15:54.131163   790 layer_factory.hpp:77] Creating layer data
I0131 03:15:54.131209   790 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 03:15:54.132779   790 net.cpp:94] Creating Layer data
I0131 03:15:54.132809   790 net.cpp:409] data -> data
I0131 03:15:54.132834   790 net.cpp:409] data -> label
I0131 03:15:54.132958   825 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0131 03:15:54.132982   825 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0131 03:15:54.133216   790 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0131 03:15:54.133360   790 data_layer.cpp:83] output data size: 50,3,227,227
I0131 03:15:54.239428   790 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 03:15:54.239490   790 net.cpp:144] Setting up data
I0131 03:15:54.239502   790 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0131 03:15:54.239507   790 net.cpp:151] Top shape: 50 (50)
I0131 03:15:54.239509   790 net.cpp:159] Memory required for data: 30917600
I0131 03:15:54.239516   790 layer_factory.hpp:77] Creating layer label_data_1_split
I0131 03:15:54.239527   790 net.cpp:94] Creating Layer label_data_1_split
I0131 03:15:54.239533   790 net.cpp:435] label_data_1_split <- label
I0131 03:15:54.239549   790 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0131 03:15:54.239559   790 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0131 03:15:54.239611   790 net.cpp:144] Setting up label_data_1_split
I0131 03:15:54.239616   790 net.cpp:151] Top shape: 50 (50)
I0131 03:15:54.239620   790 net.cpp:151] Top shape: 50 (50)
I0131 03:15:54.239640   790 net.cpp:159] Memory required for data: 30918000
I0131 03:15:54.239645   790 layer_factory.hpp:77] Creating layer conv1
I0131 03:15:54.239655   790 net.cpp:94] Creating Layer conv1
I0131 03:15:54.239658   790 net.cpp:435] conv1 <- data
I0131 03:15:54.239665   790 net.cpp:409] conv1 -> conv1
I0131 03:15:54.241714   790 net.cpp:144] Setting up conv1
I0131 03:15:54.241726   790 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:54.241729   790 net.cpp:159] Memory required for data: 88998000
I0131 03:15:54.241745   790 layer_factory.hpp:77] Creating layer bn1
I0131 03:15:54.241753   790 net.cpp:94] Creating Layer bn1
I0131 03:15:54.241756   790 net.cpp:435] bn1 <- conv1
I0131 03:15:54.241761   790 net.cpp:409] bn1 -> scale1
I0131 03:15:54.243022   790 net.cpp:144] Setting up bn1
I0131 03:15:54.243029   790 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:54.243032   790 net.cpp:159] Memory required for data: 147078000
I0131 03:15:54.243042   790 layer_factory.hpp:77] Creating layer relu1
I0131 03:15:54.243047   790 net.cpp:94] Creating Layer relu1
I0131 03:15:54.243050   790 net.cpp:435] relu1 <- scale1
I0131 03:15:54.243054   790 net.cpp:409] relu1 -> relu1
I0131 03:15:54.243078   790 net.cpp:144] Setting up relu1
I0131 03:15:54.243083   790 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:54.243084   790 net.cpp:159] Memory required for data: 205158000
I0131 03:15:54.243086   790 layer_factory.hpp:77] Creating layer pool1
I0131 03:15:54.243093   790 net.cpp:94] Creating Layer pool1
I0131 03:15:54.243094   790 net.cpp:435] pool1 <- relu1
I0131 03:15:54.243099   790 net.cpp:409] pool1 -> pool1
I0131 03:15:54.243158   790 net.cpp:144] Setting up pool1
I0131 03:15:54.243163   790 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0131 03:15:54.243165   790 net.cpp:159] Memory required for data: 219154800
I0131 03:15:54.243168   790 layer_factory.hpp:77] Creating layer conv2
I0131 03:15:54.243175   790 net.cpp:94] Creating Layer conv2
I0131 03:15:54.243177   790 net.cpp:435] conv2 <- pool1
I0131 03:15:54.243181   790 net.cpp:409] conv2 -> conv2
I0131 03:15:54.250358   790 net.cpp:144] Setting up conv2
I0131 03:15:54.250375   790 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:54.250378   790 net.cpp:159] Memory required for data: 256479600
I0131 03:15:54.250389   790 layer_factory.hpp:77] Creating layer bn2
I0131 03:15:54.250397   790 net.cpp:94] Creating Layer bn2
I0131 03:15:54.250401   790 net.cpp:435] bn2 <- conv2
I0131 03:15:54.250407   790 net.cpp:409] bn2 -> scale2
I0131 03:15:54.250980   790 net.cpp:144] Setting up bn2
I0131 03:15:54.250991   790 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:54.250995   790 net.cpp:159] Memory required for data: 293804400
I0131 03:15:54.251008   790 layer_factory.hpp:77] Creating layer relu2
I0131 03:15:54.251016   790 net.cpp:94] Creating Layer relu2
I0131 03:15:54.251021   790 net.cpp:435] relu2 <- scale2
I0131 03:15:54.251029   790 net.cpp:409] relu2 -> relu2
I0131 03:15:54.251052   790 net.cpp:144] Setting up relu2
I0131 03:15:54.251060   790 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:54.251068   790 net.cpp:159] Memory required for data: 331129200
I0131 03:15:54.251072   790 layer_factory.hpp:77] Creating layer pool2
I0131 03:15:54.251080   790 net.cpp:94] Creating Layer pool2
I0131 03:15:54.251085   790 net.cpp:435] pool2 <- relu2
I0131 03:15:54.251091   790 net.cpp:409] pool2 -> pool2
I0131 03:15:54.251125   790 net.cpp:144] Setting up pool2
I0131 03:15:54.251132   790 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:54.251137   790 net.cpp:159] Memory required for data: 339782000
I0131 03:15:54.251142   790 layer_factory.hpp:77] Creating layer conv3
I0131 03:15:54.251152   790 net.cpp:94] Creating Layer conv3
I0131 03:15:54.251157   790 net.cpp:435] conv3 <- pool2
I0131 03:15:54.251164   790 net.cpp:409] conv3 -> conv3
I0131 03:15:54.266381   790 net.cpp:144] Setting up conv3
I0131 03:15:54.266404   790 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:54.266408   790 net.cpp:159] Memory required for data: 352761200
I0131 03:15:54.266445   790 layer_factory.hpp:77] Creating layer relu3
I0131 03:15:54.266459   790 net.cpp:94] Creating Layer relu3
I0131 03:15:54.266470   790 net.cpp:435] relu3 <- conv3
I0131 03:15:54.266480   790 net.cpp:409] relu3 -> relu3
I0131 03:15:54.266513   790 net.cpp:144] Setting up relu3
I0131 03:15:54.266520   790 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:54.266525   790 net.cpp:159] Memory required for data: 365740400
I0131 03:15:54.266530   790 layer_factory.hpp:77] Creating layer conv4
I0131 03:15:54.266542   790 net.cpp:94] Creating Layer conv4
I0131 03:15:54.266548   790 net.cpp:435] conv4 <- relu3
I0131 03:15:54.266556   790 net.cpp:409] conv4 -> conv4
I0131 03:15:54.282196   790 net.cpp:144] Setting up conv4
I0131 03:15:54.282217   790 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:54.282222   790 net.cpp:159] Memory required for data: 378719600
I0131 03:15:54.282235   790 layer_factory.hpp:77] Creating layer relu4
I0131 03:15:54.282245   790 net.cpp:94] Creating Layer relu4
I0131 03:15:54.282249   790 net.cpp:435] relu4 <- conv4
I0131 03:15:54.282259   790 net.cpp:409] relu4 -> relu4
I0131 03:15:54.282286   790 net.cpp:144] Setting up relu4
I0131 03:15:54.282291   790 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:54.282294   790 net.cpp:159] Memory required for data: 391698800
I0131 03:15:54.282299   790 layer_factory.hpp:77] Creating layer conv5
I0131 03:15:54.282310   790 net.cpp:94] Creating Layer conv5
I0131 03:15:54.282313   790 net.cpp:435] conv5 <- relu4
I0131 03:15:54.282320   790 net.cpp:409] conv5 -> conv5
I0131 03:15:54.295225   790 net.cpp:144] Setting up conv5
I0131 03:15:54.295250   790 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:54.295254   790 net.cpp:159] Memory required for data: 400351600
I0131 03:15:54.295264   790 layer_factory.hpp:77] Creating layer relu5
I0131 03:15:54.295275   790 net.cpp:94] Creating Layer relu5
I0131 03:15:54.295280   790 net.cpp:435] relu5 <- conv5
I0131 03:15:54.295289   790 net.cpp:409] relu5 -> relu5
I0131 03:15:54.295330   790 net.cpp:144] Setting up relu5
I0131 03:15:54.295336   790 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:54.295341   790 net.cpp:159] Memory required for data: 409004400
I0131 03:15:54.295344   790 layer_factory.hpp:77] Creating layer pool5
I0131 03:15:54.295356   790 net.cpp:94] Creating Layer pool5
I0131 03:15:54.295362   790 net.cpp:435] pool5 <- relu5
I0131 03:15:54.295369   790 net.cpp:409] pool5 -> pool5
I0131 03:15:54.295403   790 net.cpp:144] Setting up pool5
I0131 03:15:54.295409   790 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0131 03:15:54.295414   790 net.cpp:159] Memory required for data: 410847600
I0131 03:15:54.295418   790 layer_factory.hpp:77] Creating layer fc6
I0131 03:15:54.295428   790 net.cpp:94] Creating Layer fc6
I0131 03:15:54.295431   790 net.cpp:435] fc6 <- pool5
I0131 03:15:54.295449   790 net.cpp:409] fc6 -> fc6
I0131 03:15:54.654935   790 net.cpp:144] Setting up fc6
I0131 03:15:54.654961   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.654963   790 net.cpp:159] Memory required for data: 411666800
I0131 03:15:54.654973   790 layer_factory.hpp:77] Creating layer relu6
I0131 03:15:54.654983   790 net.cpp:94] Creating Layer relu6
I0131 03:15:54.655002   790 net.cpp:435] relu6 <- fc6
I0131 03:15:54.655010   790 net.cpp:409] relu6 -> relu6
I0131 03:15:54.655036   790 net.cpp:144] Setting up relu6
I0131 03:15:54.655041   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.655043   790 net.cpp:159] Memory required for data: 412486000
I0131 03:15:54.655046   790 layer_factory.hpp:77] Creating layer drop6
I0131 03:15:54.655055   790 net.cpp:94] Creating Layer drop6
I0131 03:15:54.655058   790 net.cpp:435] drop6 <- relu6
I0131 03:15:54.655063   790 net.cpp:409] drop6 -> drop6
I0131 03:15:54.655086   790 net.cpp:144] Setting up drop6
I0131 03:15:54.655091   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.655093   790 net.cpp:159] Memory required for data: 413305200
I0131 03:15:54.655110   790 layer_factory.hpp:77] Creating layer fc7
I0131 03:15:54.655118   790 net.cpp:94] Creating Layer fc7
I0131 03:15:54.655123   790 net.cpp:435] fc7 <- drop6
I0131 03:15:54.655128   790 net.cpp:409] fc7 -> fc7
I0131 03:15:54.799773   790 net.cpp:144] Setting up fc7
I0131 03:15:54.799801   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.799804   790 net.cpp:159] Memory required for data: 414124400
I0131 03:15:54.799813   790 layer_factory.hpp:77] Creating layer bn7
I0131 03:15:54.799825   790 net.cpp:94] Creating Layer bn7
I0131 03:15:54.799829   790 net.cpp:435] bn7 <- fc7
I0131 03:15:54.799837   790 net.cpp:409] bn7 -> scale7
I0131 03:15:54.800364   790 net.cpp:144] Setting up bn7
I0131 03:15:54.800369   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.800374   790 net.cpp:159] Memory required for data: 414943600
I0131 03:15:54.800384   790 layer_factory.hpp:77] Creating layer relu7
I0131 03:15:54.800391   790 net.cpp:94] Creating Layer relu7
I0131 03:15:54.800395   790 net.cpp:435] relu7 <- scale7
I0131 03:15:54.800401   790 net.cpp:409] relu7 -> relu7
I0131 03:15:54.800422   790 net.cpp:144] Setting up relu7
I0131 03:15:54.800426   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.800431   790 net.cpp:159] Memory required for data: 415762800
I0131 03:15:54.800433   790 layer_factory.hpp:77] Creating layer drop7
I0131 03:15:54.800441   790 net.cpp:94] Creating Layer drop7
I0131 03:15:54.800443   790 net.cpp:435] drop7 <- relu7
I0131 03:15:54.800448   790 net.cpp:409] drop7 -> drop7
I0131 03:15:54.800474   790 net.cpp:144] Setting up drop7
I0131 03:15:54.800480   790 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:54.800483   790 net.cpp:159] Memory required for data: 416582000
I0131 03:15:54.800487   790 layer_factory.hpp:77] Creating layer fc8
I0131 03:15:54.800493   790 net.cpp:94] Creating Layer fc8
I0131 03:15:54.800498   790 net.cpp:435] fc8 <- drop7
I0131 03:15:54.800503   790 net.cpp:409] fc8 -> fc8
I0131 03:15:54.801395   790 net.cpp:144] Setting up fc8
I0131 03:15:54.801409   790 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:54.801412   790 net.cpp:159] Memory required for data: 416582400
I0131 03:15:54.801420   790 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0131 03:15:54.801427   790 net.cpp:94] Creating Layer fc8_fc8_0_split
I0131 03:15:54.801431   790 net.cpp:435] fc8_fc8_0_split <- fc8
I0131 03:15:54.801439   790 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0131 03:15:54.801446   790 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0131 03:15:54.801478   790 net.cpp:144] Setting up fc8_fc8_0_split
I0131 03:15:54.801483   790 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:54.801487   790 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:54.801491   790 net.cpp:159] Memory required for data: 416583200
I0131 03:15:54.801493   790 layer_factory.hpp:77] Creating layer loss
I0131 03:15:54.801501   790 net.cpp:94] Creating Layer loss
I0131 03:15:54.801504   790 net.cpp:435] loss <- fc8_fc8_0_split_0
I0131 03:15:54.801508   790 net.cpp:435] loss <- label_data_1_split_0
I0131 03:15:54.801514   790 net.cpp:409] loss -> loss
I0131 03:15:54.801524   790 layer_factory.hpp:77] Creating layer loss
I0131 03:15:54.801595   790 net.cpp:144] Setting up loss
I0131 03:15:54.801600   790 net.cpp:151] Top shape: (1)
I0131 03:15:54.801602   790 net.cpp:154]     with loss weight 1
I0131 03:15:54.801632   790 net.cpp:159] Memory required for data: 416583204
I0131 03:15:54.801635   790 layer_factory.hpp:77] Creating layer accuracy-top1
I0131 03:15:54.801642   790 net.cpp:94] Creating Layer accuracy-top1
I0131 03:15:54.801646   790 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0131 03:15:54.801651   790 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0131 03:15:54.801656   790 net.cpp:409] accuracy-top1 -> top-1
I0131 03:15:54.801664   790 net.cpp:144] Setting up accuracy-top1
I0131 03:15:54.801669   790 net.cpp:151] Top shape: (1)
I0131 03:15:54.801672   790 net.cpp:159] Memory required for data: 416583208
I0131 03:15:54.801676   790 net.cpp:222] accuracy-top1 does not need backward computation.
I0131 03:15:54.801697   790 net.cpp:220] loss needs backward computation.
I0131 03:15:54.801702   790 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0131 03:15:54.801704   790 net.cpp:220] fc8 needs backward computation.
I0131 03:15:54.801709   790 net.cpp:220] drop7 needs backward computation.
I0131 03:15:54.801712   790 net.cpp:220] relu7 needs backward computation.
I0131 03:15:54.801717   790 net.cpp:220] bn7 needs backward computation.
I0131 03:15:54.801720   790 net.cpp:220] fc7 needs backward computation.
I0131 03:15:54.801725   790 net.cpp:220] drop6 needs backward computation.
I0131 03:15:54.801728   790 net.cpp:220] relu6 needs backward computation.
I0131 03:15:54.801733   790 net.cpp:220] fc6 needs backward computation.
I0131 03:15:54.801736   790 net.cpp:220] pool5 needs backward computation.
I0131 03:15:54.801740   790 net.cpp:220] relu5 needs backward computation.
I0131 03:15:54.801744   790 net.cpp:220] conv5 needs backward computation.
I0131 03:15:54.801748   790 net.cpp:220] relu4 needs backward computation.
I0131 03:15:54.801751   790 net.cpp:220] conv4 needs backward computation.
I0131 03:15:54.801756   790 net.cpp:220] relu3 needs backward computation.
I0131 03:15:54.801759   790 net.cpp:220] conv3 needs backward computation.
I0131 03:15:54.801764   790 net.cpp:220] pool2 needs backward computation.
I0131 03:15:54.801766   790 net.cpp:220] relu2 needs backward computation.
I0131 03:15:54.801771   790 net.cpp:220] bn2 needs backward computation.
I0131 03:15:54.801774   790 net.cpp:220] conv2 needs backward computation.
I0131 03:15:54.801779   790 net.cpp:220] pool1 needs backward computation.
I0131 03:15:54.801782   790 net.cpp:220] relu1 needs backward computation.
I0131 03:15:54.801786   790 net.cpp:220] bn1 needs backward computation.
I0131 03:15:54.801789   790 net.cpp:220] conv1 needs backward computation.
I0131 03:15:54.801793   790 net.cpp:222] label_data_1_split does not need backward computation.
I0131 03:15:54.801798   790 net.cpp:222] data does not need backward computation.
I0131 03:15:54.801802   790 net.cpp:264] This network produces output loss
I0131 03:15:54.801805   790 net.cpp:264] This network produces output top-1
I0131 03:15:54.801827   790 net.cpp:284] Network initialization done.
I0131 03:15:54.887044   790 model_transformer.cpp:80] layer: data
I0131 03:15:54.887079   790 model_transformer.cpp:80] layer: conv1
I0131 03:15:54.887228   790 model_transformer.cpp:80] layer: bn1
I0131 03:15:54.887274   790 model_transformer.cpp:80] layer: relu1
I0131 03:15:54.887279   790 model_transformer.cpp:80] layer: pool1
I0131 03:15:54.887287   790 model_transformer.cpp:80] layer: conv2
I0131 03:15:54.890082   790 model_transformer.cpp:80] layer: bn2
I0131 03:15:54.890134   790 model_transformer.cpp:80] layer: relu2
I0131 03:15:54.890142   790 model_transformer.cpp:80] layer: pool2
I0131 03:15:54.890149   790 model_transformer.cpp:80] layer: conv3
I0131 03:15:54.892648   790 model_transformer.cpp:80] layer: relu3
I0131 03:15:54.892660   790 model_transformer.cpp:80] layer: conv4
I0131 03:15:54.899179   790 model_transformer.cpp:80] layer: relu4
I0131 03:15:54.899201   790 model_transformer.cpp:80] layer: conv5
I0131 03:15:54.901360   790 model_transformer.cpp:80] layer: relu5
I0131 03:15:54.901372   790 model_transformer.cpp:80] layer: pool5
I0131 03:15:54.901391   790 model_transformer.cpp:80] layer: fc6
I0131 03:15:56.463841   790 model_transformer.cpp:80] layer: relu6
I0131 03:15:56.463870   790 model_transformer.cpp:80] layer: drop6
I0131 03:15:56.463876   790 model_transformer.cpp:80] layer: fc7
I0131 03:15:56.760097   790 model_transformer.cpp:80] layer: bn7
I0131 03:15:56.760243   790 model_transformer.cpp:80] layer: relu7
I0131 03:15:56.760252   790 model_transformer.cpp:80] layer: drop7
I0131 03:15:56.760259   790 model_transformer.cpp:80] layer: fc8
I0131 03:15:56.760372   790 model_transformer.cpp:80] layer: loss
Output transformed caffemodel: transformed.caffemodel

# get flops and the number of parameters of a model
$PRUNE_ROOT/deephi_compress stat -model ${WORK_DIR}/train_val.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_stat_alexnetBNnoLRN.txt
I0131 03:15:58.891110   857 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0131 03:15:58.893219   857 net.cpp:52] Initializing net from parameters: 
name: "alexnetBNnoLRN m2 (as m3 but less DROP and less BN)"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 106
    mean_value: 116
    mean_value: 124
  }
  data_param {
    source: "cats-vs-dogs/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc7"
  top: "bn7"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "bn7"
  top: "scale7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "scale7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
I0131 03:15:58.893430   857 layer_factory.hpp:77] Creating layer data
I0131 03:15:58.894522   857 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 03:15:58.895231   857 net.cpp:94] Creating Layer data
I0131 03:15:58.895258   857 net.cpp:409] data -> data
I0131 03:15:58.895283   857 net.cpp:409] data -> label
I0131 03:15:58.897016   892 db_lmdb.cpp:35] Opened lmdb cats-vs-dogs/input/lmdb/valid_lmdb
I0131 03:15:58.897054   892 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0131 03:15:58.897348   857 data_layer.cpp:78] ReshapePrefetch 50, 3, 227, 227
I0131 03:15:58.897379   857 data_layer.cpp:83] output data size: 50,3,227,227
I0131 03:15:58.984671   857 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0131 03:15:58.984735   857 net.cpp:144] Setting up data
I0131 03:15:58.984742   857 net.cpp:151] Top shape: 50 3 227 227 (7729350)
I0131 03:15:58.984745   857 net.cpp:151] Top shape: 50 (50)
I0131 03:15:58.984764   857 net.cpp:159] Memory required for data: 30917600
I0131 03:15:58.984774   857 layer_factory.hpp:77] Creating layer label_data_1_split
I0131 03:15:58.984784   857 net.cpp:94] Creating Layer label_data_1_split
I0131 03:15:58.984788   857 net.cpp:435] label_data_1_split <- label
I0131 03:15:58.984815   857 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0131 03:15:58.984825   857 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0131 03:15:58.984836   857 net.cpp:144] Setting up label_data_1_split
I0131 03:15:58.984839   857 net.cpp:151] Top shape: 50 (50)
I0131 03:15:58.984843   857 net.cpp:151] Top shape: 50 (50)
I0131 03:15:58.984844   857 net.cpp:159] Memory required for data: 30918000
I0131 03:15:58.984848   857 layer_factory.hpp:77] Creating layer conv1
I0131 03:15:58.984856   857 net.cpp:94] Creating Layer conv1
I0131 03:15:58.984859   857 net.cpp:435] conv1 <- data
I0131 03:15:58.984863   857 net.cpp:409] conv1 -> conv1
I0131 03:15:58.985476   857 net.cpp:144] Setting up conv1
I0131 03:15:58.985481   857 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:58.985484   857 net.cpp:159] Memory required for data: 88998000
I0131 03:15:58.985502   857 layer_factory.hpp:77] Creating layer bn1
I0131 03:15:58.985509   857 net.cpp:94] Creating Layer bn1
I0131 03:15:58.985512   857 net.cpp:435] bn1 <- conv1
I0131 03:15:58.985533   857 net.cpp:409] bn1 -> bn1
I0131 03:15:58.985569   857 net.cpp:144] Setting up bn1
I0131 03:15:58.985574   857 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:58.985579   857 net.cpp:159] Memory required for data: 147078000
I0131 03:15:58.985589   857 layer_factory.hpp:77] Creating layer scale1
I0131 03:15:58.985595   857 net.cpp:94] Creating Layer scale1
I0131 03:15:58.985599   857 net.cpp:435] scale1 <- bn1
I0131 03:15:58.985602   857 net.cpp:409] scale1 -> scale1
I0131 03:15:58.985611   857 layer_factory.hpp:77] Creating layer scale1
I0131 03:15:58.985628   857 net.cpp:144] Setting up scale1
I0131 03:15:58.985635   857 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:58.985636   857 net.cpp:159] Memory required for data: 205158000
I0131 03:15:58.985642   857 layer_factory.hpp:77] Creating layer relu1
I0131 03:15:58.985647   857 net.cpp:94] Creating Layer relu1
I0131 03:15:58.985651   857 net.cpp:435] relu1 <- scale1
I0131 03:15:58.985654   857 net.cpp:409] relu1 -> relu1
I0131 03:15:58.985664   857 net.cpp:144] Setting up relu1
I0131 03:15:58.985667   857 net.cpp:151] Top shape: 50 96 55 55 (14520000)
I0131 03:15:58.985671   857 net.cpp:159] Memory required for data: 263238000
I0131 03:15:58.985673   857 layer_factory.hpp:77] Creating layer pool1
I0131 03:15:58.985678   857 net.cpp:94] Creating Layer pool1
I0131 03:15:58.985680   857 net.cpp:435] pool1 <- relu1
I0131 03:15:58.985685   857 net.cpp:409] pool1 -> pool1
I0131 03:15:58.985697   857 net.cpp:144] Setting up pool1
I0131 03:15:58.985700   857 net.cpp:151] Top shape: 50 96 27 27 (3499200)
I0131 03:15:58.985702   857 net.cpp:159] Memory required for data: 277234800
I0131 03:15:58.985705   857 layer_factory.hpp:77] Creating layer conv2
I0131 03:15:58.985713   857 net.cpp:94] Creating Layer conv2
I0131 03:15:58.985715   857 net.cpp:435] conv2 <- pool1
I0131 03:15:58.985720   857 net.cpp:409] conv2 -> conv2
I0131 03:15:58.991394   857 net.cpp:144] Setting up conv2
I0131 03:15:58.991415   857 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:58.991417   857 net.cpp:159] Memory required for data: 314559600
I0131 03:15:58.991422   857 layer_factory.hpp:77] Creating layer bn2
I0131 03:15:58.991430   857 net.cpp:94] Creating Layer bn2
I0131 03:15:58.991433   857 net.cpp:435] bn2 <- conv2
I0131 03:15:58.991438   857 net.cpp:409] bn2 -> bn2
I0131 03:15:58.991485   857 net.cpp:144] Setting up bn2
I0131 03:15:58.991492   857 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:58.991493   857 net.cpp:159] Memory required for data: 351884400
I0131 03:15:58.991502   857 layer_factory.hpp:77] Creating layer scale2
I0131 03:15:58.991506   857 net.cpp:94] Creating Layer scale2
I0131 03:15:58.991510   857 net.cpp:435] scale2 <- bn2
I0131 03:15:58.991514   857 net.cpp:409] scale2 -> scale2
I0131 03:15:58.991523   857 layer_factory.hpp:77] Creating layer scale2
I0131 03:15:58.991534   857 net.cpp:144] Setting up scale2
I0131 03:15:58.991540   857 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:58.991542   857 net.cpp:159] Memory required for data: 389209200
I0131 03:15:58.991550   857 layer_factory.hpp:77] Creating layer relu2
I0131 03:15:58.991554   857 net.cpp:94] Creating Layer relu2
I0131 03:15:58.991557   857 net.cpp:435] relu2 <- scale2
I0131 03:15:58.991562   857 net.cpp:409] relu2 -> relu2
I0131 03:15:58.991569   857 net.cpp:144] Setting up relu2
I0131 03:15:58.991571   857 net.cpp:151] Top shape: 50 256 27 27 (9331200)
I0131 03:15:58.991575   857 net.cpp:159] Memory required for data: 426534000
I0131 03:15:58.991577   857 layer_factory.hpp:77] Creating layer pool2
I0131 03:15:58.991581   857 net.cpp:94] Creating Layer pool2
I0131 03:15:58.991585   857 net.cpp:435] pool2 <- relu2
I0131 03:15:58.991588   857 net.cpp:409] pool2 -> pool2
I0131 03:15:58.991595   857 net.cpp:144] Setting up pool2
I0131 03:15:58.991598   857 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:58.991600   857 net.cpp:159] Memory required for data: 435186800
I0131 03:15:58.991603   857 layer_factory.hpp:77] Creating layer conv3
I0131 03:15:58.991621   857 net.cpp:94] Creating Layer conv3
I0131 03:15:58.991624   857 net.cpp:435] conv3 <- pool2
I0131 03:15:58.991629   857 net.cpp:409] conv3 -> conv3
I0131 03:15:59.000041   857 net.cpp:144] Setting up conv3
I0131 03:15:59.000062   857 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:59.000064   857 net.cpp:159] Memory required for data: 448166000
I0131 03:15:59.000069   857 layer_factory.hpp:77] Creating layer relu3
I0131 03:15:59.000075   857 net.cpp:94] Creating Layer relu3
I0131 03:15:59.000077   857 net.cpp:435] relu3 <- conv3
I0131 03:15:59.000082   857 net.cpp:409] relu3 -> relu3
I0131 03:15:59.000087   857 net.cpp:144] Setting up relu3
I0131 03:15:59.000092   857 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:59.000093   857 net.cpp:159] Memory required for data: 461145200
I0131 03:15:59.000097   857 layer_factory.hpp:77] Creating layer conv4
I0131 03:15:59.000102   857 net.cpp:94] Creating Layer conv4
I0131 03:15:59.000105   857 net.cpp:435] conv4 <- relu3
I0131 03:15:59.000109   857 net.cpp:409] conv4 -> conv4
I0131 03:15:59.013742   857 net.cpp:144] Setting up conv4
I0131 03:15:59.013758   857 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:59.013761   857 net.cpp:159] Memory required for data: 474124400
I0131 03:15:59.013767   857 layer_factory.hpp:77] Creating layer relu4
I0131 03:15:59.013772   857 net.cpp:94] Creating Layer relu4
I0131 03:15:59.013774   857 net.cpp:435] relu4 <- conv4
I0131 03:15:59.013780   857 net.cpp:409] relu4 -> relu4
I0131 03:15:59.013787   857 net.cpp:144] Setting up relu4
I0131 03:15:59.013789   857 net.cpp:151] Top shape: 50 384 13 13 (3244800)
I0131 03:15:59.013792   857 net.cpp:159] Memory required for data: 487103600
I0131 03:15:59.013794   857 layer_factory.hpp:77] Creating layer conv5
I0131 03:15:59.013803   857 net.cpp:94] Creating Layer conv5
I0131 03:15:59.013804   857 net.cpp:435] conv5 <- relu4
I0131 03:15:59.013808   857 net.cpp:409] conv5 -> conv5
I0131 03:15:59.022120   857 net.cpp:144] Setting up conv5
I0131 03:15:59.022135   857 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:59.022137   857 net.cpp:159] Memory required for data: 495756400
I0131 03:15:59.022142   857 layer_factory.hpp:77] Creating layer relu5
I0131 03:15:59.022147   857 net.cpp:94] Creating Layer relu5
I0131 03:15:59.022150   857 net.cpp:435] relu5 <- conv5
I0131 03:15:59.022155   857 net.cpp:409] relu5 -> relu5
I0131 03:15:59.022161   857 net.cpp:144] Setting up relu5
I0131 03:15:59.022163   857 net.cpp:151] Top shape: 50 256 13 13 (2163200)
I0131 03:15:59.022166   857 net.cpp:159] Memory required for data: 504409200
I0131 03:15:59.022167   857 layer_factory.hpp:77] Creating layer pool5
I0131 03:15:59.022172   857 net.cpp:94] Creating Layer pool5
I0131 03:15:59.022177   857 net.cpp:435] pool5 <- relu5
I0131 03:15:59.022181   857 net.cpp:409] pool5 -> pool5
I0131 03:15:59.022191   857 net.cpp:144] Setting up pool5
I0131 03:15:59.022194   857 net.cpp:151] Top shape: 50 256 6 6 (460800)
I0131 03:15:59.022197   857 net.cpp:159] Memory required for data: 506252400
I0131 03:15:59.022198   857 layer_factory.hpp:77] Creating layer fc6
I0131 03:15:59.022204   857 net.cpp:94] Creating Layer fc6
I0131 03:15:59.022207   857 net.cpp:435] fc6 <- pool5
I0131 03:15:59.022210   857 net.cpp:409] fc6 -> fc6
I0131 03:15:59.362529   857 net.cpp:144] Setting up fc6
I0131 03:15:59.362550   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.362553   857 net.cpp:159] Memory required for data: 507071600
I0131 03:15:59.362577   857 layer_factory.hpp:77] Creating layer relu6
I0131 03:15:59.362586   857 net.cpp:94] Creating Layer relu6
I0131 03:15:59.362588   857 net.cpp:435] relu6 <- fc6
I0131 03:15:59.362594   857 net.cpp:409] relu6 -> relu6
I0131 03:15:59.362606   857 net.cpp:144] Setting up relu6
I0131 03:15:59.362608   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.362612   857 net.cpp:159] Memory required for data: 507890800
I0131 03:15:59.362613   857 layer_factory.hpp:77] Creating layer drop6
I0131 03:15:59.362618   857 net.cpp:94] Creating Layer drop6
I0131 03:15:59.362638   857 net.cpp:435] drop6 <- relu6
I0131 03:15:59.362643   857 net.cpp:409] drop6 -> drop6
I0131 03:15:59.362648   857 net.cpp:144] Setting up drop6
I0131 03:15:59.362651   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.362653   857 net.cpp:159] Memory required for data: 508710000
I0131 03:15:59.362655   857 layer_factory.hpp:77] Creating layer fc7
I0131 03:15:59.362661   857 net.cpp:94] Creating Layer fc7
I0131 03:15:59.362663   857 net.cpp:435] fc7 <- drop6
I0131 03:15:59.362668   857 net.cpp:409] fc7 -> fc7
I0131 03:15:59.512369   857 net.cpp:144] Setting up fc7
I0131 03:15:59.512392   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.512395   857 net.cpp:159] Memory required for data: 509529200
I0131 03:15:59.512419   857 layer_factory.hpp:77] Creating layer bn7
I0131 03:15:59.512428   857 net.cpp:94] Creating Layer bn7
I0131 03:15:59.512432   857 net.cpp:435] bn7 <- fc7
I0131 03:15:59.512440   857 net.cpp:409] bn7 -> bn7
I0131 03:15:59.512502   857 net.cpp:144] Setting up bn7
I0131 03:15:59.512507   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.512509   857 net.cpp:159] Memory required for data: 510348400
I0131 03:15:59.512537   857 layer_factory.hpp:77] Creating layer scale7
I0131 03:15:59.512543   857 net.cpp:94] Creating Layer scale7
I0131 03:15:59.512547   857 net.cpp:435] scale7 <- bn7
I0131 03:15:59.512550   857 net.cpp:409] scale7 -> scale7
I0131 03:15:59.512567   857 layer_factory.hpp:77] Creating layer scale7
I0131 03:15:59.512586   857 net.cpp:144] Setting up scale7
I0131 03:15:59.512590   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.512593   857 net.cpp:159] Memory required for data: 511167600
I0131 03:15:59.512596   857 layer_factory.hpp:77] Creating layer relu7
I0131 03:15:59.512601   857 net.cpp:94] Creating Layer relu7
I0131 03:15:59.512603   857 net.cpp:435] relu7 <- scale7
I0131 03:15:59.512609   857 net.cpp:409] relu7 -> relu7
I0131 03:15:59.512615   857 net.cpp:144] Setting up relu7
I0131 03:15:59.512619   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.512620   857 net.cpp:159] Memory required for data: 511986800
I0131 03:15:59.512622   857 layer_factory.hpp:77] Creating layer drop7
I0131 03:15:59.512627   857 net.cpp:94] Creating Layer drop7
I0131 03:15:59.512630   857 net.cpp:435] drop7 <- relu7
I0131 03:15:59.512634   857 net.cpp:409] drop7 -> drop7
I0131 03:15:59.512639   857 net.cpp:144] Setting up drop7
I0131 03:15:59.512641   857 net.cpp:151] Top shape: 50 4096 (204800)
I0131 03:15:59.512643   857 net.cpp:159] Memory required for data: 512806000
I0131 03:15:59.512645   857 layer_factory.hpp:77] Creating layer fc8
I0131 03:15:59.512653   857 net.cpp:94] Creating Layer fc8
I0131 03:15:59.512655   857 net.cpp:435] fc8 <- drop7
I0131 03:15:59.512660   857 net.cpp:409] fc8 -> fc8
I0131 03:15:59.512759   857 net.cpp:144] Setting up fc8
I0131 03:15:59.512763   857 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:59.512768   857 net.cpp:159] Memory required for data: 512806400
I0131 03:15:59.512771   857 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0131 03:15:59.512775   857 net.cpp:94] Creating Layer fc8_fc8_0_split
I0131 03:15:59.512778   857 net.cpp:435] fc8_fc8_0_split <- fc8
I0131 03:15:59.512782   857 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0131 03:15:59.512787   857 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0131 03:15:59.512792   857 net.cpp:144] Setting up fc8_fc8_0_split
I0131 03:15:59.512795   857 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:59.512799   857 net.cpp:151] Top shape: 50 2 (100)
I0131 03:15:59.512800   857 net.cpp:159] Memory required for data: 512807200
I0131 03:15:59.512802   857 layer_factory.hpp:77] Creating layer loss
I0131 03:15:59.512809   857 net.cpp:94] Creating Layer loss
I0131 03:15:59.512811   857 net.cpp:435] loss <- fc8_fc8_0_split_0
I0131 03:15:59.512814   857 net.cpp:435] loss <- label_data_1_split_0
I0131 03:15:59.512818   857 net.cpp:409] loss -> loss
I0131 03:15:59.512828   857 layer_factory.hpp:77] Creating layer loss
I0131 03:15:59.512858   857 net.cpp:144] Setting up loss
I0131 03:15:59.512863   857 net.cpp:151] Top shape: (1)
I0131 03:15:59.512866   857 net.cpp:154]     with loss weight 1
I0131 03:15:59.512892   857 net.cpp:159] Memory required for data: 512807204
I0131 03:15:59.512895   857 layer_factory.hpp:77] Creating layer accuracy-top1
I0131 03:15:59.512899   857 net.cpp:94] Creating Layer accuracy-top1
I0131 03:15:59.512902   857 net.cpp:435] accuracy-top1 <- fc8_fc8_0_split_1
I0131 03:15:59.512907   857 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0131 03:15:59.512910   857 net.cpp:409] accuracy-top1 -> top-1
I0131 03:15:59.512915   857 net.cpp:144] Setting up accuracy-top1
I0131 03:15:59.512918   857 net.cpp:151] Top shape: (1)
I0131 03:15:59.512920   857 net.cpp:159] Memory required for data: 512807208
I0131 03:15:59.512923   857 net.cpp:222] accuracy-top1 does not need backward computation.
I0131 03:15:59.512930   857 net.cpp:220] loss needs backward computation.
I0131 03:15:59.512933   857 net.cpp:220] fc8_fc8_0_split needs backward computation.
I0131 03:15:59.512936   857 net.cpp:220] fc8 needs backward computation.
I0131 03:15:59.512939   857 net.cpp:220] drop7 needs backward computation.
I0131 03:15:59.512941   857 net.cpp:220] relu7 needs backward computation.
I0131 03:15:59.512943   857 net.cpp:220] scale7 needs backward computation.
I0131 03:15:59.512946   857 net.cpp:220] bn7 needs backward computation.
I0131 03:15:59.512949   857 net.cpp:220] fc7 needs backward computation.
I0131 03:15:59.512951   857 net.cpp:220] drop6 needs backward computation.
I0131 03:15:59.512955   857 net.cpp:220] relu6 needs backward computation.
I0131 03:15:59.512959   857 net.cpp:220] fc6 needs backward computation.
I0131 03:15:59.512961   857 net.cpp:220] pool5 needs backward computation.
I0131 03:15:59.512964   857 net.cpp:220] relu5 needs backward computation.
I0131 03:15:59.512965   857 net.cpp:220] conv5 needs backward computation.
I0131 03:15:59.512969   857 net.cpp:220] relu4 needs backward computation.
I0131 03:15:59.512972   857 net.cpp:220] conv4 needs backward computation.
I0131 03:15:59.512974   857 net.cpp:220] relu3 needs backward computation.
I0131 03:15:59.512977   857 net.cpp:220] conv3 needs backward computation.
I0131 03:15:59.512979   857 net.cpp:220] pool2 needs backward computation.
I0131 03:15:59.512982   857 net.cpp:220] relu2 needs backward computation.
I0131 03:15:59.512985   857 net.cpp:220] scale2 needs backward computation.
I0131 03:15:59.512987   857 net.cpp:220] bn2 needs backward computation.
I0131 03:15:59.512990   857 net.cpp:220] conv2 needs backward computation.
I0131 03:15:59.512993   857 net.cpp:220] pool1 needs backward computation.
I0131 03:15:59.512995   857 net.cpp:220] relu1 needs backward computation.
I0131 03:15:59.512998   857 net.cpp:220] scale1 needs backward computation.
I0131 03:15:59.513000   857 net.cpp:220] bn1 needs backward computation.
I0131 03:15:59.513003   857 net.cpp:220] conv1 needs backward computation.
I0131 03:15:59.513006   857 net.cpp:222] label_data_1_split does not need backward computation.
I0131 03:15:59.513010   857 net.cpp:222] data does not need backward computation.
I0131 03:15:59.513011   857 net.cpp:264] This network produces output loss
I0131 03:15:59.513015   857 net.cpp:264] This network produces output top-1
I0131 03:15:59.513036   857 net.cpp:284] Network initialization done.
I0131 03:15:59.513131   857 net_counter.cpp:58] Convolution layer conv1 ops: 211120800
I0131 03:15:59.513135   857 net_counter.cpp:62] Convolution layer conv1 params: 34944
I0131 03:15:59.513139   857 net_counter.cpp:62] BatchNorm layer bn1 params: 385
I0131 03:15:59.513142   857 net_counter.cpp:58] Convolution layer conv2 ops: 895981824
I0131 03:15:59.513144   857 net_counter.cpp:62] Convolution layer conv2 params: 614656
I0131 03:15:59.513146   857 net_counter.cpp:62] BatchNorm layer bn2 params: 1025
I0131 03:15:59.513150   857 net_counter.cpp:58] Convolution layer conv3 ops: 299105664
I0131 03:15:59.513151   857 net_counter.cpp:62] Convolution layer conv3 params: 885120
I0131 03:15:59.513160   857 net_counter.cpp:58] Convolution layer conv4 ops: 448626048
I0131 03:15:59.513162   857 net_counter.cpp:62] Convolution layer conv4 params: 1327488
I0131 03:15:59.513165   857 net_counter.cpp:58] Convolution layer conv5 ops: 299084032
I0131 03:15:59.513167   857 net_counter.cpp:62] Convolution layer conv5 params: 884992
I0131 03:15:59.513170   857 net_counter.cpp:62] BatchNorm layer bn7 params: 16385
I0131 03:15:59.513172   857 net_counter.cpp:68] Total operations: 2153918368
I0131 03:15:59.513175   857 net_counter.cpp:69] Total params: 3764995

(bvlc1v0_py27) danieleb@Prec5820Tow:~/ML$ ls -l
total 82744
drwxrwxr-x  3 danieleb danieleb     4096 nov  1 17:12 AWS
drwxrwxr-x  5 danieleb danieleb     4096 gen 11 11:16 caffe-demo
drwxrwxr-x  6 danieleb danieleb     4096 ago  7 10:54 CaffeTutorial
drwxrwxr-x  6 danieleb danieleb     4096 gen 25 15:48 cats-vs-dogs
drwxrwxr-x  5 danieleb danieleb     4096 gen 23 12:14 CATSvsDOGS
drwxrwxr-x  8 danieleb danieleb     4096 gen 23 08:22 cifar10
drwxrwxr-x 10 danieleb danieleb     4096 gen 23 12:15 CIFAR10
drwxrwxr-x  4 danieleb danieleb     4096 ago 10 13:14 cuda_lib_8.0
drwxrwxr-x  4 danieleb danieleb     4096 apr 17  2018 cuda_lib_9.0
drwxrwxr-x  4 danieleb danieleb     4096 gen 13 15:48 Daimler
drwxrwxr-x  2 danieleb danieleb     4096 gen 19 16:47 decent_output
drwxrwxr-x  9 danieleb danieleb     4096 gen  8 21:02 DNNDK
drwxrwxr-x  4 danieleb danieleb     4096 gen 20 17:04 JonCory
-rw-rw-r--  1 danieleb danieleb      531 gen 30 15:37 logfile_2_alexnetBNnoLRN.log.test
-rw-rw-r--  1 danieleb danieleb     9241 gen 30 15:37 logfile_2_alexnetBNnoLRN.log.train
drwxr-xr-x  6 danieleb danieleb     4096 ago 23 07:03 opencv-semantic-segmentation
drwxrwxr-x  7 danieleb danieleb     4096 ago  3 11:10 server_mm
drwxrwxr-x  4 danieleb danieleb     4096 dic 16 15:15 SSD
drwxrwxr-x  3 danieleb danieleb     4096 gen 30 14:54 TechDocs-Edge-AI-Platform-Tutorials-develop
-rw-rw-r--  1 danieleb danieleb 84635669 gen 31 03:15 transformed.caffemodel
drwxr-xr-x  7 danieleb danieleb     4096 gen 25 13:05 Zucchetti
(bvlc1v0_py27) danieleb@Prec5820Tow:~/ML$ mv transformed.caffemodel cats-vs-dogs/deephi/alexnetBNnoLRN/pruning/
(bvlc1v0_py27) danieleb@Prec5820Tow:~/ML$ 



