ln: failed to create symbolic link './float.caffemodel': File exists
I0109 19:08:33.636512  2568 deephi_compress.cpp:203] Starting analysis of /home/ML/cifar10/deephi/miniVggNet/pruning/float.caffemodel
I0109 19:08:33.636693  2568 sens_analyser.cpp:145] Analysis completed 0%
I0109 19:08:43.961761  2568 sens_analyser.cpp:212] Analysing layer [conv1] done
I0109 19:08:43.961794  2568 sens_analyser.cpp:213] Analysis completed 25%
I0109 19:08:52.471073  2568 sens_analyser.cpp:212] Analysing layer [conv2] done
I0109 19:08:52.471109  2568 sens_analyser.cpp:213] Analysis completed 50%
I0109 19:09:00.968978  2568 sens_analyser.cpp:212] Analysing layer [conv3] done
I0109 19:09:00.969022  2568 sens_analyser.cpp:213] Analysis completed 75%
I0109 19:09:09.415130  2568 sens_analyser.cpp:212] Analysing layer [conv4] done
I0109 19:09:09.415179  2568 sens_analyser.cpp:213] Analysis completed 100%
I0109 19:09:09.415400  2568 deephi_compress.cpp:205] Analysis done.
Now you can compress the model with the following command:
deephi_compress compress -config config0.prototxt
I0109 19:09:09.528358  2801 pruning_runner.cpp:190] Sens info found, use it.
I0109 19:09:09.543416  2801 pruning_runner.cpp:217] Start compressing, please wait...
I0109 19:09:10.566133  2801 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 19:09:10.566453  2801 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 19:09:10.566833  2801 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:09:10.567052  2801 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:09:10.567184  2801 layer_factory.hpp:77] Creating layer data
I0109 19:09:10.567301  2801 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:10.567400  2801 net.cpp:94] Creating Layer data
I0109 19:09:10.567420  2801 net.cpp:409] data -> data
I0109 19:09:10.567435  2801 net.cpp:409] data -> label
I0109 19:09:10.568363  2828 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:09:10.568403  2828 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:09:10.568476  2801 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:09:10.568572  2801 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:09:10.573971  2801 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:10.574030  2801 net.cpp:144] Setting up data
I0109 19:09:10.574049  2801 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:09:10.574059  2801 net.cpp:151] Top shape: 50 (50)
I0109 19:09:10.574064  2801 net.cpp:159] Memory required for data: 614600
I0109 19:09:10.574084  2801 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:09:10.574098  2801 net.cpp:94] Creating Layer label_data_1_split
I0109 19:09:10.574112  2801 net.cpp:435] label_data_1_split <- label
I0109 19:09:10.574124  2801 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:09:10.574146  2801 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:09:10.574158  2801 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:09:10.574240  2801 net.cpp:144] Setting up label_data_1_split
I0109 19:09:10.574256  2801 net.cpp:151] Top shape: 50 (50)
I0109 19:09:10.574283  2801 net.cpp:151] Top shape: 50 (50)
I0109 19:09:10.574292  2801 net.cpp:151] Top shape: 50 (50)
I0109 19:09:10.574298  2801 net.cpp:159] Memory required for data: 615200
I0109 19:09:10.574304  2801 layer_factory.hpp:77] Creating layer conv1
I0109 19:09:10.574322  2801 net.cpp:94] Creating Layer conv1
I0109 19:09:10.574331  2801 net.cpp:435] conv1 <- data
I0109 19:09:10.574340  2801 net.cpp:409] conv1 -> conv1
I0109 19:09:10.575222  2801 net.cpp:144] Setting up conv1
I0109 19:09:10.575244  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.575253  2801 net.cpp:159] Memory required for data: 7168800
I0109 19:09:10.575268  2801 layer_factory.hpp:77] Creating layer bn1
I0109 19:09:10.575282  2801 net.cpp:94] Creating Layer bn1
I0109 19:09:10.575292  2801 net.cpp:435] bn1 <- conv1
I0109 19:09:10.575302  2801 net.cpp:409] bn1 -> scale1
I0109 19:09:10.576287  2801 net.cpp:144] Setting up bn1
I0109 19:09:10.576304  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.576313  2801 net.cpp:159] Memory required for data: 13722400
I0109 19:09:10.576333  2801 layer_factory.hpp:77] Creating layer relu1
I0109 19:09:10.576346  2801 net.cpp:94] Creating Layer relu1
I0109 19:09:10.576354  2801 net.cpp:435] relu1 <- scale1
I0109 19:09:10.576364  2801 net.cpp:409] relu1 -> relu1
I0109 19:09:10.576452  2801 net.cpp:144] Setting up relu1
I0109 19:09:10.576469  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.576478  2801 net.cpp:159] Memory required for data: 20276000
I0109 19:09:10.576485  2801 layer_factory.hpp:77] Creating layer conv2
I0109 19:09:10.576501  2801 net.cpp:94] Creating Layer conv2
I0109 19:09:10.576509  2801 net.cpp:435] conv2 <- relu1
I0109 19:09:10.576521  2801 net.cpp:409] conv2 -> conv2
I0109 19:09:10.577394  2801 net.cpp:144] Setting up conv2
I0109 19:09:10.577420  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.577428  2801 net.cpp:159] Memory required for data: 26829600
I0109 19:09:10.577442  2801 layer_factory.hpp:77] Creating layer bn2
I0109 19:09:10.577458  2801 net.cpp:94] Creating Layer bn2
I0109 19:09:10.577466  2801 net.cpp:435] bn2 <- conv2
I0109 19:09:10.577476  2801 net.cpp:409] bn2 -> scale2
I0109 19:09:10.578379  2801 net.cpp:144] Setting up bn2
I0109 19:09:10.578399  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.578408  2801 net.cpp:159] Memory required for data: 33383200
I0109 19:09:10.578423  2801 layer_factory.hpp:77] Creating layer relu2
I0109 19:09:10.578435  2801 net.cpp:94] Creating Layer relu2
I0109 19:09:10.578444  2801 net.cpp:435] relu2 <- scale2
I0109 19:09:10.578454  2801 net.cpp:409] relu2 -> relu2
I0109 19:09:10.578500  2801 net.cpp:144] Setting up relu2
I0109 19:09:10.578519  2801 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:10.578528  2801 net.cpp:159] Memory required for data: 39936800
I0109 19:09:10.578534  2801 layer_factory.hpp:77] Creating layer pool1
I0109 19:09:10.578549  2801 net.cpp:94] Creating Layer pool1
I0109 19:09:10.578557  2801 net.cpp:435] pool1 <- relu2
I0109 19:09:10.578567  2801 net.cpp:409] pool1 -> pool1
I0109 19:09:10.578667  2801 net.cpp:144] Setting up pool1
I0109 19:09:10.578686  2801 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:09:10.578694  2801 net.cpp:159] Memory required for data: 41575200
I0109 19:09:10.578702  2801 layer_factory.hpp:77] Creating layer drop1
I0109 19:09:10.578713  2801 net.cpp:94] Creating Layer drop1
I0109 19:09:10.578722  2801 net.cpp:435] drop1 <- pool1
I0109 19:09:10.578730  2801 net.cpp:409] drop1 -> drop1
I0109 19:09:10.578783  2801 net.cpp:144] Setting up drop1
I0109 19:09:10.578799  2801 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:09:10.578805  2801 net.cpp:159] Memory required for data: 43213600
I0109 19:09:10.578811  2801 layer_factory.hpp:77] Creating layer conv3
I0109 19:09:10.578830  2801 net.cpp:94] Creating Layer conv3
I0109 19:09:10.578836  2801 net.cpp:435] conv3 <- drop1
I0109 19:09:10.578846  2801 net.cpp:409] conv3 -> conv3
I0109 19:09:10.579798  2801 net.cpp:144] Setting up conv3
I0109 19:09:10.579854  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.579862  2801 net.cpp:159] Memory required for data: 46490400
I0109 19:09:10.579874  2801 layer_factory.hpp:77] Creating layer bn3
I0109 19:09:10.579890  2801 net.cpp:94] Creating Layer bn3
I0109 19:09:10.579900  2801 net.cpp:435] bn3 <- conv3
I0109 19:09:10.579910  2801 net.cpp:409] bn3 -> scale3
I0109 19:09:10.580569  2801 net.cpp:144] Setting up bn3
I0109 19:09:10.580590  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.580600  2801 net.cpp:159] Memory required for data: 49767200
I0109 19:09:10.580621  2801 layer_factory.hpp:77] Creating layer relu3
I0109 19:09:10.580632  2801 net.cpp:94] Creating Layer relu3
I0109 19:09:10.580639  2801 net.cpp:435] relu3 <- scale3
I0109 19:09:10.580651  2801 net.cpp:409] relu3 -> relu3
I0109 19:09:10.580688  2801 net.cpp:144] Setting up relu3
I0109 19:09:10.580705  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.580721  2801 net.cpp:159] Memory required for data: 53044000
I0109 19:09:10.580730  2801 layer_factory.hpp:77] Creating layer conv4
I0109 19:09:10.580745  2801 net.cpp:94] Creating Layer conv4
I0109 19:09:10.580754  2801 net.cpp:435] conv4 <- relu3
I0109 19:09:10.580765  2801 net.cpp:409] conv4 -> conv4
I0109 19:09:10.581270  2801 net.cpp:144] Setting up conv4
I0109 19:09:10.581321  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.581362  2801 net.cpp:159] Memory required for data: 56320800
I0109 19:09:10.581406  2801 layer_factory.hpp:77] Creating layer bn4
I0109 19:09:10.581455  2801 net.cpp:94] Creating Layer bn4
I0109 19:09:10.581496  2801 net.cpp:435] bn4 <- conv4
I0109 19:09:10.581538  2801 net.cpp:409] bn4 -> scale4
I0109 19:09:10.582728  2801 net.cpp:144] Setting up bn4
I0109 19:09:10.582754  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.582815  2801 net.cpp:159] Memory required for data: 59597600
I0109 19:09:10.582864  2801 layer_factory.hpp:77] Creating layer relu4
I0109 19:09:10.582911  2801 net.cpp:94] Creating Layer relu4
I0109 19:09:10.582957  2801 net.cpp:435] relu4 <- scale4
I0109 19:09:10.583000  2801 net.cpp:409] relu4 -> relu4
I0109 19:09:10.583076  2801 net.cpp:144] Setting up relu4
I0109 19:09:10.583118  2801 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:10.583158  2801 net.cpp:159] Memory required for data: 62874400
I0109 19:09:10.583204  2801 layer_factory.hpp:77] Creating layer pool2
I0109 19:09:10.583254  2801 net.cpp:94] Creating Layer pool2
I0109 19:09:10.583297  2801 net.cpp:435] pool2 <- relu4
I0109 19:09:10.583340  2801 net.cpp:409] pool2 -> pool2
I0109 19:09:10.583434  2801 net.cpp:144] Setting up pool2
I0109 19:09:10.583474  2801 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:09:10.583513  2801 net.cpp:159] Memory required for data: 63693600
I0109 19:09:10.583564  2801 layer_factory.hpp:77] Creating layer drop2
I0109 19:09:10.583606  2801 net.cpp:94] Creating Layer drop2
I0109 19:09:10.583652  2801 net.cpp:435] drop2 <- pool2
I0109 19:09:10.583696  2801 net.cpp:409] drop2 -> drop2
I0109 19:09:10.583791  2801 net.cpp:144] Setting up drop2
I0109 19:09:10.583839  2801 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:09:10.583858  2801 net.cpp:159] Memory required for data: 64512800
I0109 19:09:10.583865  2801 layer_factory.hpp:77] Creating layer fc1
I0109 19:09:10.583942  2801 net.cpp:94] Creating Layer fc1
I0109 19:09:10.583974  2801 net.cpp:435] fc1 <- drop2
I0109 19:09:10.584012  2801 net.cpp:409] fc1 -> fc1
I0109 19:09:10.604933  2801 net.cpp:144] Setting up fc1
I0109 19:09:10.604960  2801 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:10.604965  2801 net.cpp:159] Memory required for data: 64615200
I0109 19:09:10.604974  2801 layer_factory.hpp:77] Creating layer bn5
I0109 19:09:10.604988  2801 net.cpp:94] Creating Layer bn5
I0109 19:09:10.605006  2801 net.cpp:435] bn5 <- fc1
I0109 19:09:10.605020  2801 net.cpp:409] bn5 -> scale5
I0109 19:09:10.605655  2801 net.cpp:144] Setting up bn5
I0109 19:09:10.605674  2801 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:10.605679  2801 net.cpp:159] Memory required for data: 64717600
I0109 19:09:10.605722  2801 layer_factory.hpp:77] Creating layer relu5
I0109 19:09:10.605744  2801 net.cpp:94] Creating Layer relu5
I0109 19:09:10.605752  2801 net.cpp:435] relu5 <- scale5
I0109 19:09:10.605764  2801 net.cpp:409] relu5 -> relu5
I0109 19:09:10.605813  2801 net.cpp:144] Setting up relu5
I0109 19:09:10.605831  2801 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:10.605837  2801 net.cpp:159] Memory required for data: 64820000
I0109 19:09:10.605842  2801 layer_factory.hpp:77] Creating layer drop3
I0109 19:09:10.605854  2801 net.cpp:94] Creating Layer drop3
I0109 19:09:10.605870  2801 net.cpp:435] drop3 <- relu5
I0109 19:09:10.605880  2801 net.cpp:409] drop3 -> drop3
I0109 19:09:10.605932  2801 net.cpp:144] Setting up drop3
I0109 19:09:10.605947  2801 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:10.605952  2801 net.cpp:159] Memory required for data: 64922400
I0109 19:09:10.605955  2801 layer_factory.hpp:77] Creating layer fc2
I0109 19:09:10.605968  2801 net.cpp:94] Creating Layer fc2
I0109 19:09:10.605983  2801 net.cpp:435] fc2 <- drop3
I0109 19:09:10.605995  2801 net.cpp:409] fc2 -> fc2
I0109 19:09:10.606161  2801 net.cpp:144] Setting up fc2
I0109 19:09:10.606178  2801 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:10.606181  2801 net.cpp:159] Memory required for data: 64924400
I0109 19:09:10.606189  2801 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:09:10.606200  2801 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:09:10.606228  2801 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:09:10.606248  2801 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:09:10.606262  2801 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:09:10.606272  2801 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:09:10.606346  2801 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:09:10.606362  2801 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:10.606371  2801 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:10.606379  2801 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:10.606384  2801 net.cpp:159] Memory required for data: 64930400
I0109 19:09:10.606396  2801 layer_factory.hpp:77] Creating layer loss
I0109 19:09:10.606408  2801 net.cpp:94] Creating Layer loss
I0109 19:09:10.606417  2801 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:09:10.606432  2801 net.cpp:435] loss <- label_data_1_split_0
I0109 19:09:10.606444  2801 net.cpp:409] loss -> loss
I0109 19:09:10.606474  2801 layer_factory.hpp:77] Creating layer loss
I0109 19:09:10.606586  2801 net.cpp:144] Setting up loss
I0109 19:09:10.606604  2801 net.cpp:151] Top shape: (1)
I0109 19:09:10.606611  2801 net.cpp:154]     with loss weight 1
I0109 19:09:10.606634  2801 net.cpp:159] Memory required for data: 64930404
I0109 19:09:10.606642  2801 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:09:10.606662  2801 net.cpp:94] Creating Layer accuracy-top1
I0109 19:09:10.606672  2801 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:09:10.606679  2801 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:09:10.606698  2801 net.cpp:409] accuracy-top1 -> top-1
I0109 19:09:10.606714  2801 net.cpp:144] Setting up accuracy-top1
I0109 19:09:10.606727  2801 net.cpp:151] Top shape: (1)
I0109 19:09:10.606734  2801 net.cpp:159] Memory required for data: 64930408
I0109 19:09:10.606740  2801 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:09:10.606752  2801 net.cpp:94] Creating Layer accuracy-top5
I0109 19:09:10.606765  2801 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:09:10.606773  2801 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:09:10.606784  2801 net.cpp:409] accuracy-top5 -> top-5
I0109 19:09:10.606815  2801 net.cpp:144] Setting up accuracy-top5
I0109 19:09:10.606832  2801 net.cpp:151] Top shape: (1)
I0109 19:09:10.606838  2801 net.cpp:159] Memory required for data: 64930412
I0109 19:09:10.606844  2801 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:09:10.606861  2801 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:09:10.606884  2801 net.cpp:220] loss needs backward computation.
I0109 19:09:10.606892  2801 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:09:10.606905  2801 net.cpp:220] fc2 needs backward computation.
I0109 19:09:10.606914  2801 net.cpp:220] drop3 needs backward computation.
I0109 19:09:10.606921  2801 net.cpp:220] relu5 needs backward computation.
I0109 19:09:10.606941  2801 net.cpp:220] bn5 needs backward computation.
I0109 19:09:10.606958  2801 net.cpp:220] fc1 needs backward computation.
I0109 19:09:10.606967  2801 net.cpp:220] drop2 needs backward computation.
I0109 19:09:10.606974  2801 net.cpp:220] pool2 needs backward computation.
I0109 19:09:10.606984  2801 net.cpp:220] relu4 needs backward computation.
I0109 19:09:10.606992  2801 net.cpp:220] bn4 needs backward computation.
I0109 19:09:10.606998  2801 net.cpp:220] conv4 needs backward computation.
I0109 19:09:10.607004  2801 net.cpp:220] relu3 needs backward computation.
I0109 19:09:10.607012  2801 net.cpp:220] bn3 needs backward computation.
I0109 19:09:10.607017  2801 net.cpp:220] conv3 needs backward computation.
I0109 19:09:10.607025  2801 net.cpp:220] drop1 needs backward computation.
I0109 19:09:10.607033  2801 net.cpp:220] pool1 needs backward computation.
I0109 19:09:10.607040  2801 net.cpp:220] relu2 needs backward computation.
I0109 19:09:10.607048  2801 net.cpp:220] bn2 needs backward computation.
I0109 19:09:10.607056  2801 net.cpp:220] conv2 needs backward computation.
I0109 19:09:10.607065  2801 net.cpp:220] relu1 needs backward computation.
I0109 19:09:10.607072  2801 net.cpp:220] bn1 needs backward computation.
I0109 19:09:10.607080  2801 net.cpp:220] conv1 needs backward computation.
I0109 19:09:10.607089  2801 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:09:10.607100  2801 net.cpp:222] data does not need backward computation.
I0109 19:09:10.607106  2801 net.cpp:264] This network produces output loss
I0109 19:09:10.607112  2801 net.cpp:264] This network produces output top-1
I0109 19:09:10.607127  2801 net.cpp:264] This network produces output top-5
I0109 19:09:10.607164  2801 net.cpp:284] Network initialization done.
W0109 19:09:10.607475  2801 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:10.607877  2801 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:10.608157  2801 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:10.608458  2801 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:10.610539  2801 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I0109 19:09:10.610637  2801 caffe_interface.cpp:363] Running for 180 iterations.
I0109 19:09:10.620457  2801 caffe_interface.cpp:125] Batch 0, loss = 0.451429
I0109 19:09:10.620492  2801 caffe_interface.cpp:125] Batch 0, top-1 = 0.86
I0109 19:09:10.620506  2801 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0109 19:09:10.624979  2801 caffe_interface.cpp:125] Batch 1, loss = 0.21581
I0109 19:09:10.625005  2801 caffe_interface.cpp:125] Batch 1, top-1 = 0.92
I0109 19:09:10.625017  2801 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 19:09:10.629477  2801 caffe_interface.cpp:125] Batch 2, loss = 0.691021
I0109 19:09:10.629503  2801 caffe_interface.cpp:125] Batch 2, top-1 = 0.74
I0109 19:09:10.629516  2801 caffe_interface.cpp:125] Batch 2, top-5 = 0.98
I0109 19:09:10.634006  2801 caffe_interface.cpp:125] Batch 3, loss = 0.151094
I0109 19:09:10.634030  2801 caffe_interface.cpp:125] Batch 3, top-1 = 0.94
I0109 19:09:10.634042  2801 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 19:09:10.638510  2801 caffe_interface.cpp:125] Batch 4, loss = 0.410462
I0109 19:09:10.638550  2801 caffe_interface.cpp:125] Batch 4, top-1 = 0.86
I0109 19:09:10.638562  2801 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 19:09:10.643055  2801 caffe_interface.cpp:125] Batch 5, loss = 0.49295
I0109 19:09:10.643082  2801 caffe_interface.cpp:125] Batch 5, top-1 = 0.8
I0109 19:09:10.643093  2801 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 19:09:10.647557  2801 caffe_interface.cpp:125] Batch 6, loss = 0.712207
I0109 19:09:10.647581  2801 caffe_interface.cpp:125] Batch 6, top-1 = 0.84
I0109 19:09:10.647593  2801 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 19:09:10.652096  2801 caffe_interface.cpp:125] Batch 7, loss = 0.259046
I0109 19:09:10.652122  2801 caffe_interface.cpp:125] Batch 7, top-1 = 0.92
I0109 19:09:10.652132  2801 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 19:09:10.656608  2801 caffe_interface.cpp:125] Batch 8, loss = 0.726851
I0109 19:09:10.656633  2801 caffe_interface.cpp:125] Batch 8, top-1 = 0.76
I0109 19:09:10.656644  2801 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0109 19:09:10.661087  2801 caffe_interface.cpp:125] Batch 9, loss = 0.457058
I0109 19:09:10.661113  2801 caffe_interface.cpp:125] Batch 9, top-1 = 0.9
I0109 19:09:10.661123  2801 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 19:09:10.665624  2801 caffe_interface.cpp:125] Batch 10, loss = 0.40086
I0109 19:09:10.665647  2801 caffe_interface.cpp:125] Batch 10, top-1 = 0.86
I0109 19:09:10.665658  2801 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 19:09:10.670102  2801 caffe_interface.cpp:125] Batch 11, loss = 0.302908
I0109 19:09:10.670127  2801 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I0109 19:09:10.670137  2801 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 19:09:10.674593  2801 caffe_interface.cpp:125] Batch 12, loss = 0.537062
I0109 19:09:10.674618  2801 caffe_interface.cpp:125] Batch 12, top-1 = 0.84
I0109 19:09:10.674628  2801 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0109 19:09:10.679078  2801 caffe_interface.cpp:125] Batch 13, loss = 0.271153
I0109 19:09:10.679105  2801 caffe_interface.cpp:125] Batch 13, top-1 = 0.92
I0109 19:09:10.679116  2801 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0109 19:09:10.683583  2801 caffe_interface.cpp:125] Batch 14, loss = 0.342391
I0109 19:09:10.683607  2801 caffe_interface.cpp:125] Batch 14, top-1 = 0.92
I0109 19:09:10.683619  2801 caffe_interface.cpp:125] Batch 14, top-5 = 0.98
I0109 19:09:10.688069  2801 caffe_interface.cpp:125] Batch 15, loss = 0.544798
I0109 19:09:10.688093  2801 caffe_interface.cpp:125] Batch 15, top-1 = 0.84
I0109 19:09:10.688104  2801 caffe_interface.cpp:125] Batch 15, top-5 = 0.96
I0109 19:09:10.692562  2801 caffe_interface.cpp:125] Batch 16, loss = 0.200327
I0109 19:09:10.692587  2801 caffe_interface.cpp:125] Batch 16, top-1 = 0.9
I0109 19:09:10.692597  2801 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0109 19:09:10.697062  2801 caffe_interface.cpp:125] Batch 17, loss = 0.327393
I0109 19:09:10.697086  2801 caffe_interface.cpp:125] Batch 17, top-1 = 0.88
I0109 19:09:10.697098  2801 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0109 19:09:10.701517  2801 caffe_interface.cpp:125] Batch 18, loss = 0.555222
I0109 19:09:10.701541  2801 caffe_interface.cpp:125] Batch 18, top-1 = 0.82
I0109 19:09:10.701553  2801 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 19:09:10.705991  2801 caffe_interface.cpp:125] Batch 19, loss = 0.540179
I0109 19:09:10.706014  2801 caffe_interface.cpp:125] Batch 19, top-1 = 0.88
I0109 19:09:10.706025  2801 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0109 19:09:10.710464  2801 caffe_interface.cpp:125] Batch 20, loss = 0.283588
I0109 19:09:10.710489  2801 caffe_interface.cpp:125] Batch 20, top-1 = 0.88
I0109 19:09:10.710500  2801 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0109 19:09:10.714969  2801 caffe_interface.cpp:125] Batch 21, loss = 0.515416
I0109 19:09:10.714995  2801 caffe_interface.cpp:125] Batch 21, top-1 = 0.84
I0109 19:09:10.715006  2801 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0109 19:09:10.719486  2801 caffe_interface.cpp:125] Batch 22, loss = 0.441543
I0109 19:09:10.719527  2801 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0109 19:09:10.719540  2801 caffe_interface.cpp:125] Batch 22, top-5 = 0.98
I0109 19:09:10.724011  2801 caffe_interface.cpp:125] Batch 23, loss = 0.352001
I0109 19:09:10.724037  2801 caffe_interface.cpp:125] Batch 23, top-1 = 0.9
I0109 19:09:10.724048  2801 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 19:09:10.728523  2801 caffe_interface.cpp:125] Batch 24, loss = 0.401251
I0109 19:09:10.728549  2801 caffe_interface.cpp:125] Batch 24, top-1 = 0.88
I0109 19:09:10.728560  2801 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0109 19:09:10.733036  2801 caffe_interface.cpp:125] Batch 25, loss = 0.676198
I0109 19:09:10.733062  2801 caffe_interface.cpp:125] Batch 25, top-1 = 0.86
I0109 19:09:10.733073  2801 caffe_interface.cpp:125] Batch 25, top-5 = 0.96
I0109 19:09:10.737532  2801 caffe_interface.cpp:125] Batch 26, loss = 0.63959
I0109 19:09:10.737558  2801 caffe_interface.cpp:125] Batch 26, top-1 = 0.86
I0109 19:09:10.737571  2801 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0109 19:09:10.742069  2801 caffe_interface.cpp:125] Batch 27, loss = 0.287956
I0109 19:09:10.742095  2801 caffe_interface.cpp:125] Batch 27, top-1 = 0.88
I0109 19:09:10.742106  2801 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 19:09:10.746564  2801 caffe_interface.cpp:125] Batch 28, loss = 0.25015
I0109 19:09:10.746589  2801 caffe_interface.cpp:125] Batch 28, top-1 = 0.92
I0109 19:09:10.746600  2801 caffe_interface.cpp:125] Batch 28, top-5 = 1
I0109 19:09:10.751077  2801 caffe_interface.cpp:125] Batch 29, loss = 0.219149
I0109 19:09:10.751101  2801 caffe_interface.cpp:125] Batch 29, top-1 = 0.86
I0109 19:09:10.751112  2801 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 19:09:10.755581  2801 caffe_interface.cpp:125] Batch 30, loss = 0.378068
I0109 19:09:10.755606  2801 caffe_interface.cpp:125] Batch 30, top-1 = 0.9
I0109 19:09:10.755619  2801 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0109 19:09:10.760071  2801 caffe_interface.cpp:125] Batch 31, loss = 0.37174
I0109 19:09:10.760095  2801 caffe_interface.cpp:125] Batch 31, top-1 = 0.9
I0109 19:09:10.760107  2801 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 19:09:10.764570  2801 caffe_interface.cpp:125] Batch 32, loss = 0.772969
I0109 19:09:10.764595  2801 caffe_interface.cpp:125] Batch 32, top-1 = 0.76
I0109 19:09:10.764605  2801 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 19:09:10.769069  2801 caffe_interface.cpp:125] Batch 33, loss = 0.388453
I0109 19:09:10.769094  2801 caffe_interface.cpp:125] Batch 33, top-1 = 0.88
I0109 19:09:10.769105  2801 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 19:09:10.773561  2801 caffe_interface.cpp:125] Batch 34, loss = 0.578849
I0109 19:09:10.773600  2801 caffe_interface.cpp:125] Batch 34, top-1 = 0.8
I0109 19:09:10.773613  2801 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0109 19:09:10.778069  2801 caffe_interface.cpp:125] Batch 35, loss = 0.472213
I0109 19:09:10.778092  2801 caffe_interface.cpp:125] Batch 35, top-1 = 0.84
I0109 19:09:10.778103  2801 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 19:09:10.782573  2801 caffe_interface.cpp:125] Batch 36, loss = 0.58749
I0109 19:09:10.782598  2801 caffe_interface.cpp:125] Batch 36, top-1 = 0.84
I0109 19:09:10.782609  2801 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0109 19:09:10.787072  2801 caffe_interface.cpp:125] Batch 37, loss = 0.534851
I0109 19:09:10.787097  2801 caffe_interface.cpp:125] Batch 37, top-1 = 0.84
I0109 19:09:10.787108  2801 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0109 19:09:10.791568  2801 caffe_interface.cpp:125] Batch 38, loss = 0.646524
I0109 19:09:10.791592  2801 caffe_interface.cpp:125] Batch 38, top-1 = 0.86
I0109 19:09:10.791604  2801 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0109 19:09:10.796051  2801 caffe_interface.cpp:125] Batch 39, loss = 0.277057
I0109 19:09:10.796077  2801 caffe_interface.cpp:125] Batch 39, top-1 = 0.92
I0109 19:09:10.796087  2801 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 19:09:10.800518  2801 caffe_interface.cpp:125] Batch 40, loss = 0.317836
I0109 19:09:10.800556  2801 caffe_interface.cpp:125] Batch 40, top-1 = 0.86
I0109 19:09:10.800567  2801 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0109 19:09:10.804960  2801 caffe_interface.cpp:125] Batch 41, loss = 0.405369
I0109 19:09:10.804986  2801 caffe_interface.cpp:125] Batch 41, top-1 = 0.82
I0109 19:09:10.804996  2801 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 19:09:10.809454  2801 caffe_interface.cpp:125] Batch 42, loss = 0.362858
I0109 19:09:10.809479  2801 caffe_interface.cpp:125] Batch 42, top-1 = 0.88
I0109 19:09:10.809491  2801 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0109 19:09:10.813948  2801 caffe_interface.cpp:125] Batch 43, loss = 0.32939
I0109 19:09:10.813972  2801 caffe_interface.cpp:125] Batch 43, top-1 = 0.92
I0109 19:09:10.813982  2801 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0109 19:09:10.818423  2801 caffe_interface.cpp:125] Batch 44, loss = 0.20203
I0109 19:09:10.818447  2801 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I0109 19:09:10.818459  2801 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0109 19:09:10.822921  2801 caffe_interface.cpp:125] Batch 45, loss = 0.55078
I0109 19:09:10.822945  2801 caffe_interface.cpp:125] Batch 45, top-1 = 0.82
I0109 19:09:10.822957  2801 caffe_interface.cpp:125] Batch 45, top-5 = 0.96
I0109 19:09:10.827430  2801 caffe_interface.cpp:125] Batch 46, loss = 0.544623
I0109 19:09:10.827453  2801 caffe_interface.cpp:125] Batch 46, top-1 = 0.78
I0109 19:09:10.827466  2801 caffe_interface.cpp:125] Batch 46, top-5 = 0.98
I0109 19:09:10.831928  2801 caffe_interface.cpp:125] Batch 47, loss = 0.669055
I0109 19:09:10.831953  2801 caffe_interface.cpp:125] Batch 47, top-1 = 0.82
I0109 19:09:10.831964  2801 caffe_interface.cpp:125] Batch 47, top-5 = 0.98
I0109 19:09:10.836405  2801 caffe_interface.cpp:125] Batch 48, loss = 0.260576
I0109 19:09:10.836429  2801 caffe_interface.cpp:125] Batch 48, top-1 = 0.9
I0109 19:09:10.836441  2801 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0109 19:09:10.840896  2801 caffe_interface.cpp:125] Batch 49, loss = 0.565233
I0109 19:09:10.840921  2801 caffe_interface.cpp:125] Batch 49, top-1 = 0.84
I0109 19:09:10.840932  2801 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0109 19:09:10.845399  2801 caffe_interface.cpp:125] Batch 50, loss = 0.409269
I0109 19:09:10.845423  2801 caffe_interface.cpp:125] Batch 50, top-1 = 0.9
I0109 19:09:10.845434  2801 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0109 19:09:10.849839  2801 caffe_interface.cpp:125] Batch 51, loss = 0.561045
I0109 19:09:10.849864  2801 caffe_interface.cpp:125] Batch 51, top-1 = 0.88
I0109 19:09:10.849874  2801 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0109 19:09:10.854288  2801 caffe_interface.cpp:125] Batch 52, loss = 0.506467
I0109 19:09:10.854313  2801 caffe_interface.cpp:125] Batch 52, top-1 = 0.82
I0109 19:09:10.854323  2801 caffe_interface.cpp:125] Batch 52, top-5 = 1
I0109 19:09:10.858712  2801 caffe_interface.cpp:125] Batch 53, loss = 0.264567
I0109 19:09:10.858738  2801 caffe_interface.cpp:125] Batch 53, top-1 = 0.86
I0109 19:09:10.858749  2801 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0109 19:09:10.863159  2801 caffe_interface.cpp:125] Batch 54, loss = 0.439792
I0109 19:09:10.863183  2801 caffe_interface.cpp:125] Batch 54, top-1 = 0.86
I0109 19:09:10.863195  2801 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0109 19:09:10.867594  2801 caffe_interface.cpp:125] Batch 55, loss = 0.446282
I0109 19:09:10.867617  2801 caffe_interface.cpp:125] Batch 55, top-1 = 0.92
I0109 19:09:10.867628  2801 caffe_interface.cpp:125] Batch 55, top-5 = 0.98
I0109 19:09:10.872040  2801 caffe_interface.cpp:125] Batch 56, loss = 0.265818
I0109 19:09:10.872064  2801 caffe_interface.cpp:125] Batch 56, top-1 = 0.9
I0109 19:09:10.872081  2801 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0109 19:09:10.876497  2801 caffe_interface.cpp:125] Batch 57, loss = 0.339547
I0109 19:09:10.876523  2801 caffe_interface.cpp:125] Batch 57, top-1 = 0.84
I0109 19:09:10.876533  2801 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0109 19:09:10.880954  2801 caffe_interface.cpp:125] Batch 58, loss = 0.549117
I0109 19:09:10.880978  2801 caffe_interface.cpp:125] Batch 58, top-1 = 0.82
I0109 19:09:10.880991  2801 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0109 19:09:10.885394  2801 caffe_interface.cpp:125] Batch 59, loss = 0.347689
I0109 19:09:10.885421  2801 caffe_interface.cpp:125] Batch 59, top-1 = 0.9
I0109 19:09:10.885432  2801 caffe_interface.cpp:125] Batch 59, top-5 = 0.98
I0109 19:09:10.889835  2801 caffe_interface.cpp:125] Batch 60, loss = 0.415495
I0109 19:09:10.889860  2801 caffe_interface.cpp:125] Batch 60, top-1 = 0.88
I0109 19:09:10.889870  2801 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0109 19:09:10.894279  2801 caffe_interface.cpp:125] Batch 61, loss = 0.262305
I0109 19:09:10.894304  2801 caffe_interface.cpp:125] Batch 61, top-1 = 0.92
I0109 19:09:10.894315  2801 caffe_interface.cpp:125] Batch 61, top-5 = 1
I0109 19:09:10.898725  2801 caffe_interface.cpp:125] Batch 62, loss = 0.343609
I0109 19:09:10.898749  2801 caffe_interface.cpp:125] Batch 62, top-1 = 0.9
I0109 19:09:10.898762  2801 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0109 19:09:10.903132  2801 caffe_interface.cpp:125] Batch 63, loss = 0.531528
I0109 19:09:10.903157  2801 caffe_interface.cpp:125] Batch 63, top-1 = 0.84
I0109 19:09:10.903168  2801 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0109 19:09:10.907547  2801 caffe_interface.cpp:125] Batch 64, loss = 0.273508
I0109 19:09:10.907572  2801 caffe_interface.cpp:125] Batch 64, top-1 = 0.9
I0109 19:09:10.907582  2801 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 19:09:10.911979  2801 caffe_interface.cpp:125] Batch 65, loss = 0.252341
I0109 19:09:10.912005  2801 caffe_interface.cpp:125] Batch 65, top-1 = 0.88
I0109 19:09:10.912016  2801 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 19:09:10.916445  2801 caffe_interface.cpp:125] Batch 66, loss = 0.564477
I0109 19:09:10.916469  2801 caffe_interface.cpp:125] Batch 66, top-1 = 0.8
I0109 19:09:10.916481  2801 caffe_interface.cpp:125] Batch 66, top-5 = 0.96
I0109 19:09:10.920883  2801 caffe_interface.cpp:125] Batch 67, loss = 0.221715
I0109 19:09:10.920908  2801 caffe_interface.cpp:125] Batch 67, top-1 = 0.88
I0109 19:09:10.920919  2801 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 19:09:10.925324  2801 caffe_interface.cpp:125] Batch 68, loss = 0.458989
I0109 19:09:10.925349  2801 caffe_interface.cpp:125] Batch 68, top-1 = 0.84
I0109 19:09:10.925360  2801 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0109 19:09:10.929769  2801 caffe_interface.cpp:125] Batch 69, loss = 0.58209
I0109 19:09:10.929793  2801 caffe_interface.cpp:125] Batch 69, top-1 = 0.8
I0109 19:09:10.929805  2801 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 19:09:10.934212  2801 caffe_interface.cpp:125] Batch 70, loss = 0.472788
I0109 19:09:10.934237  2801 caffe_interface.cpp:125] Batch 70, top-1 = 0.84
I0109 19:09:10.934248  2801 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0109 19:09:10.938678  2801 caffe_interface.cpp:125] Batch 71, loss = 0.289449
I0109 19:09:10.938702  2801 caffe_interface.cpp:125] Batch 71, top-1 = 0.88
I0109 19:09:10.938714  2801 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0109 19:09:10.943120  2801 caffe_interface.cpp:125] Batch 72, loss = 0.197738
I0109 19:09:10.943145  2801 caffe_interface.cpp:125] Batch 72, top-1 = 0.92
I0109 19:09:10.943156  2801 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 19:09:10.947544  2801 caffe_interface.cpp:125] Batch 73, loss = 0.56415
I0109 19:09:10.947571  2801 caffe_interface.cpp:125] Batch 73, top-1 = 0.88
I0109 19:09:10.947580  2801 caffe_interface.cpp:125] Batch 73, top-5 = 1
I0109 19:09:10.951985  2801 caffe_interface.cpp:125] Batch 74, loss = 0.57368
I0109 19:09:10.952011  2801 caffe_interface.cpp:125] Batch 74, top-1 = 0.86
I0109 19:09:10.952024  2801 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 19:09:10.956454  2801 caffe_interface.cpp:125] Batch 75, loss = 0.445188
I0109 19:09:10.956478  2801 caffe_interface.cpp:125] Batch 75, top-1 = 0.82
I0109 19:09:10.956490  2801 caffe_interface.cpp:125] Batch 75, top-5 = 1
I0109 19:09:10.960911  2801 caffe_interface.cpp:125] Batch 76, loss = 0.384738
I0109 19:09:10.960937  2801 caffe_interface.cpp:125] Batch 76, top-1 = 0.88
I0109 19:09:10.960947  2801 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 19:09:10.965342  2801 caffe_interface.cpp:125] Batch 77, loss = 0.251382
I0109 19:09:10.965368  2801 caffe_interface.cpp:125] Batch 77, top-1 = 0.9
I0109 19:09:10.965379  2801 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0109 19:09:10.969792  2801 caffe_interface.cpp:125] Batch 78, loss = 0.549722
I0109 19:09:10.969816  2801 caffe_interface.cpp:125] Batch 78, top-1 = 0.86
I0109 19:09:10.969828  2801 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0109 19:09:10.974223  2801 caffe_interface.cpp:125] Batch 79, loss = 0.525054
I0109 19:09:10.974247  2801 caffe_interface.cpp:125] Batch 79, top-1 = 0.86
I0109 19:09:10.974258  2801 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0109 19:09:10.978691  2801 caffe_interface.cpp:125] Batch 80, loss = 0.313293
I0109 19:09:10.978716  2801 caffe_interface.cpp:125] Batch 80, top-1 = 0.88
I0109 19:09:10.978727  2801 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0109 19:09:10.983142  2801 caffe_interface.cpp:125] Batch 81, loss = 0.518506
I0109 19:09:10.983168  2801 caffe_interface.cpp:125] Batch 81, top-1 = 0.88
I0109 19:09:10.983180  2801 caffe_interface.cpp:125] Batch 81, top-5 = 0.96
I0109 19:09:10.987587  2801 caffe_interface.cpp:125] Batch 82, loss = 0.512834
I0109 19:09:10.987613  2801 caffe_interface.cpp:125] Batch 82, top-1 = 0.78
I0109 19:09:10.987625  2801 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 19:09:10.992014  2801 caffe_interface.cpp:125] Batch 83, loss = 0.466193
I0109 19:09:10.992039  2801 caffe_interface.cpp:125] Batch 83, top-1 = 0.9
I0109 19:09:10.992050  2801 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0109 19:09:10.996444  2801 caffe_interface.cpp:125] Batch 84, loss = 0.516521
I0109 19:09:10.996469  2801 caffe_interface.cpp:125] Batch 84, top-1 = 0.86
I0109 19:09:10.996480  2801 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 19:09:11.000874  2801 caffe_interface.cpp:125] Batch 85, loss = 0.432362
I0109 19:09:11.000900  2801 caffe_interface.cpp:125] Batch 85, top-1 = 0.84
I0109 19:09:11.000911  2801 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 19:09:11.005304  2801 caffe_interface.cpp:125] Batch 86, loss = 0.445809
I0109 19:09:11.005328  2801 caffe_interface.cpp:125] Batch 86, top-1 = 0.84
I0109 19:09:11.005340  2801 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 19:09:11.009809  2801 caffe_interface.cpp:125] Batch 87, loss = 0.661083
I0109 19:09:11.009833  2801 caffe_interface.cpp:125] Batch 87, top-1 = 0.82
I0109 19:09:11.009845  2801 caffe_interface.cpp:125] Batch 87, top-5 = 0.98
I0109 19:09:11.014283  2801 caffe_interface.cpp:125] Batch 88, loss = 0.609008
I0109 19:09:11.014307  2801 caffe_interface.cpp:125] Batch 88, top-1 = 0.86
I0109 19:09:11.014318  2801 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 19:09:11.018759  2801 caffe_interface.cpp:125] Batch 89, loss = 0.397284
I0109 19:09:11.018784  2801 caffe_interface.cpp:125] Batch 89, top-1 = 0.82
I0109 19:09:11.018795  2801 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0109 19:09:11.023252  2801 caffe_interface.cpp:125] Batch 90, loss = 0.181164
I0109 19:09:11.023277  2801 caffe_interface.cpp:125] Batch 90, top-1 = 0.92
I0109 19:09:11.023288  2801 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 19:09:11.027755  2801 caffe_interface.cpp:125] Batch 91, loss = 0.272626
I0109 19:09:11.027778  2801 caffe_interface.cpp:125] Batch 91, top-1 = 0.9
I0109 19:09:11.027789  2801 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 19:09:11.032251  2801 caffe_interface.cpp:125] Batch 92, loss = 0.403544
I0109 19:09:11.032276  2801 caffe_interface.cpp:125] Batch 92, top-1 = 0.86
I0109 19:09:11.032286  2801 caffe_interface.cpp:125] Batch 92, top-5 = 1
I0109 19:09:11.036758  2801 caffe_interface.cpp:125] Batch 93, loss = 0.190572
I0109 19:09:11.036785  2801 caffe_interface.cpp:125] Batch 93, top-1 = 0.92
I0109 19:09:11.036795  2801 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 19:09:11.041276  2801 caffe_interface.cpp:125] Batch 94, loss = 0.369997
I0109 19:09:11.041301  2801 caffe_interface.cpp:125] Batch 94, top-1 = 0.84
I0109 19:09:11.041313  2801 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0109 19:09:11.045781  2801 caffe_interface.cpp:125] Batch 95, loss = 0.390539
I0109 19:09:11.045805  2801 caffe_interface.cpp:125] Batch 95, top-1 = 0.84
I0109 19:09:11.045816  2801 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 19:09:11.050282  2801 caffe_interface.cpp:125] Batch 96, loss = 0.350143
I0109 19:09:11.050307  2801 caffe_interface.cpp:125] Batch 96, top-1 = 0.88
I0109 19:09:11.050318  2801 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 19:09:11.054775  2801 caffe_interface.cpp:125] Batch 97, loss = 0.526447
I0109 19:09:11.054801  2801 caffe_interface.cpp:125] Batch 97, top-1 = 0.82
I0109 19:09:11.054812  2801 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0109 19:09:11.059268  2801 caffe_interface.cpp:125] Batch 98, loss = 0.513369
I0109 19:09:11.059293  2801 caffe_interface.cpp:125] Batch 98, top-1 = 0.82
I0109 19:09:11.059305  2801 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0109 19:09:11.063772  2801 caffe_interface.cpp:125] Batch 99, loss = 0.555849
I0109 19:09:11.063797  2801 caffe_interface.cpp:125] Batch 99, top-1 = 0.86
I0109 19:09:11.063808  2801 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0109 19:09:11.068255  2801 caffe_interface.cpp:125] Batch 100, loss = 0.325566
I0109 19:09:11.068280  2801 caffe_interface.cpp:125] Batch 100, top-1 = 0.88
I0109 19:09:11.068291  2801 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 19:09:11.072755  2801 caffe_interface.cpp:125] Batch 101, loss = 0.398439
I0109 19:09:11.072780  2801 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0109 19:09:11.072791  2801 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 19:09:11.077250  2801 caffe_interface.cpp:125] Batch 102, loss = 0.358144
I0109 19:09:11.077275  2801 caffe_interface.cpp:125] Batch 102, top-1 = 0.86
I0109 19:09:11.077287  2801 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0109 19:09:11.081756  2801 caffe_interface.cpp:125] Batch 103, loss = 0.386444
I0109 19:09:11.081779  2801 caffe_interface.cpp:125] Batch 103, top-1 = 0.86
I0109 19:09:11.081790  2801 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 19:09:11.086257  2801 caffe_interface.cpp:125] Batch 104, loss = 0.270611
I0109 19:09:11.086282  2801 caffe_interface.cpp:125] Batch 104, top-1 = 0.92
I0109 19:09:11.086293  2801 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 19:09:11.090746  2801 caffe_interface.cpp:125] Batch 105, loss = 0.240836
I0109 19:09:11.090771  2801 caffe_interface.cpp:125] Batch 105, top-1 = 0.94
I0109 19:09:11.090783  2801 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 19:09:11.095312  2801 caffe_interface.cpp:125] Batch 106, loss = 0.386469
I0109 19:09:11.095371  2801 caffe_interface.cpp:125] Batch 106, top-1 = 0.9
I0109 19:09:11.095381  2801 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0109 19:09:11.099862  2801 caffe_interface.cpp:125] Batch 107, loss = 0.477024
I0109 19:09:11.099889  2801 caffe_interface.cpp:125] Batch 107, top-1 = 0.82
I0109 19:09:11.099900  2801 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0109 19:09:11.104322  2801 caffe_interface.cpp:125] Batch 108, loss = 0.634321
I0109 19:09:11.104347  2801 caffe_interface.cpp:125] Batch 108, top-1 = 0.76
I0109 19:09:11.104359  2801 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0109 19:09:11.108821  2801 caffe_interface.cpp:125] Batch 109, loss = 0.264949
I0109 19:09:11.108849  2801 caffe_interface.cpp:125] Batch 109, top-1 = 0.92
I0109 19:09:11.108860  2801 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0109 19:09:11.113328  2801 caffe_interface.cpp:125] Batch 110, loss = 0.146709
I0109 19:09:11.113353  2801 caffe_interface.cpp:125] Batch 110, top-1 = 0.96
I0109 19:09:11.113366  2801 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 19:09:11.117856  2801 caffe_interface.cpp:125] Batch 111, loss = 0.29963
I0109 19:09:11.117882  2801 caffe_interface.cpp:125] Batch 111, top-1 = 0.9
I0109 19:09:11.117920  2801 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0109 19:09:11.122422  2801 caffe_interface.cpp:125] Batch 112, loss = 0.545889
I0109 19:09:11.122447  2801 caffe_interface.cpp:125] Batch 112, top-1 = 0.84
I0109 19:09:11.122458  2801 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0109 19:09:11.126951  2801 caffe_interface.cpp:125] Batch 113, loss = 0.364551
I0109 19:09:11.126977  2801 caffe_interface.cpp:125] Batch 113, top-1 = 0.82
I0109 19:09:11.126988  2801 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0109 19:09:11.131484  2801 caffe_interface.cpp:125] Batch 114, loss = 0.581199
I0109 19:09:11.131508  2801 caffe_interface.cpp:125] Batch 114, top-1 = 0.84
I0109 19:09:11.131520  2801 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0109 19:09:11.136013  2801 caffe_interface.cpp:125] Batch 115, loss = 0.305752
I0109 19:09:11.136039  2801 caffe_interface.cpp:125] Batch 115, top-1 = 0.88
I0109 19:09:11.136049  2801 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 19:09:11.140525  2801 caffe_interface.cpp:125] Batch 116, loss = 0.480771
I0109 19:09:11.140550  2801 caffe_interface.cpp:125] Batch 116, top-1 = 0.82
I0109 19:09:11.140561  2801 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0109 19:09:11.145052  2801 caffe_interface.cpp:125] Batch 117, loss = 0.223867
I0109 19:09:11.145079  2801 caffe_interface.cpp:125] Batch 117, top-1 = 0.92
I0109 19:09:11.145090  2801 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0109 19:09:11.149571  2801 caffe_interface.cpp:125] Batch 118, loss = 0.429438
I0109 19:09:11.149616  2801 caffe_interface.cpp:125] Batch 118, top-1 = 0.84
I0109 19:09:11.149629  2801 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0109 19:09:11.154139  2801 caffe_interface.cpp:125] Batch 119, loss = 0.206008
I0109 19:09:11.154172  2801 caffe_interface.cpp:125] Batch 119, top-1 = 0.9
I0109 19:09:11.154183  2801 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 19:09:11.158679  2801 caffe_interface.cpp:125] Batch 120, loss = 0.487334
I0109 19:09:11.158712  2801 caffe_interface.cpp:125] Batch 120, top-1 = 0.82
I0109 19:09:11.158723  2801 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0109 19:09:11.163218  2801 caffe_interface.cpp:125] Batch 121, loss = 0.414258
I0109 19:09:11.163251  2801 caffe_interface.cpp:125] Batch 121, top-1 = 0.88
I0109 19:09:11.163262  2801 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 19:09:11.167762  2801 caffe_interface.cpp:125] Batch 122, loss = 0.431947
I0109 19:09:11.167798  2801 caffe_interface.cpp:125] Batch 122, top-1 = 0.82
I0109 19:09:11.167810  2801 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 19:09:11.172296  2801 caffe_interface.cpp:125] Batch 123, loss = 0.464502
I0109 19:09:11.172322  2801 caffe_interface.cpp:125] Batch 123, top-1 = 0.78
I0109 19:09:11.172333  2801 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0109 19:09:11.176810  2801 caffe_interface.cpp:125] Batch 124, loss = 0.315538
I0109 19:09:11.176836  2801 caffe_interface.cpp:125] Batch 124, top-1 = 0.88
I0109 19:09:11.176846  2801 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 19:09:11.181327  2801 caffe_interface.cpp:125] Batch 125, loss = 0.61172
I0109 19:09:11.181354  2801 caffe_interface.cpp:125] Batch 125, top-1 = 0.82
I0109 19:09:11.181365  2801 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0109 19:09:11.185853  2801 caffe_interface.cpp:125] Batch 126, loss = 0.553588
I0109 19:09:11.185878  2801 caffe_interface.cpp:125] Batch 126, top-1 = 0.84
I0109 19:09:11.185889  2801 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 19:09:11.190353  2801 caffe_interface.cpp:125] Batch 127, loss = 0.375387
I0109 19:09:11.190378  2801 caffe_interface.cpp:125] Batch 127, top-1 = 0.84
I0109 19:09:11.190389  2801 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 19:09:11.194866  2801 caffe_interface.cpp:125] Batch 128, loss = 0.683211
I0109 19:09:11.194890  2801 caffe_interface.cpp:125] Batch 128, top-1 = 0.8
I0109 19:09:11.194901  2801 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0109 19:09:11.199383  2801 caffe_interface.cpp:125] Batch 129, loss = 0.244718
I0109 19:09:11.199409  2801 caffe_interface.cpp:125] Batch 129, top-1 = 0.9
I0109 19:09:11.199445  2801 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 19:09:11.203910  2801 caffe_interface.cpp:125] Batch 130, loss = 0.258589
I0109 19:09:11.203938  2801 caffe_interface.cpp:125] Batch 130, top-1 = 0.88
I0109 19:09:11.203949  2801 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 19:09:11.208398  2801 caffe_interface.cpp:125] Batch 131, loss = 0.613398
I0109 19:09:11.208425  2801 caffe_interface.cpp:125] Batch 131, top-1 = 0.82
I0109 19:09:11.208436  2801 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 19:09:11.212925  2801 caffe_interface.cpp:125] Batch 132, loss = 0.297058
I0109 19:09:11.212951  2801 caffe_interface.cpp:125] Batch 132, top-1 = 0.92
I0109 19:09:11.212962  2801 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 19:09:11.217461  2801 caffe_interface.cpp:125] Batch 133, loss = 0.43513
I0109 19:09:11.217491  2801 caffe_interface.cpp:125] Batch 133, top-1 = 0.9
I0109 19:09:11.217504  2801 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0109 19:09:11.221974  2801 caffe_interface.cpp:125] Batch 134, loss = 0.532062
I0109 19:09:11.221998  2801 caffe_interface.cpp:125] Batch 134, top-1 = 0.8
I0109 19:09:11.222012  2801 caffe_interface.cpp:125] Batch 134, top-5 = 0.96
I0109 19:09:11.226495  2801 caffe_interface.cpp:125] Batch 135, loss = 0.34533
I0109 19:09:11.226521  2801 caffe_interface.cpp:125] Batch 135, top-1 = 0.9
I0109 19:09:11.226531  2801 caffe_interface.cpp:125] Batch 135, top-5 = 1
I0109 19:09:11.231032  2801 caffe_interface.cpp:125] Batch 136, loss = 0.157281
I0109 19:09:11.231058  2801 caffe_interface.cpp:125] Batch 136, top-1 = 0.94
I0109 19:09:11.231070  2801 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 19:09:11.235543  2801 caffe_interface.cpp:125] Batch 137, loss = 0.421044
I0109 19:09:11.235576  2801 caffe_interface.cpp:125] Batch 137, top-1 = 0.88
I0109 19:09:11.235587  2801 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 19:09:11.240082  2801 caffe_interface.cpp:125] Batch 138, loss = 0.552953
I0109 19:09:11.240113  2801 caffe_interface.cpp:125] Batch 138, top-1 = 0.84
I0109 19:09:11.240126  2801 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0109 19:09:11.244601  2801 caffe_interface.cpp:125] Batch 139, loss = 0.291285
I0109 19:09:11.244626  2801 caffe_interface.cpp:125] Batch 139, top-1 = 0.86
I0109 19:09:11.244637  2801 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 19:09:11.249126  2801 caffe_interface.cpp:125] Batch 140, loss = 0.575776
I0109 19:09:11.249153  2801 caffe_interface.cpp:125] Batch 140, top-1 = 0.82
I0109 19:09:11.249163  2801 caffe_interface.cpp:125] Batch 140, top-5 = 0.98
I0109 19:09:11.253679  2801 caffe_interface.cpp:125] Batch 141, loss = 0.42951
I0109 19:09:11.253712  2801 caffe_interface.cpp:125] Batch 141, top-1 = 0.9
I0109 19:09:11.253723  2801 caffe_interface.cpp:125] Batch 141, top-5 = 1
I0109 19:09:11.258253  2801 caffe_interface.cpp:125] Batch 142, loss = 0.33363
I0109 19:09:11.258291  2801 caffe_interface.cpp:125] Batch 142, top-1 = 0.88
I0109 19:09:11.258302  2801 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 19:09:11.262791  2801 caffe_interface.cpp:125] Batch 143, loss = 0.153718
I0109 19:09:11.262827  2801 caffe_interface.cpp:125] Batch 143, top-1 = 0.94
I0109 19:09:11.262840  2801 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 19:09:11.267350  2801 caffe_interface.cpp:125] Batch 144, loss = 0.454523
I0109 19:09:11.267382  2801 caffe_interface.cpp:125] Batch 144, top-1 = 0.84
I0109 19:09:11.267393  2801 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0109 19:09:11.271898  2801 caffe_interface.cpp:125] Batch 145, loss = 0.453189
I0109 19:09:11.271927  2801 caffe_interface.cpp:125] Batch 145, top-1 = 0.88
I0109 19:09:11.271939  2801 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0109 19:09:11.276434  2801 caffe_interface.cpp:125] Batch 146, loss = 0.411758
I0109 19:09:11.276461  2801 caffe_interface.cpp:125] Batch 146, top-1 = 0.86
I0109 19:09:11.276473  2801 caffe_interface.cpp:125] Batch 146, top-5 = 0.96
I0109 19:09:11.280949  2801 caffe_interface.cpp:125] Batch 147, loss = 0.307716
I0109 19:09:11.280994  2801 caffe_interface.cpp:125] Batch 147, top-1 = 0.9
I0109 19:09:11.281006  2801 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0109 19:09:11.285511  2801 caffe_interface.cpp:125] Batch 148, loss = 0.592642
I0109 19:09:11.285538  2801 caffe_interface.cpp:125] Batch 148, top-1 = 0.82
I0109 19:09:11.285549  2801 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0109 19:09:11.290128  2801 caffe_interface.cpp:125] Batch 149, loss = 0.506465
I0109 19:09:11.290181  2801 caffe_interface.cpp:125] Batch 149, top-1 = 0.82
I0109 19:09:11.290192  2801 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0109 19:09:11.294750  2801 caffe_interface.cpp:125] Batch 150, loss = 0.202226
I0109 19:09:11.294800  2801 caffe_interface.cpp:125] Batch 150, top-1 = 0.92
I0109 19:09:11.294811  2801 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0109 19:09:11.299302  2801 caffe_interface.cpp:125] Batch 151, loss = 0.18674
I0109 19:09:11.299332  2801 caffe_interface.cpp:125] Batch 151, top-1 = 0.94
I0109 19:09:11.299342  2801 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 19:09:11.303843  2801 caffe_interface.cpp:125] Batch 152, loss = 0.503274
I0109 19:09:11.303884  2801 caffe_interface.cpp:125] Batch 152, top-1 = 0.84
I0109 19:09:11.303894  2801 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0109 19:09:11.308369  2801 caffe_interface.cpp:125] Batch 153, loss = 0.189258
I0109 19:09:11.308396  2801 caffe_interface.cpp:125] Batch 153, top-1 = 0.94
I0109 19:09:11.308408  2801 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 19:09:11.312892  2801 caffe_interface.cpp:125] Batch 154, loss = 0.16919
I0109 19:09:11.312918  2801 caffe_interface.cpp:125] Batch 154, top-1 = 0.94
I0109 19:09:11.312930  2801 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 19:09:11.317435  2801 caffe_interface.cpp:125] Batch 155, loss = 0.17538
I0109 19:09:11.317468  2801 caffe_interface.cpp:125] Batch 155, top-1 = 0.9
I0109 19:09:11.317479  2801 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 19:09:11.321990  2801 caffe_interface.cpp:125] Batch 156, loss = 0.35959
I0109 19:09:11.322021  2801 caffe_interface.cpp:125] Batch 156, top-1 = 0.9
I0109 19:09:11.322032  2801 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 19:09:11.326524  2801 caffe_interface.cpp:125] Batch 157, loss = 0.385105
I0109 19:09:11.326550  2801 caffe_interface.cpp:125] Batch 157, top-1 = 0.88
I0109 19:09:11.326561  2801 caffe_interface.cpp:125] Batch 157, top-5 = 0.98
I0109 19:09:11.331022  2801 caffe_interface.cpp:125] Batch 158, loss = 0.651593
I0109 19:09:11.331048  2801 caffe_interface.cpp:125] Batch 158, top-1 = 0.8
I0109 19:09:11.331059  2801 caffe_interface.cpp:125] Batch 158, top-5 = 0.94
I0109 19:09:11.335532  2801 caffe_interface.cpp:125] Batch 159, loss = 0.315728
I0109 19:09:11.335557  2801 caffe_interface.cpp:125] Batch 159, top-1 = 0.9
I0109 19:09:11.335568  2801 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0109 19:09:11.340062  2801 caffe_interface.cpp:125] Batch 160, loss = 0.503809
I0109 19:09:11.340090  2801 caffe_interface.cpp:125] Batch 160, top-1 = 0.84
I0109 19:09:11.340101  2801 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0109 19:09:11.344588  2801 caffe_interface.cpp:125] Batch 161, loss = 0.203547
I0109 19:09:11.344615  2801 caffe_interface.cpp:125] Batch 161, top-1 = 0.94
I0109 19:09:11.344626  2801 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0109 19:09:11.349110  2801 caffe_interface.cpp:125] Batch 162, loss = 0.408813
I0109 19:09:11.349136  2801 caffe_interface.cpp:125] Batch 162, top-1 = 0.88
I0109 19:09:11.349148  2801 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0109 19:09:11.353670  2801 caffe_interface.cpp:125] Batch 163, loss = 0.345717
I0109 19:09:11.353694  2801 caffe_interface.cpp:125] Batch 163, top-1 = 0.86
I0109 19:09:11.353704  2801 caffe_interface.cpp:125] Batch 163, top-5 = 0.98
I0109 19:09:11.358186  2801 caffe_interface.cpp:125] Batch 164, loss = 0.483195
I0109 19:09:11.358211  2801 caffe_interface.cpp:125] Batch 164, top-1 = 0.84
I0109 19:09:11.358223  2801 caffe_interface.cpp:125] Batch 164, top-5 = 1
I0109 19:09:11.362725  2801 caffe_interface.cpp:125] Batch 165, loss = 0.483674
I0109 19:09:11.362749  2801 caffe_interface.cpp:125] Batch 165, top-1 = 0.82
I0109 19:09:11.362761  2801 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 19:09:11.367233  2801 caffe_interface.cpp:125] Batch 166, loss = 0.186482
I0109 19:09:11.367257  2801 caffe_interface.cpp:125] Batch 166, top-1 = 0.92
I0109 19:09:11.367270  2801 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0109 19:09:11.371729  2801 caffe_interface.cpp:125] Batch 167, loss = 0.551196
I0109 19:09:11.371754  2801 caffe_interface.cpp:125] Batch 167, top-1 = 0.86
I0109 19:09:11.371765  2801 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0109 19:09:11.376253  2801 caffe_interface.cpp:125] Batch 168, loss = 0.667836
I0109 19:09:11.376277  2801 caffe_interface.cpp:125] Batch 168, top-1 = 0.74
I0109 19:09:11.376288  2801 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0109 19:09:11.380787  2801 caffe_interface.cpp:125] Batch 169, loss = 0.593102
I0109 19:09:11.380812  2801 caffe_interface.cpp:125] Batch 169, top-1 = 0.8
I0109 19:09:11.380823  2801 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0109 19:09:11.385294  2801 caffe_interface.cpp:125] Batch 170, loss = 0.16333
I0109 19:09:11.385318  2801 caffe_interface.cpp:125] Batch 170, top-1 = 0.96
I0109 19:09:11.385331  2801 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0109 19:09:11.389806  2801 caffe_interface.cpp:125] Batch 171, loss = 0.268929
I0109 19:09:11.389829  2801 caffe_interface.cpp:125] Batch 171, top-1 = 0.86
I0109 19:09:11.389840  2801 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 19:09:11.394335  2801 caffe_interface.cpp:125] Batch 172, loss = 0.263616
I0109 19:09:11.394359  2801 caffe_interface.cpp:125] Batch 172, top-1 = 0.9
I0109 19:09:11.394371  2801 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 19:09:11.398861  2801 caffe_interface.cpp:125] Batch 173, loss = 0.606458
I0109 19:09:11.398887  2801 caffe_interface.cpp:125] Batch 173, top-1 = 0.8
I0109 19:09:11.398898  2801 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0109 19:09:11.403342  2801 caffe_interface.cpp:125] Batch 174, loss = 0.732185
I0109 19:09:11.403367  2801 caffe_interface.cpp:125] Batch 174, top-1 = 0.76
I0109 19:09:11.403379  2801 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0109 19:09:11.407795  2801 caffe_interface.cpp:125] Batch 175, loss = 0.460581
I0109 19:09:11.407819  2801 caffe_interface.cpp:125] Batch 175, top-1 = 0.86
I0109 19:09:11.407830  2801 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0109 19:09:11.412293  2801 caffe_interface.cpp:125] Batch 176, loss = 0.62234
I0109 19:09:11.412317  2801 caffe_interface.cpp:125] Batch 176, top-1 = 0.82
I0109 19:09:11.412328  2801 caffe_interface.cpp:125] Batch 176, top-5 = 0.98
I0109 19:09:11.416808  2801 caffe_interface.cpp:125] Batch 177, loss = 0.423273
I0109 19:09:11.416833  2801 caffe_interface.cpp:125] Batch 177, top-1 = 0.9
I0109 19:09:11.416844  2801 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 19:09:11.421324  2801 caffe_interface.cpp:125] Batch 178, loss = 0.193263
I0109 19:09:11.421349  2801 caffe_interface.cpp:125] Batch 178, top-1 = 0.9
I0109 19:09:11.421360  2801 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 19:09:11.425846  2801 caffe_interface.cpp:125] Batch 179, loss = 0.2088
I0109 19:09:11.425873  2801 caffe_interface.cpp:125] Batch 179, top-1 = 0.94
I0109 19:09:11.425884  2801 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0109 19:09:11.425891  2801 caffe_interface.cpp:130] Loss: 0.412943
I0109 19:09:11.425906  2801 caffe_interface.cpp:142] loss = 0.412943 (* 1 = 0.412943 loss)
I0109 19:09:11.425917  2801 caffe_interface.cpp:142] top-1 = 0.865
I0109 19:09:11.425930  2801 caffe_interface.cpp:142] top-5 = 0.993555
I0109 19:09:11.440593  2801 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/sparse.caffemodel
I0109 19:09:11.440647  2801 pruning_runner.cpp:320] summary of REGULAR compression with rate 0:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.864999831    | 0              |
+-------------------------------------------------------------------+
| Weights        | 68389          | 68389          | 0%             |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 49053696       | 0%             |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config0.prototxt
I0109 19:09:11.559887  2836 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0109 19:09:11.672911  2836 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 19:09:11.673460  2836 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 19:09:11.673486  2836 caffe_interface.cpp:493] Using GPUs 0
I0109 19:09:11.673764  2836 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 19:09:12.337502  2836 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt"
type: "SGD"
I0109 19:09:12.337697  2836 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0109 19:09:12.338019  2836 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 19:09:12.338047  2836 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 19:09:12.338052  2836 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 19:09:12.338227  2836 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 19:09:12.338321  2836 layer_factory.hpp:77] Creating layer data
I0109 19:09:12.338452  2836 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:12.338677  2836 net.cpp:94] Creating Layer data
I0109 19:09:12.338718  2836 net.cpp:409] data -> data
I0109 19:09:12.338781  2836 net.cpp:409] data -> label
I0109 19:09:12.339913  2848 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 19:09:12.339963  2848 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 19:09:12.340134  2836 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 19:09:12.340241  2836 data_layer.cpp:83] output data size: 128,3,32,32
I0109 19:09:12.351236  2836 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:12.351935  2836 net.cpp:144] Setting up data
I0109 19:09:12.351953  2836 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 19:09:12.351959  2836 net.cpp:151] Top shape: 128 (128)
I0109 19:09:12.351963  2836 net.cpp:159] Memory required for data: 1573376
I0109 19:09:12.351969  2836 layer_factory.hpp:77] Creating layer conv1
I0109 19:09:12.351994  2836 net.cpp:94] Creating Layer conv1
I0109 19:09:12.352003  2836 net.cpp:435] conv1 <- data
I0109 19:09:12.352078  2836 net.cpp:409] conv1 -> conv1
I0109 19:09:12.353631  2836 net.cpp:144] Setting up conv1
I0109 19:09:12.353654  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.353662  2836 net.cpp:159] Memory required for data: 18350592
I0109 19:09:12.353760  2836 layer_factory.hpp:77] Creating layer bn1
I0109 19:09:12.353786  2836 net.cpp:94] Creating Layer bn1
I0109 19:09:12.353828  2836 net.cpp:435] bn1 <- conv1
I0109 19:09:12.353910  2836 net.cpp:409] bn1 -> scale1
I0109 19:09:12.354915  2836 net.cpp:144] Setting up bn1
I0109 19:09:12.354969  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.355020  2836 net.cpp:159] Memory required for data: 35127808
I0109 19:09:12.355077  2836 layer_factory.hpp:77] Creating layer relu1
I0109 19:09:12.355119  2836 net.cpp:94] Creating Layer relu1
I0109 19:09:12.355162  2836 net.cpp:435] relu1 <- scale1
I0109 19:09:12.355209  2836 net.cpp:409] relu1 -> relu1
I0109 19:09:12.355298  2836 net.cpp:144] Setting up relu1
I0109 19:09:12.355350  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.355392  2836 net.cpp:159] Memory required for data: 51905024
I0109 19:09:12.355443  2836 layer_factory.hpp:77] Creating layer conv2
I0109 19:09:12.355496  2836 net.cpp:94] Creating Layer conv2
I0109 19:09:12.355516  2836 net.cpp:435] conv2 <- relu1
I0109 19:09:12.355528  2836 net.cpp:409] conv2 -> conv2
I0109 19:09:12.356688  2836 net.cpp:144] Setting up conv2
I0109 19:09:12.356717  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.356727  2836 net.cpp:159] Memory required for data: 68682240
I0109 19:09:12.356775  2836 layer_factory.hpp:77] Creating layer bn2
I0109 19:09:12.356832  2836 net.cpp:94] Creating Layer bn2
I0109 19:09:12.356878  2836 net.cpp:435] bn2 <- conv2
I0109 19:09:12.356923  2836 net.cpp:409] bn2 -> scale2
I0109 19:09:12.357933  2836 net.cpp:144] Setting up bn2
I0109 19:09:12.357990  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.358034  2836 net.cpp:159] Memory required for data: 85459456
I0109 19:09:12.358086  2836 layer_factory.hpp:77] Creating layer relu2
I0109 19:09:12.358132  2836 net.cpp:94] Creating Layer relu2
I0109 19:09:12.358151  2836 net.cpp:435] relu2 <- scale2
I0109 19:09:12.358163  2836 net.cpp:409] relu2 -> relu2
I0109 19:09:12.358271  2836 net.cpp:144] Setting up relu2
I0109 19:09:12.358319  2836 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:09:12.358358  2836 net.cpp:159] Memory required for data: 102236672
I0109 19:09:12.358377  2836 layer_factory.hpp:77] Creating layer pool1
I0109 19:09:12.358425  2836 net.cpp:94] Creating Layer pool1
I0109 19:09:12.358469  2836 net.cpp:435] pool1 <- relu2
I0109 19:09:12.358512  2836 net.cpp:409] pool1 -> pool1
I0109 19:09:12.358618  2836 net.cpp:144] Setting up pool1
I0109 19:09:12.358635  2836 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:09:12.358642  2836 net.cpp:159] Memory required for data: 106430976
I0109 19:09:12.358711  2836 layer_factory.hpp:77] Creating layer drop1
I0109 19:09:12.358753  2836 net.cpp:94] Creating Layer drop1
I0109 19:09:12.358796  2836 net.cpp:435] drop1 <- pool1
I0109 19:09:12.358832  2836 net.cpp:409] drop1 -> drop1
I0109 19:09:12.359052  2836 net.cpp:144] Setting up drop1
I0109 19:09:12.359133  2836 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:09:12.359205  2836 net.cpp:159] Memory required for data: 110625280
I0109 19:09:12.359277  2836 layer_factory.hpp:77] Creating layer conv3
I0109 19:09:12.359351  2836 net.cpp:94] Creating Layer conv3
I0109 19:09:12.359433  2836 net.cpp:435] conv3 <- drop1
I0109 19:09:12.359477  2836 net.cpp:409] conv3 -> conv3
I0109 19:09:12.361007  2836 net.cpp:144] Setting up conv3
I0109 19:09:12.361029  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.361037  2836 net.cpp:159] Memory required for data: 119013888
I0109 19:09:12.361052  2836 layer_factory.hpp:77] Creating layer bn3
I0109 19:09:12.361140  2836 net.cpp:94] Creating Layer bn3
I0109 19:09:12.361150  2836 net.cpp:435] bn3 <- conv3
I0109 19:09:12.361227  2836 net.cpp:409] bn3 -> scale3
I0109 19:09:12.362305  2836 net.cpp:144] Setting up bn3
I0109 19:09:12.362385  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.362432  2836 net.cpp:159] Memory required for data: 127402496
I0109 19:09:12.362552  2836 layer_factory.hpp:77] Creating layer relu3
I0109 19:09:12.362627  2836 net.cpp:94] Creating Layer relu3
I0109 19:09:12.362699  2836 net.cpp:435] relu3 <- scale3
I0109 19:09:12.362768  2836 net.cpp:409] relu3 -> relu3
I0109 19:09:12.362869  2836 net.cpp:144] Setting up relu3
I0109 19:09:12.362951  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.362989  2836 net.cpp:159] Memory required for data: 135791104
I0109 19:09:12.363093  2836 layer_factory.hpp:77] Creating layer conv4
I0109 19:09:12.363122  2836 net.cpp:94] Creating Layer conv4
I0109 19:09:12.363131  2836 net.cpp:435] conv4 <- relu3
I0109 19:09:12.363216  2836 net.cpp:409] conv4 -> conv4
I0109 19:09:12.364217  2836 net.cpp:144] Setting up conv4
I0109 19:09:12.364306  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.364379  2836 net.cpp:159] Memory required for data: 144179712
I0109 19:09:12.364459  2836 layer_factory.hpp:77] Creating layer bn4
I0109 19:09:12.364537  2836 net.cpp:94] Creating Layer bn4
I0109 19:09:12.364552  2836 net.cpp:435] bn4 <- conv4
I0109 19:09:12.364564  2836 net.cpp:409] bn4 -> scale4
I0109 19:09:12.365703  2836 net.cpp:144] Setting up bn4
I0109 19:09:12.365725  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.365733  2836 net.cpp:159] Memory required for data: 152568320
I0109 19:09:12.365814  2836 layer_factory.hpp:77] Creating layer relu4
I0109 19:09:12.365836  2836 net.cpp:94] Creating Layer relu4
I0109 19:09:12.365844  2836 net.cpp:435] relu4 <- scale4
I0109 19:09:12.365914  2836 net.cpp:409] relu4 -> relu4
I0109 19:09:12.366030  2836 net.cpp:144] Setting up relu4
I0109 19:09:12.366116  2836 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:09:12.366190  2836 net.cpp:159] Memory required for data: 160956928
I0109 19:09:12.366258  2836 layer_factory.hpp:77] Creating layer pool2
I0109 19:09:12.366330  2836 net.cpp:94] Creating Layer pool2
I0109 19:09:12.366394  2836 net.cpp:435] pool2 <- relu4
I0109 19:09:12.366462  2836 net.cpp:409] pool2 -> pool2
I0109 19:09:12.366583  2836 net.cpp:144] Setting up pool2
I0109 19:09:12.366663  2836 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:09:12.366729  2836 net.cpp:159] Memory required for data: 163054080
I0109 19:09:12.366792  2836 layer_factory.hpp:77] Creating layer drop2
I0109 19:09:12.366863  2836 net.cpp:94] Creating Layer drop2
I0109 19:09:12.366930  2836 net.cpp:435] drop2 <- pool2
I0109 19:09:12.366952  2836 net.cpp:409] drop2 -> drop2
I0109 19:09:12.367044  2836 net.cpp:144] Setting up drop2
I0109 19:09:12.367065  2836 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:09:12.367074  2836 net.cpp:159] Memory required for data: 165151232
I0109 19:09:12.367080  2836 layer_factory.hpp:77] Creating layer fc1
I0109 19:09:12.367158  2836 net.cpp:94] Creating Layer fc1
I0109 19:09:12.367172  2836 net.cpp:435] fc1 <- drop2
I0109 19:09:12.367247  2836 net.cpp:409] fc1 -> fc1
I0109 19:09:12.389827  2836 net.cpp:144] Setting up fc1
I0109 19:09:12.389860  2836 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:09:12.389868  2836 net.cpp:159] Memory required for data: 165413376
I0109 19:09:12.389884  2836 layer_factory.hpp:77] Creating layer bn5
I0109 19:09:12.389905  2836 net.cpp:94] Creating Layer bn5
I0109 19:09:12.389914  2836 net.cpp:435] bn5 <- fc1
I0109 19:09:12.389927  2836 net.cpp:409] bn5 -> scale5
I0109 19:09:12.390578  2836 net.cpp:144] Setting up bn5
I0109 19:09:12.390600  2836 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:09:12.390609  2836 net.cpp:159] Memory required for data: 165675520
I0109 19:09:12.390635  2836 layer_factory.hpp:77] Creating layer relu5
I0109 19:09:12.390655  2836 net.cpp:94] Creating Layer relu5
I0109 19:09:12.390662  2836 net.cpp:435] relu5 <- scale5
I0109 19:09:12.390674  2836 net.cpp:409] relu5 -> relu5
I0109 19:09:12.390730  2836 net.cpp:144] Setting up relu5
I0109 19:09:12.390748  2836 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:09:12.390753  2836 net.cpp:159] Memory required for data: 165937664
I0109 19:09:12.390758  2836 layer_factory.hpp:77] Creating layer drop3
I0109 19:09:12.390769  2836 net.cpp:94] Creating Layer drop3
I0109 19:09:12.390784  2836 net.cpp:435] drop3 <- relu5
I0109 19:09:12.390795  2836 net.cpp:409] drop3 -> drop3
I0109 19:09:12.390847  2836 net.cpp:144] Setting up drop3
I0109 19:09:12.390861  2836 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:09:12.390866  2836 net.cpp:159] Memory required for data: 166199808
I0109 19:09:12.390871  2836 layer_factory.hpp:77] Creating layer fc2
I0109 19:09:12.390884  2836 net.cpp:94] Creating Layer fc2
I0109 19:09:12.390898  2836 net.cpp:435] fc2 <- drop3
I0109 19:09:12.390911  2836 net.cpp:409] fc2 -> fc2
I0109 19:09:12.391090  2836 net.cpp:144] Setting up fc2
I0109 19:09:12.391105  2836 net.cpp:151] Top shape: 128 10 (1280)
I0109 19:09:12.391109  2836 net.cpp:159] Memory required for data: 166204928
I0109 19:09:12.391119  2836 layer_factory.hpp:77] Creating layer loss
I0109 19:09:12.391137  2836 net.cpp:94] Creating Layer loss
I0109 19:09:12.391152  2836 net.cpp:435] loss <- fc2
I0109 19:09:12.391160  2836 net.cpp:435] loss <- label
I0109 19:09:12.391170  2836 net.cpp:409] loss -> loss
I0109 19:09:12.391193  2836 layer_factory.hpp:77] Creating layer loss
I0109 19:09:12.392036  2836 net.cpp:144] Setting up loss
I0109 19:09:12.392060  2836 net.cpp:151] Top shape: (1)
I0109 19:09:12.392065  2836 net.cpp:154]     with loss weight 1
I0109 19:09:12.392093  2836 net.cpp:159] Memory required for data: 166204932
I0109 19:09:12.392102  2836 net.cpp:220] loss needs backward computation.
I0109 19:09:12.392134  2836 net.cpp:220] fc2 needs backward computation.
I0109 19:09:12.392144  2836 net.cpp:220] drop3 needs backward computation.
I0109 19:09:12.392151  2836 net.cpp:220] relu5 needs backward computation.
I0109 19:09:12.392158  2836 net.cpp:220] bn5 needs backward computation.
I0109 19:09:12.392164  2836 net.cpp:220] fc1 needs backward computation.
I0109 19:09:12.392179  2836 net.cpp:220] drop2 needs backward computation.
I0109 19:09:12.392186  2836 net.cpp:220] pool2 needs backward computation.
I0109 19:09:12.392194  2836 net.cpp:220] relu4 needs backward computation.
I0109 19:09:12.392199  2836 net.cpp:220] bn4 needs backward computation.
I0109 19:09:12.392215  2836 net.cpp:220] conv4 needs backward computation.
I0109 19:09:12.392222  2836 net.cpp:220] relu3 needs backward computation.
I0109 19:09:12.392230  2836 net.cpp:220] bn3 needs backward computation.
I0109 19:09:12.392254  2836 net.cpp:220] conv3 needs backward computation.
I0109 19:09:12.392271  2836 net.cpp:220] drop1 needs backward computation.
I0109 19:09:12.392280  2836 net.cpp:220] pool1 needs backward computation.
I0109 19:09:12.392287  2836 net.cpp:220] relu2 needs backward computation.
I0109 19:09:12.392302  2836 net.cpp:220] bn2 needs backward computation.
I0109 19:09:12.392311  2836 net.cpp:220] conv2 needs backward computation.
I0109 19:09:12.392318  2836 net.cpp:220] relu1 needs backward computation.
I0109 19:09:12.392350  2836 net.cpp:220] bn1 needs backward computation.
I0109 19:09:12.392357  2836 net.cpp:220] conv1 needs backward computation.
I0109 19:09:12.392365  2836 net.cpp:222] data does not need backward computation.
I0109 19:09:12.392372  2836 net.cpp:264] This network produces output loss
I0109 19:09:12.392417  2836 net.cpp:284] Network initialization done.
I0109 19:09:12.392801  2836 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0109 19:09:12.392865  2836 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:09:12.393097  2836 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:09:12.393241  2836 layer_factory.hpp:77] Creating layer data
I0109 19:09:12.393307  2836 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:12.393457  2836 net.cpp:94] Creating Layer data
I0109 19:09:12.393478  2836 net.cpp:409] data -> data
I0109 19:09:12.393501  2836 net.cpp:409] data -> label
I0109 19:09:12.395079  2854 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:09:12.395120  2854 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:09:12.395225  2836 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:09:12.395390  2836 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:09:12.403456  2836 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:09:12.403524  2836 net.cpp:144] Setting up data
I0109 19:09:12.403570  2836 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:09:12.403609  2836 net.cpp:151] Top shape: 50 (50)
I0109 19:09:12.403640  2836 net.cpp:159] Memory required for data: 614600
I0109 19:09:12.403673  2836 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:09:12.403713  2836 net.cpp:94] Creating Layer label_data_1_split
I0109 19:09:12.403748  2836 net.cpp:435] label_data_1_split <- label
I0109 19:09:12.403789  2836 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:09:12.403828  2836 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:09:12.403868  2836 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:09:12.403983  2836 net.cpp:144] Setting up label_data_1_split
I0109 19:09:12.404004  2836 net.cpp:151] Top shape: 50 (50)
I0109 19:09:12.404014  2836 net.cpp:151] Top shape: 50 (50)
I0109 19:09:12.404021  2836 net.cpp:151] Top shape: 50 (50)
I0109 19:09:12.404026  2836 net.cpp:159] Memory required for data: 615200
I0109 19:09:12.404032  2836 layer_factory.hpp:77] Creating layer conv1
I0109 19:09:12.404057  2836 net.cpp:94] Creating Layer conv1
I0109 19:09:12.404088  2836 net.cpp:435] conv1 <- data
I0109 19:09:12.404125  2836 net.cpp:409] conv1 -> conv1
I0109 19:09:12.404518  2836 net.cpp:144] Setting up conv1
I0109 19:09:12.404539  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.404546  2836 net.cpp:159] Memory required for data: 7168800
I0109 19:09:12.404561  2836 layer_factory.hpp:77] Creating layer bn1
I0109 19:09:12.404577  2836 net.cpp:94] Creating Layer bn1
I0109 19:09:12.404610  2836 net.cpp:435] bn1 <- conv1
I0109 19:09:12.404652  2836 net.cpp:409] bn1 -> scale1
I0109 19:09:12.405858  2836 net.cpp:144] Setting up bn1
I0109 19:09:12.405879  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.405887  2836 net.cpp:159] Memory required for data: 13722400
I0109 19:09:12.405911  2836 layer_factory.hpp:77] Creating layer relu1
I0109 19:09:12.405928  2836 net.cpp:94] Creating Layer relu1
I0109 19:09:12.405937  2836 net.cpp:435] relu1 <- scale1
I0109 19:09:12.405951  2836 net.cpp:409] relu1 -> relu1
I0109 19:09:12.405997  2836 net.cpp:144] Setting up relu1
I0109 19:09:12.406009  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.406015  2836 net.cpp:159] Memory required for data: 20276000
I0109 19:09:12.406023  2836 layer_factory.hpp:77] Creating layer conv2
I0109 19:09:12.406042  2836 net.cpp:94] Creating Layer conv2
I0109 19:09:12.406059  2836 net.cpp:435] conv2 <- relu1
I0109 19:09:12.406075  2836 net.cpp:409] conv2 -> conv2
I0109 19:09:12.406631  2836 net.cpp:144] Setting up conv2
I0109 19:09:12.406661  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.406669  2836 net.cpp:159] Memory required for data: 26829600
I0109 19:09:12.406694  2836 layer_factory.hpp:77] Creating layer bn2
I0109 19:09:12.406714  2836 net.cpp:94] Creating Layer bn2
I0109 19:09:12.406726  2836 net.cpp:435] bn2 <- conv2
I0109 19:09:12.406751  2836 net.cpp:409] bn2 -> scale2
I0109 19:09:12.407693  2836 net.cpp:144] Setting up bn2
I0109 19:09:12.407721  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.407728  2836 net.cpp:159] Memory required for data: 33383200
I0109 19:09:12.407743  2836 layer_factory.hpp:77] Creating layer relu2
I0109 19:09:12.407758  2836 net.cpp:94] Creating Layer relu2
I0109 19:09:12.407768  2836 net.cpp:435] relu2 <- scale2
I0109 19:09:12.407778  2836 net.cpp:409] relu2 -> relu2
I0109 19:09:12.407820  2836 net.cpp:144] Setting up relu2
I0109 19:09:12.407840  2836 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:09:12.407848  2836 net.cpp:159] Memory required for data: 39936800
I0109 19:09:12.407855  2836 layer_factory.hpp:77] Creating layer pool1
I0109 19:09:12.407869  2836 net.cpp:94] Creating Layer pool1
I0109 19:09:12.407879  2836 net.cpp:435] pool1 <- relu2
I0109 19:09:12.407891  2836 net.cpp:409] pool1 -> pool1
I0109 19:09:12.407975  2836 net.cpp:144] Setting up pool1
I0109 19:09:12.407992  2836 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:09:12.407999  2836 net.cpp:159] Memory required for data: 41575200
I0109 19:09:12.408005  2836 layer_factory.hpp:77] Creating layer drop1
I0109 19:09:12.408015  2836 net.cpp:94] Creating Layer drop1
I0109 19:09:12.408028  2836 net.cpp:435] drop1 <- pool1
I0109 19:09:12.408042  2836 net.cpp:409] drop1 -> drop1
I0109 19:09:12.408107  2836 net.cpp:144] Setting up drop1
I0109 19:09:12.408123  2836 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:09:12.408129  2836 net.cpp:159] Memory required for data: 43213600
I0109 19:09:12.408134  2836 layer_factory.hpp:77] Creating layer conv3
I0109 19:09:12.408152  2836 net.cpp:94] Creating Layer conv3
I0109 19:09:12.408164  2836 net.cpp:435] conv3 <- drop1
I0109 19:09:12.408180  2836 net.cpp:409] conv3 -> conv3
I0109 19:09:12.408612  2836 net.cpp:144] Setting up conv3
I0109 19:09:12.408632  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.408637  2836 net.cpp:159] Memory required for data: 46490400
I0109 19:09:12.408643  2836 layer_factory.hpp:77] Creating layer bn3
I0109 19:09:12.408666  2836 net.cpp:94] Creating Layer bn3
I0109 19:09:12.408681  2836 net.cpp:435] bn3 <- conv3
I0109 19:09:12.408691  2836 net.cpp:409] bn3 -> scale3
I0109 19:09:12.409576  2836 net.cpp:144] Setting up bn3
I0109 19:09:12.409642  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.409692  2836 net.cpp:159] Memory required for data: 49767200
I0109 19:09:12.409723  2836 layer_factory.hpp:77] Creating layer relu3
I0109 19:09:12.409775  2836 net.cpp:94] Creating Layer relu3
I0109 19:09:12.409792  2836 net.cpp:435] relu3 <- scale3
I0109 19:09:12.409811  2836 net.cpp:409] relu3 -> relu3
I0109 19:09:12.409921  2836 net.cpp:144] Setting up relu3
I0109 19:09:12.409978  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.409996  2836 net.cpp:159] Memory required for data: 53044000
I0109 19:09:12.410308  2836 layer_factory.hpp:77] Creating layer conv4
I0109 19:09:12.410336  2836 net.cpp:94] Creating Layer conv4
I0109 19:09:12.410346  2836 net.cpp:435] conv4 <- relu3
I0109 19:09:12.410356  2836 net.cpp:409] conv4 -> conv4
I0109 19:09:12.411316  2836 net.cpp:144] Setting up conv4
I0109 19:09:12.411339  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.411347  2836 net.cpp:159] Memory required for data: 56320800
I0109 19:09:12.411357  2836 layer_factory.hpp:77] Creating layer bn4
I0109 19:09:12.411377  2836 net.cpp:94] Creating Layer bn4
I0109 19:09:12.411386  2836 net.cpp:435] bn4 <- conv4
I0109 19:09:12.411399  2836 net.cpp:409] bn4 -> scale4
I0109 19:09:12.412793  2836 net.cpp:144] Setting up bn4
I0109 19:09:12.412820  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.412827  2836 net.cpp:159] Memory required for data: 59597600
I0109 19:09:12.412843  2836 layer_factory.hpp:77] Creating layer relu4
I0109 19:09:12.412861  2836 net.cpp:94] Creating Layer relu4
I0109 19:09:12.412869  2836 net.cpp:435] relu4 <- scale4
I0109 19:09:12.412909  2836 net.cpp:409] relu4 -> relu4
I0109 19:09:12.413072  2836 net.cpp:144] Setting up relu4
I0109 19:09:12.413090  2836 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:09:12.413097  2836 net.cpp:159] Memory required for data: 62874400
I0109 19:09:12.413105  2836 layer_factory.hpp:77] Creating layer pool2
I0109 19:09:12.413120  2836 net.cpp:94] Creating Layer pool2
I0109 19:09:12.413127  2836 net.cpp:435] pool2 <- relu4
I0109 19:09:12.413142  2836 net.cpp:409] pool2 -> pool2
I0109 19:09:12.413203  2836 net.cpp:144] Setting up pool2
I0109 19:09:12.413209  2836 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:09:12.413213  2836 net.cpp:159] Memory required for data: 63693600
I0109 19:09:12.413218  2836 layer_factory.hpp:77] Creating layer drop2
I0109 19:09:12.413226  2836 net.cpp:94] Creating Layer drop2
I0109 19:09:12.413229  2836 net.cpp:435] drop2 <- pool2
I0109 19:09:12.413235  2836 net.cpp:409] drop2 -> drop2
I0109 19:09:12.413275  2836 net.cpp:144] Setting up drop2
I0109 19:09:12.413295  2836 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:09:12.413298  2836 net.cpp:159] Memory required for data: 64512800
I0109 19:09:12.413301  2836 layer_factory.hpp:77] Creating layer fc1
I0109 19:09:12.413311  2836 net.cpp:94] Creating Layer fc1
I0109 19:09:12.413314  2836 net.cpp:435] fc1 <- drop2
I0109 19:09:12.413322  2836 net.cpp:409] fc1 -> fc1
I0109 19:09:12.436249  2836 net.cpp:144] Setting up fc1
I0109 19:09:12.436286  2836 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:12.436291  2836 net.cpp:159] Memory required for data: 64615200
I0109 19:09:12.436305  2836 layer_factory.hpp:77] Creating layer bn5
I0109 19:09:12.436322  2836 net.cpp:94] Creating Layer bn5
I0109 19:09:12.436329  2836 net.cpp:435] bn5 <- fc1
I0109 19:09:12.436342  2836 net.cpp:409] bn5 -> scale5
I0109 19:09:12.437033  2836 net.cpp:144] Setting up bn5
I0109 19:09:12.437052  2836 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:12.437057  2836 net.cpp:159] Memory required for data: 64717600
I0109 19:09:12.437073  2836 layer_factory.hpp:77] Creating layer relu5
I0109 19:09:12.437085  2836 net.cpp:94] Creating Layer relu5
I0109 19:09:12.437090  2836 net.cpp:435] relu5 <- scale5
I0109 19:09:12.437104  2836 net.cpp:409] relu5 -> relu5
I0109 19:09:12.437150  2836 net.cpp:144] Setting up relu5
I0109 19:09:12.437168  2836 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:12.437173  2836 net.cpp:159] Memory required for data: 64820000
I0109 19:09:12.437180  2836 layer_factory.hpp:77] Creating layer drop3
I0109 19:09:12.437188  2836 net.cpp:94] Creating Layer drop3
I0109 19:09:12.437203  2836 net.cpp:435] drop3 <- relu5
I0109 19:09:12.437217  2836 net.cpp:409] drop3 -> drop3
I0109 19:09:12.437285  2836 net.cpp:144] Setting up drop3
I0109 19:09:12.437300  2836 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:09:12.437304  2836 net.cpp:159] Memory required for data: 64922400
I0109 19:09:12.437309  2836 layer_factory.hpp:77] Creating layer fc2
I0109 19:09:12.437325  2836 net.cpp:94] Creating Layer fc2
I0109 19:09:12.437340  2836 net.cpp:435] fc2 <- drop3
I0109 19:09:12.437353  2836 net.cpp:409] fc2 -> fc2
I0109 19:09:12.437557  2836 net.cpp:144] Setting up fc2
I0109 19:09:12.437574  2836 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:12.437579  2836 net.cpp:159] Memory required for data: 64924400
I0109 19:09:12.437608  2836 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:09:12.437629  2836 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:09:12.437638  2836 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:09:12.437647  2836 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:09:12.437659  2836 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:09:12.437692  2836 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:09:12.437777  2836 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:09:12.437793  2836 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:12.437803  2836 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:12.437808  2836 net.cpp:151] Top shape: 50 10 (500)
I0109 19:09:12.437814  2836 net.cpp:159] Memory required for data: 64930400
I0109 19:09:12.437820  2836 layer_factory.hpp:77] Creating layer loss
I0109 19:09:12.437834  2836 net.cpp:94] Creating Layer loss
I0109 19:09:12.437839  2836 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:09:12.437871  2836 net.cpp:435] loss <- label_data_1_split_0
I0109 19:09:12.437883  2836 net.cpp:409] loss -> loss
I0109 19:09:12.437908  2836 layer_factory.hpp:77] Creating layer loss
I0109 19:09:12.438030  2836 net.cpp:144] Setting up loss
I0109 19:09:12.438047  2836 net.cpp:151] Top shape: (1)
I0109 19:09:12.438055  2836 net.cpp:154]     with loss weight 1
I0109 19:09:12.438081  2836 net.cpp:159] Memory required for data: 64930404
I0109 19:09:12.438089  2836 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:09:12.438107  2836 net.cpp:94] Creating Layer accuracy-top1
I0109 19:09:12.438129  2836 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:09:12.438148  2836 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:09:12.438182  2836 net.cpp:409] accuracy-top1 -> top-1
I0109 19:09:12.438207  2836 net.cpp:144] Setting up accuracy-top1
I0109 19:09:12.438217  2836 net.cpp:151] Top shape: (1)
I0109 19:09:12.438222  2836 net.cpp:159] Memory required for data: 64930408
I0109 19:09:12.438228  2836 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:09:12.438241  2836 net.cpp:94] Creating Layer accuracy-top5
I0109 19:09:12.438248  2836 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:09:12.438261  2836 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:09:12.438297  2836 net.cpp:409] accuracy-top5 -> top-5
I0109 19:09:12.438323  2836 net.cpp:144] Setting up accuracy-top5
I0109 19:09:12.438331  2836 net.cpp:151] Top shape: (1)
I0109 19:09:12.438336  2836 net.cpp:159] Memory required for data: 64930412
I0109 19:09:12.438345  2836 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:09:12.438354  2836 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:09:12.438361  2836 net.cpp:220] loss needs backward computation.
I0109 19:09:12.438369  2836 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:09:12.438374  2836 net.cpp:220] fc2 needs backward computation.
I0109 19:09:12.438383  2836 net.cpp:220] drop3 needs backward computation.
I0109 19:09:12.438390  2836 net.cpp:220] relu5 needs backward computation.
I0109 19:09:12.438395  2836 net.cpp:220] bn5 needs backward computation.
I0109 19:09:12.438401  2836 net.cpp:220] fc1 needs backward computation.
I0109 19:09:12.438410  2836 net.cpp:220] drop2 needs backward computation.
I0109 19:09:12.438416  2836 net.cpp:220] pool2 needs backward computation.
I0109 19:09:12.438423  2836 net.cpp:220] relu4 needs backward computation.
I0109 19:09:12.438431  2836 net.cpp:220] bn4 needs backward computation.
I0109 19:09:12.438441  2836 net.cpp:220] conv4 needs backward computation.
I0109 19:09:12.438446  2836 net.cpp:220] relu3 needs backward computation.
I0109 19:09:12.438452  2836 net.cpp:220] bn3 needs backward computation.
I0109 19:09:12.438460  2836 net.cpp:220] conv3 needs backward computation.
I0109 19:09:12.438468  2836 net.cpp:220] drop1 needs backward computation.
I0109 19:09:12.438475  2836 net.cpp:220] pool1 needs backward computation.
I0109 19:09:12.438482  2836 net.cpp:220] relu2 needs backward computation.
I0109 19:09:12.438488  2836 net.cpp:220] bn2 needs backward computation.
I0109 19:09:12.438496  2836 net.cpp:220] conv2 needs backward computation.
I0109 19:09:12.438503  2836 net.cpp:220] relu1 needs backward computation.
I0109 19:09:12.438510  2836 net.cpp:220] bn1 needs backward computation.
I0109 19:09:12.438516  2836 net.cpp:220] conv1 needs backward computation.
I0109 19:09:12.438526  2836 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:09:12.438539  2836 net.cpp:222] data does not need backward computation.
I0109 19:09:12.438546  2836 net.cpp:264] This network produces output loss
I0109 19:09:12.438552  2836 net.cpp:264] This network produces output top-1
I0109 19:09:12.438562  2836 net.cpp:264] This network produces output top-5
I0109 19:09:12.438596  2836 net.cpp:284] Network initialization done.
I0109 19:09:12.438730  2836 solver.cpp:63] Solver scaffolding done.
I0109 19:09:12.440037  2836 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/sparse.caffemodel
W0109 19:09:12.463790  2836 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.464318  2836 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.464642  2836 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.464993  2836 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.467139  2836 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.485040  2836 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.485348  2836 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.485692  2836 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.486032  2836 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0109 19:09:12.487983  2836 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I0109 19:09:12.488114  2836 caffe_interface.cpp:527] Starting Optimization
I0109 19:09:12.488132  2836 solver.cpp:335] Solving 
I0109 19:09:12.488142  2836 solver.cpp:336] Learning Rate Policy: poly
I0109 19:09:12.489265  2836 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 19:09:13.298002  2836 solver.cpp:517]     Test net output #0: loss = 0.412943 (* 1 = 0.412943 loss)
I0109 19:09:13.298058  2836 solver.cpp:517]     Test net output #1: top-1 = 0.865
I0109 19:09:13.298070  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993555
I0109 19:09:13.346004  2836 solver.cpp:266] Iteration 0 (0 iter/s, 0.857847s/100 iter), loss = 0.112639
I0109 19:09:13.346093  2836 solver.cpp:285]     Train net output #0: loss = 0.112639 (* 1 = 0.112639 loss)
I0109 19:09:13.346133  2836 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 19:09:16.568424  2836 solver.cpp:266] Iteration 100 (31.0335 iter/s, 3.22232s/100 iter), loss = 0.187184
I0109 19:09:16.568486  2836 solver.cpp:285]     Train net output #0: loss = 0.187184 (* 1 = 0.187184 loss)
I0109 19:09:16.568500  2836 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 19:09:19.799504  2836 solver.cpp:266] Iteration 200 (30.9499 iter/s, 3.23103s/100 iter), loss = 0.290015
I0109 19:09:19.799569  2836 solver.cpp:285]     Train net output #0: loss = 0.290015 (* 1 = 0.290015 loss)
I0109 19:09:19.799582  2836 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 19:09:23.032131  2836 solver.cpp:266] Iteration 300 (30.9351 iter/s, 3.23257s/100 iter), loss = 0.369575
I0109 19:09:23.032193  2836 solver.cpp:285]     Train net output #0: loss = 0.369575 (* 1 = 0.369575 loss)
I0109 19:09:23.032207  2836 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 19:09:26.277781  2836 solver.cpp:266] Iteration 400 (30.811 iter/s, 3.2456s/100 iter), loss = 0.380601
I0109 19:09:26.277846  2836 solver.cpp:285]     Train net output #0: loss = 0.380601 (* 1 = 0.380601 loss)
I0109 19:09:26.277859  2836 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 19:09:29.521410  2836 solver.cpp:266] Iteration 500 (30.8304 iter/s, 3.24355s/100 iter), loss = 0.302052
I0109 19:09:29.521474  2836 solver.cpp:285]     Train net output #0: loss = 0.302052 (* 1 = 0.302052 loss)
I0109 19:09:29.521486  2836 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 19:09:32.773667  2836 solver.cpp:266] Iteration 600 (30.7484 iter/s, 3.2522s/100 iter), loss = 0.351163
I0109 19:09:32.773732  2836 solver.cpp:285]     Train net output #0: loss = 0.351163 (* 1 = 0.351163 loss)
I0109 19:09:32.773744  2836 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 19:09:36.030395  2836 solver.cpp:266] Iteration 700 (30.7062 iter/s, 3.25667s/100 iter), loss = 0.348503
I0109 19:09:36.030457  2836 solver.cpp:285]     Train net output #0: loss = 0.348503 (* 1 = 0.348503 loss)
I0109 19:09:36.030472  2836 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 19:09:39.282053  2836 solver.cpp:266] Iteration 800 (30.754 iter/s, 3.25161s/100 iter), loss = 0.215238
I0109 19:09:39.282119  2836 solver.cpp:285]     Train net output #0: loss = 0.215238 (* 1 = 0.215238 loss)
I0109 19:09:39.282166  2836 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 19:09:42.534965  2836 solver.cpp:266] Iteration 900 (30.7425 iter/s, 3.25283s/100 iter), loss = 0.263368
I0109 19:09:42.535099  2836 solver.cpp:285]     Train net output #0: loss = 0.263368 (* 1 = 0.263368 loss)
I0109 19:09:42.535120  2836 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 19:09:45.767946  2836 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 19:09:46.577215  2836 solver.cpp:517]     Test net output #0: loss = 0.870433 (* 1 = 0.870433 loss)
I0109 19:09:46.577255  2836 solver.cpp:517]     Test net output #1: top-1 = 0.770222
I0109 19:09:46.577263  2836 solver.cpp:517]     Test net output #2: top-5 = 0.975
I0109 19:09:46.607589  2836 solver.cpp:266] Iteration 1000 (24.5549 iter/s, 4.07251s/100 iter), loss = 0.186952
I0109 19:09:46.607648  2836 solver.cpp:285]     Train net output #0: loss = 0.186952 (* 1 = 0.186952 loss)
I0109 19:09:46.607664  2836 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 19:09:49.866291  2836 solver.cpp:266] Iteration 1100 (30.6875 iter/s, 3.25865s/100 iter), loss = 0.242886
I0109 19:09:49.866353  2836 solver.cpp:285]     Train net output #0: loss = 0.242886 (* 1 = 0.242886 loss)
I0109 19:09:49.866366  2836 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 19:09:53.125356  2836 solver.cpp:266] Iteration 1200 (30.6841 iter/s, 3.25901s/100 iter), loss = 0.342492
I0109 19:09:53.125421  2836 solver.cpp:285]     Train net output #0: loss = 0.342492 (* 1 = 0.342492 loss)
I0109 19:09:53.125432  2836 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 19:09:56.384519  2836 solver.cpp:266] Iteration 1300 (30.6832 iter/s, 3.25911s/100 iter), loss = 0.218129
I0109 19:09:56.384583  2836 solver.cpp:285]     Train net output #0: loss = 0.218129 (* 1 = 0.218129 loss)
I0109 19:09:56.384594  2836 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 19:09:59.645078  2836 solver.cpp:266] Iteration 1400 (30.6703 iter/s, 3.26048s/100 iter), loss = 0.282146
I0109 19:09:59.645148  2836 solver.cpp:285]     Train net output #0: loss = 0.282146 (* 1 = 0.282146 loss)
I0109 19:09:59.645159  2836 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 19:10:02.904448  2836 solver.cpp:266] Iteration 1500 (30.6813 iter/s, 3.25931s/100 iter), loss = 0.315838
I0109 19:10:02.904511  2836 solver.cpp:285]     Train net output #0: loss = 0.315838 (* 1 = 0.315838 loss)
I0109 19:10:02.904525  2836 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 19:10:06.165604  2836 solver.cpp:266] Iteration 1600 (30.6645 iter/s, 3.2611s/100 iter), loss = 0.323698
I0109 19:10:06.165666  2836 solver.cpp:285]     Train net output #0: loss = 0.323698 (* 1 = 0.323698 loss)
I0109 19:10:06.165679  2836 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 19:10:09.429529  2836 solver.cpp:266] Iteration 1700 (30.6384 iter/s, 3.26387s/100 iter), loss = 0.322067
I0109 19:10:09.429615  2836 solver.cpp:285]     Train net output #0: loss = 0.322067 (* 1 = 0.322067 loss)
I0109 19:10:09.429628  2836 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 19:10:12.690901  2836 solver.cpp:266] Iteration 1800 (30.6629 iter/s, 3.26127s/100 iter), loss = 0.362925
I0109 19:10:12.691046  2836 solver.cpp:285]     Train net output #0: loss = 0.362925 (* 1 = 0.362925 loss)
I0109 19:10:12.691061  2836 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 19:10:15.953258  2836 solver.cpp:266] Iteration 1900 (30.6539 iter/s, 3.26223s/100 iter), loss = 0.310285
I0109 19:10:15.953320  2836 solver.cpp:285]     Train net output #0: loss = 0.310285 (* 1 = 0.310285 loss)
I0109 19:10:15.953333  2836 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 19:10:19.186136  2836 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 19:10:20.004319  2836 solver.cpp:517]     Test net output #0: loss = 0.642263 (* 1 = 0.642263 loss)
I0109 19:10:20.004357  2836 solver.cpp:517]     Test net output #1: top-1 = 0.808
I0109 19:10:20.004367  2836 solver.cpp:517]     Test net output #2: top-5 = 0.986222
I0109 19:10:20.035221  2836 solver.cpp:266] Iteration 2000 (24.4983 iter/s, 4.08192s/100 iter), loss = 0.265132
I0109 19:10:20.035272  2836 solver.cpp:285]     Train net output #0: loss = 0.265132 (* 1 = 0.265132 loss)
I0109 19:10:20.035290  2836 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 19:10:23.297775  2836 solver.cpp:266] Iteration 2100 (30.6512 iter/s, 3.26251s/100 iter), loss = 0.237262
I0109 19:10:23.297847  2836 solver.cpp:285]     Train net output #0: loss = 0.237262 (* 1 = 0.237262 loss)
I0109 19:10:23.297860  2836 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 19:10:26.565429  2836 solver.cpp:266] Iteration 2200 (30.6038 iter/s, 3.26757s/100 iter), loss = 0.295535
I0109 19:10:26.565501  2836 solver.cpp:285]     Train net output #0: loss = 0.295535 (* 1 = 0.295535 loss)
I0109 19:10:26.565515  2836 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 19:10:29.841862  2836 solver.cpp:266] Iteration 2300 (30.5216 iter/s, 3.27637s/100 iter), loss = 0.321086
I0109 19:10:29.841945  2836 solver.cpp:285]     Train net output #0: loss = 0.321086 (* 1 = 0.321086 loss)
I0109 19:10:29.841964  2836 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 19:10:33.126278  2836 solver.cpp:266] Iteration 2400 (30.4474 iter/s, 3.28435s/100 iter), loss = 0.18176
I0109 19:10:33.126340  2836 solver.cpp:285]     Train net output #0: loss = 0.18176 (* 1 = 0.18176 loss)
I0109 19:10:33.126353  2836 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 19:10:36.422392  2836 solver.cpp:266] Iteration 2500 (30.3392 iter/s, 3.29607s/100 iter), loss = 0.342372
I0109 19:10:36.422456  2836 solver.cpp:285]     Train net output #0: loss = 0.342372 (* 1 = 0.342372 loss)
I0109 19:10:36.422468  2836 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 19:10:39.704617  2836 solver.cpp:266] Iteration 2600 (30.4678 iter/s, 3.28215s/100 iter), loss = 0.276775
I0109 19:10:39.704682  2836 solver.cpp:285]     Train net output #0: loss = 0.276775 (* 1 = 0.276775 loss)
I0109 19:10:39.704695  2836 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 19:10:43.006273  2836 solver.cpp:266] Iteration 2700 (30.2883 iter/s, 3.3016s/100 iter), loss = 0.254843
I0109 19:10:43.006490  2836 solver.cpp:285]     Train net output #0: loss = 0.254843 (* 1 = 0.254843 loss)
I0109 19:10:43.006510  2836 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 19:10:46.313684  2836 solver.cpp:266] Iteration 2800 (30.2369 iter/s, 3.30722s/100 iter), loss = 0.46081
I0109 19:10:46.313747  2836 solver.cpp:285]     Train net output #0: loss = 0.46081 (* 1 = 0.46081 loss)
I0109 19:10:46.313760  2836 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 19:10:49.621358  2836 solver.cpp:266] Iteration 2900 (30.2334 iter/s, 3.3076s/100 iter), loss = 0.254191
I0109 19:10:49.621423  2836 solver.cpp:285]     Train net output #0: loss = 0.254191 (* 1 = 0.254191 loss)
I0109 19:10:49.621438  2836 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 19:10:52.896229  2836 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 19:10:53.719032  2836 solver.cpp:517]     Test net output #0: loss = 0.576586 (* 1 = 0.576586 loss)
I0109 19:10:53.719067  2836 solver.cpp:517]     Test net output #1: top-1 = 0.822667
I0109 19:10:53.719075  2836 solver.cpp:517]     Test net output #2: top-5 = 0.987112
I0109 19:10:53.750144  2836 solver.cpp:266] Iteration 3000 (24.2204 iter/s, 4.12875s/100 iter), loss = 0.305609
I0109 19:10:53.750180  2836 solver.cpp:285]     Train net output #0: loss = 0.305609 (* 1 = 0.305609 loss)
I0109 19:10:53.750195  2836 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 19:10:57.059954  2836 solver.cpp:266] Iteration 3100 (30.2134 iter/s, 3.30979s/100 iter), loss = 0.300276
I0109 19:10:57.060019  2836 solver.cpp:285]     Train net output #0: loss = 0.300276 (* 1 = 0.300276 loss)
I0109 19:10:57.060031  2836 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 19:11:00.375313  2836 solver.cpp:266] Iteration 3200 (30.1631 iter/s, 3.31531s/100 iter), loss = 0.290263
I0109 19:11:00.375402  2836 solver.cpp:285]     Train net output #0: loss = 0.290263 (* 1 = 0.290263 loss)
I0109 19:11:00.375422  2836 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 19:11:03.691859  2836 solver.cpp:266] Iteration 3300 (30.1527 iter/s, 3.31645s/100 iter), loss = 0.221092
I0109 19:11:03.691925  2836 solver.cpp:285]     Train net output #0: loss = 0.221092 (* 1 = 0.221092 loss)
I0109 19:11:03.691937  2836 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 19:11:07.006151  2836 solver.cpp:266] Iteration 3400 (30.1728 iter/s, 3.31425s/100 iter), loss = 0.236463
I0109 19:11:07.006217  2836 solver.cpp:285]     Train net output #0: loss = 0.236463 (* 1 = 0.236463 loss)
I0109 19:11:07.006229  2836 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 19:11:10.329404  2836 solver.cpp:266] Iteration 3500 (30.0914 iter/s, 3.32321s/100 iter), loss = 0.272816
I0109 19:11:10.329470  2836 solver.cpp:285]     Train net output #0: loss = 0.272816 (* 1 = 0.272816 loss)
I0109 19:11:10.329483  2836 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 19:11:13.655630  2836 solver.cpp:266] Iteration 3600 (30.0648 iter/s, 3.32615s/100 iter), loss = 0.276928
I0109 19:11:13.655843  2836 solver.cpp:285]     Train net output #0: loss = 0.276928 (* 1 = 0.276928 loss)
I0109 19:11:13.655863  2836 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 19:11:16.980201  2836 solver.cpp:266] Iteration 3700 (30.0808 iter/s, 3.32438s/100 iter), loss = 0.216472
I0109 19:11:16.980273  2836 solver.cpp:285]     Train net output #0: loss = 0.216472 (* 1 = 0.216472 loss)
I0109 19:11:16.980290  2836 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 19:11:20.317682  2836 solver.cpp:266] Iteration 3800 (29.9632 iter/s, 3.33743s/100 iter), loss = 0.302691
I0109 19:11:20.317759  2836 solver.cpp:285]     Train net output #0: loss = 0.302691 (* 1 = 0.302691 loss)
I0109 19:11:20.317775  2836 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 19:11:23.655170  2836 solver.cpp:266] Iteration 3900 (29.9634 iter/s, 3.33741s/100 iter), loss = 0.264125
I0109 19:11:23.655236  2836 solver.cpp:285]     Train net output #0: loss = 0.264125 (* 1 = 0.264125 loss)
I0109 19:11:23.655248  2836 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 19:11:26.969774  2836 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 19:11:27.790356  2836 solver.cpp:517]     Test net output #0: loss = 0.520279 (* 1 = 0.520279 loss)
I0109 19:11:27.790396  2836 solver.cpp:517]     Test net output #1: top-1 = 0.833333
I0109 19:11:27.790405  2836 solver.cpp:517]     Test net output #2: top-5 = 0.99
I0109 19:11:27.821722  2836 solver.cpp:266] Iteration 4000 (24.0009 iter/s, 4.16652s/100 iter), loss = 0.179384
I0109 19:11:27.821774  2836 solver.cpp:285]     Train net output #0: loss = 0.179384 (* 1 = 0.179384 loss)
I0109 19:11:27.821791  2836 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 19:11:31.161429  2836 solver.cpp:266] Iteration 4100 (29.943 iter/s, 3.33968s/100 iter), loss = 0.212472
I0109 19:11:31.161505  2836 solver.cpp:285]     Train net output #0: loss = 0.212472 (* 1 = 0.212472 loss)
I0109 19:11:31.161521  2836 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 19:11:34.500757  2836 solver.cpp:266] Iteration 4200 (29.9468 iter/s, 3.33925s/100 iter), loss = 0.288013
I0109 19:11:34.500824  2836 solver.cpp:285]     Train net output #0: loss = 0.288013 (* 1 = 0.288013 loss)
I0109 19:11:34.500838  2836 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 19:11:37.850576  2836 solver.cpp:266] Iteration 4300 (29.8527 iter/s, 3.34978s/100 iter), loss = 0.240374
I0109 19:11:37.850638  2836 solver.cpp:285]     Train net output #0: loss = 0.240374 (* 1 = 0.240374 loss)
I0109 19:11:37.850652  2836 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 19:11:41.202553  2836 solver.cpp:266] Iteration 4400 (29.8335 iter/s, 3.35193s/100 iter), loss = 0.24934
I0109 19:11:41.202633  2836 solver.cpp:285]     Train net output #0: loss = 0.24934 (* 1 = 0.24934 loss)
I0109 19:11:41.202648  2836 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 19:11:44.551599  2836 solver.cpp:266] Iteration 4500 (29.86 iter/s, 3.34896s/100 iter), loss = 0.257186
I0109 19:11:44.551810  2836 solver.cpp:285]     Train net output #0: loss = 0.257186 (* 1 = 0.257186 loss)
I0109 19:11:44.551826  2836 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 19:11:47.903266  2836 solver.cpp:266] Iteration 4600 (29.8375 iter/s, 3.35149s/100 iter), loss = 0.381903
I0109 19:11:47.903331  2836 solver.cpp:285]     Train net output #0: loss = 0.381903 (* 1 = 0.381903 loss)
I0109 19:11:47.903343  2836 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 19:11:51.252957  2836 solver.cpp:266] Iteration 4700 (29.8538 iter/s, 3.34965s/100 iter), loss = 0.201021
I0109 19:11:51.253023  2836 solver.cpp:285]     Train net output #0: loss = 0.201021 (* 1 = 0.201021 loss)
I0109 19:11:51.253037  2836 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 19:11:54.601338  2836 solver.cpp:266] Iteration 4800 (29.8658 iter/s, 3.34831s/100 iter), loss = 0.311493
I0109 19:11:54.601409  2836 solver.cpp:285]     Train net output #0: loss = 0.311493 (* 1 = 0.311493 loss)
I0109 19:11:54.601423  2836 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 19:11:57.952057  2836 solver.cpp:266] Iteration 4900 (29.8447 iter/s, 3.35067s/100 iter), loss = 0.200646
I0109 19:11:57.952139  2836 solver.cpp:285]     Train net output #0: loss = 0.200646 (* 1 = 0.200646 loss)
I0109 19:11:57.952154  2836 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 19:12:01.266218  2836 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 19:12:02.091868  2836 solver.cpp:517]     Test net output #0: loss = 0.618237 (* 1 = 0.618237 loss)
I0109 19:12:02.091908  2836 solver.cpp:517]     Test net output #1: top-1 = 0.816222
I0109 19:12:02.091917  2836 solver.cpp:517]     Test net output #2: top-5 = 0.990444
I0109 19:12:02.123198  2836 solver.cpp:266] Iteration 5000 (23.9745 iter/s, 4.1711s/100 iter), loss = 0.172545
I0109 19:12:02.123235  2836 solver.cpp:285]     Train net output #0: loss = 0.172545 (* 1 = 0.172545 loss)
I0109 19:12:02.123250  2836 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 19:12:05.477430  2836 solver.cpp:266] Iteration 5100 (29.8135 iter/s, 3.35419s/100 iter), loss = 0.188789
I0109 19:12:05.477511  2836 solver.cpp:285]     Train net output #0: loss = 0.188789 (* 1 = 0.188789 loss)
I0109 19:12:05.477527  2836 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 19:12:08.825469  2836 solver.cpp:266] Iteration 5200 (29.8687 iter/s, 3.34799s/100 iter), loss = 0.242195
I0109 19:12:08.825531  2836 solver.cpp:285]     Train net output #0: loss = 0.242195 (* 1 = 0.242195 loss)
I0109 19:12:08.825544  2836 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 19:12:12.170835  2836 solver.cpp:266] Iteration 5300 (29.8924 iter/s, 3.34533s/100 iter), loss = 0.252221
I0109 19:12:12.170899  2836 solver.cpp:285]     Train net output #0: loss = 0.252221 (* 1 = 0.252221 loss)
I0109 19:12:12.170912  2836 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 19:12:15.509002  2836 solver.cpp:266] Iteration 5400 (29.9571 iter/s, 3.3381s/100 iter), loss = 0.152214
I0109 19:12:15.509162  2836 solver.cpp:285]     Train net output #0: loss = 0.152214 (* 1 = 0.152214 loss)
I0109 19:12:15.509178  2836 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 19:12:18.859563  2836 solver.cpp:266] Iteration 5500 (29.8469 iter/s, 3.35043s/100 iter), loss = 0.217195
I0109 19:12:18.859629  2836 solver.cpp:285]     Train net output #0: loss = 0.217195 (* 1 = 0.217195 loss)
I0109 19:12:18.859643  2836 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 19:12:22.201731  2836 solver.cpp:266] Iteration 5600 (29.921 iter/s, 3.34213s/100 iter), loss = 0.275965
I0109 19:12:22.201797  2836 solver.cpp:285]     Train net output #0: loss = 0.275965 (* 1 = 0.275965 loss)
I0109 19:12:22.201808  2836 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 19:12:25.552615  2836 solver.cpp:266] Iteration 5700 (29.8434 iter/s, 3.35082s/100 iter), loss = 0.227394
I0109 19:12:25.552676  2836 solver.cpp:285]     Train net output #0: loss = 0.227394 (* 1 = 0.227394 loss)
I0109 19:12:25.552688  2836 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 19:12:28.879165  2836 solver.cpp:266] Iteration 5800 (30.0615 iter/s, 3.32652s/100 iter), loss = 0.170879
I0109 19:12:28.879230  2836 solver.cpp:285]     Train net output #0: loss = 0.170879 (* 1 = 0.170879 loss)
I0109 19:12:28.879242  2836 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 19:12:32.201226  2836 solver.cpp:266] Iteration 5900 (30.1021 iter/s, 3.32203s/100 iter), loss = 0.235116
I0109 19:12:32.201292  2836 solver.cpp:285]     Train net output #0: loss = 0.235116 (* 1 = 0.235116 loss)
I0109 19:12:32.201305  2836 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 19:12:35.489370  2836 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 19:12:36.310545  2836 solver.cpp:517]     Test net output #0: loss = 0.489709 (* 1 = 0.489709 loss)
I0109 19:12:36.310582  2836 solver.cpp:517]     Test net output #1: top-1 = 0.841444
I0109 19:12:36.310590  2836 solver.cpp:517]     Test net output #2: top-5 = 0.990444
I0109 19:12:36.341500  2836 solver.cpp:266] Iteration 6000 (24.1531 iter/s, 4.14025s/100 iter), loss = 0.253373
I0109 19:12:36.341536  2836 solver.cpp:285]     Train net output #0: loss = 0.253373 (* 1 = 0.253373 loss)
I0109 19:12:36.341552  2836 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 19:12:39.641132  2836 solver.cpp:266] Iteration 6100 (30.3068 iter/s, 3.29959s/100 iter), loss = 0.290647
I0109 19:12:39.641211  2836 solver.cpp:285]     Train net output #0: loss = 0.290647 (* 1 = 0.290647 loss)
I0109 19:12:39.641225  2836 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 19:12:42.954416  2836 solver.cpp:266] Iteration 6200 (30.182 iter/s, 3.31323s/100 iter), loss = 0.244891
I0109 19:12:42.954483  2836 solver.cpp:285]     Train net output #0: loss = 0.244891 (* 1 = 0.244891 loss)
I0109 19:12:42.954496  2836 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 19:12:46.270171  2836 solver.cpp:266] Iteration 6300 (30.1594 iter/s, 3.31572s/100 iter), loss = 0.177732
I0109 19:12:46.270388  2836 solver.cpp:285]     Train net output #0: loss = 0.177732 (* 1 = 0.177732 loss)
I0109 19:12:46.270407  2836 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 19:12:49.590389  2836 solver.cpp:266] Iteration 6400 (30.1204 iter/s, 3.32s/100 iter), loss = 0.215697
I0109 19:12:49.590462  2836 solver.cpp:285]     Train net output #0: loss = 0.215697 (* 1 = 0.215697 loss)
I0109 19:12:49.590476  2836 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 19:12:52.909723  2836 solver.cpp:266] Iteration 6500 (30.127 iter/s, 3.31929s/100 iter), loss = 0.208242
I0109 19:12:52.909801  2836 solver.cpp:285]     Train net output #0: loss = 0.208242 (* 1 = 0.208242 loss)
I0109 19:12:52.909816  2836 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 19:12:56.220182  2836 solver.cpp:266] Iteration 6600 (30.2077 iter/s, 3.31041s/100 iter), loss = 0.141517
I0109 19:12:56.220255  2836 solver.cpp:285]     Train net output #0: loss = 0.141517 (* 1 = 0.141517 loss)
I0109 19:12:56.220274  2836 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 19:12:59.540925  2836 solver.cpp:266] Iteration 6700 (30.1144 iter/s, 3.32067s/100 iter), loss = 0.273912
I0109 19:12:59.540992  2836 solver.cpp:285]     Train net output #0: loss = 0.273912 (* 1 = 0.273912 loss)
I0109 19:12:59.541007  2836 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 19:13:02.868281  2836 solver.cpp:266] Iteration 6800 (30.0543 iter/s, 3.32731s/100 iter), loss = 0.289096
I0109 19:13:02.868361  2836 solver.cpp:285]     Train net output #0: loss = 0.289096 (* 1 = 0.289096 loss)
I0109 19:13:02.868377  2836 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 19:13:06.188467  2836 solver.cpp:266] Iteration 6900 (30.1193 iter/s, 3.32013s/100 iter), loss = 0.153717
I0109 19:13:06.188539  2836 solver.cpp:285]     Train net output #0: loss = 0.153717 (* 1 = 0.153717 loss)
I0109 19:13:06.188552  2836 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 19:13:09.472390  2836 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 19:13:10.297298  2836 solver.cpp:517]     Test net output #0: loss = 0.5647 (* 1 = 0.5647 loss)
I0109 19:13:10.297338  2836 solver.cpp:517]     Test net output #1: top-1 = 0.821889
I0109 19:13:10.297346  2836 solver.cpp:517]     Test net output #2: top-5 = 0.988445
I0109 19:13:10.328629  2836 solver.cpp:266] Iteration 7000 (24.1539 iter/s, 4.14012s/100 iter), loss = 0.192981
I0109 19:13:10.328709  2836 solver.cpp:285]     Train net output #0: loss = 0.192981 (* 1 = 0.192981 loss)
I0109 19:13:10.328727  2836 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 19:13:13.650427  2836 solver.cpp:266] Iteration 7100 (30.1049 iter/s, 3.32172s/100 iter), loss = 0.272019
I0109 19:13:13.650496  2836 solver.cpp:285]     Train net output #0: loss = 0.272019 (* 1 = 0.272019 loss)
I0109 19:13:13.650509  2836 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 19:13:16.987241  2836 solver.cpp:266] Iteration 7200 (29.9691 iter/s, 3.33677s/100 iter), loss = 0.292521
I0109 19:13:16.987473  2836 solver.cpp:285]     Train net output #0: loss = 0.292521 (* 1 = 0.292521 loss)
I0109 19:13:16.987489  2836 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 19:13:20.316315  2836 solver.cpp:266] Iteration 7300 (30.0402 iter/s, 3.32887s/100 iter), loss = 0.191401
I0109 19:13:20.316393  2836 solver.cpp:285]     Train net output #0: loss = 0.191401 (* 1 = 0.191401 loss)
I0109 19:13:20.316412  2836 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 19:13:23.635238  2836 solver.cpp:266] Iteration 7400 (30.1309 iter/s, 3.31885s/100 iter), loss = 0.158644
I0109 19:13:23.635308  2836 solver.cpp:285]     Train net output #0: loss = 0.158644 (* 1 = 0.158644 loss)
I0109 19:13:23.635325  2836 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 19:13:26.966150  2836 solver.cpp:266] Iteration 7500 (30.0222 iter/s, 3.33087s/100 iter), loss = 0.18882
I0109 19:13:26.966238  2836 solver.cpp:285]     Train net output #0: loss = 0.18882 (* 1 = 0.18882 loss)
I0109 19:13:26.966256  2836 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 19:13:30.314803  2836 solver.cpp:266] Iteration 7600 (29.8632 iter/s, 3.3486s/100 iter), loss = 0.228343
I0109 19:13:30.314872  2836 solver.cpp:285]     Train net output #0: loss = 0.228343 (* 1 = 0.228343 loss)
I0109 19:13:30.314884  2836 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 19:13:33.668542  2836 solver.cpp:266] Iteration 7700 (29.8181 iter/s, 3.35367s/100 iter), loss = 0.16788
I0109 19:13:33.668609  2836 solver.cpp:285]     Train net output #0: loss = 0.16788 (* 1 = 0.16788 loss)
I0109 19:13:33.668622  2836 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 19:13:37.019485  2836 solver.cpp:266] Iteration 7800 (29.8427 iter/s, 3.3509s/100 iter), loss = 0.17402
I0109 19:13:37.019552  2836 solver.cpp:285]     Train net output #0: loss = 0.17402 (* 1 = 0.17402 loss)
I0109 19:13:37.019565  2836 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 19:13:40.365408  2836 solver.cpp:266] Iteration 7900 (29.8874 iter/s, 3.34589s/100 iter), loss = 0.198588
I0109 19:13:40.365474  2836 solver.cpp:285]     Train net output #0: loss = 0.198588 (* 1 = 0.198588 loss)
I0109 19:13:40.365487  2836 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 19:13:43.679553  2836 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 19:13:44.506981  2836 solver.cpp:517]     Test net output #0: loss = 0.468325 (* 1 = 0.468325 loss)
I0109 19:13:44.507021  2836 solver.cpp:517]     Test net output #1: top-1 = 0.850777
I0109 19:13:44.507030  2836 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:13:44.538370  2836 solver.cpp:266] Iteration 8000 (23.9641 iter/s, 4.17291s/100 iter), loss = 0.192868
I0109 19:13:44.538409  2836 solver.cpp:285]     Train net output #0: loss = 0.192868 (* 1 = 0.192868 loss)
I0109 19:13:44.538424  2836 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 19:13:47.875715  2836 solver.cpp:266] Iteration 8100 (29.9641 iter/s, 3.33733s/100 iter), loss = 0.247957
I0109 19:13:47.875881  2836 solver.cpp:285]     Train net output #0: loss = 0.247957 (* 1 = 0.247957 loss)
I0109 19:13:47.875896  2836 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 19:13:51.217767  2836 solver.cpp:266] Iteration 8200 (29.9229 iter/s, 3.34192s/100 iter), loss = 0.163279
I0109 19:13:51.217831  2836 solver.cpp:285]     Train net output #0: loss = 0.163279 (* 1 = 0.163279 loss)
I0109 19:13:51.217844  2836 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 19:13:54.562014  2836 solver.cpp:266] Iteration 8300 (29.9026 iter/s, 3.34419s/100 iter), loss = 0.30017
I0109 19:13:54.562080  2836 solver.cpp:285]     Train net output #0: loss = 0.30017 (* 1 = 0.30017 loss)
I0109 19:13:54.562094  2836 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 19:13:57.914584  2836 solver.cpp:266] Iteration 8400 (29.8282 iter/s, 3.35254s/100 iter), loss = 0.17494
I0109 19:13:57.914647  2836 solver.cpp:285]     Train net output #0: loss = 0.17494 (* 1 = 0.17494 loss)
I0109 19:13:57.914660  2836 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 19:14:01.264612  2836 solver.cpp:266] Iteration 8500 (29.8508 iter/s, 3.34999s/100 iter), loss = 0.237633
I0109 19:14:01.264678  2836 solver.cpp:285]     Train net output #0: loss = 0.237633 (* 1 = 0.237633 loss)
I0109 19:14:01.264690  2836 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 19:14:04.611239  2836 solver.cpp:266] Iteration 8600 (29.8814 iter/s, 3.34657s/100 iter), loss = 0.214269
I0109 19:14:04.611304  2836 solver.cpp:285]     Train net output #0: loss = 0.214269 (* 1 = 0.214269 loss)
I0109 19:14:04.611315  2836 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 19:14:07.962519  2836 solver.cpp:266] Iteration 8700 (29.8396 iter/s, 3.35125s/100 iter), loss = 0.210334
I0109 19:14:07.962584  2836 solver.cpp:285]     Train net output #0: loss = 0.210334 (* 1 = 0.210334 loss)
I0109 19:14:07.962597  2836 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 19:14:11.306198  2836 solver.cpp:266] Iteration 8800 (29.9075 iter/s, 3.34364s/100 iter), loss = 0.146146
I0109 19:14:11.306272  2836 solver.cpp:285]     Train net output #0: loss = 0.146146 (* 1 = 0.146146 loss)
I0109 19:14:11.306286  2836 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 19:14:14.652427  2836 solver.cpp:266] Iteration 8900 (29.885 iter/s, 3.34616s/100 iter), loss = 0.265345
I0109 19:14:14.652492  2836 solver.cpp:285]     Train net output #0: loss = 0.265345 (* 1 = 0.265345 loss)
I0109 19:14:14.652503  2836 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 19:14:17.968492  2836 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 19:14:18.798472  2836 solver.cpp:517]     Test net output #0: loss = 0.472577 (* 1 = 0.472577 loss)
I0109 19:14:18.798512  2836 solver.cpp:517]     Test net output #1: top-1 = 0.852777
I0109 19:14:18.798521  2836 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 19:14:18.829834  2836 solver.cpp:266] Iteration 9000 (23.9384 iter/s, 4.17738s/100 iter), loss = 0.156565
I0109 19:14:18.829895  2836 solver.cpp:285]     Train net output #0: loss = 0.156565 (* 1 = 0.156565 loss)
I0109 19:14:18.829910  2836 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 19:14:22.181607  2836 solver.cpp:266] Iteration 9100 (29.8353 iter/s, 3.35174s/100 iter), loss = 0.177966
I0109 19:14:22.181672  2836 solver.cpp:285]     Train net output #0: loss = 0.177966 (* 1 = 0.177966 loss)
I0109 19:14:22.181685  2836 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 19:14:25.537600  2836 solver.cpp:266] Iteration 9200 (29.7981 iter/s, 3.35592s/100 iter), loss = 0.269326
I0109 19:14:25.537668  2836 solver.cpp:285]     Train net output #0: loss = 0.269326 (* 1 = 0.269326 loss)
I0109 19:14:25.537681  2836 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 19:14:28.882364  2836 solver.cpp:266] Iteration 9300 (29.8978 iter/s, 3.34473s/100 iter), loss = 0.17968
I0109 19:14:28.882431  2836 solver.cpp:285]     Train net output #0: loss = 0.17968 (* 1 = 0.17968 loss)
I0109 19:14:28.882441  2836 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 19:14:32.236881  2836 solver.cpp:266] Iteration 9400 (29.8109 iter/s, 3.35448s/100 iter), loss = 0.209977
I0109 19:14:32.236943  2836 solver.cpp:285]     Train net output #0: loss = 0.209977 (* 1 = 0.209977 loss)
I0109 19:14:32.236955  2836 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 19:14:35.592355  2836 solver.cpp:266] Iteration 9500 (29.8025 iter/s, 3.35542s/100 iter), loss = 0.177077
I0109 19:14:35.592418  2836 solver.cpp:285]     Train net output #0: loss = 0.177077 (* 1 = 0.177077 loss)
I0109 19:14:35.592432  2836 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 19:14:38.950997  2836 solver.cpp:266] Iteration 9600 (29.7742 iter/s, 3.35861s/100 iter), loss = 0.171496
I0109 19:14:38.951059  2836 solver.cpp:285]     Train net output #0: loss = 0.171496 (* 1 = 0.171496 loss)
I0109 19:14:38.951072  2836 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 19:14:42.311508  2836 solver.cpp:266] Iteration 9700 (29.7577 iter/s, 3.36048s/100 iter), loss = 0.152207
I0109 19:14:42.311590  2836 solver.cpp:285]     Train net output #0: loss = 0.152207 (* 1 = 0.152207 loss)
I0109 19:14:42.311606  2836 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 19:14:45.676740  2836 solver.cpp:266] Iteration 9800 (29.7163 iter/s, 3.36515s/100 iter), loss = 0.13267
I0109 19:14:45.676821  2836 solver.cpp:285]     Train net output #0: loss = 0.13267 (* 1 = 0.13267 loss)
I0109 19:14:45.676836  2836 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 19:14:49.035521  2836 solver.cpp:266] Iteration 9900 (29.7731 iter/s, 3.35873s/100 iter), loss = 0.137209
I0109 19:14:49.035708  2836 solver.cpp:285]     Train net output #0: loss = 0.137209 (* 1 = 0.137209 loss)
I0109 19:14:49.035725  2836 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 19:14:52.362195  2836 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 19:14:53.190811  2836 solver.cpp:517]     Test net output #0: loss = 0.489349 (* 1 = 0.489349 loss)
I0109 19:14:53.190850  2836 solver.cpp:517]     Test net output #1: top-1 = 0.848
I0109 19:14:53.190860  2836 solver.cpp:517]     Test net output #2: top-5 = 0.991222
I0109 19:14:53.222185  2836 solver.cpp:266] Iteration 10000 (23.8861 iter/s, 4.18653s/100 iter), loss = 0.146675
I0109 19:14:53.222220  2836 solver.cpp:285]     Train net output #0: loss = 0.146675 (* 1 = 0.146675 loss)
I0109 19:14:53.222235  2836 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 19:14:56.580703  2836 solver.cpp:266] Iteration 10100 (29.7754 iter/s, 3.35848s/100 iter), loss = 0.133439
I0109 19:14:56.580786  2836 solver.cpp:285]     Train net output #0: loss = 0.133439 (* 1 = 0.133439 loss)
I0109 19:14:56.580802  2836 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 19:14:59.941608  2836 solver.cpp:266] Iteration 10200 (29.7544 iter/s, 3.36085s/100 iter), loss = 0.126491
I0109 19:14:59.941674  2836 solver.cpp:285]     Train net output #0: loss = 0.126491 (* 1 = 0.126491 loss)
I0109 19:14:59.941686  2836 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 19:15:03.302435  2836 solver.cpp:266] Iteration 10300 (29.7549 iter/s, 3.3608s/100 iter), loss = 0.116571
I0109 19:15:03.302500  2836 solver.cpp:285]     Train net output #0: loss = 0.116571 (* 1 = 0.116571 loss)
I0109 19:15:03.302512  2836 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 19:15:06.660954  2836 solver.cpp:266] Iteration 10400 (29.7756 iter/s, 3.35846s/100 iter), loss = 0.179898
I0109 19:15:06.661036  2836 solver.cpp:285]     Train net output #0: loss = 0.179898 (* 1 = 0.179898 loss)
I0109 19:15:06.661049  2836 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 19:15:10.024629  2836 solver.cpp:266] Iteration 10500 (29.7298 iter/s, 3.36363s/100 iter), loss = 0.218963
I0109 19:15:10.024695  2836 solver.cpp:285]     Train net output #0: loss = 0.218963 (* 1 = 0.218963 loss)
I0109 19:15:10.024708  2836 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 19:15:13.382047  2836 solver.cpp:266] Iteration 10600 (29.7851 iter/s, 3.35739s/100 iter), loss = 0.142248
I0109 19:15:13.382113  2836 solver.cpp:285]     Train net output #0: loss = 0.142248 (* 1 = 0.142248 loss)
I0109 19:15:13.382125  2836 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 19:15:16.738554  2836 solver.cpp:266] Iteration 10700 (29.7934 iter/s, 3.35645s/100 iter), loss = 0.139628
I0109 19:15:16.738621  2836 solver.cpp:285]     Train net output #0: loss = 0.139628 (* 1 = 0.139628 loss)
I0109 19:15:16.738636  2836 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 19:15:20.100531  2836 solver.cpp:266] Iteration 10800 (29.7446 iter/s, 3.36195s/100 iter), loss = 0.126903
I0109 19:15:20.100714  2836 solver.cpp:285]     Train net output #0: loss = 0.126903 (* 1 = 0.126903 loss)
I0109 19:15:20.100733  2836 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 19:15:23.460389  2836 solver.cpp:266] Iteration 10900 (29.7644 iter/s, 3.35972s/100 iter), loss = 0.155064
I0109 19:15:23.460454  2836 solver.cpp:285]     Train net output #0: loss = 0.155064 (* 1 = 0.155064 loss)
I0109 19:15:23.460466  2836 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 19:15:26.789347  2836 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 19:15:27.613551  2836 solver.cpp:517]     Test net output #0: loss = 0.483616 (* 1 = 0.483616 loss)
I0109 19:15:27.613610  2836 solver.cpp:517]     Test net output #1: top-1 = 0.846111
I0109 19:15:27.613623  2836 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:15:27.644944  2836 solver.cpp:266] Iteration 11000 (23.8976 iter/s, 4.18452s/100 iter), loss = 0.107904
I0109 19:15:27.644986  2836 solver.cpp:285]     Train net output #0: loss = 0.107904 (* 1 = 0.107904 loss)
I0109 19:15:27.645007  2836 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 19:15:31.001447  2836 solver.cpp:266] Iteration 11100 (29.7929 iter/s, 3.3565s/100 iter), loss = 0.144133
I0109 19:15:31.001513  2836 solver.cpp:285]     Train net output #0: loss = 0.144133 (* 1 = 0.144133 loss)
I0109 19:15:31.001526  2836 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 19:15:34.365057  2836 solver.cpp:266] Iteration 11200 (29.7302 iter/s, 3.36358s/100 iter), loss = 0.245662
I0109 19:15:34.365133  2836 solver.cpp:285]     Train net output #0: loss = 0.245662 (* 1 = 0.245662 loss)
I0109 19:15:34.365147  2836 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 19:15:37.731803  2836 solver.cpp:266] Iteration 11300 (29.7028 iter/s, 3.36668s/100 iter), loss = 0.174059
I0109 19:15:37.731874  2836 solver.cpp:285]     Train net output #0: loss = 0.174059 (* 1 = 0.174059 loss)
I0109 19:15:37.731890  2836 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 19:15:41.095342  2836 solver.cpp:266] Iteration 11400 (29.7308 iter/s, 3.36351s/100 iter), loss = 0.20073
I0109 19:15:41.095410  2836 solver.cpp:285]     Train net output #0: loss = 0.20073 (* 1 = 0.20073 loss)
I0109 19:15:41.095423  2836 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 19:15:44.452811  2836 solver.cpp:266] Iteration 11500 (29.7846 iter/s, 3.35744s/100 iter), loss = 0.146702
I0109 19:15:44.452877  2836 solver.cpp:285]     Train net output #0: loss = 0.146702 (* 1 = 0.146702 loss)
I0109 19:15:44.452889  2836 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 19:15:47.813514  2836 solver.cpp:266] Iteration 11600 (29.7561 iter/s, 3.36066s/100 iter), loss = 0.225177
I0109 19:15:47.813580  2836 solver.cpp:285]     Train net output #0: loss = 0.225177 (* 1 = 0.225177 loss)
I0109 19:15:47.813604  2836 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 19:15:51.168973  2836 solver.cpp:266] Iteration 11700 (29.8024 iter/s, 3.35544s/100 iter), loss = 0.127137
I0109 19:15:51.169122  2836 solver.cpp:285]     Train net output #0: loss = 0.127137 (* 1 = 0.127137 loss)
I0109 19:15:51.169137  2836 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 19:15:54.522542  2836 solver.cpp:266] Iteration 11800 (29.8201 iter/s, 3.35344s/100 iter), loss = 0.16224
I0109 19:15:54.522611  2836 solver.cpp:285]     Train net output #0: loss = 0.16224 (* 1 = 0.16224 loss)
I0109 19:15:54.522624  2836 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 19:15:57.873967  2836 solver.cpp:266] Iteration 11900 (29.8383 iter/s, 3.3514s/100 iter), loss = 0.157535
I0109 19:15:57.874033  2836 solver.cpp:285]     Train net output #0: loss = 0.157535 (* 1 = 0.157535 loss)
I0109 19:15:57.874047  2836 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 19:16:01.194087  2836 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 19:16:02.024260  2836 solver.cpp:517]     Test net output #0: loss = 0.4701 (* 1 = 0.4701 loss)
I0109 19:16:02.024300  2836 solver.cpp:517]     Test net output #1: top-1 = 0.850778
I0109 19:16:02.024307  2836 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:16:02.055692  2836 solver.cpp:266] Iteration 12000 (23.9136 iter/s, 4.18171s/100 iter), loss = 0.136107
I0109 19:16:02.055750  2836 solver.cpp:285]     Train net output #0: loss = 0.136107 (* 1 = 0.136107 loss)
I0109 19:16:02.055765  2836 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 19:16:05.411664  2836 solver.cpp:266] Iteration 12100 (29.7978 iter/s, 3.35595s/100 iter), loss = 0.207904
I0109 19:16:05.411742  2836 solver.cpp:285]     Train net output #0: loss = 0.207904 (* 1 = 0.207904 loss)
I0109 19:16:05.411757  2836 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 19:16:08.777340  2836 solver.cpp:266] Iteration 12200 (29.7123 iter/s, 3.36561s/100 iter), loss = 0.141431
I0109 19:16:08.777415  2836 solver.cpp:285]     Train net output #0: loss = 0.141431 (* 1 = 0.141431 loss)
I0109 19:16:08.777428  2836 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 19:16:12.138401  2836 solver.cpp:266] Iteration 12300 (29.7528 iter/s, 3.36103s/100 iter), loss = 0.168505
I0109 19:16:12.138468  2836 solver.cpp:285]     Train net output #0: loss = 0.168505 (* 1 = 0.168505 loss)
I0109 19:16:12.138480  2836 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 19:16:15.498558  2836 solver.cpp:266] Iteration 12400 (29.761 iter/s, 3.3601s/100 iter), loss = 0.0988783
I0109 19:16:15.498637  2836 solver.cpp:285]     Train net output #0: loss = 0.0988782 (* 1 = 0.0988782 loss)
I0109 19:16:15.498652  2836 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 19:16:18.860154  2836 solver.cpp:266] Iteration 12500 (29.7481 iter/s, 3.36156s/100 iter), loss = 0.130041
I0109 19:16:18.860239  2836 solver.cpp:285]     Train net output #0: loss = 0.130041 (* 1 = 0.130041 loss)
I0109 19:16:18.860260  2836 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 19:16:22.220765  2836 solver.cpp:266] Iteration 12600 (29.7568 iter/s, 3.36057s/100 iter), loss = 0.105087
I0109 19:16:22.220964  2836 solver.cpp:285]     Train net output #0: loss = 0.105087 (* 1 = 0.105087 loss)
I0109 19:16:22.220985  2836 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 19:16:25.584452  2836 solver.cpp:266] Iteration 12700 (29.7309 iter/s, 3.36351s/100 iter), loss = 0.119713
I0109 19:16:25.584533  2836 solver.cpp:285]     Train net output #0: loss = 0.119713 (* 1 = 0.119713 loss)
I0109 19:16:25.584549  2836 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 19:16:28.946419  2836 solver.cpp:266] Iteration 12800 (29.7448 iter/s, 3.36193s/100 iter), loss = 0.146382
I0109 19:16:28.946491  2836 solver.cpp:285]     Train net output #0: loss = 0.146382 (* 1 = 0.146382 loss)
I0109 19:16:28.946503  2836 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 19:16:32.307427  2836 solver.cpp:266] Iteration 12900 (29.7532 iter/s, 3.36098s/100 iter), loss = 0.203629
I0109 19:16:32.307490  2836 solver.cpp:285]     Train net output #0: loss = 0.203629 (* 1 = 0.203629 loss)
I0109 19:16:32.307502  2836 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 19:16:35.635987  2836 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 19:16:36.464095  2836 solver.cpp:517]     Test net output #0: loss = 0.465414 (* 1 = 0.465414 loss)
I0109 19:16:36.464133  2836 solver.cpp:517]     Test net output #1: top-1 = 0.854555
I0109 19:16:36.464140  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993111
I0109 19:16:36.495409  2836 solver.cpp:266] Iteration 13000 (23.8779 iter/s, 4.18798s/100 iter), loss = 0.149244
I0109 19:16:36.495481  2836 solver.cpp:285]     Train net output #0: loss = 0.149244 (* 1 = 0.149244 loss)
I0109 19:16:36.495494  2836 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 19:16:39.858184  2836 solver.cpp:266] Iteration 13100 (29.7376 iter/s, 3.36275s/100 iter), loss = 0.105435
I0109 19:16:39.858253  2836 solver.cpp:285]     Train net output #0: loss = 0.105435 (* 1 = 0.105435 loss)
I0109 19:16:39.858265  2836 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 19:16:43.219000  2836 solver.cpp:266] Iteration 13200 (29.7549 iter/s, 3.3608s/100 iter), loss = 0.19848
I0109 19:16:43.219063  2836 solver.cpp:285]     Train net output #0: loss = 0.19848 (* 1 = 0.19848 loss)
I0109 19:16:43.219075  2836 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 19:16:46.582345  2836 solver.cpp:266] Iteration 13300 (29.7327 iter/s, 3.3633s/100 iter), loss = 0.10063
I0109 19:16:46.582417  2836 solver.cpp:285]     Train net output #0: loss = 0.10063 (* 1 = 0.10063 loss)
I0109 19:16:46.582434  2836 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 19:16:49.949002  2836 solver.cpp:266] Iteration 13400 (29.7033 iter/s, 3.36663s/100 iter), loss = 0.141516
I0109 19:16:49.949100  2836 solver.cpp:285]     Train net output #0: loss = 0.141516 (* 1 = 0.141516 loss)
I0109 19:16:49.949121  2836 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 19:16:53.312860  2836 solver.cpp:266] Iteration 13500 (29.7282 iter/s, 3.36381s/100 iter), loss = 0.144683
I0109 19:16:53.313102  2836 solver.cpp:285]     Train net output #0: loss = 0.144683 (* 1 = 0.144683 loss)
I0109 19:16:53.313120  2836 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 19:16:56.677039  2836 solver.cpp:266] Iteration 13600 (29.7269 iter/s, 3.36396s/100 iter), loss = 0.119344
I0109 19:16:56.677109  2836 solver.cpp:285]     Train net output #0: loss = 0.119344 (* 1 = 0.119344 loss)
I0109 19:16:56.677124  2836 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 19:17:00.048408  2836 solver.cpp:266] Iteration 13700 (29.6618 iter/s, 3.37134s/100 iter), loss = 0.102108
I0109 19:17:00.048483  2836 solver.cpp:285]     Train net output #0: loss = 0.102108 (* 1 = 0.102108 loss)
I0109 19:17:00.048497  2836 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 19:17:03.415443  2836 solver.cpp:266] Iteration 13800 (29.7 iter/s, 3.36701s/100 iter), loss = 0.13308
I0109 19:17:03.415510  2836 solver.cpp:285]     Train net output #0: loss = 0.13308 (* 1 = 0.13308 loss)
I0109 19:17:03.415524  2836 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 19:17:06.776155  2836 solver.cpp:266] Iteration 13900 (29.7561 iter/s, 3.36066s/100 iter), loss = 0.178788
I0109 19:17:06.776247  2836 solver.cpp:285]     Train net output #0: loss = 0.178788 (* 1 = 0.178788 loss)
I0109 19:17:06.776264  2836 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 19:17:10.109781  2836 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 19:17:10.940062  2836 solver.cpp:517]     Test net output #0: loss = 0.446007 (* 1 = 0.446007 loss)
I0109 19:17:10.940109  2836 solver.cpp:517]     Test net output #1: top-1 = 0.860222
I0109 19:17:10.940116  2836 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0109 19:17:10.971527  2836 solver.cpp:266] Iteration 14000 (23.836 iter/s, 4.19533s/100 iter), loss = 0.157654
I0109 19:17:10.971612  2836 solver.cpp:285]     Train net output #0: loss = 0.157654 (* 1 = 0.157654 loss)
I0109 19:17:10.971629  2836 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 19:17:14.327549  2836 solver.cpp:266] Iteration 14100 (29.7975 iter/s, 3.35598s/100 iter), loss = 0.147151
I0109 19:17:14.327615  2836 solver.cpp:285]     Train net output #0: loss = 0.147151 (* 1 = 0.147151 loss)
I0109 19:17:14.327628  2836 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 19:17:17.683241  2836 solver.cpp:266] Iteration 14200 (29.8006 iter/s, 3.35564s/100 iter), loss = 0.143589
I0109 19:17:17.683323  2836 solver.cpp:285]     Train net output #0: loss = 0.143589 (* 1 = 0.143589 loss)
I0109 19:17:17.683338  2836 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 19:17:21.031711  2836 solver.cpp:266] Iteration 14300 (29.8647 iter/s, 3.34843s/100 iter), loss = 0.204995
I0109 19:17:21.031777  2836 solver.cpp:285]     Train net output #0: loss = 0.204995 (* 1 = 0.204995 loss)
I0109 19:17:21.031790  2836 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 19:17:24.383508  2836 solver.cpp:266] Iteration 14400 (29.835 iter/s, 3.35177s/100 iter), loss = 0.117989
I0109 19:17:24.383708  2836 solver.cpp:285]     Train net output #0: loss = 0.117989 (* 1 = 0.117989 loss)
I0109 19:17:24.383728  2836 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 19:17:27.720193  2836 solver.cpp:266] Iteration 14500 (29.9715 iter/s, 3.3365s/100 iter), loss = 0.0997176
I0109 19:17:27.720259  2836 solver.cpp:285]     Train net output #0: loss = 0.0997176 (* 1 = 0.0997176 loss)
I0109 19:17:27.720271  2836 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 19:17:31.070317  2836 solver.cpp:266] Iteration 14600 (29.8499 iter/s, 3.3501s/100 iter), loss = 0.166378
I0109 19:17:31.070384  2836 solver.cpp:285]     Train net output #0: loss = 0.166378 (* 1 = 0.166378 loss)
I0109 19:17:31.070399  2836 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 19:17:34.418864  2836 solver.cpp:266] Iteration 14700 (29.864 iter/s, 3.34851s/100 iter), loss = 0.146983
I0109 19:17:34.418927  2836 solver.cpp:285]     Train net output #0: loss = 0.146983 (* 1 = 0.146983 loss)
I0109 19:17:34.418941  2836 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 19:17:37.766394  2836 solver.cpp:266] Iteration 14800 (29.8733 iter/s, 3.34748s/100 iter), loss = 0.158801
I0109 19:17:37.766456  2836 solver.cpp:285]     Train net output #0: loss = 0.158801 (* 1 = 0.158801 loss)
I0109 19:17:37.766469  2836 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 19:17:41.106958  2836 solver.cpp:266] Iteration 14900 (29.9353 iter/s, 3.34053s/100 iter), loss = 0.156881
I0109 19:17:41.107029  2836 solver.cpp:285]     Train net output #0: loss = 0.156881 (* 1 = 0.156881 loss)
I0109 19:17:41.107043  2836 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 19:17:44.420511  2836 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 19:17:45.244168  2836 solver.cpp:517]     Test net output #0: loss = 0.440379 (* 1 = 0.440379 loss)
I0109 19:17:45.244210  2836 solver.cpp:517]     Test net output #1: top-1 = 0.864111
I0109 19:17:45.244216  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 19:17:45.275475  2836 solver.cpp:266] Iteration 15000 (23.9895 iter/s, 4.16848s/100 iter), loss = 0.165778
I0109 19:17:45.275550  2836 solver.cpp:285]     Train net output #0: loss = 0.165778 (* 1 = 0.165778 loss)
I0109 19:17:45.275568  2836 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 19:17:48.609199  2836 solver.cpp:266] Iteration 15100 (29.9971 iter/s, 3.33365s/100 iter), loss = 0.13759
I0109 19:17:48.609266  2836 solver.cpp:285]     Train net output #0: loss = 0.13759 (* 1 = 0.13759 loss)
I0109 19:17:48.609282  2836 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 19:17:51.957970  2836 solver.cpp:266] Iteration 15200 (29.862 iter/s, 3.34874s/100 iter), loss = 0.142268
I0109 19:17:51.958037  2836 solver.cpp:285]     Train net output #0: loss = 0.142268 (* 1 = 0.142268 loss)
I0109 19:17:51.958051  2836 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 19:17:55.308205  2836 solver.cpp:266] Iteration 15300 (29.8489 iter/s, 3.3502s/100 iter), loss = 0.202166
I0109 19:17:55.308408  2836 solver.cpp:285]     Train net output #0: loss = 0.202166 (* 1 = 0.202166 loss)
I0109 19:17:55.308423  2836 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 19:17:58.653923  2836 solver.cpp:266] Iteration 15400 (29.8907 iter/s, 3.34552s/100 iter), loss = 0.133654
I0109 19:17:58.654009  2836 solver.cpp:285]     Train net output #0: loss = 0.133654 (* 1 = 0.133654 loss)
I0109 19:17:58.654023  2836 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 19:18:01.999603  2836 solver.cpp:266] Iteration 15500 (29.8897 iter/s, 3.34563s/100 iter), loss = 0.158786
I0109 19:18:01.999671  2836 solver.cpp:285]     Train net output #0: loss = 0.158786 (* 1 = 0.158786 loss)
I0109 19:18:01.999686  2836 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 19:18:05.341086  2836 solver.cpp:266] Iteration 15600 (29.9271 iter/s, 3.34145s/100 iter), loss = 0.0847909
I0109 19:18:05.341153  2836 solver.cpp:285]     Train net output #0: loss = 0.084791 (* 1 = 0.084791 loss)
I0109 19:18:05.341166  2836 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 19:18:08.693522  2836 solver.cpp:266] Iteration 15700 (29.8296 iter/s, 3.35237s/100 iter), loss = 0.159975
I0109 19:18:08.693606  2836 solver.cpp:285]     Train net output #0: loss = 0.159975 (* 1 = 0.159975 loss)
I0109 19:18:08.693620  2836 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 19:18:12.037079  2836 solver.cpp:266] Iteration 15800 (29.9086 iter/s, 3.34352s/100 iter), loss = 0.128602
I0109 19:18:12.037142  2836 solver.cpp:285]     Train net output #0: loss = 0.128602 (* 1 = 0.128602 loss)
I0109 19:18:12.037154  2836 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 19:18:15.384899  2836 solver.cpp:266] Iteration 15900 (29.8705 iter/s, 3.34779s/100 iter), loss = 0.154016
I0109 19:18:15.384980  2836 solver.cpp:285]     Train net output #0: loss = 0.154016 (* 1 = 0.154016 loss)
I0109 19:18:15.384995  2836 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 19:18:18.698298  2836 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 19:18:19.522953  2836 solver.cpp:517]     Test net output #0: loss = 0.444137 (* 1 = 0.444137 loss)
I0109 19:18:19.522992  2836 solver.cpp:517]     Test net output #1: top-1 = 0.860889
I0109 19:18:19.523002  2836 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0109 19:18:19.554286  2836 solver.cpp:266] Iteration 16000 (23.9847 iter/s, 4.16932s/100 iter), loss = 0.13956
I0109 19:18:19.554342  2836 solver.cpp:285]     Train net output #0: loss = 0.13956 (* 1 = 0.13956 loss)
I0109 19:18:19.554358  2836 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 19:18:22.903713  2836 solver.cpp:266] Iteration 16100 (29.8561 iter/s, 3.3494s/100 iter), loss = 0.167629
I0109 19:18:22.903782  2836 solver.cpp:285]     Train net output #0: loss = 0.167629 (* 1 = 0.167629 loss)
I0109 19:18:22.903796  2836 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 19:18:26.252113  2836 solver.cpp:266] Iteration 16200 (29.8653 iter/s, 3.34836s/100 iter), loss = 0.114617
I0109 19:18:26.252305  2836 solver.cpp:285]     Train net output #0: loss = 0.114617 (* 1 = 0.114617 loss)
I0109 19:18:26.252321  2836 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 19:18:29.600018  2836 solver.cpp:266] Iteration 16300 (29.8711 iter/s, 3.34772s/100 iter), loss = 0.129416
I0109 19:18:29.600086  2836 solver.cpp:285]     Train net output #0: loss = 0.129416 (* 1 = 0.129416 loss)
I0109 19:18:29.600100  2836 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 19:18:32.948801  2836 solver.cpp:266] Iteration 16400 (29.8619 iter/s, 3.34875s/100 iter), loss = 0.166796
I0109 19:18:32.948864  2836 solver.cpp:285]     Train net output #0: loss = 0.166796 (* 1 = 0.166796 loss)
I0109 19:18:32.948876  2836 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 19:18:36.305297  2836 solver.cpp:266] Iteration 16500 (29.7932 iter/s, 3.35647s/100 iter), loss = 0.0848438
I0109 19:18:36.305367  2836 solver.cpp:285]     Train net output #0: loss = 0.0848439 (* 1 = 0.0848439 loss)
I0109 19:18:36.305382  2836 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 19:18:39.653786  2836 solver.cpp:266] Iteration 16600 (29.8648 iter/s, 3.34843s/100 iter), loss = 0.10721
I0109 19:18:39.653851  2836 solver.cpp:285]     Train net output #0: loss = 0.10721 (* 1 = 0.10721 loss)
I0109 19:18:39.653862  2836 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 19:18:43.004503  2836 solver.cpp:266] Iteration 16700 (29.8447 iter/s, 3.35068s/100 iter), loss = 0.125433
I0109 19:18:43.004568  2836 solver.cpp:285]     Train net output #0: loss = 0.125433 (* 1 = 0.125433 loss)
I0109 19:18:43.004583  2836 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 19:18:46.353519  2836 solver.cpp:266] Iteration 16800 (29.8598 iter/s, 3.34898s/100 iter), loss = 0.11354
I0109 19:18:46.353600  2836 solver.cpp:285]     Train net output #0: loss = 0.11354 (* 1 = 0.11354 loss)
I0109 19:18:46.353619  2836 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 19:18:49.699602  2836 solver.cpp:266] Iteration 16900 (29.8862 iter/s, 3.34602s/100 iter), loss = 0.100104
I0109 19:18:49.699668  2836 solver.cpp:285]     Train net output #0: loss = 0.100104 (* 1 = 0.100104 loss)
I0109 19:18:49.699681  2836 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 19:18:53.015910  2836 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 19:18:53.841430  2836 solver.cpp:517]     Test net output #0: loss = 0.423535 (* 1 = 0.423535 loss)
I0109 19:18:53.841471  2836 solver.cpp:517]     Test net output #1: top-1 = 0.867666
I0109 19:18:53.841480  2836 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:18:53.872797  2836 solver.cpp:266] Iteration 17000 (23.9626 iter/s, 4.17318s/100 iter), loss = 0.134423
I0109 19:18:53.872840  2836 solver.cpp:285]     Train net output #0: loss = 0.134423 (* 1 = 0.134423 loss)
I0109 19:18:53.872855  2836 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 19:18:57.227747  2836 solver.cpp:266] Iteration 17100 (29.8068 iter/s, 3.35493s/100 iter), loss = 0.13007
I0109 19:18:57.227963  2836 solver.cpp:285]     Train net output #0: loss = 0.13007 (* 1 = 0.13007 loss)
I0109 19:18:57.227982  2836 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 19:19:00.582367  2836 solver.cpp:266] Iteration 17200 (29.8115 iter/s, 3.35442s/100 iter), loss = 0.101399
I0109 19:19:00.582432  2836 solver.cpp:285]     Train net output #0: loss = 0.101399 (* 1 = 0.101399 loss)
I0109 19:19:00.582444  2836 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 19:19:03.941421  2836 solver.cpp:266] Iteration 17300 (29.7706 iter/s, 3.35902s/100 iter), loss = 0.106527
I0109 19:19:03.941488  2836 solver.cpp:285]     Train net output #0: loss = 0.106527 (* 1 = 0.106527 loss)
I0109 19:19:03.941500  2836 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 19:19:07.295210  2836 solver.cpp:266] Iteration 17400 (29.8173 iter/s, 3.35376s/100 iter), loss = 0.127439
I0109 19:19:07.295282  2836 solver.cpp:285]     Train net output #0: loss = 0.127439 (* 1 = 0.127439 loss)
I0109 19:19:07.295295  2836 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 19:19:10.659499  2836 solver.cpp:266] Iteration 17500 (29.7246 iter/s, 3.36422s/100 iter), loss = 0.105129
I0109 19:19:10.659587  2836 solver.cpp:285]     Train net output #0: loss = 0.105129 (* 1 = 0.105129 loss)
I0109 19:19:10.659603  2836 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 19:19:14.017477  2836 solver.cpp:266] Iteration 17600 (29.7803 iter/s, 3.35793s/100 iter), loss = 0.0862491
I0109 19:19:14.017542  2836 solver.cpp:285]     Train net output #0: loss = 0.0862491 (* 1 = 0.0862491 loss)
I0109 19:19:14.017555  2836 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 19:19:17.373755  2836 solver.cpp:266] Iteration 17700 (29.7952 iter/s, 3.35624s/100 iter), loss = 0.107986
I0109 19:19:17.373834  2836 solver.cpp:285]     Train net output #0: loss = 0.107986 (* 1 = 0.107986 loss)
I0109 19:19:17.373849  2836 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 19:19:20.736595  2836 solver.cpp:266] Iteration 17800 (29.7374 iter/s, 3.36277s/100 iter), loss = 0.106329
I0109 19:19:20.736676  2836 solver.cpp:285]     Train net output #0: loss = 0.106329 (* 1 = 0.106329 loss)
I0109 19:19:20.736690  2836 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 19:19:24.092382  2836 solver.cpp:266] Iteration 17900 (29.7997 iter/s, 3.35574s/100 iter), loss = 0.105353
I0109 19:19:24.092448  2836 solver.cpp:285]     Train net output #0: loss = 0.105353 (* 1 = 0.105353 loss)
I0109 19:19:24.092461  2836 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 19:19:27.411317  2836 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 19:19:28.241391  2836 solver.cpp:517]     Test net output #0: loss = 0.435558 (* 1 = 0.435558 loss)
I0109 19:19:28.241434  2836 solver.cpp:517]     Test net output #1: top-1 = 0.865999
I0109 19:19:28.241449  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993444
I0109 19:19:28.272773  2836 solver.cpp:266] Iteration 18000 (23.9214 iter/s, 4.18037s/100 iter), loss = 0.0950788
I0109 19:19:28.272861  2836 solver.cpp:285]     Train net output #0: loss = 0.0950789 (* 1 = 0.0950789 loss)
I0109 19:19:28.272882  2836 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 19:19:31.632561  2836 solver.cpp:266] Iteration 18100 (29.7644 iter/s, 3.35972s/100 iter), loss = 0.149473
I0109 19:19:31.632632  2836 solver.cpp:285]     Train net output #0: loss = 0.149473 (* 1 = 0.149473 loss)
I0109 19:19:31.632650  2836 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 19:19:34.990459  2836 solver.cpp:266] Iteration 18200 (29.7808 iter/s, 3.35786s/100 iter), loss = 0.134281
I0109 19:19:34.990540  2836 solver.cpp:285]     Train net output #0: loss = 0.134281 (* 1 = 0.134281 loss)
I0109 19:19:34.990559  2836 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 19:19:38.349985  2836 solver.cpp:266] Iteration 18300 (29.7665 iter/s, 3.35949s/100 iter), loss = 0.0965124
I0109 19:19:38.350057  2836 solver.cpp:285]     Train net output #0: loss = 0.0965124 (* 1 = 0.0965124 loss)
I0109 19:19:38.350076  2836 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 19:19:41.708073  2836 solver.cpp:266] Iteration 18400 (29.7794 iter/s, 3.35803s/100 iter), loss = 0.128362
I0109 19:19:41.708144  2836 solver.cpp:285]     Train net output #0: loss = 0.128362 (* 1 = 0.128362 loss)
I0109 19:19:41.708163  2836 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 19:19:45.071027  2836 solver.cpp:266] Iteration 18500 (29.7361 iter/s, 3.36292s/100 iter), loss = 0.156928
I0109 19:19:45.071112  2836 solver.cpp:285]     Train net output #0: loss = 0.156928 (* 1 = 0.156928 loss)
I0109 19:19:45.071128  2836 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 19:19:48.432080  2836 solver.cpp:266] Iteration 18600 (29.753 iter/s, 3.36101s/100 iter), loss = 0.0560445
I0109 19:19:48.432147  2836 solver.cpp:285]     Train net output #0: loss = 0.0560446 (* 1 = 0.0560446 loss)
I0109 19:19:48.432159  2836 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 19:19:51.782438  2836 solver.cpp:266] Iteration 18700 (29.8481 iter/s, 3.3503s/100 iter), loss = 0.101292
I0109 19:19:51.782506  2836 solver.cpp:285]     Train net output #0: loss = 0.101292 (* 1 = 0.101292 loss)
I0109 19:19:51.782521  2836 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 19:19:55.143013  2836 solver.cpp:266] Iteration 18800 (29.7571 iter/s, 3.36054s/100 iter), loss = 0.107517
I0109 19:19:55.143080  2836 solver.cpp:285]     Train net output #0: loss = 0.107517 (* 1 = 0.107517 loss)
I0109 19:19:55.143093  2836 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 19:19:58.502485  2836 solver.cpp:266] Iteration 18900 (29.7669 iter/s, 3.35944s/100 iter), loss = 0.0853669
I0109 19:19:58.502677  2836 solver.cpp:285]     Train net output #0: loss = 0.0853669 (* 1 = 0.0853669 loss)
I0109 19:19:58.502696  2836 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 19:20:01.829108  2836 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 19:20:02.655649  2836 solver.cpp:517]     Test net output #0: loss = 0.416489 (* 1 = 0.416489 loss)
I0109 19:20:02.655688  2836 solver.cpp:517]     Test net output #1: top-1 = 0.870333
I0109 19:20:02.655694  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 19:20:02.686974  2836 solver.cpp:266] Iteration 19000 (23.8987 iter/s, 4.18432s/100 iter), loss = 0.0837045
I0109 19:20:02.687032  2836 solver.cpp:285]     Train net output #0: loss = 0.0837045 (* 1 = 0.0837045 loss)
I0109 19:20:02.687047  2836 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 19:20:06.040772  2836 solver.cpp:266] Iteration 19100 (29.8172 iter/s, 3.35377s/100 iter), loss = 0.0940766
I0109 19:20:06.040865  2836 solver.cpp:285]     Train net output #0: loss = 0.0940766 (* 1 = 0.0940766 loss)
I0109 19:20:06.040884  2836 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 19:20:09.392396  2836 solver.cpp:266] Iteration 19200 (29.8367 iter/s, 3.35157s/100 iter), loss = 0.175939
I0109 19:20:09.392465  2836 solver.cpp:285]     Train net output #0: loss = 0.175939 (* 1 = 0.175939 loss)
I0109 19:20:09.392479  2836 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 19:20:12.742699  2836 solver.cpp:266] Iteration 19300 (29.8485 iter/s, 3.35025s/100 iter), loss = 0.108187
I0109 19:20:12.742765  2836 solver.cpp:285]     Train net output #0: loss = 0.108187 (* 1 = 0.108187 loss)
I0109 19:20:12.742777  2836 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 19:20:16.103699  2836 solver.cpp:266] Iteration 19400 (29.7533 iter/s, 3.36097s/100 iter), loss = 0.120605
I0109 19:20:16.103761  2836 solver.cpp:285]     Train net output #0: loss = 0.120605 (* 1 = 0.120605 loss)
I0109 19:20:16.103773  2836 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 19:20:19.462427  2836 solver.cpp:266] Iteration 19500 (29.7734 iter/s, 3.35871s/100 iter), loss = 0.064806
I0109 19:20:19.462493  2836 solver.cpp:285]     Train net output #0: loss = 0.0648061 (* 1 = 0.0648061 loss)
I0109 19:20:19.462507  2836 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 19:20:22.821964  2836 solver.cpp:266] Iteration 19600 (29.7665 iter/s, 3.35949s/100 iter), loss = 0.132763
I0109 19:20:22.822027  2836 solver.cpp:285]     Train net output #0: loss = 0.132763 (* 1 = 0.132763 loss)
I0109 19:20:22.822039  2836 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 19:20:26.179143  2836 solver.cpp:266] Iteration 19700 (29.7871 iter/s, 3.35716s/100 iter), loss = 0.102449
I0109 19:20:26.179216  2836 solver.cpp:285]     Train net output #0: loss = 0.102449 (* 1 = 0.102449 loss)
I0109 19:20:26.179230  2836 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 19:20:29.539372  2836 solver.cpp:266] Iteration 19800 (29.7604 iter/s, 3.36017s/100 iter), loss = 0.11754
I0109 19:20:29.539582  2836 solver.cpp:285]     Train net output #0: loss = 0.11754 (* 1 = 0.11754 loss)
I0109 19:20:29.539602  2836 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 19:20:32.899178  2836 solver.cpp:266] Iteration 19900 (29.7651 iter/s, 3.35964s/100 iter), loss = 0.118148
I0109 19:20:32.899242  2836 solver.cpp:285]     Train net output #0: loss = 0.118148 (* 1 = 0.118148 loss)
I0109 19:20:32.899255  2836 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 19:20:36.226286  2836 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/_iter_20000.caffemodel
I0109 19:20:36.319648  2836 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/_iter_20000.solverstate
I0109 19:20:36.345866  2836 solver.cpp:378] Iteration 20000, loss = 0.0167433
I0109 19:20:36.345913  2836 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 19:20:37.164301  2836 solver.cpp:517]     Test net output #0: loss = 0.42401 (* 1 = 0.42401 loss)
I0109 19:20:37.164342  2836 solver.cpp:517]     Test net output #1: top-1 = 0.870888
I0109 19:20:37.164351  2836 solver.cpp:517]     Test net output #2: top-5 = 0.993333
I0109 19:20:37.164358  2836 solver.cpp:386] Optimization Done (29.3945 iter/s).
I0109 19:20:37.164366  2836 caffe_interface.cpp:530] Optimization Done.
I0109 19:20:37.329325  2879 pruning_runner.cpp:190] Sens info found, use it.
I0109 19:20:37.379251  2879 pruning_runner.cpp:217] Start compressing, please wait...
I0109 19:20:38.436569  2879 pruning_runner.cpp:264] Compression complete 1.17777%
I0109 19:20:38.746944  2879 pruning_runner.cpp:264] Compression complete 2.32811%
I0109 19:20:39.056064  2879 pruning_runner.cpp:264] Compression complete 4.55029%
I0109 19:20:39.361764  2879 pruning_runner.cpp:264] Compression complete 8.70449%
I0109 19:20:39.671301  2879 pruning_runner.cpp:264] Compression complete 16.015%
I0109 19:20:39.984131  2879 pruning_runner.cpp:264] Compression complete 27.6084%
I0109 19:20:40.292922  2879 pruning_runner.cpp:264] Compression complete 60.404%
I0109 19:20:40.607661  2879 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 19:20:40.608003  2879 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 19:20:40.608417  2879 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:20:40.608669  2879 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:20:40.608852  2879 layer_factory.hpp:77] Creating layer data
I0109 19:20:40.608937  2879 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:40.609055  2879 net.cpp:94] Creating Layer data
I0109 19:20:40.609086  2879 net.cpp:409] data -> data
I0109 19:20:40.609113  2879 net.cpp:409] data -> label
I0109 19:20:40.609962  3032 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:20:40.609997  3032 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:20:40.610083  2879 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:20:40.610208  2879 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:20:40.616623  2879 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:40.617323  2879 net.cpp:144] Setting up data
I0109 19:20:40.617343  2879 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:20:40.617353  2879 net.cpp:151] Top shape: 50 (50)
I0109 19:20:40.617358  2879 net.cpp:159] Memory required for data: 614600
I0109 19:20:40.617367  2879 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:20:40.617419  2879 net.cpp:94] Creating Layer label_data_1_split
I0109 19:20:40.617444  2879 net.cpp:435] label_data_1_split <- label
I0109 19:20:40.617466  2879 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:20:40.617482  2879 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:20:40.617522  2879 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:20:40.617640  2879 net.cpp:144] Setting up label_data_1_split
I0109 19:20:40.617660  2879 net.cpp:151] Top shape: 50 (50)
I0109 19:20:40.617668  2879 net.cpp:151] Top shape: 50 (50)
I0109 19:20:40.617676  2879 net.cpp:151] Top shape: 50 (50)
I0109 19:20:40.617697  2879 net.cpp:159] Memory required for data: 615200
I0109 19:20:40.617727  2879 layer_factory.hpp:77] Creating layer conv1
I0109 19:20:40.617769  2879 net.cpp:94] Creating Layer conv1
I0109 19:20:40.617787  2879 net.cpp:435] conv1 <- data
I0109 19:20:40.617801  2879 net.cpp:409] conv1 -> conv1
I0109 19:20:40.618985  2879 net.cpp:144] Setting up conv1
I0109 19:20:40.619009  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.619019  2879 net.cpp:159] Memory required for data: 7168800
I0109 19:20:40.619035  2879 layer_factory.hpp:77] Creating layer bn1
I0109 19:20:40.619065  2879 net.cpp:94] Creating Layer bn1
I0109 19:20:40.619087  2879 net.cpp:435] bn1 <- conv1
I0109 19:20:40.619110  2879 net.cpp:409] bn1 -> scale1
I0109 19:20:40.619768  2879 net.cpp:144] Setting up bn1
I0109 19:20:40.619791  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.619801  2879 net.cpp:159] Memory required for data: 13722400
I0109 19:20:40.619823  2879 layer_factory.hpp:77] Creating layer relu1
I0109 19:20:40.619853  2879 net.cpp:94] Creating Layer relu1
I0109 19:20:40.619869  2879 net.cpp:435] relu1 <- scale1
I0109 19:20:40.619881  2879 net.cpp:409] relu1 -> relu1
I0109 19:20:40.619935  2879 net.cpp:144] Setting up relu1
I0109 19:20:40.619954  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.619962  2879 net.cpp:159] Memory required for data: 20276000
I0109 19:20:40.619968  2879 layer_factory.hpp:77] Creating layer conv2
I0109 19:20:40.619997  2879 net.cpp:94] Creating Layer conv2
I0109 19:20:40.620019  2879 net.cpp:435] conv2 <- relu1
I0109 19:20:40.620044  2879 net.cpp:409] conv2 -> conv2
I0109 19:20:40.621049  2879 net.cpp:144] Setting up conv2
I0109 19:20:40.621073  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.621081  2879 net.cpp:159] Memory required for data: 26829600
I0109 19:20:40.621095  2879 layer_factory.hpp:77] Creating layer bn2
I0109 19:20:40.621126  2879 net.cpp:94] Creating Layer bn2
I0109 19:20:40.621150  2879 net.cpp:435] bn2 <- conv2
I0109 19:20:40.621184  2879 net.cpp:409] bn2 -> scale2
I0109 19:20:40.621899  2879 net.cpp:144] Setting up bn2
I0109 19:20:40.621923  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.621932  2879 net.cpp:159] Memory required for data: 33383200
I0109 19:20:40.621948  2879 layer_factory.hpp:77] Creating layer relu2
I0109 19:20:40.621975  2879 net.cpp:94] Creating Layer relu2
I0109 19:20:40.621996  2879 net.cpp:435] relu2 <- scale2
I0109 19:20:40.622030  2879 net.cpp:409] relu2 -> relu2
I0109 19:20:40.622081  2879 net.cpp:144] Setting up relu2
I0109 19:20:40.622100  2879 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:40.622108  2879 net.cpp:159] Memory required for data: 39936800
I0109 19:20:40.622117  2879 layer_factory.hpp:77] Creating layer pool1
I0109 19:20:40.622143  2879 net.cpp:94] Creating Layer pool1
I0109 19:20:40.622164  2879 net.cpp:435] pool1 <- relu2
I0109 19:20:40.622189  2879 net.cpp:409] pool1 -> pool1
I0109 19:20:40.622247  2879 net.cpp:144] Setting up pool1
I0109 19:20:40.622265  2879 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:20:40.622273  2879 net.cpp:159] Memory required for data: 41575200
I0109 19:20:40.622279  2879 layer_factory.hpp:77] Creating layer drop1
I0109 19:20:40.622304  2879 net.cpp:94] Creating Layer drop1
I0109 19:20:40.622331  2879 net.cpp:435] drop1 <- pool1
I0109 19:20:40.622361  2879 net.cpp:409] drop1 -> drop1
I0109 19:20:40.622437  2879 net.cpp:144] Setting up drop1
I0109 19:20:40.622457  2879 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:20:40.622464  2879 net.cpp:159] Memory required for data: 43213600
I0109 19:20:40.622470  2879 layer_factory.hpp:77] Creating layer conv3
I0109 19:20:40.622484  2879 net.cpp:94] Creating Layer conv3
I0109 19:20:40.622509  2879 net.cpp:435] conv3 <- drop1
I0109 19:20:40.622540  2879 net.cpp:409] conv3 -> conv3
I0109 19:20:40.623718  2879 net.cpp:144] Setting up conv3
I0109 19:20:40.623739  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.623749  2879 net.cpp:159] Memory required for data: 46490400
I0109 19:20:40.623761  2879 layer_factory.hpp:77] Creating layer bn3
I0109 19:20:40.623791  2879 net.cpp:94] Creating Layer bn3
I0109 19:20:40.623821  2879 net.cpp:435] bn3 <- conv3
I0109 19:20:40.623852  2879 net.cpp:409] bn3 -> scale3
I0109 19:20:40.624599  2879 net.cpp:144] Setting up bn3
I0109 19:20:40.624624  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.624631  2879 net.cpp:159] Memory required for data: 49767200
I0109 19:20:40.624653  2879 layer_factory.hpp:77] Creating layer relu3
I0109 19:20:40.624680  2879 net.cpp:94] Creating Layer relu3
I0109 19:20:40.624703  2879 net.cpp:435] relu3 <- scale3
I0109 19:20:40.624730  2879 net.cpp:409] relu3 -> relu3
I0109 19:20:40.624788  2879 net.cpp:144] Setting up relu3
I0109 19:20:40.624819  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.624835  2879 net.cpp:159] Memory required for data: 53044000
I0109 19:20:40.624856  2879 layer_factory.hpp:77] Creating layer conv4
I0109 19:20:40.624887  2879 net.cpp:94] Creating Layer conv4
I0109 19:20:40.624912  2879 net.cpp:435] conv4 <- relu3
I0109 19:20:40.624940  2879 net.cpp:409] conv4 -> conv4
I0109 19:20:40.625481  2879 net.cpp:144] Setting up conv4
I0109 19:20:40.625511  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.625527  2879 net.cpp:159] Memory required for data: 56320800
I0109 19:20:40.625542  2879 layer_factory.hpp:77] Creating layer bn4
I0109 19:20:40.625556  2879 net.cpp:94] Creating Layer bn4
I0109 19:20:40.625566  2879 net.cpp:435] bn4 <- conv4
I0109 19:20:40.625577  2879 net.cpp:409] bn4 -> scale4
I0109 19:20:40.626279  2879 net.cpp:144] Setting up bn4
I0109 19:20:40.626302  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.626312  2879 net.cpp:159] Memory required for data: 59597600
I0109 19:20:40.626329  2879 layer_factory.hpp:77] Creating layer relu4
I0109 19:20:40.626343  2879 net.cpp:94] Creating Layer relu4
I0109 19:20:40.626350  2879 net.cpp:435] relu4 <- scale4
I0109 19:20:40.626361  2879 net.cpp:409] relu4 -> relu4
I0109 19:20:40.626399  2879 net.cpp:144] Setting up relu4
I0109 19:20:40.626420  2879 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:40.626426  2879 net.cpp:159] Memory required for data: 62874400
I0109 19:20:40.626431  2879 layer_factory.hpp:77] Creating layer pool2
I0109 19:20:40.626441  2879 net.cpp:94] Creating Layer pool2
I0109 19:20:40.626447  2879 net.cpp:435] pool2 <- relu4
I0109 19:20:40.626454  2879 net.cpp:409] pool2 -> pool2
I0109 19:20:40.626518  2879 net.cpp:144] Setting up pool2
I0109 19:20:40.626538  2879 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:20:40.626544  2879 net.cpp:159] Memory required for data: 63693600
I0109 19:20:40.626551  2879 layer_factory.hpp:77] Creating layer drop2
I0109 19:20:40.626561  2879 net.cpp:94] Creating Layer drop2
I0109 19:20:40.626574  2879 net.cpp:435] drop2 <- pool2
I0109 19:20:40.626583  2879 net.cpp:409] drop2 -> drop2
I0109 19:20:40.626634  2879 net.cpp:144] Setting up drop2
I0109 19:20:40.626652  2879 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:20:40.626659  2879 net.cpp:159] Memory required for data: 64512800
I0109 19:20:40.626667  2879 layer_factory.hpp:77] Creating layer fc1
I0109 19:20:40.626678  2879 net.cpp:94] Creating Layer fc1
I0109 19:20:40.626775  2879 net.cpp:435] fc1 <- drop2
I0109 19:20:40.626794  2879 net.cpp:409] fc1 -> fc1
I0109 19:20:40.648624  2879 net.cpp:144] Setting up fc1
I0109 19:20:40.648653  2879 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:40.648663  2879 net.cpp:159] Memory required for data: 64615200
I0109 19:20:40.648674  2879 layer_factory.hpp:77] Creating layer bn5
I0109 19:20:40.648690  2879 net.cpp:94] Creating Layer bn5
I0109 19:20:40.648723  2879 net.cpp:435] bn5 <- fc1
I0109 19:20:40.648749  2879 net.cpp:409] bn5 -> scale5
I0109 19:20:40.649335  2879 net.cpp:144] Setting up bn5
I0109 19:20:40.649355  2879 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:40.649363  2879 net.cpp:159] Memory required for data: 64717600
I0109 19:20:40.649387  2879 layer_factory.hpp:77] Creating layer relu5
I0109 19:20:40.649405  2879 net.cpp:94] Creating Layer relu5
I0109 19:20:40.649413  2879 net.cpp:435] relu5 <- scale5
I0109 19:20:40.649423  2879 net.cpp:409] relu5 -> relu5
I0109 19:20:40.649466  2879 net.cpp:144] Setting up relu5
I0109 19:20:40.649482  2879 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:40.649489  2879 net.cpp:159] Memory required for data: 64820000
I0109 19:20:40.649497  2879 layer_factory.hpp:77] Creating layer drop3
I0109 19:20:40.649509  2879 net.cpp:94] Creating Layer drop3
I0109 19:20:40.649523  2879 net.cpp:435] drop3 <- relu5
I0109 19:20:40.649533  2879 net.cpp:409] drop3 -> drop3
I0109 19:20:40.649629  2879 net.cpp:144] Setting up drop3
I0109 19:20:40.649647  2879 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:40.649652  2879 net.cpp:159] Memory required for data: 64922400
I0109 19:20:40.649657  2879 layer_factory.hpp:77] Creating layer fc2
I0109 19:20:40.649670  2879 net.cpp:94] Creating Layer fc2
I0109 19:20:40.649688  2879 net.cpp:435] fc2 <- drop3
I0109 19:20:40.649700  2879 net.cpp:409] fc2 -> fc2
I0109 19:20:40.649875  2879 net.cpp:144] Setting up fc2
I0109 19:20:40.649891  2879 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:40.649896  2879 net.cpp:159] Memory required for data: 64924400
I0109 19:20:40.649904  2879 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:20:40.649919  2879 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:20:40.649945  2879 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:20:40.649966  2879 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:20:40.649981  2879 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:20:40.649991  2879 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:20:40.650060  2879 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:20:40.650077  2879 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:40.650085  2879 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:40.650092  2879 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:40.650099  2879 net.cpp:159] Memory required for data: 64930400
I0109 19:20:40.650113  2879 layer_factory.hpp:77] Creating layer loss
I0109 19:20:40.650126  2879 net.cpp:94] Creating Layer loss
I0109 19:20:40.650140  2879 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:20:40.650151  2879 net.cpp:435] loss <- label_data_1_split_0
I0109 19:20:40.650163  2879 net.cpp:409] loss -> loss
I0109 19:20:40.650183  2879 layer_factory.hpp:77] Creating layer loss
I0109 19:20:40.650279  2879 net.cpp:144] Setting up loss
I0109 19:20:40.650295  2879 net.cpp:151] Top shape: (1)
I0109 19:20:40.650302  2879 net.cpp:154]     with loss weight 1
I0109 19:20:40.650326  2879 net.cpp:159] Memory required for data: 64930404
I0109 19:20:40.650336  2879 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:20:40.650347  2879 net.cpp:94] Creating Layer accuracy-top1
I0109 19:20:40.650362  2879 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:20:40.650372  2879 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:20:40.650389  2879 net.cpp:409] accuracy-top1 -> top-1
I0109 19:20:40.650404  2879 net.cpp:144] Setting up accuracy-top1
I0109 19:20:40.650420  2879 net.cpp:151] Top shape: (1)
I0109 19:20:40.650427  2879 net.cpp:159] Memory required for data: 64930408
I0109 19:20:40.650434  2879 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:20:40.650444  2879 net.cpp:94] Creating Layer accuracy-top5
I0109 19:20:40.650471  2879 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:20:40.650481  2879 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:20:40.650493  2879 net.cpp:409] accuracy-top5 -> top-5
I0109 19:20:40.650512  2879 net.cpp:144] Setting up accuracy-top5
I0109 19:20:40.650529  2879 net.cpp:151] Top shape: (1)
I0109 19:20:40.650537  2879 net.cpp:159] Memory required for data: 64930412
I0109 19:20:40.650543  2879 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:20:40.650558  2879 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:20:40.650568  2879 net.cpp:220] loss needs backward computation.
I0109 19:20:40.650581  2879 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:20:40.650589  2879 net.cpp:220] fc2 needs backward computation.
I0109 19:20:40.650595  2879 net.cpp:220] drop3 needs backward computation.
I0109 19:20:40.650610  2879 net.cpp:220] relu5 needs backward computation.
I0109 19:20:40.650619  2879 net.cpp:220] bn5 needs backward computation.
I0109 19:20:40.650625  2879 net.cpp:220] fc1 needs backward computation.
I0109 19:20:40.650631  2879 net.cpp:220] drop2 needs backward computation.
I0109 19:20:40.650660  2879 net.cpp:220] pool2 needs backward computation.
I0109 19:20:40.650673  2879 net.cpp:220] relu4 needs backward computation.
I0109 19:20:40.650681  2879 net.cpp:220] bn4 needs backward computation.
I0109 19:20:40.650688  2879 net.cpp:220] conv4 needs backward computation.
I0109 19:20:40.650696  2879 net.cpp:220] relu3 needs backward computation.
I0109 19:20:40.650709  2879 net.cpp:220] bn3 needs backward computation.
I0109 19:20:40.650717  2879 net.cpp:220] conv3 needs backward computation.
I0109 19:20:40.650724  2879 net.cpp:220] drop1 needs backward computation.
I0109 19:20:40.650732  2879 net.cpp:220] pool1 needs backward computation.
I0109 19:20:40.650744  2879 net.cpp:220] relu2 needs backward computation.
I0109 19:20:40.650753  2879 net.cpp:220] bn2 needs backward computation.
I0109 19:20:40.650758  2879 net.cpp:220] conv2 needs backward computation.
I0109 19:20:40.650773  2879 net.cpp:220] relu1 needs backward computation.
I0109 19:20:40.650780  2879 net.cpp:220] bn1 needs backward computation.
I0109 19:20:40.650787  2879 net.cpp:220] conv1 needs backward computation.
I0109 19:20:40.650795  2879 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:20:40.650811  2879 net.cpp:222] data does not need backward computation.
I0109 19:20:40.650818  2879 net.cpp:264] This network produces output loss
I0109 19:20:40.650825  2879 net.cpp:264] This network produces output top-1
I0109 19:20:40.650832  2879 net.cpp:264] This network produces output top-5
I0109 19:20:40.650874  2879 net.cpp:284] Network initialization done.
I0109 19:20:40.654551  2879 caffe_interface.cpp:363] Running for 180 iterations.
I0109 19:20:40.666342  2879 caffe_interface.cpp:125] Batch 0, loss = 0.57646
I0109 19:20:40.666376  2879 caffe_interface.cpp:125] Batch 0, top-1 = 0.82
I0109 19:20:40.666389  2879 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0109 19:20:40.672685  2879 caffe_interface.cpp:125] Batch 1, loss = 0.315611
I0109 19:20:40.672711  2879 caffe_interface.cpp:125] Batch 1, top-1 = 0.86
I0109 19:20:40.672721  2879 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 19:20:40.679006  2879 caffe_interface.cpp:125] Batch 2, loss = 0.827132
I0109 19:20:40.679033  2879 caffe_interface.cpp:125] Batch 2, top-1 = 0.74
I0109 19:20:40.679042  2879 caffe_interface.cpp:125] Batch 2, top-5 = 0.98
I0109 19:20:40.685359  2879 caffe_interface.cpp:125] Batch 3, loss = 0.231999
I0109 19:20:40.685386  2879 caffe_interface.cpp:125] Batch 3, top-1 = 0.92
I0109 19:20:40.685397  2879 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 19:20:40.691696  2879 caffe_interface.cpp:125] Batch 4, loss = 0.44816
I0109 19:20:40.691721  2879 caffe_interface.cpp:125] Batch 4, top-1 = 0.82
I0109 19:20:40.691733  2879 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 19:20:40.698014  2879 caffe_interface.cpp:125] Batch 5, loss = 0.544863
I0109 19:20:40.698037  2879 caffe_interface.cpp:125] Batch 5, top-1 = 0.84
I0109 19:20:40.698067  2879 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 19:20:40.704330  2879 caffe_interface.cpp:125] Batch 6, loss = 0.852113
I0109 19:20:40.704355  2879 caffe_interface.cpp:125] Batch 6, top-1 = 0.74
I0109 19:20:40.704365  2879 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 19:20:40.710600  2879 caffe_interface.cpp:125] Batch 7, loss = 0.420676
I0109 19:20:40.710625  2879 caffe_interface.cpp:125] Batch 7, top-1 = 0.88
I0109 19:20:40.710635  2879 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 19:20:40.716902  2879 caffe_interface.cpp:125] Batch 8, loss = 0.772553
I0109 19:20:40.716926  2879 caffe_interface.cpp:125] Batch 8, top-1 = 0.76
I0109 19:20:40.716938  2879 caffe_interface.cpp:125] Batch 8, top-5 = 0.96
I0109 19:20:40.723220  2879 caffe_interface.cpp:125] Batch 9, loss = 0.604803
I0109 19:20:40.723245  2879 caffe_interface.cpp:125] Batch 9, top-1 = 0.82
I0109 19:20:40.723255  2879 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 19:20:40.729538  2879 caffe_interface.cpp:125] Batch 10, loss = 0.402544
I0109 19:20:40.729562  2879 caffe_interface.cpp:125] Batch 10, top-1 = 0.84
I0109 19:20:40.729573  2879 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 19:20:40.735852  2879 caffe_interface.cpp:125] Batch 11, loss = 0.264919
I0109 19:20:40.735877  2879 caffe_interface.cpp:125] Batch 11, top-1 = 0.92
I0109 19:20:40.735886  2879 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 19:20:40.742151  2879 caffe_interface.cpp:125] Batch 12, loss = 0.604478
I0109 19:20:40.742175  2879 caffe_interface.cpp:125] Batch 12, top-1 = 0.76
I0109 19:20:40.742187  2879 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0109 19:20:40.748450  2879 caffe_interface.cpp:125] Batch 13, loss = 0.414513
I0109 19:20:40.748476  2879 caffe_interface.cpp:125] Batch 13, top-1 = 0.86
I0109 19:20:40.748486  2879 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0109 19:20:40.754761  2879 caffe_interface.cpp:125] Batch 14, loss = 0.524234
I0109 19:20:40.754786  2879 caffe_interface.cpp:125] Batch 14, top-1 = 0.82
I0109 19:20:40.754796  2879 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0109 19:20:40.761054  2879 caffe_interface.cpp:125] Batch 15, loss = 0.792049
I0109 19:20:40.761080  2879 caffe_interface.cpp:125] Batch 15, top-1 = 0.78
I0109 19:20:40.761090  2879 caffe_interface.cpp:125] Batch 15, top-5 = 0.96
I0109 19:20:40.767475  2879 caffe_interface.cpp:125] Batch 16, loss = 0.354057
I0109 19:20:40.767500  2879 caffe_interface.cpp:125] Batch 16, top-1 = 0.9
I0109 19:20:40.767508  2879 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0109 19:20:40.773906  2879 caffe_interface.cpp:125] Batch 17, loss = 0.457209
I0109 19:20:40.773936  2879 caffe_interface.cpp:125] Batch 17, top-1 = 0.84
I0109 19:20:40.773946  2879 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0109 19:20:40.779763  2879 caffe_interface.cpp:125] Batch 18, loss = 0.599386
I0109 19:20:40.779788  2879 caffe_interface.cpp:125] Batch 18, top-1 = 0.82
I0109 19:20:40.779793  2879 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 19:20:40.785534  2879 caffe_interface.cpp:125] Batch 19, loss = 0.480261
I0109 19:20:40.785557  2879 caffe_interface.cpp:125] Batch 19, top-1 = 0.9
I0109 19:20:40.785563  2879 caffe_interface.cpp:125] Batch 19, top-5 = 0.96
I0109 19:20:40.791339  2879 caffe_interface.cpp:125] Batch 20, loss = 0.348518
I0109 19:20:40.791362  2879 caffe_interface.cpp:125] Batch 20, top-1 = 0.9
I0109 19:20:40.791368  2879 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0109 19:20:40.797124  2879 caffe_interface.cpp:125] Batch 21, loss = 0.686297
I0109 19:20:40.797147  2879 caffe_interface.cpp:125] Batch 21, top-1 = 0.8
I0109 19:20:40.797152  2879 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0109 19:20:40.802881  2879 caffe_interface.cpp:125] Batch 22, loss = 0.562823
I0109 19:20:40.802902  2879 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0109 19:20:40.802907  2879 caffe_interface.cpp:125] Batch 22, top-5 = 0.98
I0109 19:20:40.808670  2879 caffe_interface.cpp:125] Batch 23, loss = 0.491431
I0109 19:20:40.808708  2879 caffe_interface.cpp:125] Batch 23, top-1 = 0.86
I0109 19:20:40.808715  2879 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 19:20:40.814483  2879 caffe_interface.cpp:125] Batch 24, loss = 0.54126
I0109 19:20:40.814507  2879 caffe_interface.cpp:125] Batch 24, top-1 = 0.86
I0109 19:20:40.814512  2879 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0109 19:20:40.820248  2879 caffe_interface.cpp:125] Batch 25, loss = 0.794679
I0109 19:20:40.820271  2879 caffe_interface.cpp:125] Batch 25, top-1 = 0.82
I0109 19:20:40.820276  2879 caffe_interface.cpp:125] Batch 25, top-5 = 0.96
I0109 19:20:40.826031  2879 caffe_interface.cpp:125] Batch 26, loss = 0.880973
I0109 19:20:40.826053  2879 caffe_interface.cpp:125] Batch 26, top-1 = 0.82
I0109 19:20:40.826058  2879 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0109 19:20:40.831801  2879 caffe_interface.cpp:125] Batch 27, loss = 0.367804
I0109 19:20:40.831825  2879 caffe_interface.cpp:125] Batch 27, top-1 = 0.88
I0109 19:20:40.831830  2879 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 19:20:40.837610  2879 caffe_interface.cpp:125] Batch 28, loss = 0.369391
I0109 19:20:40.837628  2879 caffe_interface.cpp:125] Batch 28, top-1 = 0.84
I0109 19:20:40.837635  2879 caffe_interface.cpp:125] Batch 28, top-5 = 1
I0109 19:20:40.843396  2879 caffe_interface.cpp:125] Batch 29, loss = 0.220665
I0109 19:20:40.843420  2879 caffe_interface.cpp:125] Batch 29, top-1 = 0.9
I0109 19:20:40.843425  2879 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 19:20:40.849196  2879 caffe_interface.cpp:125] Batch 30, loss = 0.545188
I0109 19:20:40.849218  2879 caffe_interface.cpp:125] Batch 30, top-1 = 0.86
I0109 19:20:40.849223  2879 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0109 19:20:40.854974  2879 caffe_interface.cpp:125] Batch 31, loss = 0.489652
I0109 19:20:40.854997  2879 caffe_interface.cpp:125] Batch 31, top-1 = 0.86
I0109 19:20:40.855002  2879 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 19:20:40.860697  2879 caffe_interface.cpp:125] Batch 32, loss = 0.783494
I0109 19:20:40.860718  2879 caffe_interface.cpp:125] Batch 32, top-1 = 0.74
I0109 19:20:40.860724  2879 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 19:20:40.866456  2879 caffe_interface.cpp:125] Batch 33, loss = 0.431278
I0109 19:20:40.866480  2879 caffe_interface.cpp:125] Batch 33, top-1 = 0.86
I0109 19:20:40.866487  2879 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 19:20:40.872216  2879 caffe_interface.cpp:125] Batch 34, loss = 0.720085
I0109 19:20:40.872239  2879 caffe_interface.cpp:125] Batch 34, top-1 = 0.78
I0109 19:20:40.872244  2879 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0109 19:20:40.877985  2879 caffe_interface.cpp:125] Batch 35, loss = 0.604627
I0109 19:20:40.878007  2879 caffe_interface.cpp:125] Batch 35, top-1 = 0.8
I0109 19:20:40.878013  2879 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 19:20:40.883793  2879 caffe_interface.cpp:125] Batch 36, loss = 0.625372
I0109 19:20:40.883816  2879 caffe_interface.cpp:125] Batch 36, top-1 = 0.82
I0109 19:20:40.883822  2879 caffe_interface.cpp:125] Batch 36, top-5 = 0.98
I0109 19:20:40.889573  2879 caffe_interface.cpp:125] Batch 37, loss = 0.705945
I0109 19:20:40.889608  2879 caffe_interface.cpp:125] Batch 37, top-1 = 0.76
I0109 19:20:40.889616  2879 caffe_interface.cpp:125] Batch 37, top-5 = 0.98
I0109 19:20:40.895392  2879 caffe_interface.cpp:125] Batch 38, loss = 0.798137
I0109 19:20:40.895413  2879 caffe_interface.cpp:125] Batch 38, top-1 = 0.84
I0109 19:20:40.895419  2879 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0109 19:20:40.901170  2879 caffe_interface.cpp:125] Batch 39, loss = 0.295071
I0109 19:20:40.901192  2879 caffe_interface.cpp:125] Batch 39, top-1 = 0.92
I0109 19:20:40.901197  2879 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 19:20:40.906987  2879 caffe_interface.cpp:125] Batch 40, loss = 0.394285
I0109 19:20:40.907011  2879 caffe_interface.cpp:125] Batch 40, top-1 = 0.84
I0109 19:20:40.907016  2879 caffe_interface.cpp:125] Batch 40, top-5 = 0.98
I0109 19:20:40.912756  2879 caffe_interface.cpp:125] Batch 41, loss = 0.368842
I0109 19:20:40.912796  2879 caffe_interface.cpp:125] Batch 41, top-1 = 0.88
I0109 19:20:40.912802  2879 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 19:20:40.918541  2879 caffe_interface.cpp:125] Batch 42, loss = 0.420098
I0109 19:20:40.918563  2879 caffe_interface.cpp:125] Batch 42, top-1 = 0.84
I0109 19:20:40.918570  2879 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0109 19:20:40.924343  2879 caffe_interface.cpp:125] Batch 43, loss = 0.357506
I0109 19:20:40.924366  2879 caffe_interface.cpp:125] Batch 43, top-1 = 0.92
I0109 19:20:40.924371  2879 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0109 19:20:40.930132  2879 caffe_interface.cpp:125] Batch 44, loss = 0.406705
I0109 19:20:40.930155  2879 caffe_interface.cpp:125] Batch 44, top-1 = 0.9
I0109 19:20:40.930161  2879 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0109 19:20:40.935912  2879 caffe_interface.cpp:125] Batch 45, loss = 0.665714
I0109 19:20:40.935933  2879 caffe_interface.cpp:125] Batch 45, top-1 = 0.8
I0109 19:20:40.935940  2879 caffe_interface.cpp:125] Batch 45, top-5 = 0.96
I0109 19:20:40.941740  2879 caffe_interface.cpp:125] Batch 46, loss = 0.818374
I0109 19:20:40.941769  2879 caffe_interface.cpp:125] Batch 46, top-1 = 0.76
I0109 19:20:40.941778  2879 caffe_interface.cpp:125] Batch 46, top-5 = 0.94
I0109 19:20:40.947027  2879 caffe_interface.cpp:125] Batch 47, loss = 0.888068
I0109 19:20:40.947052  2879 caffe_interface.cpp:125] Batch 47, top-1 = 0.82
I0109 19:20:40.947058  2879 caffe_interface.cpp:125] Batch 47, top-5 = 0.96
I0109 19:20:40.952297  2879 caffe_interface.cpp:125] Batch 48, loss = 0.403445
I0109 19:20:40.952320  2879 caffe_interface.cpp:125] Batch 48, top-1 = 0.88
I0109 19:20:40.952327  2879 caffe_interface.cpp:125] Batch 48, top-5 = 0.98
I0109 19:20:40.957535  2879 caffe_interface.cpp:125] Batch 49, loss = 0.659205
I0109 19:20:40.957556  2879 caffe_interface.cpp:125] Batch 49, top-1 = 0.8
I0109 19:20:40.957561  2879 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0109 19:20:40.962792  2879 caffe_interface.cpp:125] Batch 50, loss = 0.416761
I0109 19:20:40.962816  2879 caffe_interface.cpp:125] Batch 50, top-1 = 0.92
I0109 19:20:40.962822  2879 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0109 19:20:40.968056  2879 caffe_interface.cpp:125] Batch 51, loss = 0.61235
I0109 19:20:40.968080  2879 caffe_interface.cpp:125] Batch 51, top-1 = 0.86
I0109 19:20:40.968084  2879 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0109 19:20:40.973317  2879 caffe_interface.cpp:125] Batch 52, loss = 0.591803
I0109 19:20:40.973341  2879 caffe_interface.cpp:125] Batch 52, top-1 = 0.86
I0109 19:20:40.973348  2879 caffe_interface.cpp:125] Batch 52, top-5 = 1
I0109 19:20:40.978556  2879 caffe_interface.cpp:125] Batch 53, loss = 0.571736
I0109 19:20:40.978579  2879 caffe_interface.cpp:125] Batch 53, top-1 = 0.76
I0109 19:20:40.978585  2879 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0109 19:20:40.983836  2879 caffe_interface.cpp:125] Batch 54, loss = 0.623948
I0109 19:20:40.983857  2879 caffe_interface.cpp:125] Batch 54, top-1 = 0.78
I0109 19:20:40.983863  2879 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0109 19:20:40.989089  2879 caffe_interface.cpp:125] Batch 55, loss = 0.458413
I0109 19:20:40.989112  2879 caffe_interface.cpp:125] Batch 55, top-1 = 0.86
I0109 19:20:40.989117  2879 caffe_interface.cpp:125] Batch 55, top-5 = 0.98
I0109 19:20:40.994333  2879 caffe_interface.cpp:125] Batch 56, loss = 0.334475
I0109 19:20:40.994355  2879 caffe_interface.cpp:125] Batch 56, top-1 = 0.9
I0109 19:20:40.994362  2879 caffe_interface.cpp:125] Batch 56, top-5 = 0.98
I0109 19:20:40.999609  2879 caffe_interface.cpp:125] Batch 57, loss = 0.418676
I0109 19:20:40.999632  2879 caffe_interface.cpp:125] Batch 57, top-1 = 0.86
I0109 19:20:40.999637  2879 caffe_interface.cpp:125] Batch 57, top-5 = 0.96
I0109 19:20:41.004859  2879 caffe_interface.cpp:125] Batch 58, loss = 0.651064
I0109 19:20:41.004881  2879 caffe_interface.cpp:125] Batch 58, top-1 = 0.8
I0109 19:20:41.004887  2879 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0109 19:20:41.010134  2879 caffe_interface.cpp:125] Batch 59, loss = 0.354631
I0109 19:20:41.010175  2879 caffe_interface.cpp:125] Batch 59, top-1 = 0.82
I0109 19:20:41.010181  2879 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0109 19:20:41.015413  2879 caffe_interface.cpp:125] Batch 60, loss = 0.575819
I0109 19:20:41.015435  2879 caffe_interface.cpp:125] Batch 60, top-1 = 0.8
I0109 19:20:41.015441  2879 caffe_interface.cpp:125] Batch 60, top-5 = 0.98
I0109 19:20:41.020680  2879 caffe_interface.cpp:125] Batch 61, loss = 0.314317
I0109 19:20:41.020702  2879 caffe_interface.cpp:125] Batch 61, top-1 = 0.9
I0109 19:20:41.020707  2879 caffe_interface.cpp:125] Batch 61, top-5 = 1
I0109 19:20:41.025949  2879 caffe_interface.cpp:125] Batch 62, loss = 0.340913
I0109 19:20:41.025971  2879 caffe_interface.cpp:125] Batch 62, top-1 = 0.92
I0109 19:20:41.025976  2879 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0109 19:20:41.031199  2879 caffe_interface.cpp:125] Batch 63, loss = 0.486319
I0109 19:20:41.031222  2879 caffe_interface.cpp:125] Batch 63, top-1 = 0.88
I0109 19:20:41.031227  2879 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0109 19:20:41.036481  2879 caffe_interface.cpp:125] Batch 64, loss = 0.384718
I0109 19:20:41.036504  2879 caffe_interface.cpp:125] Batch 64, top-1 = 0.88
I0109 19:20:41.036510  2879 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 19:20:41.041738  2879 caffe_interface.cpp:125] Batch 65, loss = 0.420924
I0109 19:20:41.041759  2879 caffe_interface.cpp:125] Batch 65, top-1 = 0.84
I0109 19:20:41.041764  2879 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 19:20:41.046998  2879 caffe_interface.cpp:125] Batch 66, loss = 0.818157
I0109 19:20:41.047019  2879 caffe_interface.cpp:125] Batch 66, top-1 = 0.76
I0109 19:20:41.047024  2879 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0109 19:20:41.052248  2879 caffe_interface.cpp:125] Batch 67, loss = 0.3203
I0109 19:20:41.052271  2879 caffe_interface.cpp:125] Batch 67, top-1 = 0.9
I0109 19:20:41.052276  2879 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 19:20:41.057474  2879 caffe_interface.cpp:125] Batch 68, loss = 0.807565
I0109 19:20:41.057497  2879 caffe_interface.cpp:125] Batch 68, top-1 = 0.7
I0109 19:20:41.057502  2879 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0109 19:20:41.062762  2879 caffe_interface.cpp:125] Batch 69, loss = 0.662967
I0109 19:20:41.062789  2879 caffe_interface.cpp:125] Batch 69, top-1 = 0.76
I0109 19:20:41.062798  2879 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 19:20:41.068084  2879 caffe_interface.cpp:125] Batch 70, loss = 0.604642
I0109 19:20:41.068106  2879 caffe_interface.cpp:125] Batch 70, top-1 = 0.86
I0109 19:20:41.068112  2879 caffe_interface.cpp:125] Batch 70, top-5 = 0.96
I0109 19:20:41.073346  2879 caffe_interface.cpp:125] Batch 71, loss = 0.419067
I0109 19:20:41.073370  2879 caffe_interface.cpp:125] Batch 71, top-1 = 0.86
I0109 19:20:41.073376  2879 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0109 19:20:41.078598  2879 caffe_interface.cpp:125] Batch 72, loss = 0.277422
I0109 19:20:41.078620  2879 caffe_interface.cpp:125] Batch 72, top-1 = 0.88
I0109 19:20:41.078626  2879 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 19:20:41.083855  2879 caffe_interface.cpp:125] Batch 73, loss = 0.713197
I0109 19:20:41.083878  2879 caffe_interface.cpp:125] Batch 73, top-1 = 0.84
I0109 19:20:41.083883  2879 caffe_interface.cpp:125] Batch 73, top-5 = 1
I0109 19:20:41.089120  2879 caffe_interface.cpp:125] Batch 74, loss = 0.617022
I0109 19:20:41.089143  2879 caffe_interface.cpp:125] Batch 74, top-1 = 0.86
I0109 19:20:41.089148  2879 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 19:20:41.094390  2879 caffe_interface.cpp:125] Batch 75, loss = 0.528481
I0109 19:20:41.094413  2879 caffe_interface.cpp:125] Batch 75, top-1 = 0.84
I0109 19:20:41.094418  2879 caffe_interface.cpp:125] Batch 75, top-5 = 0.96
I0109 19:20:41.099645  2879 caffe_interface.cpp:125] Batch 76, loss = 0.576959
I0109 19:20:41.099666  2879 caffe_interface.cpp:125] Batch 76, top-1 = 0.86
I0109 19:20:41.099673  2879 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 19:20:41.104957  2879 caffe_interface.cpp:125] Batch 77, loss = 0.260926
I0109 19:20:41.104980  2879 caffe_interface.cpp:125] Batch 77, top-1 = 0.94
I0109 19:20:41.104985  2879 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0109 19:20:41.110293  2879 caffe_interface.cpp:125] Batch 78, loss = 0.797469
I0109 19:20:41.110323  2879 caffe_interface.cpp:125] Batch 78, top-1 = 0.74
I0109 19:20:41.110332  2879 caffe_interface.cpp:125] Batch 78, top-5 = 0.98
I0109 19:20:41.115234  2879 caffe_interface.cpp:125] Batch 79, loss = 0.611523
I0109 19:20:41.115258  2879 caffe_interface.cpp:125] Batch 79, top-1 = 0.86
I0109 19:20:41.115264  2879 caffe_interface.cpp:125] Batch 79, top-5 = 0.96
I0109 19:20:41.120116  2879 caffe_interface.cpp:125] Batch 80, loss = 0.549499
I0109 19:20:41.120139  2879 caffe_interface.cpp:125] Batch 80, top-1 = 0.82
I0109 19:20:41.120146  2879 caffe_interface.cpp:125] Batch 80, top-5 = 0.98
I0109 19:20:41.124989  2879 caffe_interface.cpp:125] Batch 81, loss = 0.715426
I0109 19:20:41.125011  2879 caffe_interface.cpp:125] Batch 81, top-1 = 0.88
I0109 19:20:41.125016  2879 caffe_interface.cpp:125] Batch 81, top-5 = 0.96
I0109 19:20:41.129866  2879 caffe_interface.cpp:125] Batch 82, loss = 0.388205
I0109 19:20:41.129889  2879 caffe_interface.cpp:125] Batch 82, top-1 = 0.88
I0109 19:20:41.129894  2879 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 19:20:41.134765  2879 caffe_interface.cpp:125] Batch 83, loss = 0.500628
I0109 19:20:41.134789  2879 caffe_interface.cpp:125] Batch 83, top-1 = 0.82
I0109 19:20:41.134795  2879 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0109 19:20:41.139648  2879 caffe_interface.cpp:125] Batch 84, loss = 0.654188
I0109 19:20:41.139672  2879 caffe_interface.cpp:125] Batch 84, top-1 = 0.74
I0109 19:20:41.139678  2879 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 19:20:41.144513  2879 caffe_interface.cpp:125] Batch 85, loss = 0.551355
I0109 19:20:41.144536  2879 caffe_interface.cpp:125] Batch 85, top-1 = 0.76
I0109 19:20:41.144542  2879 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 19:20:41.149405  2879 caffe_interface.cpp:125] Batch 86, loss = 0.501905
I0109 19:20:41.149428  2879 caffe_interface.cpp:125] Batch 86, top-1 = 0.76
I0109 19:20:41.149433  2879 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 19:20:41.154266  2879 caffe_interface.cpp:125] Batch 87, loss = 0.651262
I0109 19:20:41.154290  2879 caffe_interface.cpp:125] Batch 87, top-1 = 0.84
I0109 19:20:41.154295  2879 caffe_interface.cpp:125] Batch 87, top-5 = 0.96
I0109 19:20:41.159104  2879 caffe_interface.cpp:125] Batch 88, loss = 0.611873
I0109 19:20:41.159126  2879 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0109 19:20:41.159133  2879 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 19:20:41.163970  2879 caffe_interface.cpp:125] Batch 89, loss = 0.684977
I0109 19:20:41.163995  2879 caffe_interface.cpp:125] Batch 89, top-1 = 0.78
I0109 19:20:41.164000  2879 caffe_interface.cpp:125] Batch 89, top-5 = 0.98
I0109 19:20:41.168860  2879 caffe_interface.cpp:125] Batch 90, loss = 0.223031
I0109 19:20:41.168884  2879 caffe_interface.cpp:125] Batch 90, top-1 = 0.88
I0109 19:20:41.168889  2879 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 19:20:41.173743  2879 caffe_interface.cpp:125] Batch 91, loss = 0.439278
I0109 19:20:41.173768  2879 caffe_interface.cpp:125] Batch 91, top-1 = 0.82
I0109 19:20:41.173772  2879 caffe_interface.cpp:125] Batch 91, top-5 = 0.98
I0109 19:20:41.178620  2879 caffe_interface.cpp:125] Batch 92, loss = 0.43468
I0109 19:20:41.178643  2879 caffe_interface.cpp:125] Batch 92, top-1 = 0.84
I0109 19:20:41.178650  2879 caffe_interface.cpp:125] Batch 92, top-5 = 1
I0109 19:20:41.183490  2879 caffe_interface.cpp:125] Batch 93, loss = 0.408744
I0109 19:20:41.183512  2879 caffe_interface.cpp:125] Batch 93, top-1 = 0.9
I0109 19:20:41.183518  2879 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 19:20:41.188346  2879 caffe_interface.cpp:125] Batch 94, loss = 0.521647
I0109 19:20:41.188369  2879 caffe_interface.cpp:125] Batch 94, top-1 = 0.88
I0109 19:20:41.188375  2879 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0109 19:20:41.193235  2879 caffe_interface.cpp:125] Batch 95, loss = 0.56777
I0109 19:20:41.193259  2879 caffe_interface.cpp:125] Batch 95, top-1 = 0.86
I0109 19:20:41.193264  2879 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 19:20:41.198108  2879 caffe_interface.cpp:125] Batch 96, loss = 0.505232
I0109 19:20:41.198130  2879 caffe_interface.cpp:125] Batch 96, top-1 = 0.82
I0109 19:20:41.198137  2879 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 19:20:41.202967  2879 caffe_interface.cpp:125] Batch 97, loss = 0.654526
I0109 19:20:41.202989  2879 caffe_interface.cpp:125] Batch 97, top-1 = 0.8
I0109 19:20:41.202994  2879 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0109 19:20:41.207856  2879 caffe_interface.cpp:125] Batch 98, loss = 0.659664
I0109 19:20:41.207880  2879 caffe_interface.cpp:125] Batch 98, top-1 = 0.8
I0109 19:20:41.207885  2879 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0109 19:20:41.212744  2879 caffe_interface.cpp:125] Batch 99, loss = 0.656554
I0109 19:20:41.212769  2879 caffe_interface.cpp:125] Batch 99, top-1 = 0.82
I0109 19:20:41.212774  2879 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0109 19:20:41.217618  2879 caffe_interface.cpp:125] Batch 100, loss = 0.342687
I0109 19:20:41.217639  2879 caffe_interface.cpp:125] Batch 100, top-1 = 0.88
I0109 19:20:41.217645  2879 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 19:20:41.222501  2879 caffe_interface.cpp:125] Batch 101, loss = 0.64948
I0109 19:20:41.222522  2879 caffe_interface.cpp:125] Batch 101, top-1 = 0.8
I0109 19:20:41.222528  2879 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 19:20:41.227385  2879 caffe_interface.cpp:125] Batch 102, loss = 0.360758
I0109 19:20:41.227407  2879 caffe_interface.cpp:125] Batch 102, top-1 = 0.9
I0109 19:20:41.227412  2879 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0109 19:20:41.232259  2879 caffe_interface.cpp:125] Batch 103, loss = 0.619568
I0109 19:20:41.232282  2879 caffe_interface.cpp:125] Batch 103, top-1 = 0.8
I0109 19:20:41.232287  2879 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 19:20:41.237115  2879 caffe_interface.cpp:125] Batch 104, loss = 0.320178
I0109 19:20:41.237138  2879 caffe_interface.cpp:125] Batch 104, top-1 = 0.86
I0109 19:20:41.237144  2879 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 19:20:41.241981  2879 caffe_interface.cpp:125] Batch 105, loss = 0.284827
I0109 19:20:41.242008  2879 caffe_interface.cpp:125] Batch 105, top-1 = 0.88
I0109 19:20:41.242015  2879 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 19:20:41.246851  2879 caffe_interface.cpp:125] Batch 106, loss = 0.463298
I0109 19:20:41.246873  2879 caffe_interface.cpp:125] Batch 106, top-1 = 0.88
I0109 19:20:41.246879  2879 caffe_interface.cpp:125] Batch 106, top-5 = 0.98
I0109 19:20:41.251713  2879 caffe_interface.cpp:125] Batch 107, loss = 0.767927
I0109 19:20:41.251737  2879 caffe_interface.cpp:125] Batch 107, top-1 = 0.84
I0109 19:20:41.251744  2879 caffe_interface.cpp:125] Batch 107, top-5 = 0.96
I0109 19:20:41.256574  2879 caffe_interface.cpp:125] Batch 108, loss = 0.556756
I0109 19:20:41.256597  2879 caffe_interface.cpp:125] Batch 108, top-1 = 0.78
I0109 19:20:41.256603  2879 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0109 19:20:41.261420  2879 caffe_interface.cpp:125] Batch 109, loss = 0.347207
I0109 19:20:41.261443  2879 caffe_interface.cpp:125] Batch 109, top-1 = 0.86
I0109 19:20:41.261448  2879 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0109 19:20:41.266289  2879 caffe_interface.cpp:125] Batch 110, loss = 0.293823
I0109 19:20:41.266314  2879 caffe_interface.cpp:125] Batch 110, top-1 = 0.92
I0109 19:20:41.266319  2879 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 19:20:41.271159  2879 caffe_interface.cpp:125] Batch 111, loss = 0.315443
I0109 19:20:41.271183  2879 caffe_interface.cpp:125] Batch 111, top-1 = 0.9
I0109 19:20:41.271188  2879 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0109 19:20:41.276100  2879 caffe_interface.cpp:125] Batch 112, loss = 0.69422
I0109 19:20:41.276130  2879 caffe_interface.cpp:125] Batch 112, top-1 = 0.82
I0109 19:20:41.276162  2879 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0109 19:20:41.280913  2879 caffe_interface.cpp:125] Batch 113, loss = 0.735392
I0109 19:20:41.280938  2879 caffe_interface.cpp:125] Batch 113, top-1 = 0.74
I0109 19:20:41.280944  2879 caffe_interface.cpp:125] Batch 113, top-5 = 0.98
I0109 19:20:41.285497  2879 caffe_interface.cpp:125] Batch 114, loss = 0.588282
I0109 19:20:41.285521  2879 caffe_interface.cpp:125] Batch 114, top-1 = 0.84
I0109 19:20:41.285526  2879 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0109 19:20:41.290092  2879 caffe_interface.cpp:125] Batch 115, loss = 0.420712
I0109 19:20:41.290117  2879 caffe_interface.cpp:125] Batch 115, top-1 = 0.82
I0109 19:20:41.290122  2879 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 19:20:41.294651  2879 caffe_interface.cpp:125] Batch 116, loss = 0.65015
I0109 19:20:41.294674  2879 caffe_interface.cpp:125] Batch 116, top-1 = 0.8
I0109 19:20:41.294682  2879 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0109 19:20:41.299213  2879 caffe_interface.cpp:125] Batch 117, loss = 0.391435
I0109 19:20:41.299235  2879 caffe_interface.cpp:125] Batch 117, top-1 = 0.86
I0109 19:20:41.299240  2879 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0109 19:20:41.303773  2879 caffe_interface.cpp:125] Batch 118, loss = 0.725746
I0109 19:20:41.303795  2879 caffe_interface.cpp:125] Batch 118, top-1 = 0.78
I0109 19:20:41.303800  2879 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0109 19:20:41.308305  2879 caffe_interface.cpp:125] Batch 119, loss = 0.258459
I0109 19:20:41.308327  2879 caffe_interface.cpp:125] Batch 119, top-1 = 0.92
I0109 19:20:41.308332  2879 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 19:20:41.312844  2879 caffe_interface.cpp:125] Batch 120, loss = 0.643186
I0109 19:20:41.312866  2879 caffe_interface.cpp:125] Batch 120, top-1 = 0.78
I0109 19:20:41.312872  2879 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0109 19:20:41.317391  2879 caffe_interface.cpp:125] Batch 121, loss = 0.52274
I0109 19:20:41.317412  2879 caffe_interface.cpp:125] Batch 121, top-1 = 0.8
I0109 19:20:41.317417  2879 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 19:20:41.321950  2879 caffe_interface.cpp:125] Batch 122, loss = 0.514703
I0109 19:20:41.321974  2879 caffe_interface.cpp:125] Batch 122, top-1 = 0.8
I0109 19:20:41.321979  2879 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 19:20:41.326508  2879 caffe_interface.cpp:125] Batch 123, loss = 0.456106
I0109 19:20:41.326530  2879 caffe_interface.cpp:125] Batch 123, top-1 = 0.84
I0109 19:20:41.326535  2879 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0109 19:20:41.331063  2879 caffe_interface.cpp:125] Batch 124, loss = 0.427271
I0109 19:20:41.331084  2879 caffe_interface.cpp:125] Batch 124, top-1 = 0.86
I0109 19:20:41.331089  2879 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 19:20:41.335604  2879 caffe_interface.cpp:125] Batch 125, loss = 0.587712
I0109 19:20:41.335625  2879 caffe_interface.cpp:125] Batch 125, top-1 = 0.86
I0109 19:20:41.335631  2879 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0109 19:20:41.340171  2879 caffe_interface.cpp:125] Batch 126, loss = 0.731904
I0109 19:20:41.340193  2879 caffe_interface.cpp:125] Batch 126, top-1 = 0.8
I0109 19:20:41.340199  2879 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 19:20:41.344730  2879 caffe_interface.cpp:125] Batch 127, loss = 0.353377
I0109 19:20:41.344753  2879 caffe_interface.cpp:125] Batch 127, top-1 = 0.84
I0109 19:20:41.344759  2879 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 19:20:41.349289  2879 caffe_interface.cpp:125] Batch 128, loss = 0.847151
I0109 19:20:41.349318  2879 caffe_interface.cpp:125] Batch 128, top-1 = 0.78
I0109 19:20:41.349328  2879 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0109 19:20:41.353888  2879 caffe_interface.cpp:125] Batch 129, loss = 0.432991
I0109 19:20:41.353909  2879 caffe_interface.cpp:125] Batch 129, top-1 = 0.84
I0109 19:20:41.353914  2879 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 19:20:41.358438  2879 caffe_interface.cpp:125] Batch 130, loss = 0.458827
I0109 19:20:41.358479  2879 caffe_interface.cpp:125] Batch 130, top-1 = 0.86
I0109 19:20:41.358486  2879 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 19:20:41.362974  2879 caffe_interface.cpp:125] Batch 131, loss = 0.682863
I0109 19:20:41.362998  2879 caffe_interface.cpp:125] Batch 131, top-1 = 0.8
I0109 19:20:41.363003  2879 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 19:20:41.367553  2879 caffe_interface.cpp:125] Batch 132, loss = 0.355736
I0109 19:20:41.367578  2879 caffe_interface.cpp:125] Batch 132, top-1 = 0.9
I0109 19:20:41.367585  2879 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 19:20:41.372110  2879 caffe_interface.cpp:125] Batch 133, loss = 0.380979
I0109 19:20:41.372133  2879 caffe_interface.cpp:125] Batch 133, top-1 = 0.86
I0109 19:20:41.372138  2879 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0109 19:20:41.376663  2879 caffe_interface.cpp:125] Batch 134, loss = 0.692192
I0109 19:20:41.376688  2879 caffe_interface.cpp:125] Batch 134, top-1 = 0.8
I0109 19:20:41.376694  2879 caffe_interface.cpp:125] Batch 134, top-5 = 0.96
I0109 19:20:41.381239  2879 caffe_interface.cpp:125] Batch 135, loss = 0.378652
I0109 19:20:41.381263  2879 caffe_interface.cpp:125] Batch 135, top-1 = 0.88
I0109 19:20:41.381269  2879 caffe_interface.cpp:125] Batch 135, top-5 = 1
I0109 19:20:41.385838  2879 caffe_interface.cpp:125] Batch 136, loss = 0.354062
I0109 19:20:41.385859  2879 caffe_interface.cpp:125] Batch 136, top-1 = 0.78
I0109 19:20:41.385866  2879 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 19:20:41.390427  2879 caffe_interface.cpp:125] Batch 137, loss = 0.404434
I0109 19:20:41.390450  2879 caffe_interface.cpp:125] Batch 137, top-1 = 0.9
I0109 19:20:41.390456  2879 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 19:20:41.395020  2879 caffe_interface.cpp:125] Batch 138, loss = 0.685846
I0109 19:20:41.395043  2879 caffe_interface.cpp:125] Batch 138, top-1 = 0.84
I0109 19:20:41.395048  2879 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0109 19:20:41.399619  2879 caffe_interface.cpp:125] Batch 139, loss = 0.590317
I0109 19:20:41.399646  2879 caffe_interface.cpp:125] Batch 139, top-1 = 0.82
I0109 19:20:41.399650  2879 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 19:20:41.404227  2879 caffe_interface.cpp:125] Batch 140, loss = 0.67873
I0109 19:20:41.404251  2879 caffe_interface.cpp:125] Batch 140, top-1 = 0.8
I0109 19:20:41.404258  2879 caffe_interface.cpp:125] Batch 140, top-5 = 0.96
I0109 19:20:41.408784  2879 caffe_interface.cpp:125] Batch 141, loss = 0.371062
I0109 19:20:41.408807  2879 caffe_interface.cpp:125] Batch 141, top-1 = 0.92
I0109 19:20:41.408813  2879 caffe_interface.cpp:125] Batch 141, top-5 = 1
I0109 19:20:41.413365  2879 caffe_interface.cpp:125] Batch 142, loss = 0.360283
I0109 19:20:41.413388  2879 caffe_interface.cpp:125] Batch 142, top-1 = 0.88
I0109 19:20:41.413394  2879 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 19:20:41.417946  2879 caffe_interface.cpp:125] Batch 143, loss = 0.210918
I0109 19:20:41.417969  2879 caffe_interface.cpp:125] Batch 143, top-1 = 0.92
I0109 19:20:41.417974  2879 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 19:20:41.422520  2879 caffe_interface.cpp:125] Batch 144, loss = 0.334666
I0109 19:20:41.422544  2879 caffe_interface.cpp:125] Batch 144, top-1 = 0.88
I0109 19:20:41.422550  2879 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0109 19:20:41.427098  2879 caffe_interface.cpp:125] Batch 145, loss = 0.628872
I0109 19:20:41.427120  2879 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0109 19:20:41.427126  2879 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0109 19:20:41.431704  2879 caffe_interface.cpp:125] Batch 146, loss = 0.568368
I0109 19:20:41.431728  2879 caffe_interface.cpp:125] Batch 146, top-1 = 0.84
I0109 19:20:41.431733  2879 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0109 19:20:41.436285  2879 caffe_interface.cpp:125] Batch 147, loss = 0.511537
I0109 19:20:41.436309  2879 caffe_interface.cpp:125] Batch 147, top-1 = 0.82
I0109 19:20:41.436314  2879 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0109 19:20:41.440879  2879 caffe_interface.cpp:125] Batch 148, loss = 0.721032
I0109 19:20:41.440903  2879 caffe_interface.cpp:125] Batch 148, top-1 = 0.8
I0109 19:20:41.440910  2879 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0109 19:20:41.445508  2879 caffe_interface.cpp:125] Batch 149, loss = 0.610248
I0109 19:20:41.445533  2879 caffe_interface.cpp:125] Batch 149, top-1 = 0.8
I0109 19:20:41.445539  2879 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0109 19:20:41.450028  2879 caffe_interface.cpp:125] Batch 150, loss = 0.352573
I0109 19:20:41.450052  2879 caffe_interface.cpp:125] Batch 150, top-1 = 0.88
I0109 19:20:41.450057  2879 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0109 19:20:41.454541  2879 caffe_interface.cpp:125] Batch 151, loss = 0.333416
I0109 19:20:41.454565  2879 caffe_interface.cpp:125] Batch 151, top-1 = 0.88
I0109 19:20:41.454571  2879 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 19:20:41.459048  2879 caffe_interface.cpp:125] Batch 152, loss = 0.579072
I0109 19:20:41.459071  2879 caffe_interface.cpp:125] Batch 152, top-1 = 0.84
I0109 19:20:41.459077  2879 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0109 19:20:41.463500  2879 caffe_interface.cpp:125] Batch 153, loss = 0.229783
I0109 19:20:41.463523  2879 caffe_interface.cpp:125] Batch 153, top-1 = 0.88
I0109 19:20:41.463529  2879 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 19:20:41.468004  2879 caffe_interface.cpp:125] Batch 154, loss = 0.303051
I0109 19:20:41.468029  2879 caffe_interface.cpp:125] Batch 154, top-1 = 0.88
I0109 19:20:41.468034  2879 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 19:20:41.472507  2879 caffe_interface.cpp:125] Batch 155, loss = 0.190963
I0109 19:20:41.472532  2879 caffe_interface.cpp:125] Batch 155, top-1 = 0.92
I0109 19:20:41.472537  2879 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 19:20:41.476994  2879 caffe_interface.cpp:125] Batch 156, loss = 0.478046
I0109 19:20:41.477016  2879 caffe_interface.cpp:125] Batch 156, top-1 = 0.86
I0109 19:20:41.477023  2879 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 19:20:41.481518  2879 caffe_interface.cpp:125] Batch 157, loss = 0.468684
I0109 19:20:41.481541  2879 caffe_interface.cpp:125] Batch 157, top-1 = 0.88
I0109 19:20:41.481546  2879 caffe_interface.cpp:125] Batch 157, top-5 = 0.98
I0109 19:20:41.486009  2879 caffe_interface.cpp:125] Batch 158, loss = 0.740968
I0109 19:20:41.486032  2879 caffe_interface.cpp:125] Batch 158, top-1 = 0.8
I0109 19:20:41.486037  2879 caffe_interface.cpp:125] Batch 158, top-5 = 0.96
I0109 19:20:41.490504  2879 caffe_interface.cpp:125] Batch 159, loss = 0.417805
I0109 19:20:41.490528  2879 caffe_interface.cpp:125] Batch 159, top-1 = 0.84
I0109 19:20:41.490533  2879 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0109 19:20:41.494998  2879 caffe_interface.cpp:125] Batch 160, loss = 0.646196
I0109 19:20:41.495023  2879 caffe_interface.cpp:125] Batch 160, top-1 = 0.8
I0109 19:20:41.495028  2879 caffe_interface.cpp:125] Batch 160, top-5 = 0.98
I0109 19:20:41.499481  2879 caffe_interface.cpp:125] Batch 161, loss = 0.337878
I0109 19:20:41.499505  2879 caffe_interface.cpp:125] Batch 161, top-1 = 0.9
I0109 19:20:41.499509  2879 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0109 19:20:41.503984  2879 caffe_interface.cpp:125] Batch 162, loss = 0.585543
I0109 19:20:41.504009  2879 caffe_interface.cpp:125] Batch 162, top-1 = 0.8
I0109 19:20:41.504014  2879 caffe_interface.cpp:125] Batch 162, top-5 = 0.98
I0109 19:20:41.508481  2879 caffe_interface.cpp:125] Batch 163, loss = 0.423316
I0109 19:20:41.508504  2879 caffe_interface.cpp:125] Batch 163, top-1 = 0.84
I0109 19:20:41.508509  2879 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0109 19:20:41.512959  2879 caffe_interface.cpp:125] Batch 164, loss = 0.685068
I0109 19:20:41.512982  2879 caffe_interface.cpp:125] Batch 164, top-1 = 0.74
I0109 19:20:41.512989  2879 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0109 19:20:41.517458  2879 caffe_interface.cpp:125] Batch 165, loss = 0.450785
I0109 19:20:41.517482  2879 caffe_interface.cpp:125] Batch 165, top-1 = 0.84
I0109 19:20:41.517504  2879 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 19:20:41.521991  2879 caffe_interface.cpp:125] Batch 166, loss = 0.420376
I0109 19:20:41.522014  2879 caffe_interface.cpp:125] Batch 166, top-1 = 0.84
I0109 19:20:41.522019  2879 caffe_interface.cpp:125] Batch 166, top-5 = 0.98
I0109 19:20:41.526475  2879 caffe_interface.cpp:125] Batch 167, loss = 0.657508
I0109 19:20:41.526499  2879 caffe_interface.cpp:125] Batch 167, top-1 = 0.8
I0109 19:20:41.526504  2879 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0109 19:20:41.530997  2879 caffe_interface.cpp:125] Batch 168, loss = 0.731709
I0109 19:20:41.531020  2879 caffe_interface.cpp:125] Batch 168, top-1 = 0.76
I0109 19:20:41.531028  2879 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0109 19:20:41.535496  2879 caffe_interface.cpp:125] Batch 169, loss = 0.754796
I0109 19:20:41.535519  2879 caffe_interface.cpp:125] Batch 169, top-1 = 0.8
I0109 19:20:41.535526  2879 caffe_interface.cpp:125] Batch 169, top-5 = 1
I0109 19:20:41.540002  2879 caffe_interface.cpp:125] Batch 170, loss = 0.305407
I0109 19:20:41.540026  2879 caffe_interface.cpp:125] Batch 170, top-1 = 0.88
I0109 19:20:41.540032  2879 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0109 19:20:41.544520  2879 caffe_interface.cpp:125] Batch 171, loss = 0.346429
I0109 19:20:41.544545  2879 caffe_interface.cpp:125] Batch 171, top-1 = 0.88
I0109 19:20:41.544550  2879 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 19:20:41.549019  2879 caffe_interface.cpp:125] Batch 172, loss = 0.334565
I0109 19:20:41.549044  2879 caffe_interface.cpp:125] Batch 172, top-1 = 0.9
I0109 19:20:41.549051  2879 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 19:20:41.553544  2879 caffe_interface.cpp:125] Batch 173, loss = 0.772887
I0109 19:20:41.553568  2879 caffe_interface.cpp:125] Batch 173, top-1 = 0.78
I0109 19:20:41.553575  2879 caffe_interface.cpp:125] Batch 173, top-5 = 0.98
I0109 19:20:41.558075  2879 caffe_interface.cpp:125] Batch 174, loss = 0.915541
I0109 19:20:41.558099  2879 caffe_interface.cpp:125] Batch 174, top-1 = 0.76
I0109 19:20:41.558104  2879 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0109 19:20:41.562528  2879 caffe_interface.cpp:125] Batch 175, loss = 0.593237
I0109 19:20:41.562553  2879 caffe_interface.cpp:125] Batch 175, top-1 = 0.78
I0109 19:20:41.562559  2879 caffe_interface.cpp:125] Batch 175, top-5 = 0.98
I0109 19:20:41.567134  2879 caffe_interface.cpp:125] Batch 176, loss = 0.664633
I0109 19:20:41.567164  2879 caffe_interface.cpp:125] Batch 176, top-1 = 0.78
I0109 19:20:41.567175  2879 caffe_interface.cpp:125] Batch 176, top-5 = 1
I0109 19:20:41.571709  2879 caffe_interface.cpp:125] Batch 177, loss = 0.664701
I0109 19:20:41.571734  2879 caffe_interface.cpp:125] Batch 177, top-1 = 0.8
I0109 19:20:41.571740  2879 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 19:20:41.576212  2879 caffe_interface.cpp:125] Batch 178, loss = 0.270286
I0109 19:20:41.576236  2879 caffe_interface.cpp:125] Batch 178, top-1 = 0.92
I0109 19:20:41.576242  2879 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 19:20:41.580713  2879 caffe_interface.cpp:125] Batch 179, loss = 0.392399
I0109 19:20:41.580735  2879 caffe_interface.cpp:125] Batch 179, top-1 = 0.84
I0109 19:20:41.580741  2879 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0109 19:20:41.580746  2879 caffe_interface.cpp:130] Loss: 0.521759
I0109 19:20:41.580755  2879 caffe_interface.cpp:142] loss = 0.521759 (* 1 = 0.521759 loss)
I0109 19:20:41.580763  2879 caffe_interface.cpp:142] top-1 = 0.837333
I0109 19:20:41.580771  2879 caffe_interface.cpp:142] top-5 = 0.989778
I0109 19:20:41.595542  2879 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/sparse.caffemodel
I0109 19:20:41.595584  2879 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.1:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.837333143    | -0.027666688   |
+-------------------------------------------------------------------+
| Weights        | 68389          | 62561          | -8.52183723%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 44292608       | -9.70587158%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config1.prototxt
I0109 19:20:41.710947  3041 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0109 19:20:41.823065  3041 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 19:20:41.823627  3041 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 19:20:41.823659  3041 caffe_interface.cpp:493] Using GPUs 0
I0109 19:20:41.823945  3041 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 19:20:42.488720  3041 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt"
type: "SGD"
I0109 19:20:42.488931  3041 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0109 19:20:42.489315  3041 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 19:20:42.489344  3041 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 19:20:42.489349  3041 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 19:20:42.489622  3041 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 19:20:42.489728  3041 layer_factory.hpp:77] Creating layer data
I0109 19:20:42.489959  3041 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:42.490223  3041 net.cpp:94] Creating Layer data
I0109 19:20:42.490264  3041 net.cpp:409] data -> data
I0109 19:20:42.490286  3041 net.cpp:409] data -> label
I0109 19:20:42.491441  3052 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 19:20:42.491498  3052 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 19:20:42.491626  3041 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 19:20:42.491761  3041 data_layer.cpp:83] output data size: 128,3,32,32
I0109 19:20:42.502725  3041 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:42.502799  3041 net.cpp:144] Setting up data
I0109 19:20:42.502822  3041 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 19:20:42.502832  3041 net.cpp:151] Top shape: 128 (128)
I0109 19:20:42.502840  3041 net.cpp:159] Memory required for data: 1573376
I0109 19:20:42.502857  3041 layer_factory.hpp:77] Creating layer conv1
I0109 19:20:42.502882  3041 net.cpp:94] Creating Layer conv1
I0109 19:20:42.502897  3041 net.cpp:435] conv1 <- data
I0109 19:20:42.502926  3041 net.cpp:409] conv1 -> conv1
I0109 19:20:42.504354  3041 net.cpp:144] Setting up conv1
I0109 19:20:42.504374  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.504379  3041 net.cpp:159] Memory required for data: 18350592
I0109 19:20:42.504400  3041 layer_factory.hpp:77] Creating layer bn1
I0109 19:20:42.504431  3041 net.cpp:94] Creating Layer bn1
I0109 19:20:42.504446  3041 net.cpp:435] bn1 <- conv1
I0109 19:20:42.504460  3041 net.cpp:409] bn1 -> scale1
I0109 19:20:42.505530  3041 net.cpp:144] Setting up bn1
I0109 19:20:42.505547  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.505606  3041 net.cpp:159] Memory required for data: 35127808
I0109 19:20:42.505635  3041 layer_factory.hpp:77] Creating layer relu1
I0109 19:20:42.505658  3041 net.cpp:94] Creating Layer relu1
I0109 19:20:42.505666  3041 net.cpp:435] relu1 <- scale1
I0109 19:20:42.505686  3041 net.cpp:409] relu1 -> relu1
I0109 19:20:42.505774  3041 net.cpp:144] Setting up relu1
I0109 19:20:42.505790  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.505795  3041 net.cpp:159] Memory required for data: 51905024
I0109 19:20:42.505832  3041 layer_factory.hpp:77] Creating layer conv2
I0109 19:20:42.505852  3041 net.cpp:94] Creating Layer conv2
I0109 19:20:42.505869  3041 net.cpp:435] conv2 <- relu1
I0109 19:20:42.505882  3041 net.cpp:409] conv2 -> conv2
I0109 19:20:42.506988  3041 net.cpp:144] Setting up conv2
I0109 19:20:42.507015  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.507021  3041 net.cpp:159] Memory required for data: 68682240
I0109 19:20:42.507036  3041 layer_factory.hpp:77] Creating layer bn2
I0109 19:20:42.507055  3041 net.cpp:94] Creating Layer bn2
I0109 19:20:42.507069  3041 net.cpp:435] bn2 <- conv2
I0109 19:20:42.507084  3041 net.cpp:409] bn2 -> scale2
I0109 19:20:42.508141  3041 net.cpp:144] Setting up bn2
I0109 19:20:42.508159  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.508163  3041 net.cpp:159] Memory required for data: 85459456
I0109 19:20:42.508177  3041 layer_factory.hpp:77] Creating layer relu2
I0109 19:20:42.508189  3041 net.cpp:94] Creating Layer relu2
I0109 19:20:42.508204  3041 net.cpp:435] relu2 <- scale2
I0109 19:20:42.508216  3041 net.cpp:409] relu2 -> relu2
I0109 19:20:42.508262  3041 net.cpp:144] Setting up relu2
I0109 19:20:42.508283  3041 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:20:42.508291  3041 net.cpp:159] Memory required for data: 102236672
I0109 19:20:42.508297  3041 layer_factory.hpp:77] Creating layer pool1
I0109 19:20:42.508318  3041 net.cpp:94] Creating Layer pool1
I0109 19:20:42.508333  3041 net.cpp:435] pool1 <- relu2
I0109 19:20:42.508345  3041 net.cpp:409] pool1 -> pool1
I0109 19:20:42.508576  3041 net.cpp:144] Setting up pool1
I0109 19:20:42.508596  3041 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:20:42.508603  3041 net.cpp:159] Memory required for data: 106430976
I0109 19:20:42.508611  3041 layer_factory.hpp:77] Creating layer drop1
I0109 19:20:42.508661  3041 net.cpp:94] Creating Layer drop1
I0109 19:20:42.508734  3041 net.cpp:435] drop1 <- pool1
I0109 19:20:42.508761  3041 net.cpp:409] drop1 -> drop1
I0109 19:20:42.508831  3041 net.cpp:144] Setting up drop1
I0109 19:20:42.508843  3041 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:20:42.508852  3041 net.cpp:159] Memory required for data: 110625280
I0109 19:20:42.508860  3041 layer_factory.hpp:77] Creating layer conv3
I0109 19:20:42.508882  3041 net.cpp:94] Creating Layer conv3
I0109 19:20:42.508898  3041 net.cpp:435] conv3 <- drop1
I0109 19:20:42.509073  3041 net.cpp:409] conv3 -> conv3
I0109 19:20:42.510542  3041 net.cpp:144] Setting up conv3
I0109 19:20:42.510587  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.510592  3041 net.cpp:159] Memory required for data: 119013888
I0109 19:20:42.510601  3041 layer_factory.hpp:77] Creating layer bn3
I0109 19:20:42.510617  3041 net.cpp:94] Creating Layer bn3
I0109 19:20:42.510633  3041 net.cpp:435] bn3 <- conv3
I0109 19:20:42.510715  3041 net.cpp:409] bn3 -> scale3
I0109 19:20:42.511724  3041 net.cpp:144] Setting up bn3
I0109 19:20:42.511746  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.511752  3041 net.cpp:159] Memory required for data: 127402496
I0109 19:20:42.511786  3041 layer_factory.hpp:77] Creating layer relu3
I0109 19:20:42.511812  3041 net.cpp:94] Creating Layer relu3
I0109 19:20:42.511821  3041 net.cpp:435] relu3 <- scale3
I0109 19:20:42.511842  3041 net.cpp:409] relu3 -> relu3
I0109 19:20:42.511898  3041 net.cpp:144] Setting up relu3
I0109 19:20:42.511922  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.511929  3041 net.cpp:159] Memory required for data: 135791104
I0109 19:20:42.511943  3041 layer_factory.hpp:77] Creating layer conv4
I0109 19:20:42.512053  3041 net.cpp:94] Creating Layer conv4
I0109 19:20:42.512059  3041 net.cpp:435] conv4 <- relu3
I0109 19:20:42.512074  3041 net.cpp:409] conv4 -> conv4
I0109 19:20:42.512926  3041 net.cpp:144] Setting up conv4
I0109 19:20:42.512945  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.512953  3041 net.cpp:159] Memory required for data: 144179712
I0109 19:20:42.512964  3041 layer_factory.hpp:77] Creating layer bn4
I0109 19:20:42.512977  3041 net.cpp:94] Creating Layer bn4
I0109 19:20:42.512984  3041 net.cpp:435] bn4 <- conv4
I0109 19:20:42.512995  3041 net.cpp:409] bn4 -> scale4
I0109 19:20:42.514160  3041 net.cpp:144] Setting up bn4
I0109 19:20:42.514179  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.514183  3041 net.cpp:159] Memory required for data: 152568320
I0109 19:20:42.514196  3041 layer_factory.hpp:77] Creating layer relu4
I0109 19:20:42.514209  3041 net.cpp:94] Creating Layer relu4
I0109 19:20:42.514223  3041 net.cpp:435] relu4 <- scale4
I0109 19:20:42.514235  3041 net.cpp:409] relu4 -> relu4
I0109 19:20:42.514348  3041 net.cpp:144] Setting up relu4
I0109 19:20:42.514365  3041 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:20:42.514380  3041 net.cpp:159] Memory required for data: 160956928
I0109 19:20:42.514387  3041 layer_factory.hpp:77] Creating layer pool2
I0109 19:20:42.514401  3041 net.cpp:94] Creating Layer pool2
I0109 19:20:42.514415  3041 net.cpp:435] pool2 <- relu4
I0109 19:20:42.514426  3041 net.cpp:409] pool2 -> pool2
I0109 19:20:42.514602  3041 net.cpp:144] Setting up pool2
I0109 19:20:42.514619  3041 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:20:42.514626  3041 net.cpp:159] Memory required for data: 163054080
I0109 19:20:42.514636  3041 layer_factory.hpp:77] Creating layer drop2
I0109 19:20:42.514649  3041 net.cpp:94] Creating Layer drop2
I0109 19:20:42.514657  3041 net.cpp:435] drop2 <- pool2
I0109 19:20:42.514667  3041 net.cpp:409] drop2 -> drop2
I0109 19:20:42.514719  3041 net.cpp:144] Setting up drop2
I0109 19:20:42.514729  3041 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:20:42.514736  3041 net.cpp:159] Memory required for data: 165151232
I0109 19:20:42.514747  3041 layer_factory.hpp:77] Creating layer fc1
I0109 19:20:42.514762  3041 net.cpp:94] Creating Layer fc1
I0109 19:20:42.514770  3041 net.cpp:435] fc1 <- drop2
I0109 19:20:42.514801  3041 net.cpp:409] fc1 -> fc1
I0109 19:20:42.538506  3041 net.cpp:144] Setting up fc1
I0109 19:20:42.538542  3041 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:20:42.538547  3041 net.cpp:159] Memory required for data: 165413376
I0109 19:20:42.538568  3041 layer_factory.hpp:77] Creating layer bn5
I0109 19:20:42.538591  3041 net.cpp:94] Creating Layer bn5
I0109 19:20:42.538607  3041 net.cpp:435] bn5 <- fc1
I0109 19:20:42.538624  3041 net.cpp:409] bn5 -> scale5
I0109 19:20:42.539247  3041 net.cpp:144] Setting up bn5
I0109 19:20:42.539265  3041 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:20:42.539270  3041 net.cpp:159] Memory required for data: 165675520
I0109 19:20:42.539294  3041 layer_factory.hpp:77] Creating layer relu5
I0109 19:20:42.539315  3041 net.cpp:94] Creating Layer relu5
I0109 19:20:42.539324  3041 net.cpp:435] relu5 <- scale5
I0109 19:20:42.539337  3041 net.cpp:409] relu5 -> relu5
I0109 19:20:42.539377  3041 net.cpp:144] Setting up relu5
I0109 19:20:42.539393  3041 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:20:42.539397  3041 net.cpp:159] Memory required for data: 165937664
I0109 19:20:42.539402  3041 layer_factory.hpp:77] Creating layer drop3
I0109 19:20:42.539413  3041 net.cpp:94] Creating Layer drop3
I0109 19:20:42.539428  3041 net.cpp:435] drop3 <- relu5
I0109 19:20:42.539440  3041 net.cpp:409] drop3 -> drop3
I0109 19:20:42.539496  3041 net.cpp:144] Setting up drop3
I0109 19:20:42.539510  3041 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:20:42.539515  3041 net.cpp:159] Memory required for data: 166199808
I0109 19:20:42.539520  3041 layer_factory.hpp:77] Creating layer fc2
I0109 19:20:42.539533  3041 net.cpp:94] Creating Layer fc2
I0109 19:20:42.539546  3041 net.cpp:435] fc2 <- drop3
I0109 19:20:42.539561  3041 net.cpp:409] fc2 -> fc2
I0109 19:20:42.539757  3041 net.cpp:144] Setting up fc2
I0109 19:20:42.539772  3041 net.cpp:151] Top shape: 128 10 (1280)
I0109 19:20:42.539777  3041 net.cpp:159] Memory required for data: 166204928
I0109 19:20:42.539785  3041 layer_factory.hpp:77] Creating layer loss
I0109 19:20:42.539798  3041 net.cpp:94] Creating Layer loss
I0109 19:20:42.539811  3041 net.cpp:435] loss <- fc2
I0109 19:20:42.539820  3041 net.cpp:435] loss <- label
I0109 19:20:42.539834  3041 net.cpp:409] loss -> loss
I0109 19:20:42.539850  3041 layer_factory.hpp:77] Creating layer loss
I0109 19:20:42.540706  3041 net.cpp:144] Setting up loss
I0109 19:20:42.540729  3041 net.cpp:151] Top shape: (1)
I0109 19:20:42.540733  3041 net.cpp:154]     with loss weight 1
I0109 19:20:42.540762  3041 net.cpp:159] Memory required for data: 166204932
I0109 19:20:42.540771  3041 net.cpp:220] loss needs backward computation.
I0109 19:20:42.540807  3041 net.cpp:220] fc2 needs backward computation.
I0109 19:20:42.540817  3041 net.cpp:220] drop3 needs backward computation.
I0109 19:20:42.540822  3041 net.cpp:220] relu5 needs backward computation.
I0109 19:20:42.540832  3041 net.cpp:220] bn5 needs backward computation.
I0109 19:20:42.540839  3041 net.cpp:220] fc1 needs backward computation.
I0109 19:20:42.540849  3041 net.cpp:220] drop2 needs backward computation.
I0109 19:20:42.540856  3041 net.cpp:220] pool2 needs backward computation.
I0109 19:20:42.540863  3041 net.cpp:220] relu4 needs backward computation.
I0109 19:20:42.540879  3041 net.cpp:220] bn4 needs backward computation.
I0109 19:20:42.540887  3041 net.cpp:220] conv4 needs backward computation.
I0109 19:20:42.540895  3041 net.cpp:220] relu3 needs backward computation.
I0109 19:20:42.540902  3041 net.cpp:220] bn3 needs backward computation.
I0109 19:20:42.540912  3041 net.cpp:220] conv3 needs backward computation.
I0109 19:20:42.540922  3041 net.cpp:220] drop1 needs backward computation.
I0109 19:20:42.540930  3041 net.cpp:220] pool1 needs backward computation.
I0109 19:20:42.540940  3041 net.cpp:220] relu2 needs backward computation.
I0109 19:20:42.540947  3041 net.cpp:220] bn2 needs backward computation.
I0109 19:20:42.540957  3041 net.cpp:220] conv2 needs backward computation.
I0109 19:20:42.540964  3041 net.cpp:220] relu1 needs backward computation.
I0109 19:20:42.540995  3041 net.cpp:220] bn1 needs backward computation.
I0109 19:20:42.541004  3041 net.cpp:220] conv1 needs backward computation.
I0109 19:20:42.541013  3041 net.cpp:222] data does not need backward computation.
I0109 19:20:42.541029  3041 net.cpp:264] This network produces output loss
I0109 19:20:42.541065  3041 net.cpp:284] Network initialization done.
I0109 19:20:42.541437  3041 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0109 19:20:42.541496  3041 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:20:42.541751  3041 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:20:42.541913  3041 layer_factory.hpp:77] Creating layer data
I0109 19:20:42.541980  3041 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:42.543781  3041 net.cpp:94] Creating Layer data
I0109 19:20:42.543802  3041 net.cpp:409] data -> data
I0109 19:20:42.543820  3041 net.cpp:409] data -> label
I0109 19:20:42.543853  3058 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:20:42.543890  3058 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:20:42.544603  3041 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:20:42.544739  3041 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:20:42.552688  3041 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:20:42.552758  3041 net.cpp:144] Setting up data
I0109 19:20:42.552780  3041 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:20:42.552790  3041 net.cpp:151] Top shape: 50 (50)
I0109 19:20:42.552796  3041 net.cpp:159] Memory required for data: 614600
I0109 19:20:42.552804  3041 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:20:42.552827  3041 net.cpp:94] Creating Layer label_data_1_split
I0109 19:20:42.552836  3041 net.cpp:435] label_data_1_split <- label
I0109 19:20:42.552848  3041 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:20:42.552872  3041 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:20:42.552884  3041 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:20:42.553001  3041 net.cpp:144] Setting up label_data_1_split
I0109 19:20:42.553016  3041 net.cpp:151] Top shape: 50 (50)
I0109 19:20:42.553021  3041 net.cpp:151] Top shape: 50 (50)
I0109 19:20:42.553026  3041 net.cpp:151] Top shape: 50 (50)
I0109 19:20:42.553031  3041 net.cpp:159] Memory required for data: 615200
I0109 19:20:42.553038  3041 layer_factory.hpp:77] Creating layer conv1
I0109 19:20:42.553066  3041 net.cpp:94] Creating Layer conv1
I0109 19:20:42.553074  3041 net.cpp:435] conv1 <- data
I0109 19:20:42.553086  3041 net.cpp:409] conv1 -> conv1
I0109 19:20:42.553444  3041 net.cpp:144] Setting up conv1
I0109 19:20:42.553463  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.553467  3041 net.cpp:159] Memory required for data: 7168800
I0109 19:20:42.553479  3041 layer_factory.hpp:77] Creating layer bn1
I0109 19:20:42.553494  3041 net.cpp:94] Creating Layer bn1
I0109 19:20:42.553510  3041 net.cpp:435] bn1 <- conv1
I0109 19:20:42.553522  3041 net.cpp:409] bn1 -> scale1
I0109 19:20:42.554551  3041 net.cpp:144] Setting up bn1
I0109 19:20:42.554569  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.554574  3041 net.cpp:159] Memory required for data: 13722400
I0109 19:20:42.554590  3041 layer_factory.hpp:77] Creating layer relu1
I0109 19:20:42.554613  3041 net.cpp:94] Creating Layer relu1
I0109 19:20:42.554621  3041 net.cpp:435] relu1 <- scale1
I0109 19:20:42.554632  3041 net.cpp:409] relu1 -> relu1
I0109 19:20:42.554832  3041 net.cpp:144] Setting up relu1
I0109 19:20:42.554849  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.554854  3041 net.cpp:159] Memory required for data: 20276000
I0109 19:20:42.554859  3041 layer_factory.hpp:77] Creating layer conv2
I0109 19:20:42.554875  3041 net.cpp:94] Creating Layer conv2
I0109 19:20:42.554890  3041 net.cpp:435] conv2 <- relu1
I0109 19:20:42.554903  3041 net.cpp:409] conv2 -> conv2
I0109 19:20:42.555706  3041 net.cpp:144] Setting up conv2
I0109 19:20:42.555729  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.555737  3041 net.cpp:159] Memory required for data: 26829600
I0109 19:20:42.555759  3041 layer_factory.hpp:77] Creating layer bn2
I0109 19:20:42.555780  3041 net.cpp:94] Creating Layer bn2
I0109 19:20:42.555789  3041 net.cpp:435] bn2 <- conv2
I0109 19:20:42.555801  3041 net.cpp:409] bn2 -> scale2
I0109 19:20:42.557427  3041 net.cpp:144] Setting up bn2
I0109 19:20:42.557451  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.557459  3041 net.cpp:159] Memory required for data: 33383200
I0109 19:20:42.557476  3041 layer_factory.hpp:77] Creating layer relu2
I0109 19:20:42.557494  3041 net.cpp:94] Creating Layer relu2
I0109 19:20:42.557502  3041 net.cpp:435] relu2 <- scale2
I0109 19:20:42.557513  3041 net.cpp:409] relu2 -> relu2
I0109 19:20:42.557642  3041 net.cpp:144] Setting up relu2
I0109 19:20:42.557665  3041 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:20:42.557673  3041 net.cpp:159] Memory required for data: 39936800
I0109 19:20:42.557678  3041 layer_factory.hpp:77] Creating layer pool1
I0109 19:20:42.557693  3041 net.cpp:94] Creating Layer pool1
I0109 19:20:42.557729  3041 net.cpp:435] pool1 <- relu2
I0109 19:20:42.557756  3041 net.cpp:409] pool1 -> pool1
I0109 19:20:42.557869  3041 net.cpp:144] Setting up pool1
I0109 19:20:42.557907  3041 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:20:42.557929  3041 net.cpp:159] Memory required for data: 41575200
I0109 19:20:42.557937  3041 layer_factory.hpp:77] Creating layer drop1
I0109 19:20:42.557950  3041 net.cpp:94] Creating Layer drop1
I0109 19:20:42.557984  3041 net.cpp:435] drop1 <- pool1
I0109 19:20:42.557996  3041 net.cpp:409] drop1 -> drop1
I0109 19:20:42.558069  3041 net.cpp:144] Setting up drop1
I0109 19:20:42.558104  3041 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:20:42.558118  3041 net.cpp:159] Memory required for data: 43213600
I0109 19:20:42.558125  3041 layer_factory.hpp:77] Creating layer conv3
I0109 19:20:42.558145  3041 net.cpp:94] Creating Layer conv3
I0109 19:20:42.558168  3041 net.cpp:435] conv3 <- drop1
I0109 19:20:42.558181  3041 net.cpp:409] conv3 -> conv3
I0109 19:20:42.559159  3041 net.cpp:144] Setting up conv3
I0109 19:20:42.559181  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.559190  3041 net.cpp:159] Memory required for data: 46490400
I0109 19:20:42.559201  3041 layer_factory.hpp:77] Creating layer bn3
I0109 19:20:42.559223  3041 net.cpp:94] Creating Layer bn3
I0109 19:20:42.559232  3041 net.cpp:435] bn3 <- conv3
I0109 19:20:42.559245  3041 net.cpp:409] bn3 -> scale3
I0109 19:20:42.560797  3041 net.cpp:144] Setting up bn3
I0109 19:20:42.560820  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.560830  3041 net.cpp:159] Memory required for data: 49767200
I0109 19:20:42.560853  3041 layer_factory.hpp:77] Creating layer relu3
I0109 19:20:42.560870  3041 net.cpp:94] Creating Layer relu3
I0109 19:20:42.560878  3041 net.cpp:435] relu3 <- scale3
I0109 19:20:42.560889  3041 net.cpp:409] relu3 -> relu3
I0109 19:20:42.560940  3041 net.cpp:144] Setting up relu3
I0109 19:20:42.560962  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.560969  3041 net.cpp:159] Memory required for data: 53044000
I0109 19:20:42.560976  3041 layer_factory.hpp:77] Creating layer conv4
I0109 19:20:42.561000  3041 net.cpp:94] Creating Layer conv4
I0109 19:20:42.561009  3041 net.cpp:435] conv4 <- relu3
I0109 19:20:42.561022  3041 net.cpp:409] conv4 -> conv4
I0109 19:20:42.561998  3041 net.cpp:144] Setting up conv4
I0109 19:20:42.562024  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.562032  3041 net.cpp:159] Memory required for data: 56320800
I0109 19:20:42.562043  3041 layer_factory.hpp:77] Creating layer bn4
I0109 19:20:42.562067  3041 net.cpp:94] Creating Layer bn4
I0109 19:20:42.562075  3041 net.cpp:435] bn4 <- conv4
I0109 19:20:42.562088  3041 net.cpp:409] bn4 -> scale4
I0109 19:20:42.563648  3041 net.cpp:144] Setting up bn4
I0109 19:20:42.563673  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.563679  3041 net.cpp:159] Memory required for data: 59597600
I0109 19:20:42.563695  3041 layer_factory.hpp:77] Creating layer relu4
I0109 19:20:42.563715  3041 net.cpp:94] Creating Layer relu4
I0109 19:20:42.563724  3041 net.cpp:435] relu4 <- scale4
I0109 19:20:42.563735  3041 net.cpp:409] relu4 -> relu4
I0109 19:20:42.563778  3041 net.cpp:144] Setting up relu4
I0109 19:20:42.563822  3041 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:20:42.563830  3041 net.cpp:159] Memory required for data: 62874400
I0109 19:20:42.563838  3041 layer_factory.hpp:77] Creating layer pool2
I0109 19:20:42.563854  3041 net.cpp:94] Creating Layer pool2
I0109 19:20:42.563861  3041 net.cpp:435] pool2 <- relu4
I0109 19:20:42.563872  3041 net.cpp:409] pool2 -> pool2
I0109 19:20:42.563944  3041 net.cpp:144] Setting up pool2
I0109 19:20:42.563956  3041 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:20:42.563963  3041 net.cpp:159] Memory required for data: 63693600
I0109 19:20:42.563971  3041 layer_factory.hpp:77] Creating layer drop2
I0109 19:20:42.563982  3041 net.cpp:94] Creating Layer drop2
I0109 19:20:42.563997  3041 net.cpp:435] drop2 <- pool2
I0109 19:20:42.564007  3041 net.cpp:409] drop2 -> drop2
I0109 19:20:42.564070  3041 net.cpp:144] Setting up drop2
I0109 19:20:42.564100  3041 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:20:42.564106  3041 net.cpp:159] Memory required for data: 64512800
I0109 19:20:42.564112  3041 layer_factory.hpp:77] Creating layer fc1
I0109 19:20:42.564127  3041 net.cpp:94] Creating Layer fc1
I0109 19:20:42.564142  3041 net.cpp:435] fc1 <- drop2
I0109 19:20:42.564155  3041 net.cpp:409] fc1 -> fc1
I0109 19:20:42.587108  3041 net.cpp:144] Setting up fc1
I0109 19:20:42.587146  3041 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:42.587150  3041 net.cpp:159] Memory required for data: 64615200
I0109 19:20:42.587164  3041 layer_factory.hpp:77] Creating layer bn5
I0109 19:20:42.587185  3041 net.cpp:94] Creating Layer bn5
I0109 19:20:42.587203  3041 net.cpp:435] bn5 <- fc1
I0109 19:20:42.587219  3041 net.cpp:409] bn5 -> scale5
I0109 19:20:42.587915  3041 net.cpp:144] Setting up bn5
I0109 19:20:42.587934  3041 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:42.587939  3041 net.cpp:159] Memory required for data: 64717600
I0109 19:20:42.587962  3041 layer_factory.hpp:77] Creating layer relu5
I0109 19:20:42.587982  3041 net.cpp:94] Creating Layer relu5
I0109 19:20:42.587990  3041 net.cpp:435] relu5 <- scale5
I0109 19:20:42.588007  3041 net.cpp:409] relu5 -> relu5
I0109 19:20:42.588050  3041 net.cpp:144] Setting up relu5
I0109 19:20:42.588065  3041 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:42.588069  3041 net.cpp:159] Memory required for data: 64820000
I0109 19:20:42.588074  3041 layer_factory.hpp:77] Creating layer drop3
I0109 19:20:42.588086  3041 net.cpp:94] Creating Layer drop3
I0109 19:20:42.588100  3041 net.cpp:435] drop3 <- relu5
I0109 19:20:42.588111  3041 net.cpp:409] drop3 -> drop3
I0109 19:20:42.588174  3041 net.cpp:144] Setting up drop3
I0109 19:20:42.588191  3041 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:20:42.588194  3041 net.cpp:159] Memory required for data: 64922400
I0109 19:20:42.588198  3041 layer_factory.hpp:77] Creating layer fc2
I0109 19:20:42.588212  3041 net.cpp:94] Creating Layer fc2
I0109 19:20:42.588219  3041 net.cpp:435] fc2 <- drop3
I0109 19:20:42.588238  3041 net.cpp:409] fc2 -> fc2
I0109 19:20:42.588455  3041 net.cpp:144] Setting up fc2
I0109 19:20:42.588471  3041 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:42.588475  3041 net.cpp:159] Memory required for data: 64924400
I0109 19:20:42.588484  3041 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:20:42.588496  3041 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:20:42.588511  3041 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:20:42.588522  3041 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:20:42.588542  3041 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:20:42.588553  3041 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:20:42.588640  3041 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:20:42.588654  3041 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:42.588660  3041 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:42.588665  3041 net.cpp:151] Top shape: 50 10 (500)
I0109 19:20:42.588668  3041 net.cpp:159] Memory required for data: 64930400
I0109 19:20:42.588673  3041 layer_factory.hpp:77] Creating layer loss
I0109 19:20:42.588690  3041 net.cpp:94] Creating Layer loss
I0109 19:20:42.588699  3041 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:20:42.588709  3041 net.cpp:435] loss <- label_data_1_split_0
I0109 19:20:42.588721  3041 net.cpp:409] loss -> loss
I0109 19:20:42.588737  3041 layer_factory.hpp:77] Creating layer loss
I0109 19:20:42.588860  3041 net.cpp:144] Setting up loss
I0109 19:20:42.588876  3041 net.cpp:151] Top shape: (1)
I0109 19:20:42.588879  3041 net.cpp:154]     with loss weight 1
I0109 19:20:42.588905  3041 net.cpp:159] Memory required for data: 64930404
I0109 19:20:42.588912  3041 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:20:42.588929  3041 net.cpp:94] Creating Layer accuracy-top1
I0109 19:20:42.588937  3041 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:20:42.588945  3041 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:20:42.588984  3041 net.cpp:409] accuracy-top1 -> top-1
I0109 19:20:42.588999  3041 net.cpp:144] Setting up accuracy-top1
I0109 19:20:42.589015  3041 net.cpp:151] Top shape: (1)
I0109 19:20:42.589022  3041 net.cpp:159] Memory required for data: 64930408
I0109 19:20:42.589028  3041 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:20:42.589049  3041 net.cpp:94] Creating Layer accuracy-top5
I0109 19:20:42.589056  3041 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:20:42.589063  3041 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:20:42.589082  3041 net.cpp:409] accuracy-top5 -> top-5
I0109 19:20:42.589102  3041 net.cpp:144] Setting up accuracy-top5
I0109 19:20:42.589112  3041 net.cpp:151] Top shape: (1)
I0109 19:20:42.589118  3041 net.cpp:159] Memory required for data: 64930412
I0109 19:20:42.589131  3041 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:20:42.589141  3041 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:20:42.589156  3041 net.cpp:220] loss needs backward computation.
I0109 19:20:42.589164  3041 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:20:42.589170  3041 net.cpp:220] fc2 needs backward computation.
I0109 19:20:42.589179  3041 net.cpp:220] drop3 needs backward computation.
I0109 19:20:42.589190  3041 net.cpp:220] relu5 needs backward computation.
I0109 19:20:42.589197  3041 net.cpp:220] bn5 needs backward computation.
I0109 19:20:42.589210  3041 net.cpp:220] fc1 needs backward computation.
I0109 19:20:42.589220  3041 net.cpp:220] drop2 needs backward computation.
I0109 19:20:42.589226  3041 net.cpp:220] pool2 needs backward computation.
I0109 19:20:42.589232  3041 net.cpp:220] relu4 needs backward computation.
I0109 19:20:42.589246  3041 net.cpp:220] bn4 needs backward computation.
I0109 19:20:42.589254  3041 net.cpp:220] conv4 needs backward computation.
I0109 19:20:42.589262  3041 net.cpp:220] relu3 needs backward computation.
I0109 19:20:42.589274  3041 net.cpp:220] bn3 needs backward computation.
I0109 19:20:42.589282  3041 net.cpp:220] conv3 needs backward computation.
I0109 19:20:42.589289  3041 net.cpp:220] drop1 needs backward computation.
I0109 19:20:42.589296  3041 net.cpp:220] pool1 needs backward computation.
I0109 19:20:42.589311  3041 net.cpp:220] relu2 needs backward computation.
I0109 19:20:42.589319  3041 net.cpp:220] bn2 needs backward computation.
I0109 19:20:42.589327  3041 net.cpp:220] conv2 needs backward computation.
I0109 19:20:42.589341  3041 net.cpp:220] relu1 needs backward computation.
I0109 19:20:42.589349  3041 net.cpp:220] bn1 needs backward computation.
I0109 19:20:42.589354  3041 net.cpp:220] conv1 needs backward computation.
I0109 19:20:42.589365  3041 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:20:42.589380  3041 net.cpp:222] data does not need backward computation.
I0109 19:20:42.589386  3041 net.cpp:264] This network produces output loss
I0109 19:20:42.589393  3041 net.cpp:264] This network produces output top-1
I0109 19:20:42.589407  3041 net.cpp:264] This network produces output top-5
I0109 19:20:42.589445  3041 net.cpp:284] Network initialization done.
I0109 19:20:42.589617  3041 solver.cpp:63] Solver scaffolding done.
I0109 19:20:42.590885  3041 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/sparse.caffemodel
I0109 19:20:42.656540  3041 caffe_interface.cpp:527] Starting Optimization
I0109 19:20:42.656590  3041 solver.cpp:335] Solving 
I0109 19:20:42.656595  3041 solver.cpp:336] Learning Rate Policy: poly
I0109 19:20:42.657980  3041 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 19:20:43.479461  3041 solver.cpp:517]     Test net output #0: loss = 0.521759 (* 1 = 0.521759 loss)
I0109 19:20:43.479509  3041 solver.cpp:517]     Test net output #1: top-1 = 0.837333
I0109 19:20:43.479517  3041 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0109 19:20:43.527117  3041 solver.cpp:266] Iteration 0 (0 iter/s, 0.870467s/100 iter), loss = 0.16252
I0109 19:20:43.527166  3041 solver.cpp:285]     Train net output #0: loss = 0.16252 (* 1 = 0.16252 loss)
I0109 19:20:43.527228  3041 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 19:20:46.868381  3041 solver.cpp:266] Iteration 100 (29.929 iter/s, 3.34125s/100 iter), loss = 0.161833
I0109 19:20:46.868443  3041 solver.cpp:285]     Train net output #0: loss = 0.161833 (* 1 = 0.161833 loss)
I0109 19:20:46.868458  3041 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 19:20:50.212280  3041 solver.cpp:266] Iteration 200 (29.9055 iter/s, 3.34387s/100 iter), loss = 0.200227
I0109 19:20:50.212350  3041 solver.cpp:285]     Train net output #0: loss = 0.200227 (* 1 = 0.200227 loss)
I0109 19:20:50.212366  3041 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 19:20:53.560281  3041 solver.cpp:266] Iteration 300 (29.8691 iter/s, 3.34794s/100 iter), loss = 0.209692
I0109 19:20:53.560346  3041 solver.cpp:285]     Train net output #0: loss = 0.209692 (* 1 = 0.209692 loss)
I0109 19:20:53.560359  3041 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 19:20:56.908083  3041 solver.cpp:266] Iteration 400 (29.8706 iter/s, 3.34777s/100 iter), loss = 0.221449
I0109 19:20:56.908145  3041 solver.cpp:285]     Train net output #0: loss = 0.221449 (* 1 = 0.221449 loss)
I0109 19:20:56.908157  3041 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 19:21:00.256981  3041 solver.cpp:266] Iteration 500 (29.8608 iter/s, 3.34887s/100 iter), loss = 0.164984
I0109 19:21:00.257050  3041 solver.cpp:285]     Train net output #0: loss = 0.164984 (* 1 = 0.164984 loss)
I0109 19:21:00.257064  3041 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 19:21:03.602274  3041 solver.cpp:266] Iteration 600 (29.8933 iter/s, 3.34523s/100 iter), loss = 0.299963
I0109 19:21:03.602342  3041 solver.cpp:285]     Train net output #0: loss = 0.299963 (* 1 = 0.299963 loss)
I0109 19:21:03.602357  3041 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 19:21:06.948796  3041 solver.cpp:266] Iteration 700 (29.8821 iter/s, 3.34648s/100 iter), loss = 0.254127
I0109 19:21:06.948881  3041 solver.cpp:285]     Train net output #0: loss = 0.254127 (* 1 = 0.254127 loss)
I0109 19:21:06.948899  3041 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 19:21:10.298565  3041 solver.cpp:266] Iteration 800 (29.8533 iter/s, 3.34972s/100 iter), loss = 0.166238
I0109 19:21:10.298636  3041 solver.cpp:285]     Train net output #0: loss = 0.166238 (* 1 = 0.166238 loss)
I0109 19:21:10.298650  3041 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 19:21:13.645455  3041 solver.cpp:266] Iteration 900 (29.879 iter/s, 3.34683s/100 iter), loss = 0.20562
I0109 19:21:13.645584  3041 solver.cpp:285]     Train net output #0: loss = 0.20562 (* 1 = 0.20562 loss)
I0109 19:21:13.645615  3041 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 19:21:16.958773  3041 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 19:21:17.788077  3041 solver.cpp:517]     Test net output #0: loss = 0.816291 (* 1 = 0.816291 loss)
I0109 19:21:17.788120  3041 solver.cpp:517]     Test net output #1: top-1 = 0.798444
I0109 19:21:17.788130  3041 solver.cpp:517]     Test net output #2: top-5 = 0.984667
I0109 19:21:17.819586  3041 solver.cpp:266] Iteration 1000 (23.9576 iter/s, 4.17404s/100 iter), loss = 0.192455
I0109 19:21:17.819661  3041 solver.cpp:285]     Train net output #0: loss = 0.192455 (* 1 = 0.192455 loss)
I0109 19:21:17.819677  3041 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 19:21:21.166826  3041 solver.cpp:266] Iteration 1100 (29.8758 iter/s, 3.34719s/100 iter), loss = 0.226113
I0109 19:21:21.166911  3041 solver.cpp:285]     Train net output #0: loss = 0.226113 (* 1 = 0.226113 loss)
I0109 19:21:21.166929  3041 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 19:21:24.514230  3041 solver.cpp:266] Iteration 1200 (29.8743 iter/s, 3.34735s/100 iter), loss = 0.309008
I0109 19:21:24.514297  3041 solver.cpp:285]     Train net output #0: loss = 0.309008 (* 1 = 0.309008 loss)
I0109 19:21:24.514309  3041 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 19:21:27.858783  3041 solver.cpp:266] Iteration 1300 (29.8999 iter/s, 3.34449s/100 iter), loss = 0.216614
I0109 19:21:27.858850  3041 solver.cpp:285]     Train net output #0: loss = 0.216614 (* 1 = 0.216614 loss)
I0109 19:21:27.858863  3041 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 19:21:31.206467  3041 solver.cpp:266] Iteration 1400 (29.8717 iter/s, 3.34765s/100 iter), loss = 0.273857
I0109 19:21:31.206542  3041 solver.cpp:285]     Train net output #0: loss = 0.273857 (* 1 = 0.273857 loss)
I0109 19:21:31.206557  3041 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 19:21:34.526757  3041 solver.cpp:266] Iteration 1500 (30.1185 iter/s, 3.32022s/100 iter), loss = 0.221327
I0109 19:21:34.526835  3041 solver.cpp:285]     Train net output #0: loss = 0.221327 (* 1 = 0.221327 loss)
I0109 19:21:34.526850  3041 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 19:21:37.866775  3041 solver.cpp:266] Iteration 1600 (29.9404 iter/s, 3.33997s/100 iter), loss = 0.29632
I0109 19:21:37.866842  3041 solver.cpp:285]     Train net output #0: loss = 0.29632 (* 1 = 0.29632 loss)
I0109 19:21:37.866854  3041 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 19:21:41.193383  3041 solver.cpp:266] Iteration 1700 (30.061 iter/s, 3.32657s/100 iter), loss = 0.19822
I0109 19:21:41.193447  3041 solver.cpp:285]     Train net output #0: loss = 0.19822 (* 1 = 0.19822 loss)
I0109 19:21:41.193460  3041 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 19:21:44.519186  3041 solver.cpp:266] Iteration 1800 (30.0682 iter/s, 3.32577s/100 iter), loss = 0.271735
I0109 19:21:44.519397  3041 solver.cpp:285]     Train net output #0: loss = 0.271735 (* 1 = 0.271735 loss)
I0109 19:21:44.519412  3041 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 19:21:47.835640  3041 solver.cpp:266] Iteration 1900 (30.1545 iter/s, 3.31625s/100 iter), loss = 0.281445
I0109 19:21:47.835707  3041 solver.cpp:285]     Train net output #0: loss = 0.281445 (* 1 = 0.281445 loss)
I0109 19:21:47.835722  3041 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 19:21:51.120743  3041 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 19:21:51.946877  3041 solver.cpp:517]     Test net output #0: loss = 0.841005 (* 1 = 0.841005 loss)
I0109 19:21:51.946920  3041 solver.cpp:517]     Test net output #1: top-1 = 0.789333
I0109 19:21:51.946928  3041 solver.cpp:517]     Test net output #2: top-5 = 0.982889
I0109 19:21:51.978209  3041 solver.cpp:266] Iteration 2000 (24.1398 iter/s, 4.14254s/100 iter), loss = 0.114744
I0109 19:21:51.978288  3041 solver.cpp:285]     Train net output #0: loss = 0.114744 (* 1 = 0.114744 loss)
I0109 19:21:51.978305  3041 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 19:21:55.304286  3041 solver.cpp:266] Iteration 2100 (30.0659 iter/s, 3.32603s/100 iter), loss = 0.164877
I0109 19:21:55.304352  3041 solver.cpp:285]     Train net output #0: loss = 0.164877 (* 1 = 0.164877 loss)
I0109 19:21:55.304368  3041 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 19:21:58.621284  3041 solver.cpp:266] Iteration 2200 (30.1483 iter/s, 3.31694s/100 iter), loss = 0.230015
I0109 19:21:58.621349  3041 solver.cpp:285]     Train net output #0: loss = 0.230015 (* 1 = 0.230015 loss)
I0109 19:21:58.621361  3041 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 19:22:01.939721  3041 solver.cpp:266] Iteration 2300 (30.135 iter/s, 3.3184s/100 iter), loss = 0.268521
I0109 19:22:01.939795  3041 solver.cpp:285]     Train net output #0: loss = 0.268521 (* 1 = 0.268521 loss)
I0109 19:22:01.939815  3041 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 19:22:05.256181  3041 solver.cpp:266] Iteration 2400 (30.153 iter/s, 3.31642s/100 iter), loss = 0.144117
I0109 19:22:05.256253  3041 solver.cpp:285]     Train net output #0: loss = 0.144117 (* 1 = 0.144117 loss)
I0109 19:22:05.256273  3041 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 19:22:08.567698  3041 solver.cpp:266] Iteration 2500 (30.1982 iter/s, 3.31145s/100 iter), loss = 0.206797
I0109 19:22:08.567761  3041 solver.cpp:285]     Train net output #0: loss = 0.206797 (* 1 = 0.206797 loss)
I0109 19:22:08.567775  3041 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 19:22:11.887331  3041 solver.cpp:266] Iteration 2600 (30.1241 iter/s, 3.3196s/100 iter), loss = 0.269422
I0109 19:22:11.887396  3041 solver.cpp:285]     Train net output #0: loss = 0.269422 (* 1 = 0.269422 loss)
I0109 19:22:11.887409  3041 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 19:22:15.207145  3041 solver.cpp:266] Iteration 2700 (30.1225 iter/s, 3.31978s/100 iter), loss = 0.216328
I0109 19:22:15.207332  3041 solver.cpp:285]     Train net output #0: loss = 0.216328 (* 1 = 0.216328 loss)
I0109 19:22:15.207350  3041 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 19:22:18.529214  3041 solver.cpp:266] Iteration 2800 (30.1034 iter/s, 3.32188s/100 iter), loss = 0.286535
I0109 19:22:18.529290  3041 solver.cpp:285]     Train net output #0: loss = 0.286535 (* 1 = 0.286535 loss)
I0109 19:22:18.529305  3041 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 19:22:21.854306  3041 solver.cpp:266] Iteration 2900 (30.0748 iter/s, 3.32505s/100 iter), loss = 0.232446
I0109 19:22:21.854375  3041 solver.cpp:285]     Train net output #0: loss = 0.232446 (* 1 = 0.232446 loss)
I0109 19:22:21.854391  3041 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 19:22:25.144991  3041 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 19:22:25.971987  3041 solver.cpp:517]     Test net output #0: loss = 0.610049 (* 1 = 0.610049 loss)
I0109 19:22:25.972028  3041 solver.cpp:517]     Test net output #1: top-1 = 0.813778
I0109 19:22:25.972036  3041 solver.cpp:517]     Test net output #2: top-5 = 0.987
I0109 19:22:26.003407  3041 solver.cpp:266] Iteration 3000 (24.1018 iter/s, 4.14907s/100 iter), loss = 0.206361
I0109 19:22:26.003458  3041 solver.cpp:285]     Train net output #0: loss = 0.206361 (* 1 = 0.206361 loss)
I0109 19:22:26.003474  3041 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 19:22:29.326475  3041 solver.cpp:266] Iteration 3100 (30.0928 iter/s, 3.32305s/100 iter), loss = 0.202151
I0109 19:22:29.326536  3041 solver.cpp:285]     Train net output #0: loss = 0.202151 (* 1 = 0.202151 loss)
I0109 19:22:29.326548  3041 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 19:22:32.650341  3041 solver.cpp:266] Iteration 3200 (30.086 iter/s, 3.32381s/100 iter), loss = 0.209474
I0109 19:22:32.650410  3041 solver.cpp:285]     Train net output #0: loss = 0.209474 (* 1 = 0.209474 loss)
I0109 19:22:32.650424  3041 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 19:22:35.991099  3041 solver.cpp:266] Iteration 3300 (29.9337 iter/s, 3.34072s/100 iter), loss = 0.266503
I0109 19:22:35.991163  3041 solver.cpp:285]     Train net output #0: loss = 0.266503 (* 1 = 0.266503 loss)
I0109 19:22:35.991176  3041 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 19:22:39.325609  3041 solver.cpp:266] Iteration 3400 (29.9898 iter/s, 3.33446s/100 iter), loss = 0.143945
I0109 19:22:39.325671  3041 solver.cpp:285]     Train net output #0: loss = 0.143945 (* 1 = 0.143945 loss)
I0109 19:22:39.325685  3041 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 19:22:42.654389  3041 solver.cpp:266] Iteration 3500 (30.0416 iter/s, 3.32872s/100 iter), loss = 0.20559
I0109 19:22:42.654456  3041 solver.cpp:285]     Train net output #0: loss = 0.20559 (* 1 = 0.20559 loss)
I0109 19:22:42.654469  3041 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 19:22:45.989114  3041 solver.cpp:266] Iteration 3600 (29.9878 iter/s, 3.33469s/100 iter), loss = 0.192445
I0109 19:22:45.989274  3041 solver.cpp:285]     Train net output #0: loss = 0.192445 (* 1 = 0.192445 loss)
I0109 19:22:45.989289  3041 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 19:22:49.333953  3041 solver.cpp:266] Iteration 3700 (29.898 iter/s, 3.34471s/100 iter), loss = 0.192407
I0109 19:22:49.334028  3041 solver.cpp:285]     Train net output #0: loss = 0.192407 (* 1 = 0.192407 loss)
I0109 19:22:49.334043  3041 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 19:22:52.665429  3041 solver.cpp:266] Iteration 3800 (30.0173 iter/s, 3.33141s/100 iter), loss = 0.351722
I0109 19:22:52.665494  3041 solver.cpp:285]     Train net output #0: loss = 0.351722 (* 1 = 0.351722 loss)
I0109 19:22:52.665508  3041 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 19:22:56.007846  3041 solver.cpp:266] Iteration 3900 (29.9188 iter/s, 3.34238s/100 iter), loss = 0.216691
I0109 19:22:56.007915  3041 solver.cpp:285]     Train net output #0: loss = 0.216691 (* 1 = 0.216691 loss)
I0109 19:22:56.007928  3041 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 19:22:59.314604  3041 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 19:23:00.138684  3041 solver.cpp:517]     Test net output #0: loss = 0.554699 (* 1 = 0.554699 loss)
I0109 19:23:00.138723  3041 solver.cpp:517]     Test net output #1: top-1 = 0.831222
I0109 19:23:00.138732  3041 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0109 19:23:00.170127  3041 solver.cpp:266] Iteration 4000 (24.0255 iter/s, 4.16225s/100 iter), loss = 0.151111
I0109 19:23:00.170213  3041 solver.cpp:285]     Train net output #0: loss = 0.151111 (* 1 = 0.151111 loss)
I0109 19:23:00.170230  3041 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 19:23:03.511896  3041 solver.cpp:266] Iteration 4100 (29.9248 iter/s, 3.34171s/100 iter), loss = 0.195284
I0109 19:23:03.511982  3041 solver.cpp:285]     Train net output #0: loss = 0.195284 (* 1 = 0.195284 loss)
I0109 19:23:03.512006  3041 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 19:23:06.851655  3041 solver.cpp:266] Iteration 4200 (29.9429 iter/s, 3.33969s/100 iter), loss = 0.246756
I0109 19:23:06.851717  3041 solver.cpp:285]     Train net output #0: loss = 0.246756 (* 1 = 0.246756 loss)
I0109 19:23:06.851730  3041 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 19:23:10.203310  3041 solver.cpp:266] Iteration 4300 (29.8363 iter/s, 3.35162s/100 iter), loss = 0.227102
I0109 19:23:10.203371  3041 solver.cpp:285]     Train net output #0: loss = 0.227102 (* 1 = 0.227102 loss)
I0109 19:23:10.203383  3041 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 19:23:13.547436  3041 solver.cpp:266] Iteration 4400 (29.9037 iter/s, 3.34407s/100 iter), loss = 0.199429
I0109 19:23:13.547499  3041 solver.cpp:285]     Train net output #0: loss = 0.199429 (* 1 = 0.199429 loss)
I0109 19:23:13.547511  3041 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 19:23:16.901185  3041 solver.cpp:266] Iteration 4500 (29.8176 iter/s, 3.35372s/100 iter), loss = 0.256459
I0109 19:23:16.901373  3041 solver.cpp:285]     Train net output #0: loss = 0.256459 (* 1 = 0.256459 loss)
I0109 19:23:16.901391  3041 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 19:23:20.246824  3041 solver.cpp:266] Iteration 4600 (29.891 iter/s, 3.34549s/100 iter), loss = 0.293528
I0109 19:23:20.246896  3041 solver.cpp:285]     Train net output #0: loss = 0.293528 (* 1 = 0.293528 loss)
I0109 19:23:20.246915  3041 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 19:23:23.595316  3041 solver.cpp:266] Iteration 4700 (29.8647 iter/s, 3.34843s/100 iter), loss = 0.206045
I0109 19:23:23.595387  3041 solver.cpp:285]     Train net output #0: loss = 0.206045 (* 1 = 0.206045 loss)
I0109 19:23:23.595402  3041 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 19:23:26.945952  3041 solver.cpp:266] Iteration 4800 (29.8454 iter/s, 3.3506s/100 iter), loss = 0.366954
I0109 19:23:26.946014  3041 solver.cpp:285]     Train net output #0: loss = 0.366954 (* 1 = 0.366954 loss)
I0109 19:23:26.946027  3041 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 19:23:30.297294  3041 solver.cpp:266] Iteration 4900 (29.8391 iter/s, 3.35131s/100 iter), loss = 0.171665
I0109 19:23:30.297361  3041 solver.cpp:285]     Train net output #0: loss = 0.171665 (* 1 = 0.171665 loss)
I0109 19:23:30.297375  3041 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 19:23:33.606868  3041 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 19:23:34.434674  3041 solver.cpp:517]     Test net output #0: loss = 0.546317 (* 1 = 0.546317 loss)
I0109 19:23:34.434710  3041 solver.cpp:517]     Test net output #1: top-1 = 0.832
I0109 19:23:34.434721  3041 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0109 19:23:34.466049  3041 solver.cpp:266] Iteration 5000 (23.9881 iter/s, 4.16874s/100 iter), loss = 0.185398
I0109 19:23:34.466087  3041 solver.cpp:285]     Train net output #0: loss = 0.185398 (* 1 = 0.185398 loss)
I0109 19:23:34.466102  3041 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 19:23:37.819028  3041 solver.cpp:266] Iteration 5100 (29.8246 iter/s, 3.35294s/100 iter), loss = 0.14487
I0109 19:23:37.819110  3041 solver.cpp:285]     Train net output #0: loss = 0.14487 (* 1 = 0.14487 loss)
I0109 19:23:37.819126  3041 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 19:23:41.170437  3041 solver.cpp:266] Iteration 5200 (29.8386 iter/s, 3.35136s/100 iter), loss = 0.180736
I0109 19:23:41.170506  3041 solver.cpp:285]     Train net output #0: loss = 0.180736 (* 1 = 0.180736 loss)
I0109 19:23:41.170519  3041 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 19:23:44.521271  3041 solver.cpp:266] Iteration 5300 (29.8436 iter/s, 3.3508s/100 iter), loss = 0.247569
I0109 19:23:44.521342  3041 solver.cpp:285]     Train net output #0: loss = 0.247569 (* 1 = 0.247569 loss)
I0109 19:23:44.521358  3041 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 19:23:47.869365  3041 solver.cpp:266] Iteration 5400 (29.8683 iter/s, 3.34803s/100 iter), loss = 0.151204
I0109 19:23:47.869575  3041 solver.cpp:285]     Train net output #0: loss = 0.151204 (* 1 = 0.151204 loss)
I0109 19:23:47.869590  3041 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 19:23:51.218989  3041 solver.cpp:266] Iteration 5500 (29.8556 iter/s, 3.34945s/100 iter), loss = 0.248109
I0109 19:23:51.219058  3041 solver.cpp:285]     Train net output #0: loss = 0.248109 (* 1 = 0.248109 loss)
I0109 19:23:51.219070  3041 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 19:23:54.568228  3041 solver.cpp:266] Iteration 5600 (29.858 iter/s, 3.34918s/100 iter), loss = 0.274824
I0109 19:23:54.568292  3041 solver.cpp:285]     Train net output #0: loss = 0.274823 (* 1 = 0.274823 loss)
I0109 19:23:54.568305  3041 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 19:23:57.912245  3041 solver.cpp:266] Iteration 5700 (29.9045 iter/s, 3.34398s/100 iter), loss = 0.146917
I0109 19:23:57.912328  3041 solver.cpp:285]     Train net output #0: loss = 0.146917 (* 1 = 0.146917 loss)
I0109 19:23:57.912344  3041 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 19:24:01.261306  3041 solver.cpp:266] Iteration 5800 (29.8595 iter/s, 3.34901s/100 iter), loss = 0.107595
I0109 19:24:01.261373  3041 solver.cpp:285]     Train net output #0: loss = 0.107595 (* 1 = 0.107595 loss)
I0109 19:24:01.261386  3041 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 19:24:04.613605  3041 solver.cpp:266] Iteration 5900 (29.8309 iter/s, 3.35223s/100 iter), loss = 0.178794
I0109 19:24:04.613670  3041 solver.cpp:285]     Train net output #0: loss = 0.178794 (* 1 = 0.178794 loss)
I0109 19:24:04.613684  3041 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 19:24:07.927505  3041 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 19:24:08.756538  3041 solver.cpp:517]     Test net output #0: loss = 0.507 (* 1 = 0.507 loss)
I0109 19:24:08.756579  3041 solver.cpp:517]     Test net output #1: top-1 = 0.842556
I0109 19:24:08.756592  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0109 19:24:08.787889  3041 solver.cpp:266] Iteration 6000 (23.9563 iter/s, 4.17427s/100 iter), loss = 0.183173
I0109 19:24:08.787937  3041 solver.cpp:285]     Train net output #0: loss = 0.183173 (* 1 = 0.183173 loss)
I0109 19:24:08.787959  3041 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 19:24:12.144752  3041 solver.cpp:266] Iteration 6100 (29.7898 iter/s, 3.35685s/100 iter), loss = 0.260128
I0109 19:24:12.144824  3041 solver.cpp:285]     Train net output #0: loss = 0.260128 (* 1 = 0.260128 loss)
I0109 19:24:12.144839  3041 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 19:24:15.490562  3041 solver.cpp:266] Iteration 6200 (29.8885 iter/s, 3.34577s/100 iter), loss = 0.23271
I0109 19:24:15.490649  3041 solver.cpp:285]     Train net output #0: loss = 0.23271 (* 1 = 0.23271 loss)
I0109 19:24:15.490665  3041 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 19:24:18.837090  3041 solver.cpp:266] Iteration 6300 (29.8824 iter/s, 3.34645s/100 iter), loss = 0.177581
I0109 19:24:18.837288  3041 solver.cpp:285]     Train net output #0: loss = 0.177581 (* 1 = 0.177581 loss)
I0109 19:24:18.837307  3041 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 19:24:22.185623  3041 solver.cpp:266] Iteration 6400 (29.8652 iter/s, 3.34838s/100 iter), loss = 0.221143
I0109 19:24:22.185688  3041 solver.cpp:285]     Train net output #0: loss = 0.221143 (* 1 = 0.221143 loss)
I0109 19:24:22.185700  3041 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 19:24:25.534121  3041 solver.cpp:266] Iteration 6500 (29.8644 iter/s, 3.34847s/100 iter), loss = 0.278417
I0109 19:24:25.534186  3041 solver.cpp:285]     Train net output #0: loss = 0.278417 (* 1 = 0.278417 loss)
I0109 19:24:25.534201  3041 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 19:24:28.879741  3041 solver.cpp:266] Iteration 6600 (29.8903 iter/s, 3.34557s/100 iter), loss = 0.106502
I0109 19:24:28.879806  3041 solver.cpp:285]     Train net output #0: loss = 0.106502 (* 1 = 0.106502 loss)
I0109 19:24:28.879820  3041 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 19:24:32.230808  3041 solver.cpp:266] Iteration 6700 (29.8415 iter/s, 3.35103s/100 iter), loss = 0.209805
I0109 19:24:32.230896  3041 solver.cpp:285]     Train net output #0: loss = 0.209805 (* 1 = 0.209805 loss)
I0109 19:24:32.230911  3041 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 19:24:35.572259  3041 solver.cpp:266] Iteration 6800 (29.9278 iter/s, 3.34138s/100 iter), loss = 0.263179
I0109 19:24:35.572326  3041 solver.cpp:285]     Train net output #0: loss = 0.263179 (* 1 = 0.263179 loss)
I0109 19:24:35.572340  3041 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 19:24:38.923496  3041 solver.cpp:266] Iteration 6900 (29.84 iter/s, 3.35121s/100 iter), loss = 0.139376
I0109 19:24:38.923557  3041 solver.cpp:285]     Train net output #0: loss = 0.139375 (* 1 = 0.139375 loss)
I0109 19:24:38.923571  3041 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 19:24:42.243134  3041 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 19:24:43.072499  3041 solver.cpp:517]     Test net output #0: loss = 0.520429 (* 1 = 0.520429 loss)
I0109 19:24:43.072535  3041 solver.cpp:517]     Test net output #1: top-1 = 0.840889
I0109 19:24:43.072543  3041 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0109 19:24:43.103740  3041 solver.cpp:266] Iteration 7000 (23.9221 iter/s, 4.18024s/100 iter), loss = 0.179864
I0109 19:24:43.103780  3041 solver.cpp:285]     Train net output #0: loss = 0.179864 (* 1 = 0.179864 loss)
I0109 19:24:43.103796  3041 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 19:24:46.454051  3041 solver.cpp:266] Iteration 7100 (29.848 iter/s, 3.35031s/100 iter), loss = 0.241992
I0109 19:24:46.454115  3041 solver.cpp:285]     Train net output #0: loss = 0.241992 (* 1 = 0.241992 loss)
I0109 19:24:46.454128  3041 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 19:24:49.805829  3041 solver.cpp:266] Iteration 7200 (29.8354 iter/s, 3.35173s/100 iter), loss = 0.24299
I0109 19:24:49.806002  3041 solver.cpp:285]     Train net output #0: loss = 0.24299 (* 1 = 0.24299 loss)
I0109 19:24:49.806018  3041 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 19:24:53.153091  3041 solver.cpp:266] Iteration 7300 (29.8764 iter/s, 3.34713s/100 iter), loss = 0.194889
I0109 19:24:53.153177  3041 solver.cpp:285]     Train net output #0: loss = 0.194889 (* 1 = 0.194889 loss)
I0109 19:24:53.153193  3041 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 19:24:56.501390  3041 solver.cpp:266] Iteration 7400 (29.8663 iter/s, 3.34826s/100 iter), loss = 0.156245
I0109 19:24:56.501452  3041 solver.cpp:285]     Train net output #0: loss = 0.156245 (* 1 = 0.156245 loss)
I0109 19:24:56.501464  3041 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 19:24:59.846572  3041 solver.cpp:266] Iteration 7500 (29.8942 iter/s, 3.34513s/100 iter), loss = 0.13615
I0109 19:24:59.846634  3041 solver.cpp:285]     Train net output #0: loss = 0.13615 (* 1 = 0.13615 loss)
I0109 19:24:59.846647  3041 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 19:25:03.198938  3041 solver.cpp:266] Iteration 7600 (29.8299 iter/s, 3.35234s/100 iter), loss = 0.18798
I0109 19:25:03.199004  3041 solver.cpp:285]     Train net output #0: loss = 0.18798 (* 1 = 0.18798 loss)
I0109 19:25:03.199018  3041 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 19:25:06.542331  3041 solver.cpp:266] Iteration 7700 (29.91 iter/s, 3.34336s/100 iter), loss = 0.135575
I0109 19:25:06.542412  3041 solver.cpp:285]     Train net output #0: loss = 0.135575 (* 1 = 0.135575 loss)
I0109 19:25:06.542428  3041 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 19:25:09.885567  3041 solver.cpp:266] Iteration 7800 (29.9118 iter/s, 3.34317s/100 iter), loss = 0.155019
I0109 19:25:09.885646  3041 solver.cpp:285]     Train net output #0: loss = 0.155019 (* 1 = 0.155019 loss)
I0109 19:25:09.885659  3041 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 19:25:13.226862  3041 solver.cpp:266] Iteration 7900 (29.9289 iter/s, 3.34125s/100 iter), loss = 0.189404
I0109 19:25:13.226927  3041 solver.cpp:285]     Train net output #0: loss = 0.189404 (* 1 = 0.189404 loss)
I0109 19:25:13.226939  3041 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 19:25:16.533563  3041 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 19:25:17.356890  3041 solver.cpp:517]     Test net output #0: loss = 0.461887 (* 1 = 0.461887 loss)
I0109 19:25:17.356933  3041 solver.cpp:517]     Test net output #1: top-1 = 0.854777
I0109 19:25:17.356942  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:25:17.388195  3041 solver.cpp:266] Iteration 8000 (24.0309 iter/s, 4.16131s/100 iter), loss = 0.23087
I0109 19:25:17.388275  3041 solver.cpp:285]     Train net output #0: loss = 0.23087 (* 1 = 0.23087 loss)
I0109 19:25:17.388291  3041 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 19:25:20.717160  3041 solver.cpp:266] Iteration 8100 (30.04 iter/s, 3.32889s/100 iter), loss = 0.222338
I0109 19:25:20.717299  3041 solver.cpp:285]     Train net output #0: loss = 0.222338 (* 1 = 0.222338 loss)
I0109 19:25:20.717316  3041 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 19:25:24.040771  3041 solver.cpp:266] Iteration 8200 (30.0887 iter/s, 3.3235s/100 iter), loss = 0.154802
I0109 19:25:24.040839  3041 solver.cpp:285]     Train net output #0: loss = 0.154802 (* 1 = 0.154802 loss)
I0109 19:25:24.040853  3041 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 19:25:27.354244  3041 solver.cpp:266] Iteration 8300 (30.1801 iter/s, 3.31344s/100 iter), loss = 0.22356
I0109 19:25:27.354308  3041 solver.cpp:285]     Train net output #0: loss = 0.22356 (* 1 = 0.22356 loss)
I0109 19:25:27.354321  3041 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 19:25:30.666005  3041 solver.cpp:266] Iteration 8400 (30.196 iter/s, 3.3117s/100 iter), loss = 0.163416
I0109 19:25:30.666071  3041 solver.cpp:285]     Train net output #0: loss = 0.163416 (* 1 = 0.163416 loss)
I0109 19:25:30.666083  3041 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 19:25:33.983078  3041 solver.cpp:266] Iteration 8500 (30.1474 iter/s, 3.31704s/100 iter), loss = 0.179952
I0109 19:25:33.983145  3041 solver.cpp:285]     Train net output #0: loss = 0.179952 (* 1 = 0.179952 loss)
I0109 19:25:33.983160  3041 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 19:25:37.299402  3041 solver.cpp:266] Iteration 8600 (30.1542 iter/s, 3.31629s/100 iter), loss = 0.168834
I0109 19:25:37.299469  3041 solver.cpp:285]     Train net output #0: loss = 0.168834 (* 1 = 0.168834 loss)
I0109 19:25:37.299482  3041 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 19:25:40.619434  3041 solver.cpp:266] Iteration 8700 (30.1208 iter/s, 3.31996s/100 iter), loss = 0.185079
I0109 19:25:40.619503  3041 solver.cpp:285]     Train net output #0: loss = 0.185078 (* 1 = 0.185078 loss)
I0109 19:25:40.619516  3041 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 19:25:43.923118  3041 solver.cpp:266] Iteration 8800 (30.2696 iter/s, 3.30365s/100 iter), loss = 0.139675
I0109 19:25:43.923185  3041 solver.cpp:285]     Train net output #0: loss = 0.139675 (* 1 = 0.139675 loss)
I0109 19:25:43.923197  3041 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 19:25:47.235131  3041 solver.cpp:266] Iteration 8900 (30.1935 iter/s, 3.31198s/100 iter), loss = 0.18273
I0109 19:25:47.235196  3041 solver.cpp:285]     Train net output #0: loss = 0.18273 (* 1 = 0.18273 loss)
I0109 19:25:47.235209  3041 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 19:25:50.511598  3041 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 19:25:51.328783  3041 solver.cpp:517]     Test net output #0: loss = 0.471855 (* 1 = 0.471855 loss)
I0109 19:25:51.328953  3041 solver.cpp:517]     Test net output #1: top-1 = 0.850222
I0109 19:25:51.328965  3041 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0109 19:25:51.359848  3041 solver.cpp:266] Iteration 9000 (24.2442 iter/s, 4.1247s/100 iter), loss = 0.154894
I0109 19:25:51.359886  3041 solver.cpp:285]     Train net output #0: loss = 0.154894 (* 1 = 0.154894 loss)
I0109 19:25:51.359901  3041 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 19:25:54.662592  3041 solver.cpp:266] Iteration 9100 (30.2782 iter/s, 3.30271s/100 iter), loss = 0.11973
I0109 19:25:54.662657  3041 solver.cpp:285]     Train net output #0: loss = 0.11973 (* 1 = 0.11973 loss)
I0109 19:25:54.662672  3041 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 19:25:57.966238  3041 solver.cpp:266] Iteration 9200 (30.2699 iter/s, 3.30361s/100 iter), loss = 0.257026
I0109 19:25:57.966307  3041 solver.cpp:285]     Train net output #0: loss = 0.257026 (* 1 = 0.257026 loss)
I0109 19:25:57.966321  3041 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 19:26:01.280961  3041 solver.cpp:266] Iteration 9300 (30.1688 iter/s, 3.31468s/100 iter), loss = 0.130823
I0109 19:26:01.281046  3041 solver.cpp:285]     Train net output #0: loss = 0.130823 (* 1 = 0.130823 loss)
I0109 19:26:01.281064  3041 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 19:26:04.586598  3041 solver.cpp:266] Iteration 9400 (30.2521 iter/s, 3.30556s/100 iter), loss = 0.152666
I0109 19:26:04.586661  3041 solver.cpp:285]     Train net output #0: loss = 0.152666 (* 1 = 0.152666 loss)
I0109 19:26:04.586674  3041 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 19:26:07.900190  3041 solver.cpp:266] Iteration 9500 (30.179 iter/s, 3.31356s/100 iter), loss = 0.151175
I0109 19:26:07.900262  3041 solver.cpp:285]     Train net output #0: loss = 0.151175 (* 1 = 0.151175 loss)
I0109 19:26:07.900275  3041 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 19:26:11.221138  3041 solver.cpp:266] Iteration 9600 (30.1123 iter/s, 3.32091s/100 iter), loss = 0.139827
I0109 19:26:11.221207  3041 solver.cpp:285]     Train net output #0: loss = 0.139827 (* 1 = 0.139827 loss)
I0109 19:26:11.221221  3041 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 19:26:14.533169  3041 solver.cpp:266] Iteration 9700 (30.1933 iter/s, 3.31199s/100 iter), loss = 0.169154
I0109 19:26:14.533231  3041 solver.cpp:285]     Train net output #0: loss = 0.169154 (* 1 = 0.169154 loss)
I0109 19:26:14.533244  3041 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 19:26:17.859740  3041 solver.cpp:266] Iteration 9800 (30.0615 iter/s, 3.32651s/100 iter), loss = 0.134092
I0109 19:26:17.859802  3041 solver.cpp:285]     Train net output #0: loss = 0.134092 (* 1 = 0.134092 loss)
I0109 19:26:17.859815  3041 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 19:26:21.189177  3041 solver.cpp:266] Iteration 9900 (30.0354 iter/s, 3.32941s/100 iter), loss = 0.133788
I0109 19:26:21.189244  3041 solver.cpp:285]     Train net output #0: loss = 0.133788 (* 1 = 0.133788 loss)
I0109 19:26:21.189261  3041 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 19:26:24.478864  3041 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 19:26:25.304092  3041 solver.cpp:517]     Test net output #0: loss = 0.491404 (* 1 = 0.491404 loss)
I0109 19:26:25.304131  3041 solver.cpp:517]     Test net output #1: top-1 = 0.846666
I0109 19:26:25.304143  3041 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0109 19:26:25.335324  3041 solver.cpp:266] Iteration 10000 (24.1189 iter/s, 4.14613s/100 iter), loss = 0.145034
I0109 19:26:25.335369  3041 solver.cpp:285]     Train net output #0: loss = 0.145034 (* 1 = 0.145034 loss)
I0109 19:26:25.335391  3041 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 19:26:28.673635  3041 solver.cpp:266] Iteration 10100 (29.9556 iter/s, 3.33827s/100 iter), loss = 0.129971
I0109 19:26:28.673702  3041 solver.cpp:285]     Train net output #0: loss = 0.129971 (* 1 = 0.129971 loss)
I0109 19:26:28.673714  3041 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 19:26:31.997678  3041 solver.cpp:266] Iteration 10200 (30.0842 iter/s, 3.32401s/100 iter), loss = 0.118861
I0109 19:26:31.997747  3041 solver.cpp:285]     Train net output #0: loss = 0.11886 (* 1 = 0.11886 loss)
I0109 19:26:31.997761  3041 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 19:26:35.334415  3041 solver.cpp:266] Iteration 10300 (29.9697 iter/s, 3.3367s/100 iter), loss = 0.128919
I0109 19:26:35.334475  3041 solver.cpp:285]     Train net output #0: loss = 0.128919 (* 1 = 0.128919 loss)
I0109 19:26:35.334487  3041 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 19:26:38.659737  3041 solver.cpp:266] Iteration 10400 (30.0728 iter/s, 3.32527s/100 iter), loss = 0.214214
I0109 19:26:38.659802  3041 solver.cpp:285]     Train net output #0: loss = 0.214214 (* 1 = 0.214214 loss)
I0109 19:26:38.659816  3041 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 19:26:41.998627  3041 solver.cpp:266] Iteration 10500 (29.9504 iter/s, 3.33886s/100 iter), loss = 0.195228
I0109 19:26:41.998693  3041 solver.cpp:285]     Train net output #0: loss = 0.195228 (* 1 = 0.195228 loss)
I0109 19:26:41.998708  3041 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 19:26:45.335182  3041 solver.cpp:266] Iteration 10600 (29.9714 iter/s, 3.33652s/100 iter), loss = 0.156591
I0109 19:26:45.335252  3041 solver.cpp:285]     Train net output #0: loss = 0.156591 (* 1 = 0.156591 loss)
I0109 19:26:45.335266  3041 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 19:26:48.669219  3041 solver.cpp:266] Iteration 10700 (29.9942 iter/s, 3.33397s/100 iter), loss = 0.12764
I0109 19:26:48.669282  3041 solver.cpp:285]     Train net output #0: loss = 0.12764 (* 1 = 0.12764 loss)
I0109 19:26:48.669294  3041 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 19:26:52.014452  3041 solver.cpp:266] Iteration 10800 (29.8936 iter/s, 3.3452s/100 iter), loss = 0.118934
I0109 19:26:52.014518  3041 solver.cpp:285]     Train net output #0: loss = 0.118933 (* 1 = 0.118933 loss)
I0109 19:26:52.014530  3041 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 19:26:55.364931  3041 solver.cpp:266] Iteration 10900 (29.8468 iter/s, 3.35045s/100 iter), loss = 0.191696
I0109 19:26:55.365082  3041 solver.cpp:285]     Train net output #0: loss = 0.191696 (* 1 = 0.191696 loss)
I0109 19:26:55.365097  3041 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 19:26:58.679838  3041 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 19:26:59.501761  3041 solver.cpp:517]     Test net output #0: loss = 0.470466 (* 1 = 0.470466 loss)
I0109 19:26:59.501801  3041 solver.cpp:517]     Test net output #1: top-1 = 0.847667
I0109 19:26:59.501809  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 19:26:59.533139  3041 solver.cpp:266] Iteration 11000 (23.9917 iter/s, 4.16811s/100 iter), loss = 0.144948
I0109 19:26:59.533186  3041 solver.cpp:285]     Train net output #0: loss = 0.144948 (* 1 = 0.144948 loss)
I0109 19:26:59.533202  3041 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 19:27:02.875217  3041 solver.cpp:266] Iteration 11100 (29.9219 iter/s, 3.34203s/100 iter), loss = 0.103801
I0109 19:27:02.875279  3041 solver.cpp:285]     Train net output #0: loss = 0.103801 (* 1 = 0.103801 loss)
I0109 19:27:02.875291  3041 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 19:27:06.224318  3041 solver.cpp:266] Iteration 11200 (29.859 iter/s, 3.34907s/100 iter), loss = 0.191226
I0109 19:27:06.224378  3041 solver.cpp:285]     Train net output #0: loss = 0.191226 (* 1 = 0.191226 loss)
I0109 19:27:06.224390  3041 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 19:27:09.569628  3041 solver.cpp:266] Iteration 11300 (29.8931 iter/s, 3.34526s/100 iter), loss = 0.12108
I0109 19:27:09.569691  3041 solver.cpp:285]     Train net output #0: loss = 0.12108 (* 1 = 0.12108 loss)
I0109 19:27:09.569705  3041 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 19:27:12.913367  3041 solver.cpp:266] Iteration 11400 (29.9069 iter/s, 3.34371s/100 iter), loss = 0.157581
I0109 19:27:12.913430  3041 solver.cpp:285]     Train net output #0: loss = 0.157581 (* 1 = 0.157581 loss)
I0109 19:27:12.913442  3041 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 19:27:16.262832  3041 solver.cpp:266] Iteration 11500 (29.8558 iter/s, 3.34943s/100 iter), loss = 0.153317
I0109 19:27:16.262904  3041 solver.cpp:285]     Train net output #0: loss = 0.153317 (* 1 = 0.153317 loss)
I0109 19:27:16.262917  3041 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 19:27:19.611917  3041 solver.cpp:266] Iteration 11600 (29.8595 iter/s, 3.34902s/100 iter), loss = 0.16894
I0109 19:27:19.611982  3041 solver.cpp:285]     Train net output #0: loss = 0.168939 (* 1 = 0.168939 loss)
I0109 19:27:19.611994  3041 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 19:27:22.958330  3041 solver.cpp:266] Iteration 11700 (29.8831 iter/s, 3.34637s/100 iter), loss = 0.103225
I0109 19:27:22.958412  3041 solver.cpp:285]     Train net output #0: loss = 0.103225 (* 1 = 0.103225 loss)
I0109 19:27:22.958428  3041 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 19:27:26.301360  3041 solver.cpp:266] Iteration 11800 (29.9134 iter/s, 3.34298s/100 iter), loss = 0.141756
I0109 19:27:26.301545  3041 solver.cpp:285]     Train net output #0: loss = 0.141756 (* 1 = 0.141756 loss)
I0109 19:27:26.301561  3041 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 19:27:29.650888  3041 solver.cpp:266] Iteration 11900 (29.8565 iter/s, 3.34935s/100 iter), loss = 0.171068
I0109 19:27:29.650959  3041 solver.cpp:285]     Train net output #0: loss = 0.171068 (* 1 = 0.171068 loss)
I0109 19:27:29.650974  3041 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 19:27:32.966979  3041 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 19:27:33.790628  3041 solver.cpp:517]     Test net output #0: loss = 0.464726 (* 1 = 0.464726 loss)
I0109 19:27:33.790668  3041 solver.cpp:517]     Test net output #1: top-1 = 0.855778
I0109 19:27:33.790675  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:27:33.822012  3041 solver.cpp:266] Iteration 12000 (23.9745 iter/s, 4.1711s/100 iter), loss = 0.184946
I0109 19:27:33.822072  3041 solver.cpp:285]     Train net output #0: loss = 0.184945 (* 1 = 0.184945 loss)
I0109 19:27:33.822088  3041 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 19:27:37.177817  3041 solver.cpp:266] Iteration 12100 (29.7994 iter/s, 3.35577s/100 iter), loss = 0.183207
I0109 19:27:37.177896  3041 solver.cpp:285]     Train net output #0: loss = 0.183207 (* 1 = 0.183207 loss)
I0109 19:27:37.177911  3041 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 19:27:40.528863  3041 solver.cpp:266] Iteration 12200 (29.8418 iter/s, 3.351s/100 iter), loss = 0.0916014
I0109 19:27:40.528934  3041 solver.cpp:285]     Train net output #0: loss = 0.0916012 (* 1 = 0.0916012 loss)
I0109 19:27:40.528947  3041 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 19:27:43.876504  3041 solver.cpp:266] Iteration 12300 (29.8723 iter/s, 3.34758s/100 iter), loss = 0.176536
I0109 19:27:43.876569  3041 solver.cpp:285]     Train net output #0: loss = 0.176536 (* 1 = 0.176536 loss)
I0109 19:27:43.876582  3041 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 19:27:47.225471  3041 solver.cpp:266] Iteration 12400 (29.8602 iter/s, 3.34893s/100 iter), loss = 0.113034
I0109 19:27:47.225541  3041 solver.cpp:285]     Train net output #0: loss = 0.113034 (* 1 = 0.113034 loss)
I0109 19:27:47.225554  3041 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 19:27:50.572094  3041 solver.cpp:266] Iteration 12500 (29.8814 iter/s, 3.34656s/100 iter), loss = 0.164424
I0109 19:27:50.572160  3041 solver.cpp:285]     Train net output #0: loss = 0.164424 (* 1 = 0.164424 loss)
I0109 19:27:50.572172  3041 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 19:27:53.930276  3041 solver.cpp:266] Iteration 12600 (29.7783 iter/s, 3.35815s/100 iter), loss = 0.11728
I0109 19:27:53.930361  3041 solver.cpp:285]     Train net output #0: loss = 0.11728 (* 1 = 0.11728 loss)
I0109 19:27:53.930377  3041 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 19:27:57.273109  3041 solver.cpp:266] Iteration 12700 (29.9152 iter/s, 3.34279s/100 iter), loss = 0.126964
I0109 19:27:57.273304  3041 solver.cpp:285]     Train net output #0: loss = 0.126964 (* 1 = 0.126964 loss)
I0109 19:27:57.273319  3041 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 19:28:00.623353  3041 solver.cpp:266] Iteration 12800 (29.8502 iter/s, 3.35006s/100 iter), loss = 0.135563
I0109 19:28:00.623420  3041 solver.cpp:285]     Train net output #0: loss = 0.135563 (* 1 = 0.135563 loss)
I0109 19:28:00.623433  3041 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 19:28:03.967873  3041 solver.cpp:266] Iteration 12900 (29.9 iter/s, 3.34449s/100 iter), loss = 0.217479
I0109 19:28:03.967941  3041 solver.cpp:285]     Train net output #0: loss = 0.217478 (* 1 = 0.217478 loss)
I0109 19:28:03.967955  3041 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 19:28:07.288898  3041 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 19:28:08.118399  3041 solver.cpp:517]     Test net output #0: loss = 0.501618 (* 1 = 0.501618 loss)
I0109 19:28:08.118445  3041 solver.cpp:517]     Test net output #1: top-1 = 0.849889
I0109 19:28:08.118453  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 19:28:08.150036  3041 solver.cpp:266] Iteration 13000 (23.9112 iter/s, 4.18214s/100 iter), loss = 0.15605
I0109 19:28:08.150095  3041 solver.cpp:285]     Train net output #0: loss = 0.15605 (* 1 = 0.15605 loss)
I0109 19:28:08.150117  3041 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 19:28:11.494144  3041 solver.cpp:266] Iteration 13100 (29.9035 iter/s, 3.34409s/100 iter), loss = 0.125263
I0109 19:28:11.494208  3041 solver.cpp:285]     Train net output #0: loss = 0.125262 (* 1 = 0.125262 loss)
I0109 19:28:11.494221  3041 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 19:28:14.848230  3041 solver.cpp:266] Iteration 13200 (29.8149 iter/s, 3.35403s/100 iter), loss = 0.158664
I0109 19:28:14.848304  3041 solver.cpp:285]     Train net output #0: loss = 0.158664 (* 1 = 0.158664 loss)
I0109 19:28:14.848316  3041 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 19:28:18.198081  3041 solver.cpp:266] Iteration 13300 (29.8524 iter/s, 3.34982s/100 iter), loss = 0.122647
I0109 19:28:18.198144  3041 solver.cpp:285]     Train net output #0: loss = 0.122647 (* 1 = 0.122647 loss)
I0109 19:28:18.198158  3041 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 19:28:21.547575  3041 solver.cpp:266] Iteration 13400 (29.8555 iter/s, 3.34947s/100 iter), loss = 0.124011
I0109 19:28:21.547639  3041 solver.cpp:285]     Train net output #0: loss = 0.124011 (* 1 = 0.124011 loss)
I0109 19:28:21.547652  3041 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 19:28:24.895165  3041 solver.cpp:266] Iteration 13500 (29.8727 iter/s, 3.34753s/100 iter), loss = 0.122984
I0109 19:28:24.895232  3041 solver.cpp:285]     Train net output #0: loss = 0.122984 (* 1 = 0.122984 loss)
I0109 19:28:24.895247  3041 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 19:28:28.244271  3041 solver.cpp:266] Iteration 13600 (29.859 iter/s, 3.34908s/100 iter), loss = 0.0965469
I0109 19:28:28.244422  3041 solver.cpp:285]     Train net output #0: loss = 0.0965467 (* 1 = 0.0965467 loss)
I0109 19:28:28.244437  3041 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 19:28:31.584791  3041 solver.cpp:266] Iteration 13700 (29.9367 iter/s, 3.34038s/100 iter), loss = 0.102823
I0109 19:28:31.584859  3041 solver.cpp:285]     Train net output #0: loss = 0.102823 (* 1 = 0.102823 loss)
I0109 19:28:31.584873  3041 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 19:28:34.933923  3041 solver.cpp:266] Iteration 13800 (29.8587 iter/s, 3.34911s/100 iter), loss = 0.113938
I0109 19:28:34.933985  3041 solver.cpp:285]     Train net output #0: loss = 0.113938 (* 1 = 0.113938 loss)
I0109 19:28:34.933998  3041 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 19:28:38.284869  3041 solver.cpp:266] Iteration 13900 (29.8425 iter/s, 3.35092s/100 iter), loss = 0.139453
I0109 19:28:38.284934  3041 solver.cpp:285]     Train net output #0: loss = 0.139453 (* 1 = 0.139453 loss)
I0109 19:28:38.284945  3041 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 19:28:41.598552  3041 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 19:28:42.426573  3041 solver.cpp:517]     Test net output #0: loss = 0.483406 (* 1 = 0.483406 loss)
I0109 19:28:42.426614  3041 solver.cpp:517]     Test net output #1: top-1 = 0.851778
I0109 19:28:42.426621  3041 solver.cpp:517]     Test net output #2: top-5 = 0.991444
I0109 19:28:42.457926  3041 solver.cpp:266] Iteration 14000 (23.9633 iter/s, 4.17305s/100 iter), loss = 0.124563
I0109 19:28:42.457974  3041 solver.cpp:285]     Train net output #0: loss = 0.124563 (* 1 = 0.124563 loss)
I0109 19:28:42.457988  3041 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 19:28:45.806850  3041 solver.cpp:266] Iteration 14100 (29.8606 iter/s, 3.34889s/100 iter), loss = 0.151582
I0109 19:28:45.806918  3041 solver.cpp:285]     Train net output #0: loss = 0.151582 (* 1 = 0.151582 loss)
I0109 19:28:45.806932  3041 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 19:28:49.155225  3041 solver.cpp:266] Iteration 14200 (29.8655 iter/s, 3.34834s/100 iter), loss = 0.151215
I0109 19:28:49.155309  3041 solver.cpp:285]     Train net output #0: loss = 0.151215 (* 1 = 0.151215 loss)
I0109 19:28:49.155324  3041 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 19:28:52.511567  3041 solver.cpp:266] Iteration 14300 (29.7947 iter/s, 3.3563s/100 iter), loss = 0.168431
I0109 19:28:52.511629  3041 solver.cpp:285]     Train net output #0: loss = 0.168431 (* 1 = 0.168431 loss)
I0109 19:28:52.511642  3041 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 19:28:55.863224  3041 solver.cpp:266] Iteration 14400 (29.8364 iter/s, 3.35161s/100 iter), loss = 0.101038
I0109 19:28:55.863291  3041 solver.cpp:285]     Train net output #0: loss = 0.101038 (* 1 = 0.101038 loss)
I0109 19:28:55.863303  3041 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 19:28:59.210834  3041 solver.cpp:266] Iteration 14500 (29.8723 iter/s, 3.34759s/100 iter), loss = 0.0959051
I0109 19:28:59.211019  3041 solver.cpp:285]     Train net output #0: loss = 0.095905 (* 1 = 0.095905 loss)
I0109 19:28:59.211038  3041 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 19:29:02.557018  3041 solver.cpp:266] Iteration 14600 (29.886 iter/s, 3.34605s/100 iter), loss = 0.139085
I0109 19:29:02.557085  3041 solver.cpp:285]     Train net output #0: loss = 0.139085 (* 1 = 0.139085 loss)
I0109 19:29:02.557098  3041 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 19:29:05.909317  3041 solver.cpp:266] Iteration 14700 (29.8307 iter/s, 3.35225s/100 iter), loss = 0.126376
I0109 19:29:05.909382  3041 solver.cpp:285]     Train net output #0: loss = 0.126376 (* 1 = 0.126376 loss)
I0109 19:29:05.909396  3041 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 19:29:09.267647  3041 solver.cpp:266] Iteration 14800 (29.7769 iter/s, 3.35831s/100 iter), loss = 0.141286
I0109 19:29:09.267712  3041 solver.cpp:285]     Train net output #0: loss = 0.141286 (* 1 = 0.141286 loss)
I0109 19:29:09.267725  3041 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 19:29:12.628672  3041 solver.cpp:266] Iteration 14900 (29.7532 iter/s, 3.36098s/100 iter), loss = 0.155177
I0109 19:29:12.628736  3041 solver.cpp:285]     Train net output #0: loss = 0.155177 (* 1 = 0.155177 loss)
I0109 19:29:12.628749  3041 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 19:29:15.950500  3041 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 19:29:16.777772  3041 solver.cpp:517]     Test net output #0: loss = 0.459876 (* 1 = 0.459876 loss)
I0109 19:29:16.777814  3041 solver.cpp:517]     Test net output #1: top-1 = 0.858333
I0109 19:29:16.777824  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992444
I0109 19:29:16.809115  3041 solver.cpp:266] Iteration 15000 (23.921 iter/s, 4.18043s/100 iter), loss = 0.149594
I0109 19:29:16.809180  3041 solver.cpp:285]     Train net output #0: loss = 0.149594 (* 1 = 0.149594 loss)
I0109 19:29:16.809195  3041 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 19:29:20.160893  3041 solver.cpp:266] Iteration 15100 (29.8351 iter/s, 3.35176s/100 iter), loss = 0.137988
I0109 19:29:20.160962  3041 solver.cpp:285]     Train net output #0: loss = 0.137988 (* 1 = 0.137988 loss)
I0109 19:29:20.160974  3041 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 19:29:23.510130  3041 solver.cpp:266] Iteration 15200 (29.8578 iter/s, 3.34921s/100 iter), loss = 0.180144
I0109 19:29:23.510210  3041 solver.cpp:285]     Train net output #0: loss = 0.180144 (* 1 = 0.180144 loss)
I0109 19:29:23.510226  3041 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 19:29:26.857666  3041 solver.cpp:266] Iteration 15300 (29.8733 iter/s, 3.34747s/100 iter), loss = 0.228751
I0109 19:29:26.857729  3041 solver.cpp:285]     Train net output #0: loss = 0.228751 (* 1 = 0.228751 loss)
I0109 19:29:26.857743  3041 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 19:29:30.200839  3041 solver.cpp:266] Iteration 15400 (29.9119 iter/s, 3.34315s/100 iter), loss = 0.116529
I0109 19:29:30.201053  3041 solver.cpp:285]     Train net output #0: loss = 0.116529 (* 1 = 0.116529 loss)
I0109 19:29:30.201071  3041 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 19:29:33.532936  3041 solver.cpp:266] Iteration 15500 (30.0127 iter/s, 3.33192s/100 iter), loss = 0.133616
I0109 19:29:33.532999  3041 solver.cpp:285]     Train net output #0: loss = 0.133616 (* 1 = 0.133616 loss)
I0109 19:29:33.533011  3041 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 19:29:36.881317  3041 solver.cpp:266] Iteration 15600 (29.8657 iter/s, 3.34833s/100 iter), loss = 0.0653387
I0109 19:29:36.881402  3041 solver.cpp:285]     Train net output #0: loss = 0.0653385 (* 1 = 0.0653385 loss)
I0109 19:29:36.881424  3041 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 19:29:40.203692  3041 solver.cpp:266] Iteration 15700 (30.0994 iter/s, 3.32233s/100 iter), loss = 0.103396
I0109 19:29:40.203764  3041 solver.cpp:285]     Train net output #0: loss = 0.103396 (* 1 = 0.103396 loss)
I0109 19:29:40.203784  3041 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 19:29:43.539170  3041 solver.cpp:266] Iteration 15800 (29.981 iter/s, 3.33544s/100 iter), loss = 0.122725
I0109 19:29:43.539247  3041 solver.cpp:285]     Train net output #0: loss = 0.122725 (* 1 = 0.122725 loss)
I0109 19:29:43.539265  3041 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 19:29:46.859814  3041 solver.cpp:266] Iteration 15900 (30.1152 iter/s, 3.32058s/100 iter), loss = 0.148587
I0109 19:29:46.859881  3041 solver.cpp:285]     Train net output #0: loss = 0.148587 (* 1 = 0.148587 loss)
I0109 19:29:46.859894  3041 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 19:29:50.145678  3041 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 19:29:50.969974  3041 solver.cpp:517]     Test net output #0: loss = 0.446587 (* 1 = 0.446587 loss)
I0109 19:29:50.970012  3041 solver.cpp:517]     Test net output #1: top-1 = 0.861
I0109 19:29:50.970021  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:29:51.000949  3041 solver.cpp:266] Iteration 16000 (24.1481 iter/s, 4.14112s/100 iter), loss = 0.131929
I0109 19:29:51.000986  3041 solver.cpp:285]     Train net output #0: loss = 0.131929 (* 1 = 0.131929 loss)
I0109 19:29:51.001003  3041 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 19:29:54.318006  3041 solver.cpp:266] Iteration 16100 (30.1473 iter/s, 3.31705s/100 iter), loss = 0.206506
I0109 19:29:54.318071  3041 solver.cpp:285]     Train net output #0: loss = 0.206506 (* 1 = 0.206506 loss)
I0109 19:29:54.318085  3041 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 19:29:57.635995  3041 solver.cpp:266] Iteration 16200 (30.1393 iter/s, 3.31793s/100 iter), loss = 0.130404
I0109 19:29:57.636061  3041 solver.cpp:285]     Train net output #0: loss = 0.130404 (* 1 = 0.130404 loss)
I0109 19:29:57.636073  3041 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 19:30:00.966670  3041 solver.cpp:266] Iteration 16300 (30.0242 iter/s, 3.33064s/100 iter), loss = 0.11797
I0109 19:30:00.966861  3041 solver.cpp:285]     Train net output #0: loss = 0.11797 (* 1 = 0.11797 loss)
I0109 19:30:00.966876  3041 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 19:30:04.284940  3041 solver.cpp:266] Iteration 16400 (30.1376 iter/s, 3.31811s/100 iter), loss = 0.175295
I0109 19:30:04.285008  3041 solver.cpp:285]     Train net output #0: loss = 0.175294 (* 1 = 0.175294 loss)
I0109 19:30:04.285022  3041 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 19:30:07.608006  3041 solver.cpp:266] Iteration 16500 (30.0933 iter/s, 3.323s/100 iter), loss = 0.0997509
I0109 19:30:07.608091  3041 solver.cpp:285]     Train net output #0: loss = 0.0997507 (* 1 = 0.0997507 loss)
I0109 19:30:07.608108  3041 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 19:30:10.921424  3041 solver.cpp:266] Iteration 16600 (30.1808 iter/s, 3.31337s/100 iter), loss = 0.125311
I0109 19:30:10.921496  3041 solver.cpp:285]     Train net output #0: loss = 0.125311 (* 1 = 0.125311 loss)
I0109 19:30:10.921509  3041 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 19:30:14.235558  3041 solver.cpp:266] Iteration 16700 (30.1742 iter/s, 3.31409s/100 iter), loss = 0.0916152
I0109 19:30:14.235625  3041 solver.cpp:285]     Train net output #0: loss = 0.091615 (* 1 = 0.091615 loss)
I0109 19:30:14.235637  3041 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 19:30:17.549376  3041 solver.cpp:266] Iteration 16800 (30.177 iter/s, 3.31378s/100 iter), loss = 0.0988913
I0109 19:30:17.549453  3041 solver.cpp:285]     Train net output #0: loss = 0.0988911 (* 1 = 0.0988911 loss)
I0109 19:30:17.549473  3041 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 19:30:20.872030  3041 solver.cpp:266] Iteration 16900 (30.097 iter/s, 3.32259s/100 iter), loss = 0.136493
I0109 19:30:20.872094  3041 solver.cpp:285]     Train net output #0: loss = 0.136493 (* 1 = 0.136493 loss)
I0109 19:30:20.872107  3041 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 19:30:24.159652  3041 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 19:30:24.983983  3041 solver.cpp:517]     Test net output #0: loss = 0.428959 (* 1 = 0.428959 loss)
I0109 19:30:24.984021  3041 solver.cpp:517]     Test net output #1: top-1 = 0.867556
I0109 19:30:24.984030  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:30:25.015316  3041 solver.cpp:266] Iteration 17000 (24.1356 iter/s, 4.14326s/100 iter), loss = 0.136884
I0109 19:30:25.015374  3041 solver.cpp:285]     Train net output #0: loss = 0.136884 (* 1 = 0.136884 loss)
I0109 19:30:25.015388  3041 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 19:30:28.335001  3041 solver.cpp:266] Iteration 17100 (30.1236 iter/s, 3.31965s/100 iter), loss = 0.133056
I0109 19:30:28.335079  3041 solver.cpp:285]     Train net output #0: loss = 0.133056 (* 1 = 0.133056 loss)
I0109 19:30:28.335093  3041 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 19:30:31.642238  3041 solver.cpp:266] Iteration 17200 (30.2374 iter/s, 3.30716s/100 iter), loss = 0.101231
I0109 19:30:31.642387  3041 solver.cpp:285]     Train net output #0: loss = 0.10123 (* 1 = 0.10123 loss)
I0109 19:30:31.642403  3041 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 19:30:34.966603  3041 solver.cpp:266] Iteration 17300 (30.082 iter/s, 3.32425s/100 iter), loss = 0.0995182
I0109 19:30:34.966667  3041 solver.cpp:285]     Train net output #0: loss = 0.099518 (* 1 = 0.099518 loss)
I0109 19:30:34.966681  3041 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 19:30:38.283794  3041 solver.cpp:266] Iteration 17400 (30.1463 iter/s, 3.31716s/100 iter), loss = 0.0954023
I0109 19:30:38.283861  3041 solver.cpp:285]     Train net output #0: loss = 0.0954021 (* 1 = 0.0954021 loss)
I0109 19:30:38.283874  3041 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 19:30:41.620829  3041 solver.cpp:266] Iteration 17500 (29.9673 iter/s, 3.33697s/100 iter), loss = 0.0650135
I0109 19:30:41.620903  3041 solver.cpp:285]     Train net output #0: loss = 0.0650133 (* 1 = 0.0650133 loss)
I0109 19:30:41.620918  3041 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 19:30:44.945567  3041 solver.cpp:266] Iteration 17600 (30.0779 iter/s, 3.3247s/100 iter), loss = 0.0821316
I0109 19:30:44.945647  3041 solver.cpp:285]     Train net output #0: loss = 0.0821314 (* 1 = 0.0821314 loss)
I0109 19:30:44.945659  3041 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 19:30:48.274590  3041 solver.cpp:266] Iteration 17700 (30.0393 iter/s, 3.32897s/100 iter), loss = 0.0972958
I0109 19:30:48.274667  3041 solver.cpp:285]     Train net output #0: loss = 0.0972956 (* 1 = 0.0972956 loss)
I0109 19:30:48.274680  3041 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 19:30:51.601531  3041 solver.cpp:266] Iteration 17800 (30.0583 iter/s, 3.32687s/100 iter), loss = 0.151723
I0109 19:30:51.601608  3041 solver.cpp:285]     Train net output #0: loss = 0.151723 (* 1 = 0.151723 loss)
I0109 19:30:51.601621  3041 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 19:30:54.939607  3041 solver.cpp:266] Iteration 17900 (29.9578 iter/s, 3.33803s/100 iter), loss = 0.127098
I0109 19:30:54.939678  3041 solver.cpp:285]     Train net output #0: loss = 0.127097 (* 1 = 0.127097 loss)
I0109 19:30:54.939690  3041 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 19:30:58.229640  3041 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 19:30:59.052472  3041 solver.cpp:517]     Test net output #0: loss = 0.440894 (* 1 = 0.440894 loss)
I0109 19:30:59.052508  3041 solver.cpp:517]     Test net output #1: top-1 = 0.863111
I0109 19:30:59.052517  3041 solver.cpp:517]     Test net output #2: top-5 = 0.993111
I0109 19:30:59.083755  3041 solver.cpp:266] Iteration 18000 (24.1306 iter/s, 4.14412s/100 iter), loss = 0.100281
I0109 19:30:59.083796  3041 solver.cpp:285]     Train net output #0: loss = 0.100281 (* 1 = 0.100281 loss)
I0109 19:30:59.083811  3041 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 19:31:02.419695  3041 solver.cpp:266] Iteration 18100 (29.9766 iter/s, 3.33593s/100 iter), loss = 0.185791
I0109 19:31:02.419869  3041 solver.cpp:285]     Train net output #0: loss = 0.185791 (* 1 = 0.185791 loss)
I0109 19:31:02.419886  3041 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 19:31:05.763378  3041 solver.cpp:266] Iteration 18200 (29.9086 iter/s, 3.34352s/100 iter), loss = 0.132811
I0109 19:31:05.763443  3041 solver.cpp:285]     Train net output #0: loss = 0.13281 (* 1 = 0.13281 loss)
I0109 19:31:05.763455  3041 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 19:31:09.108918  3041 solver.cpp:266] Iteration 18300 (29.8908 iter/s, 3.34551s/100 iter), loss = 0.107184
I0109 19:31:09.108983  3041 solver.cpp:285]     Train net output #0: loss = 0.107183 (* 1 = 0.107183 loss)
I0109 19:31:09.108995  3041 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 19:31:12.458745  3041 solver.cpp:266] Iteration 18400 (29.8526 iter/s, 3.3498s/100 iter), loss = 0.129746
I0109 19:31:12.458806  3041 solver.cpp:285]     Train net output #0: loss = 0.129746 (* 1 = 0.129746 loss)
I0109 19:31:12.458819  3041 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 19:31:15.803696  3041 solver.cpp:266] Iteration 18500 (29.8964 iter/s, 3.34489s/100 iter), loss = 0.158385
I0109 19:31:15.803783  3041 solver.cpp:285]     Train net output #0: loss = 0.158384 (* 1 = 0.158384 loss)
I0109 19:31:15.803800  3041 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 19:31:19.155393  3041 solver.cpp:266] Iteration 18600 (29.8361 iter/s, 3.35165s/100 iter), loss = 0.0591696
I0109 19:31:19.155458  3041 solver.cpp:285]     Train net output #0: loss = 0.0591694 (* 1 = 0.0591694 loss)
I0109 19:31:19.155472  3041 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 19:31:22.499471  3041 solver.cpp:266] Iteration 18700 (29.9039 iter/s, 3.34405s/100 iter), loss = 0.111698
I0109 19:31:22.499536  3041 solver.cpp:285]     Train net output #0: loss = 0.111698 (* 1 = 0.111698 loss)
I0109 19:31:22.499548  3041 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 19:31:25.850327  3041 solver.cpp:266] Iteration 18800 (29.8436 iter/s, 3.3508s/100 iter), loss = 0.0850869
I0109 19:31:25.850389  3041 solver.cpp:285]     Train net output #0: loss = 0.0850866 (* 1 = 0.0850866 loss)
I0109 19:31:25.850402  3041 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 19:31:29.196848  3041 solver.cpp:266] Iteration 18900 (29.8821 iter/s, 3.34649s/100 iter), loss = 0.103517
I0109 19:31:29.196926  3041 solver.cpp:285]     Train net output #0: loss = 0.103516 (* 1 = 0.103516 loss)
I0109 19:31:29.196943  3041 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 19:31:32.515316  3041 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 19:31:33.341614  3041 solver.cpp:517]     Test net output #0: loss = 0.428207 (* 1 = 0.428207 loss)
I0109 19:31:33.341648  3041 solver.cpp:517]     Test net output #1: top-1 = 0.868444
I0109 19:31:33.341655  3041 solver.cpp:517]     Test net output #2: top-5 = 0.992666
I0109 19:31:33.372853  3041 solver.cpp:266] Iteration 19000 (23.9465 iter/s, 4.17598s/100 iter), loss = 0.129673
I0109 19:31:33.372889  3041 solver.cpp:285]     Train net output #0: loss = 0.129673 (* 1 = 0.129673 loss)
I0109 19:31:33.372903  3041 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 19:31:36.726946  3041 solver.cpp:266] Iteration 19100 (29.8146 iter/s, 3.35406s/100 iter), loss = 0.0838102
I0109 19:31:36.727012  3041 solver.cpp:285]     Train net output #0: loss = 0.0838099 (* 1 = 0.0838099 loss)
I0109 19:31:36.727025  3041 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 19:31:40.076902  3041 solver.cpp:266] Iteration 19200 (29.8515 iter/s, 3.34992s/100 iter), loss = 0.183621
I0109 19:31:40.076979  3041 solver.cpp:285]     Train net output #0: loss = 0.18362 (* 1 = 0.18362 loss)
I0109 19:31:40.076993  3041 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 19:31:43.424382  3041 solver.cpp:266] Iteration 19300 (29.8736 iter/s, 3.34743s/100 iter), loss = 0.143404
I0109 19:31:43.424455  3041 solver.cpp:285]     Train net output #0: loss = 0.143404 (* 1 = 0.143404 loss)
I0109 19:31:43.424470  3041 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 19:31:46.773118  3041 solver.cpp:266] Iteration 19400 (29.8626 iter/s, 3.34867s/100 iter), loss = 0.120724
I0109 19:31:46.773183  3041 solver.cpp:285]     Train net output #0: loss = 0.120724 (* 1 = 0.120724 loss)
I0109 19:31:46.773195  3041 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 19:31:50.124019  3041 solver.cpp:266] Iteration 19500 (29.8431 iter/s, 3.35086s/100 iter), loss = 0.083277
I0109 19:31:50.124100  3041 solver.cpp:285]     Train net output #0: loss = 0.0832769 (* 1 = 0.0832769 loss)
I0109 19:31:50.124114  3041 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 19:31:53.477962  3041 solver.cpp:266] Iteration 19600 (29.816 iter/s, 3.3539s/100 iter), loss = 0.113864
I0109 19:31:53.478025  3041 solver.cpp:285]     Train net output #0: loss = 0.113864 (* 1 = 0.113864 loss)
I0109 19:31:53.478039  3041 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 19:31:56.830621  3041 solver.cpp:266] Iteration 19700 (29.8276 iter/s, 3.3526s/100 iter), loss = 0.087388
I0109 19:31:56.830682  3041 solver.cpp:285]     Train net output #0: loss = 0.0873878 (* 1 = 0.0873878 loss)
I0109 19:31:56.830694  3041 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 19:32:00.175010  3041 solver.cpp:266] Iteration 19800 (29.9011 iter/s, 3.34436s/100 iter), loss = 0.134018
I0109 19:32:00.175074  3041 solver.cpp:285]     Train net output #0: loss = 0.134018 (* 1 = 0.134018 loss)
I0109 19:32:00.175087  3041 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 19:32:03.525343  3041 solver.cpp:266] Iteration 19900 (29.848 iter/s, 3.3503s/100 iter), loss = 0.145837
I0109 19:32:03.525522  3041 solver.cpp:285]     Train net output #0: loss = 0.145837 (* 1 = 0.145837 loss)
I0109 19:32:03.525537  3041 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 19:32:06.832360  3041 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/_iter_20000.caffemodel
I0109 19:32:06.932977  3041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/_iter_20000.solverstate
I0109 19:32:06.959201  3041 solver.cpp:378] Iteration 20000, loss = 0.0141395
I0109 19:32:06.959241  3041 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 19:32:07.777253  3041 solver.cpp:517]     Test net output #0: loss = 0.430389 (* 1 = 0.430389 loss)
I0109 19:32:07.777292  3041 solver.cpp:517]     Test net output #1: top-1 = 0.87
I0109 19:32:07.777299  3041 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 19:32:07.777305  3041 solver.cpp:386] Optimization Done (29.3747 iter/s).
I0109 19:32:07.777313  3041 caffe_interface.cpp:530] Optimization Done.
I0109 19:32:07.938989  3069 pruning_runner.cpp:190] Sens info found, use it.
I0109 19:32:07.983989  3069 pruning_runner.cpp:217] Start compressing, please wait...
I0109 19:32:09.022670  3069 pruning_runner.cpp:264] Compression complete 0.00210527%
I0109 19:32:09.337131  3069 pruning_runner.cpp:264] Compression complete 50.001%
I0109 19:32:09.648186  3069 pruning_runner.cpp:264] Compression complete 66.6676%
I0109 19:32:09.953867  3069 pruning_runner.cpp:264] Compression complete 91.6669%
I0109 19:32:10.269946  3069 pruning_runner.cpp:264] Compression complete 98.8764%
I0109 19:32:10.586922  3069 pruning_runner.cpp:264] Compression complete 99.4382%
I0109 19:32:10.904573  3069 pruning_runner.cpp:264] Compression complete 99.7183%
I0109 19:32:11.220829  3069 pruning_runner.cpp:264] Compression complete 99.8592%
I0109 19:32:11.535212  3069 pruning_runner.cpp:264] Compression complete 99.9978%
I0109 19:32:11.839619  3069 pruning_runner.cpp:264] Compression complete 99.9989%
I0109 19:32:12.146775  3069 pruning_runner.cpp:264] Compression complete 99.9994%
I0109 19:32:12.458250  3069 pruning_runner.cpp:264] Compression complete 99.9997%
I0109 19:32:12.770807  3069 pruning_runner.cpp:264] Compression complete 99.9999%
I0109 19:32:13.086171  3069 pruning_runner.cpp:264] Compression complete 100%
I0109 19:32:13.402817  3069 pruning_runner.cpp:264] Compression complete 100%
I0109 19:32:13.717655  3069 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 19:32:13.717983  3069 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 19:32:13.718389  3069 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:32:13.718631  3069 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:32:13.718782  3069 layer_factory.hpp:77] Creating layer data
I0109 19:32:13.718850  3069 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:13.718956  3069 net.cpp:94] Creating Layer data
I0109 19:32:13.718996  3069 net.cpp:409] data -> data
I0109 19:32:13.719029  3069 net.cpp:409] data -> label
I0109 19:32:13.719831  3366 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:32:13.719877  3366 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:32:13.719979  3069 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:32:13.720137  3069 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:32:13.726433  3069 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:13.726486  3069 net.cpp:144] Setting up data
I0109 19:32:13.726506  3069 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:32:13.726516  3069 net.cpp:151] Top shape: 50 (50)
I0109 19:32:13.726521  3069 net.cpp:159] Memory required for data: 614600
I0109 19:32:13.726528  3069 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:32:13.726541  3069 net.cpp:94] Creating Layer label_data_1_split
I0109 19:32:13.726555  3069 net.cpp:435] label_data_1_split <- label
I0109 19:32:13.726567  3069 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:32:13.726581  3069 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:32:13.726600  3069 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:32:13.726790  3069 net.cpp:144] Setting up label_data_1_split
I0109 19:32:13.726809  3069 net.cpp:151] Top shape: 50 (50)
I0109 19:32:13.726817  3069 net.cpp:151] Top shape: 50 (50)
I0109 19:32:13.726825  3069 net.cpp:151] Top shape: 50 (50)
I0109 19:32:13.726831  3069 net.cpp:159] Memory required for data: 615200
I0109 19:32:13.726845  3069 layer_factory.hpp:77] Creating layer conv1
I0109 19:32:13.726863  3069 net.cpp:94] Creating Layer conv1
I0109 19:32:13.726891  3069 net.cpp:435] conv1 <- data
I0109 19:32:13.726919  3069 net.cpp:409] conv1 -> conv1
I0109 19:32:13.727941  3069 net.cpp:144] Setting up conv1
I0109 19:32:13.727963  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.727972  3069 net.cpp:159] Memory required for data: 7168800
I0109 19:32:13.727988  3069 layer_factory.hpp:77] Creating layer bn1
I0109 19:32:13.728011  3069 net.cpp:94] Creating Layer bn1
I0109 19:32:13.728020  3069 net.cpp:435] bn1 <- conv1
I0109 19:32:13.728040  3069 net.cpp:409] bn1 -> scale1
I0109 19:32:13.729055  3069 net.cpp:144] Setting up bn1
I0109 19:32:13.729074  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.729112  3069 net.cpp:159] Memory required for data: 13722400
I0109 19:32:13.729135  3069 layer_factory.hpp:77] Creating layer relu1
I0109 19:32:13.729153  3069 net.cpp:94] Creating Layer relu1
I0109 19:32:13.729161  3069 net.cpp:435] relu1 <- scale1
I0109 19:32:13.729169  3069 net.cpp:409] relu1 -> relu1
I0109 19:32:13.729264  3069 net.cpp:144] Setting up relu1
I0109 19:32:13.729321  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.729372  3069 net.cpp:159] Memory required for data: 20276000
I0109 19:32:13.729421  3069 layer_factory.hpp:77] Creating layer conv2
I0109 19:32:13.729470  3069 net.cpp:94] Creating Layer conv2
I0109 19:32:13.729509  3069 net.cpp:435] conv2 <- relu1
I0109 19:32:13.729553  3069 net.cpp:409] conv2 -> conv2
I0109 19:32:13.730687  3069 net.cpp:144] Setting up conv2
I0109 19:32:13.730717  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.730726  3069 net.cpp:159] Memory required for data: 26829600
I0109 19:32:13.730783  3069 layer_factory.hpp:77] Creating layer bn2
I0109 19:32:13.730826  3069 net.cpp:94] Creating Layer bn2
I0109 19:32:13.730870  3069 net.cpp:435] bn2 <- conv2
I0109 19:32:13.730918  3069 net.cpp:409] bn2 -> scale2
I0109 19:32:13.732013  3069 net.cpp:144] Setting up bn2
I0109 19:32:13.732035  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.732044  3069 net.cpp:159] Memory required for data: 33383200
I0109 19:32:13.732100  3069 layer_factory.hpp:77] Creating layer relu2
I0109 19:32:13.732139  3069 net.cpp:94] Creating Layer relu2
I0109 19:32:13.732184  3069 net.cpp:435] relu2 <- scale2
I0109 19:32:13.732228  3069 net.cpp:409] relu2 -> relu2
I0109 19:32:13.732300  3069 net.cpp:144] Setting up relu2
I0109 19:32:13.732352  3069 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:13.732408  3069 net.cpp:159] Memory required for data: 39936800
I0109 19:32:13.732465  3069 layer_factory.hpp:77] Creating layer pool1
I0109 19:32:13.732512  3069 net.cpp:94] Creating Layer pool1
I0109 19:32:13.732553  3069 net.cpp:435] pool1 <- relu2
I0109 19:32:13.732597  3069 net.cpp:409] pool1 -> pool1
I0109 19:32:13.732692  3069 net.cpp:144] Setting up pool1
I0109 19:32:13.732743  3069 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:32:13.732790  3069 net.cpp:159] Memory required for data: 41575200
I0109 19:32:13.732838  3069 layer_factory.hpp:77] Creating layer drop1
I0109 19:32:13.732878  3069 net.cpp:94] Creating Layer drop1
I0109 19:32:13.732897  3069 net.cpp:435] drop1 <- pool1
I0109 19:32:13.732909  3069 net.cpp:409] drop1 -> drop1
I0109 19:32:13.733032  3069 net.cpp:144] Setting up drop1
I0109 19:32:13.733191  3069 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:32:13.733232  3069 net.cpp:159] Memory required for data: 43213600
I0109 19:32:13.733275  3069 layer_factory.hpp:77] Creating layer conv3
I0109 19:32:13.733319  3069 net.cpp:94] Creating Layer conv3
I0109 19:32:13.733359  3069 net.cpp:435] conv3 <- drop1
I0109 19:32:13.733388  3069 net.cpp:409] conv3 -> conv3
I0109 19:32:13.734750  3069 net.cpp:144] Setting up conv3
I0109 19:32:13.734812  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.734854  3069 net.cpp:159] Memory required for data: 46490400
I0109 19:32:13.734906  3069 layer_factory.hpp:77] Creating layer bn3
I0109 19:32:13.734953  3069 net.cpp:94] Creating Layer bn3
I0109 19:32:13.734971  3069 net.cpp:435] bn3 <- conv3
I0109 19:32:13.734987  3069 net.cpp:409] bn3 -> scale3
I0109 19:32:13.736338  3069 net.cpp:144] Setting up bn3
I0109 19:32:13.736363  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.736371  3069 net.cpp:159] Memory required for data: 49767200
I0109 19:32:13.736460  3069 layer_factory.hpp:77] Creating layer relu3
I0109 19:32:13.736483  3069 net.cpp:94] Creating Layer relu3
I0109 19:32:13.736492  3069 net.cpp:435] relu3 <- scale3
I0109 19:32:13.736563  3069 net.cpp:409] relu3 -> relu3
I0109 19:32:13.736640  3069 net.cpp:144] Setting up relu3
I0109 19:32:13.736687  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.736726  3069 net.cpp:159] Memory required for data: 53044000
I0109 19:32:13.736773  3069 layer_factory.hpp:77] Creating layer conv4
I0109 19:32:13.736822  3069 net.cpp:94] Creating Layer conv4
I0109 19:32:13.736861  3069 net.cpp:435] conv4 <- relu3
I0109 19:32:13.736905  3069 net.cpp:409] conv4 -> conv4
I0109 19:32:13.737792  3069 net.cpp:144] Setting up conv4
I0109 19:32:13.737823  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.737831  3069 net.cpp:159] Memory required for data: 56320800
I0109 19:32:13.737885  3069 layer_factory.hpp:77] Creating layer bn4
I0109 19:32:13.737964  3069 net.cpp:94] Creating Layer bn4
I0109 19:32:13.738008  3069 net.cpp:435] bn4 <- conv4
I0109 19:32:13.738054  3069 net.cpp:409] bn4 -> scale4
I0109 19:32:13.739332  3069 net.cpp:144] Setting up bn4
I0109 19:32:13.739356  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.739365  3069 net.cpp:159] Memory required for data: 59597600
I0109 19:32:13.739449  3069 layer_factory.hpp:77] Creating layer relu4
I0109 19:32:13.739464  3069 net.cpp:94] Creating Layer relu4
I0109 19:32:13.739471  3069 net.cpp:435] relu4 <- scale4
I0109 19:32:13.739483  3069 net.cpp:409] relu4 -> relu4
I0109 19:32:13.739534  3069 net.cpp:144] Setting up relu4
I0109 19:32:13.739553  3069 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:13.739560  3069 net.cpp:159] Memory required for data: 62874400
I0109 19:32:13.739567  3069 layer_factory.hpp:77] Creating layer pool2
I0109 19:32:13.739581  3069 net.cpp:94] Creating Layer pool2
I0109 19:32:13.739595  3069 net.cpp:435] pool2 <- relu4
I0109 19:32:13.739606  3069 net.cpp:409] pool2 -> pool2
I0109 19:32:13.739661  3069 net.cpp:144] Setting up pool2
I0109 19:32:13.739676  3069 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:32:13.739681  3069 net.cpp:159] Memory required for data: 63693600
I0109 19:32:13.739684  3069 layer_factory.hpp:77] Creating layer drop2
I0109 19:32:13.739711  3069 net.cpp:94] Creating Layer drop2
I0109 19:32:13.739725  3069 net.cpp:435] drop2 <- pool2
I0109 19:32:13.739734  3069 net.cpp:409] drop2 -> drop2
I0109 19:32:13.739785  3069 net.cpp:144] Setting up drop2
I0109 19:32:13.739800  3069 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:32:13.739804  3069 net.cpp:159] Memory required for data: 64512800
I0109 19:32:13.739809  3069 layer_factory.hpp:77] Creating layer fc1
I0109 19:32:13.739820  3069 net.cpp:94] Creating Layer fc1
I0109 19:32:13.739835  3069 net.cpp:435] fc1 <- drop2
I0109 19:32:13.739847  3069 net.cpp:409] fc1 -> fc1
I0109 19:32:13.760473  3069 net.cpp:144] Setting up fc1
I0109 19:32:13.760500  3069 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:13.760506  3069 net.cpp:159] Memory required for data: 64615200
I0109 19:32:13.760516  3069 layer_factory.hpp:77] Creating layer bn5
I0109 19:32:13.760535  3069 net.cpp:94] Creating Layer bn5
I0109 19:32:13.760565  3069 net.cpp:435] bn5 <- fc1
I0109 19:32:13.760604  3069 net.cpp:409] bn5 -> scale5
I0109 19:32:13.761273  3069 net.cpp:144] Setting up bn5
I0109 19:32:13.761294  3069 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:13.761301  3069 net.cpp:159] Memory required for data: 64717600
I0109 19:32:13.761327  3069 layer_factory.hpp:77] Creating layer relu5
I0109 19:32:13.761343  3069 net.cpp:94] Creating Layer relu5
I0109 19:32:13.761349  3069 net.cpp:435] relu5 <- scale5
I0109 19:32:13.761359  3069 net.cpp:409] relu5 -> relu5
I0109 19:32:13.761399  3069 net.cpp:144] Setting up relu5
I0109 19:32:13.761415  3069 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:13.761421  3069 net.cpp:159] Memory required for data: 64820000
I0109 19:32:13.761428  3069 layer_factory.hpp:77] Creating layer drop3
I0109 19:32:13.761440  3069 net.cpp:94] Creating Layer drop3
I0109 19:32:13.761449  3069 net.cpp:435] drop3 <- relu5
I0109 19:32:13.761459  3069 net.cpp:409] drop3 -> drop3
I0109 19:32:13.761539  3069 net.cpp:144] Setting up drop3
I0109 19:32:13.761556  3069 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:13.761561  3069 net.cpp:159] Memory required for data: 64922400
I0109 19:32:13.761565  3069 layer_factory.hpp:77] Creating layer fc2
I0109 19:32:13.761576  3069 net.cpp:94] Creating Layer fc2
I0109 19:32:13.761581  3069 net.cpp:435] fc2 <- drop3
I0109 19:32:13.761605  3069 net.cpp:409] fc2 -> fc2
I0109 19:32:13.761799  3069 net.cpp:144] Setting up fc2
I0109 19:32:13.761814  3069 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:13.761819  3069 net.cpp:159] Memory required for data: 64924400
I0109 19:32:13.761826  3069 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:32:13.761835  3069 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:32:13.761839  3069 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:32:13.761850  3069 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:32:13.761868  3069 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:32:13.761879  3069 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:32:13.761956  3069 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:32:13.761971  3069 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:13.761976  3069 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:13.761981  3069 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:13.761984  3069 net.cpp:159] Memory required for data: 64930400
I0109 19:32:13.761989  3069 layer_factory.hpp:77] Creating layer loss
I0109 19:32:13.761997  3069 net.cpp:94] Creating Layer loss
I0109 19:32:13.762003  3069 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:32:13.762009  3069 net.cpp:435] loss <- label_data_1_split_0
I0109 19:32:13.762023  3069 net.cpp:409] loss -> loss
I0109 19:32:13.762037  3069 layer_factory.hpp:77] Creating layer loss
I0109 19:32:13.762163  3069 net.cpp:144] Setting up loss
I0109 19:32:13.762181  3069 net.cpp:151] Top shape: (1)
I0109 19:32:13.762185  3069 net.cpp:154]     with loss weight 1
I0109 19:32:13.762207  3069 net.cpp:159] Memory required for data: 64930404
I0109 19:32:13.762214  3069 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:32:13.762248  3069 net.cpp:94] Creating Layer accuracy-top1
I0109 19:32:13.762265  3069 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:32:13.762274  3069 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:32:13.762285  3069 net.cpp:409] accuracy-top1 -> top-1
I0109 19:32:13.762308  3069 net.cpp:144] Setting up accuracy-top1
I0109 19:32:13.762320  3069 net.cpp:151] Top shape: (1)
I0109 19:32:13.762332  3069 net.cpp:159] Memory required for data: 64930408
I0109 19:32:13.762339  3069 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:32:13.762351  3069 net.cpp:94] Creating Layer accuracy-top5
I0109 19:32:13.762360  3069 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:32:13.762368  3069 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:32:13.762378  3069 net.cpp:409] accuracy-top5 -> top-5
I0109 19:32:13.762398  3069 net.cpp:144] Setting up accuracy-top5
I0109 19:32:13.762408  3069 net.cpp:151] Top shape: (1)
I0109 19:32:13.762414  3069 net.cpp:159] Memory required for data: 64930412
I0109 19:32:13.762428  3069 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:32:13.762434  3069 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:32:13.762444  3069 net.cpp:220] loss needs backward computation.
I0109 19:32:13.762452  3069 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:32:13.762459  3069 net.cpp:220] fc2 needs backward computation.
I0109 19:32:13.762467  3069 net.cpp:220] drop3 needs backward computation.
I0109 19:32:13.762475  3069 net.cpp:220] relu5 needs backward computation.
I0109 19:32:13.762482  3069 net.cpp:220] bn5 needs backward computation.
I0109 19:32:13.762490  3069 net.cpp:220] fc1 needs backward computation.
I0109 19:32:13.762495  3069 net.cpp:220] drop2 needs backward computation.
I0109 19:32:13.762511  3069 net.cpp:220] pool2 needs backward computation.
I0109 19:32:13.762516  3069 net.cpp:220] relu4 needs backward computation.
I0109 19:32:13.762522  3069 net.cpp:220] bn4 needs backward computation.
I0109 19:32:13.762529  3069 net.cpp:220] conv4 needs backward computation.
I0109 19:32:13.762537  3069 net.cpp:220] relu3 needs backward computation.
I0109 19:32:13.762544  3069 net.cpp:220] bn3 needs backward computation.
I0109 19:32:13.762552  3069 net.cpp:220] conv3 needs backward computation.
I0109 19:32:13.762559  3069 net.cpp:220] drop1 needs backward computation.
I0109 19:32:13.762567  3069 net.cpp:220] pool1 needs backward computation.
I0109 19:32:13.762576  3069 net.cpp:220] relu2 needs backward computation.
I0109 19:32:13.762583  3069 net.cpp:220] bn2 needs backward computation.
I0109 19:32:13.762588  3069 net.cpp:220] conv2 needs backward computation.
I0109 19:32:13.762595  3069 net.cpp:220] relu1 needs backward computation.
I0109 19:32:13.762603  3069 net.cpp:220] bn1 needs backward computation.
I0109 19:32:13.762610  3069 net.cpp:220] conv1 needs backward computation.
I0109 19:32:13.762619  3069 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:32:13.762635  3069 net.cpp:222] data does not need backward computation.
I0109 19:32:13.762641  3069 net.cpp:264] This network produces output loss
I0109 19:32:13.762648  3069 net.cpp:264] This network produces output top-1
I0109 19:32:13.762655  3069 net.cpp:264] This network produces output top-5
I0109 19:32:13.762696  3069 net.cpp:284] Network initialization done.
I0109 19:32:13.766512  3069 caffe_interface.cpp:363] Running for 180 iterations.
I0109 19:32:13.778558  3069 caffe_interface.cpp:125] Batch 0, loss = 0.555465
I0109 19:32:13.778589  3069 caffe_interface.cpp:125] Batch 0, top-1 = 0.84
I0109 19:32:13.778595  3069 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0109 19:32:13.784898  3069 caffe_interface.cpp:125] Batch 1, loss = 0.212038
I0109 19:32:13.784924  3069 caffe_interface.cpp:125] Batch 1, top-1 = 0.92
I0109 19:32:13.784929  3069 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 19:32:13.791209  3069 caffe_interface.cpp:125] Batch 2, loss = 0.986531
I0109 19:32:13.791234  3069 caffe_interface.cpp:125] Batch 2, top-1 = 0.76
I0109 19:32:13.791255  3069 caffe_interface.cpp:125] Batch 2, top-5 = 0.96
I0109 19:32:13.797514  3069 caffe_interface.cpp:125] Batch 3, loss = 0.200104
I0109 19:32:13.797538  3069 caffe_interface.cpp:125] Batch 3, top-1 = 0.9
I0109 19:32:13.797544  3069 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 19:32:13.803810  3069 caffe_interface.cpp:125] Batch 4, loss = 0.513575
I0109 19:32:13.803834  3069 caffe_interface.cpp:125] Batch 4, top-1 = 0.84
I0109 19:32:13.803840  3069 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 19:32:13.810084  3069 caffe_interface.cpp:125] Batch 5, loss = 0.72547
I0109 19:32:13.810107  3069 caffe_interface.cpp:125] Batch 5, top-1 = 0.74
I0109 19:32:13.810112  3069 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 19:32:13.816385  3069 caffe_interface.cpp:125] Batch 6, loss = 0.92032
I0109 19:32:13.816408  3069 caffe_interface.cpp:125] Batch 6, top-1 = 0.82
I0109 19:32:13.816414  3069 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 19:32:13.822695  3069 caffe_interface.cpp:125] Batch 7, loss = 0.220652
I0109 19:32:13.822718  3069 caffe_interface.cpp:125] Batch 7, top-1 = 0.94
I0109 19:32:13.822724  3069 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 19:32:13.828994  3069 caffe_interface.cpp:125] Batch 8, loss = 0.800873
I0109 19:32:13.829017  3069 caffe_interface.cpp:125] Batch 8, top-1 = 0.72
I0109 19:32:13.829023  3069 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0109 19:32:13.835280  3069 caffe_interface.cpp:125] Batch 9, loss = 0.420782
I0109 19:32:13.835304  3069 caffe_interface.cpp:125] Batch 9, top-1 = 0.82
I0109 19:32:13.835309  3069 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 19:32:13.841562  3069 caffe_interface.cpp:125] Batch 10, loss = 0.404671
I0109 19:32:13.841585  3069 caffe_interface.cpp:125] Batch 10, top-1 = 0.82
I0109 19:32:13.841608  3069 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 19:32:13.847877  3069 caffe_interface.cpp:125] Batch 11, loss = 0.360766
I0109 19:32:13.847899  3069 caffe_interface.cpp:125] Batch 11, top-1 = 0.86
I0109 19:32:13.847904  3069 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 19:32:13.854135  3069 caffe_interface.cpp:125] Batch 12, loss = 0.485534
I0109 19:32:13.854156  3069 caffe_interface.cpp:125] Batch 12, top-1 = 0.82
I0109 19:32:13.854162  3069 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0109 19:32:13.860405  3069 caffe_interface.cpp:125] Batch 13, loss = 0.273424
I0109 19:32:13.860429  3069 caffe_interface.cpp:125] Batch 13, top-1 = 0.92
I0109 19:32:13.860433  3069 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0109 19:32:13.866693  3069 caffe_interface.cpp:125] Batch 14, loss = 0.414023
I0109 19:32:13.866716  3069 caffe_interface.cpp:125] Batch 14, top-1 = 0.88
I0109 19:32:13.866721  3069 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0109 19:32:13.873025  3069 caffe_interface.cpp:125] Batch 15, loss = 0.684437
I0109 19:32:13.873049  3069 caffe_interface.cpp:125] Batch 15, top-1 = 0.88
I0109 19:32:13.873054  3069 caffe_interface.cpp:125] Batch 15, top-5 = 0.94
I0109 19:32:13.879314  3069 caffe_interface.cpp:125] Batch 16, loss = 0.552464
I0109 19:32:13.879338  3069 caffe_interface.cpp:125] Batch 16, top-1 = 0.84
I0109 19:32:13.879344  3069 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0109 19:32:13.885638  3069 caffe_interface.cpp:125] Batch 17, loss = 0.502032
I0109 19:32:13.885663  3069 caffe_interface.cpp:125] Batch 17, top-1 = 0.86
I0109 19:32:13.885668  3069 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0109 19:32:13.891607  3069 caffe_interface.cpp:125] Batch 18, loss = 0.386591
I0109 19:32:13.891630  3069 caffe_interface.cpp:125] Batch 18, top-1 = 0.9
I0109 19:32:13.891635  3069 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 19:32:13.897547  3069 caffe_interface.cpp:125] Batch 19, loss = 0.527365
I0109 19:32:13.897569  3069 caffe_interface.cpp:125] Batch 19, top-1 = 0.9
I0109 19:32:13.897574  3069 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0109 19:32:13.903470  3069 caffe_interface.cpp:125] Batch 20, loss = 0.32585
I0109 19:32:13.903491  3069 caffe_interface.cpp:125] Batch 20, top-1 = 0.88
I0109 19:32:13.903517  3069 caffe_interface.cpp:125] Batch 20, top-5 = 0.98
I0109 19:32:13.909394  3069 caffe_interface.cpp:125] Batch 21, loss = 0.696737
I0109 19:32:13.909416  3069 caffe_interface.cpp:125] Batch 21, top-1 = 0.8
I0109 19:32:13.909422  3069 caffe_interface.cpp:125] Batch 21, top-5 = 0.96
I0109 19:32:13.915355  3069 caffe_interface.cpp:125] Batch 22, loss = 0.690087
I0109 19:32:13.915380  3069 caffe_interface.cpp:125] Batch 22, top-1 = 0.82
I0109 19:32:13.915385  3069 caffe_interface.cpp:125] Batch 22, top-5 = 0.98
I0109 19:32:13.921303  3069 caffe_interface.cpp:125] Batch 23, loss = 0.418225
I0109 19:32:13.921325  3069 caffe_interface.cpp:125] Batch 23, top-1 = 0.86
I0109 19:32:13.921331  3069 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 19:32:13.927227  3069 caffe_interface.cpp:125] Batch 24, loss = 0.449236
I0109 19:32:13.927249  3069 caffe_interface.cpp:125] Batch 24, top-1 = 0.9
I0109 19:32:13.927255  3069 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0109 19:32:13.933207  3069 caffe_interface.cpp:125] Batch 25, loss = 0.893855
I0109 19:32:13.933229  3069 caffe_interface.cpp:125] Batch 25, top-1 = 0.84
I0109 19:32:13.933234  3069 caffe_interface.cpp:125] Batch 25, top-5 = 0.94
I0109 19:32:13.939147  3069 caffe_interface.cpp:125] Batch 26, loss = 0.998753
I0109 19:32:13.939170  3069 caffe_interface.cpp:125] Batch 26, top-1 = 0.8
I0109 19:32:13.939175  3069 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0109 19:32:13.945103  3069 caffe_interface.cpp:125] Batch 27, loss = 0.385035
I0109 19:32:13.945128  3069 caffe_interface.cpp:125] Batch 27, top-1 = 0.86
I0109 19:32:13.945133  3069 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 19:32:13.951009  3069 caffe_interface.cpp:125] Batch 28, loss = 0.431677
I0109 19:32:13.951031  3069 caffe_interface.cpp:125] Batch 28, top-1 = 0.88
I0109 19:32:13.951038  3069 caffe_interface.cpp:125] Batch 28, top-5 = 1
I0109 19:32:13.956959  3069 caffe_interface.cpp:125] Batch 29, loss = 0.359209
I0109 19:32:13.956980  3069 caffe_interface.cpp:125] Batch 29, top-1 = 0.86
I0109 19:32:13.956986  3069 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 19:32:13.962910  3069 caffe_interface.cpp:125] Batch 30, loss = 0.463917
I0109 19:32:13.962934  3069 caffe_interface.cpp:125] Batch 30, top-1 = 0.86
I0109 19:32:13.962939  3069 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0109 19:32:13.968847  3069 caffe_interface.cpp:125] Batch 31, loss = 0.451247
I0109 19:32:13.968869  3069 caffe_interface.cpp:125] Batch 31, top-1 = 0.88
I0109 19:32:13.968875  3069 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 19:32:13.974781  3069 caffe_interface.cpp:125] Batch 32, loss = 0.870264
I0109 19:32:13.974803  3069 caffe_interface.cpp:125] Batch 32, top-1 = 0.74
I0109 19:32:13.974809  3069 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 19:32:13.980718  3069 caffe_interface.cpp:125] Batch 33, loss = 0.378013
I0109 19:32:13.980741  3069 caffe_interface.cpp:125] Batch 33, top-1 = 0.88
I0109 19:32:13.980746  3069 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 19:32:13.986644  3069 caffe_interface.cpp:125] Batch 34, loss = 0.624695
I0109 19:32:13.986666  3069 caffe_interface.cpp:125] Batch 34, top-1 = 0.78
I0109 19:32:13.986672  3069 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0109 19:32:13.992594  3069 caffe_interface.cpp:125] Batch 35, loss = 0.409447
I0109 19:32:13.992617  3069 caffe_interface.cpp:125] Batch 35, top-1 = 0.86
I0109 19:32:13.992624  3069 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 19:32:13.998503  3069 caffe_interface.cpp:125] Batch 36, loss = 0.546929
I0109 19:32:13.998525  3069 caffe_interface.cpp:125] Batch 36, top-1 = 0.86
I0109 19:32:13.998531  3069 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0109 19:32:14.004467  3069 caffe_interface.cpp:125] Batch 37, loss = 0.302795
I0109 19:32:14.004492  3069 caffe_interface.cpp:125] Batch 37, top-1 = 0.9
I0109 19:32:14.004496  3069 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0109 19:32:14.010380  3069 caffe_interface.cpp:125] Batch 38, loss = 0.798341
I0109 19:32:14.010403  3069 caffe_interface.cpp:125] Batch 38, top-1 = 0.86
I0109 19:32:14.010427  3069 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0109 19:32:14.016352  3069 caffe_interface.cpp:125] Batch 39, loss = 0.168246
I0109 19:32:14.016378  3069 caffe_interface.cpp:125] Batch 39, top-1 = 0.94
I0109 19:32:14.016384  3069 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 19:32:14.022303  3069 caffe_interface.cpp:125] Batch 40, loss = 0.387076
I0109 19:32:14.022326  3069 caffe_interface.cpp:125] Batch 40, top-1 = 0.82
I0109 19:32:14.022333  3069 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0109 19:32:14.028259  3069 caffe_interface.cpp:125] Batch 41, loss = 0.357739
I0109 19:32:14.028282  3069 caffe_interface.cpp:125] Batch 41, top-1 = 0.88
I0109 19:32:14.028287  3069 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 19:32:14.034210  3069 caffe_interface.cpp:125] Batch 42, loss = 0.3591
I0109 19:32:14.034234  3069 caffe_interface.cpp:125] Batch 42, top-1 = 0.84
I0109 19:32:14.034240  3069 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0109 19:32:14.040180  3069 caffe_interface.cpp:125] Batch 43, loss = 0.339708
I0109 19:32:14.040205  3069 caffe_interface.cpp:125] Batch 43, top-1 = 0.94
I0109 19:32:14.040210  3069 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0109 19:32:14.046100  3069 caffe_interface.cpp:125] Batch 44, loss = 0.217317
I0109 19:32:14.046123  3069 caffe_interface.cpp:125] Batch 44, top-1 = 0.88
I0109 19:32:14.046129  3069 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0109 19:32:14.051888  3069 caffe_interface.cpp:125] Batch 45, loss = 0.644685
I0109 19:32:14.051914  3069 caffe_interface.cpp:125] Batch 45, top-1 = 0.8
I0109 19:32:14.051920  3069 caffe_interface.cpp:125] Batch 45, top-5 = 0.98
I0109 19:32:14.057273  3069 caffe_interface.cpp:125] Batch 46, loss = 0.60614
I0109 19:32:14.057297  3069 caffe_interface.cpp:125] Batch 46, top-1 = 0.8
I0109 19:32:14.057303  3069 caffe_interface.cpp:125] Batch 46, top-5 = 0.96
I0109 19:32:14.062680  3069 caffe_interface.cpp:125] Batch 47, loss = 0.677349
I0109 19:32:14.062705  3069 caffe_interface.cpp:125] Batch 47, top-1 = 0.82
I0109 19:32:14.062711  3069 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0109 19:32:14.068063  3069 caffe_interface.cpp:125] Batch 48, loss = 0.395528
I0109 19:32:14.068087  3069 caffe_interface.cpp:125] Batch 48, top-1 = 0.86
I0109 19:32:14.068094  3069 caffe_interface.cpp:125] Batch 48, top-5 = 0.98
I0109 19:32:14.073452  3069 caffe_interface.cpp:125] Batch 49, loss = 0.620495
I0109 19:32:14.073477  3069 caffe_interface.cpp:125] Batch 49, top-1 = 0.86
I0109 19:32:14.073483  3069 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0109 19:32:14.078874  3069 caffe_interface.cpp:125] Batch 50, loss = 0.467725
I0109 19:32:14.078899  3069 caffe_interface.cpp:125] Batch 50, top-1 = 0.9
I0109 19:32:14.078904  3069 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0109 19:32:14.084270  3069 caffe_interface.cpp:125] Batch 51, loss = 0.551456
I0109 19:32:14.084295  3069 caffe_interface.cpp:125] Batch 51, top-1 = 0.88
I0109 19:32:14.084300  3069 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0109 19:32:14.089666  3069 caffe_interface.cpp:125] Batch 52, loss = 0.65379
I0109 19:32:14.089689  3069 caffe_interface.cpp:125] Batch 52, top-1 = 0.82
I0109 19:32:14.089695  3069 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0109 19:32:14.095067  3069 caffe_interface.cpp:125] Batch 53, loss = 0.421101
I0109 19:32:14.095090  3069 caffe_interface.cpp:125] Batch 53, top-1 = 0.82
I0109 19:32:14.095095  3069 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0109 19:32:14.100435  3069 caffe_interface.cpp:125] Batch 54, loss = 0.538795
I0109 19:32:14.100458  3069 caffe_interface.cpp:125] Batch 54, top-1 = 0.82
I0109 19:32:14.100464  3069 caffe_interface.cpp:125] Batch 54, top-5 = 0.98
I0109 19:32:14.105849  3069 caffe_interface.cpp:125] Batch 55, loss = 0.305207
I0109 19:32:14.105875  3069 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I0109 19:32:14.105880  3069 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0109 19:32:14.111208  3069 caffe_interface.cpp:125] Batch 56, loss = 0.325243
I0109 19:32:14.111249  3069 caffe_interface.cpp:125] Batch 56, top-1 = 0.9
I0109 19:32:14.111258  3069 caffe_interface.cpp:125] Batch 56, top-5 = 0.98
I0109 19:32:14.116631  3069 caffe_interface.cpp:125] Batch 57, loss = 0.372655
I0109 19:32:14.116655  3069 caffe_interface.cpp:125] Batch 57, top-1 = 0.86
I0109 19:32:14.116662  3069 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0109 19:32:14.122038  3069 caffe_interface.cpp:125] Batch 58, loss = 0.655409
I0109 19:32:14.122063  3069 caffe_interface.cpp:125] Batch 58, top-1 = 0.8
I0109 19:32:14.122068  3069 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0109 19:32:14.127466  3069 caffe_interface.cpp:125] Batch 59, loss = 0.234254
I0109 19:32:14.127492  3069 caffe_interface.cpp:125] Batch 59, top-1 = 0.86
I0109 19:32:14.127498  3069 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0109 19:32:14.132861  3069 caffe_interface.cpp:125] Batch 60, loss = 0.369017
I0109 19:32:14.132884  3069 caffe_interface.cpp:125] Batch 60, top-1 = 0.86
I0109 19:32:14.132891  3069 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0109 19:32:14.138242  3069 caffe_interface.cpp:125] Batch 61, loss = 0.205124
I0109 19:32:14.138264  3069 caffe_interface.cpp:125] Batch 61, top-1 = 0.9
I0109 19:32:14.138270  3069 caffe_interface.cpp:125] Batch 61, top-5 = 1
I0109 19:32:14.143643  3069 caffe_interface.cpp:125] Batch 62, loss = 0.327671
I0109 19:32:14.143667  3069 caffe_interface.cpp:125] Batch 62, top-1 = 0.9
I0109 19:32:14.143673  3069 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0109 19:32:14.149039  3069 caffe_interface.cpp:125] Batch 63, loss = 0.505305
I0109 19:32:14.149065  3069 caffe_interface.cpp:125] Batch 63, top-1 = 0.82
I0109 19:32:14.149070  3069 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0109 19:32:14.154423  3069 caffe_interface.cpp:125] Batch 64, loss = 0.255393
I0109 19:32:14.154448  3069 caffe_interface.cpp:125] Batch 64, top-1 = 0.88
I0109 19:32:14.154454  3069 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 19:32:14.159829  3069 caffe_interface.cpp:125] Batch 65, loss = 0.319386
I0109 19:32:14.159853  3069 caffe_interface.cpp:125] Batch 65, top-1 = 0.9
I0109 19:32:14.159859  3069 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 19:32:14.165237  3069 caffe_interface.cpp:125] Batch 66, loss = 0.573486
I0109 19:32:14.165262  3069 caffe_interface.cpp:125] Batch 66, top-1 = 0.84
I0109 19:32:14.165267  3069 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0109 19:32:14.170634  3069 caffe_interface.cpp:125] Batch 67, loss = 0.275176
I0109 19:32:14.170658  3069 caffe_interface.cpp:125] Batch 67, top-1 = 0.86
I0109 19:32:14.170665  3069 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 19:32:14.176033  3069 caffe_interface.cpp:125] Batch 68, loss = 0.663626
I0109 19:32:14.176057  3069 caffe_interface.cpp:125] Batch 68, top-1 = 0.78
I0109 19:32:14.176064  3069 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0109 19:32:14.181433  3069 caffe_interface.cpp:125] Batch 69, loss = 0.780637
I0109 19:32:14.181457  3069 caffe_interface.cpp:125] Batch 69, top-1 = 0.8
I0109 19:32:14.181463  3069 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 19:32:14.186843  3069 caffe_interface.cpp:125] Batch 70, loss = 0.628644
I0109 19:32:14.186866  3069 caffe_interface.cpp:125] Batch 70, top-1 = 0.86
I0109 19:32:14.186872  3069 caffe_interface.cpp:125] Batch 70, top-5 = 0.96
I0109 19:32:14.192263  3069 caffe_interface.cpp:125] Batch 71, loss = 0.461223
I0109 19:32:14.192288  3069 caffe_interface.cpp:125] Batch 71, top-1 = 0.82
I0109 19:32:14.192294  3069 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0109 19:32:14.197692  3069 caffe_interface.cpp:125] Batch 72, loss = 0.328649
I0109 19:32:14.197715  3069 caffe_interface.cpp:125] Batch 72, top-1 = 0.9
I0109 19:32:14.197722  3069 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 19:32:14.203084  3069 caffe_interface.cpp:125] Batch 73, loss = 0.506562
I0109 19:32:14.203107  3069 caffe_interface.cpp:125] Batch 73, top-1 = 0.84
I0109 19:32:14.203114  3069 caffe_interface.cpp:125] Batch 73, top-5 = 1
I0109 19:32:14.208461  3069 caffe_interface.cpp:125] Batch 74, loss = 0.592709
I0109 19:32:14.208503  3069 caffe_interface.cpp:125] Batch 74, top-1 = 0.88
I0109 19:32:14.208509  3069 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 19:32:14.213891  3069 caffe_interface.cpp:125] Batch 75, loss = 0.40956
I0109 19:32:14.213917  3069 caffe_interface.cpp:125] Batch 75, top-1 = 0.86
I0109 19:32:14.213922  3069 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0109 19:32:14.219266  3069 caffe_interface.cpp:125] Batch 76, loss = 0.410295
I0109 19:32:14.219292  3069 caffe_interface.cpp:125] Batch 76, top-1 = 0.82
I0109 19:32:14.219298  3069 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 19:32:14.224268  3069 caffe_interface.cpp:125] Batch 77, loss = 0.315339
I0109 19:32:14.224293  3069 caffe_interface.cpp:125] Batch 77, top-1 = 0.94
I0109 19:32:14.224298  3069 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0109 19:32:14.229233  3069 caffe_interface.cpp:125] Batch 78, loss = 0.618623
I0109 19:32:14.229257  3069 caffe_interface.cpp:125] Batch 78, top-1 = 0.8
I0109 19:32:14.229262  3069 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0109 19:32:14.234218  3069 caffe_interface.cpp:125] Batch 79, loss = 0.605926
I0109 19:32:14.234243  3069 caffe_interface.cpp:125] Batch 79, top-1 = 0.88
I0109 19:32:14.234249  3069 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0109 19:32:14.239173  3069 caffe_interface.cpp:125] Batch 80, loss = 0.549384
I0109 19:32:14.239197  3069 caffe_interface.cpp:125] Batch 80, top-1 = 0.84
I0109 19:32:14.239203  3069 caffe_interface.cpp:125] Batch 80, top-5 = 0.98
I0109 19:32:14.244125  3069 caffe_interface.cpp:125] Batch 81, loss = 0.645796
I0109 19:32:14.244148  3069 caffe_interface.cpp:125] Batch 81, top-1 = 0.82
I0109 19:32:14.244154  3069 caffe_interface.cpp:125] Batch 81, top-5 = 0.96
I0109 19:32:14.249065  3069 caffe_interface.cpp:125] Batch 82, loss = 0.3539
I0109 19:32:14.249090  3069 caffe_interface.cpp:125] Batch 82, top-1 = 0.9
I0109 19:32:14.249095  3069 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 19:32:14.254042  3069 caffe_interface.cpp:125] Batch 83, loss = 0.526408
I0109 19:32:14.254067  3069 caffe_interface.cpp:125] Batch 83, top-1 = 0.84
I0109 19:32:14.254072  3069 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0109 19:32:14.259021  3069 caffe_interface.cpp:125] Batch 84, loss = 0.476827
I0109 19:32:14.259044  3069 caffe_interface.cpp:125] Batch 84, top-1 = 0.84
I0109 19:32:14.259050  3069 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 19:32:14.263984  3069 caffe_interface.cpp:125] Batch 85, loss = 0.651453
I0109 19:32:14.264008  3069 caffe_interface.cpp:125] Batch 85, top-1 = 0.8
I0109 19:32:14.264014  3069 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 19:32:14.268945  3069 caffe_interface.cpp:125] Batch 86, loss = 0.455663
I0109 19:32:14.268970  3069 caffe_interface.cpp:125] Batch 86, top-1 = 0.78
I0109 19:32:14.268975  3069 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 19:32:14.273917  3069 caffe_interface.cpp:125] Batch 87, loss = 0.637987
I0109 19:32:14.273942  3069 caffe_interface.cpp:125] Batch 87, top-1 = 0.8
I0109 19:32:14.273947  3069 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0109 19:32:14.278894  3069 caffe_interface.cpp:125] Batch 88, loss = 0.682121
I0109 19:32:14.278918  3069 caffe_interface.cpp:125] Batch 88, top-1 = 0.8
I0109 19:32:14.278924  3069 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 19:32:14.283864  3069 caffe_interface.cpp:125] Batch 89, loss = 0.609287
I0109 19:32:14.283887  3069 caffe_interface.cpp:125] Batch 89, top-1 = 0.8
I0109 19:32:14.283892  3069 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0109 19:32:14.288817  3069 caffe_interface.cpp:125] Batch 90, loss = 0.176998
I0109 19:32:14.288841  3069 caffe_interface.cpp:125] Batch 90, top-1 = 0.9
I0109 19:32:14.288846  3069 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 19:32:14.293793  3069 caffe_interface.cpp:125] Batch 91, loss = 0.278498
I0109 19:32:14.293817  3069 caffe_interface.cpp:125] Batch 91, top-1 = 0.86
I0109 19:32:14.293823  3069 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 19:32:14.298772  3069 caffe_interface.cpp:125] Batch 92, loss = 0.476381
I0109 19:32:14.298813  3069 caffe_interface.cpp:125] Batch 92, top-1 = 0.88
I0109 19:32:14.298820  3069 caffe_interface.cpp:125] Batch 92, top-5 = 1
I0109 19:32:14.303766  3069 caffe_interface.cpp:125] Batch 93, loss = 0.216482
I0109 19:32:14.303788  3069 caffe_interface.cpp:125] Batch 93, top-1 = 0.92
I0109 19:32:14.303794  3069 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 19:32:14.308743  3069 caffe_interface.cpp:125] Batch 94, loss = 0.596497
I0109 19:32:14.308766  3069 caffe_interface.cpp:125] Batch 94, top-1 = 0.88
I0109 19:32:14.308773  3069 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0109 19:32:14.313693  3069 caffe_interface.cpp:125] Batch 95, loss = 0.53168
I0109 19:32:14.313716  3069 caffe_interface.cpp:125] Batch 95, top-1 = 0.8
I0109 19:32:14.313721  3069 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 19:32:14.318683  3069 caffe_interface.cpp:125] Batch 96, loss = 0.355331
I0109 19:32:14.318708  3069 caffe_interface.cpp:125] Batch 96, top-1 = 0.88
I0109 19:32:14.318716  3069 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 19:32:14.323676  3069 caffe_interface.cpp:125] Batch 97, loss = 0.676375
I0109 19:32:14.323699  3069 caffe_interface.cpp:125] Batch 97, top-1 = 0.78
I0109 19:32:14.323705  3069 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0109 19:32:14.328630  3069 caffe_interface.cpp:125] Batch 98, loss = 0.507573
I0109 19:32:14.328653  3069 caffe_interface.cpp:125] Batch 98, top-1 = 0.82
I0109 19:32:14.328660  3069 caffe_interface.cpp:125] Batch 98, top-5 = 0.98
I0109 19:32:14.333611  3069 caffe_interface.cpp:125] Batch 99, loss = 0.472959
I0109 19:32:14.333633  3069 caffe_interface.cpp:125] Batch 99, top-1 = 0.84
I0109 19:32:14.333638  3069 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0109 19:32:14.338584  3069 caffe_interface.cpp:125] Batch 100, loss = 0.262581
I0109 19:32:14.338608  3069 caffe_interface.cpp:125] Batch 100, top-1 = 0.9
I0109 19:32:14.338614  3069 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 19:32:14.343562  3069 caffe_interface.cpp:125] Batch 101, loss = 0.540507
I0109 19:32:14.343586  3069 caffe_interface.cpp:125] Batch 101, top-1 = 0.82
I0109 19:32:14.343591  3069 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 19:32:14.348520  3069 caffe_interface.cpp:125] Batch 102, loss = 0.297669
I0109 19:32:14.348543  3069 caffe_interface.cpp:125] Batch 102, top-1 = 0.86
I0109 19:32:14.348548  3069 caffe_interface.cpp:125] Batch 102, top-5 = 1
I0109 19:32:14.353485  3069 caffe_interface.cpp:125] Batch 103, loss = 0.535036
I0109 19:32:14.353509  3069 caffe_interface.cpp:125] Batch 103, top-1 = 0.84
I0109 19:32:14.353515  3069 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 19:32:14.358460  3069 caffe_interface.cpp:125] Batch 104, loss = 0.320826
I0109 19:32:14.358484  3069 caffe_interface.cpp:125] Batch 104, top-1 = 0.86
I0109 19:32:14.358489  3069 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 19:32:14.363426  3069 caffe_interface.cpp:125] Batch 105, loss = 0.182473
I0109 19:32:14.363451  3069 caffe_interface.cpp:125] Batch 105, top-1 = 0.92
I0109 19:32:14.363456  3069 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 19:32:14.368412  3069 caffe_interface.cpp:125] Batch 106, loss = 0.626897
I0109 19:32:14.368436  3069 caffe_interface.cpp:125] Batch 106, top-1 = 0.84
I0109 19:32:14.368441  3069 caffe_interface.cpp:125] Batch 106, top-5 = 0.98
I0109 19:32:14.373391  3069 caffe_interface.cpp:125] Batch 107, loss = 0.619029
I0109 19:32:14.373416  3069 caffe_interface.cpp:125] Batch 107, top-1 = 0.82
I0109 19:32:14.373421  3069 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0109 19:32:14.378350  3069 caffe_interface.cpp:125] Batch 108, loss = 0.559347
I0109 19:32:14.378374  3069 caffe_interface.cpp:125] Batch 108, top-1 = 0.8
I0109 19:32:14.378381  3069 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0109 19:32:14.383327  3069 caffe_interface.cpp:125] Batch 109, loss = 0.41492
I0109 19:32:14.383352  3069 caffe_interface.cpp:125] Batch 109, top-1 = 0.9
I0109 19:32:14.383357  3069 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0109 19:32:14.388273  3069 caffe_interface.cpp:125] Batch 110, loss = 0.266244
I0109 19:32:14.388317  3069 caffe_interface.cpp:125] Batch 110, top-1 = 0.9
I0109 19:32:14.388324  3069 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 19:32:14.392961  3069 caffe_interface.cpp:125] Batch 111, loss = 0.286505
I0109 19:32:14.392987  3069 caffe_interface.cpp:125] Batch 111, top-1 = 0.88
I0109 19:32:14.392992  3069 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0109 19:32:14.397609  3069 caffe_interface.cpp:125] Batch 112, loss = 0.484818
I0109 19:32:14.397644  3069 caffe_interface.cpp:125] Batch 112, top-1 = 0.82
I0109 19:32:14.397653  3069 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0109 19:32:14.402297  3069 caffe_interface.cpp:125] Batch 113, loss = 0.554846
I0109 19:32:14.402330  3069 caffe_interface.cpp:125] Batch 113, top-1 = 0.82
I0109 19:32:14.402339  3069 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0109 19:32:14.406935  3069 caffe_interface.cpp:125] Batch 114, loss = 0.694976
I0109 19:32:14.406960  3069 caffe_interface.cpp:125] Batch 114, top-1 = 0.8
I0109 19:32:14.406965  3069 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0109 19:32:14.411510  3069 caffe_interface.cpp:125] Batch 115, loss = 0.397824
I0109 19:32:14.411535  3069 caffe_interface.cpp:125] Batch 115, top-1 = 0.86
I0109 19:32:14.411540  3069 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 19:32:14.416143  3069 caffe_interface.cpp:125] Batch 116, loss = 0.582659
I0109 19:32:14.416167  3069 caffe_interface.cpp:125] Batch 116, top-1 = 0.76
I0109 19:32:14.416174  3069 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0109 19:32:14.420773  3069 caffe_interface.cpp:125] Batch 117, loss = 0.387592
I0109 19:32:14.420797  3069 caffe_interface.cpp:125] Batch 117, top-1 = 0.92
I0109 19:32:14.420804  3069 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0109 19:32:14.425382  3069 caffe_interface.cpp:125] Batch 118, loss = 0.707068
I0109 19:32:14.425407  3069 caffe_interface.cpp:125] Batch 118, top-1 = 0.76
I0109 19:32:14.425413  3069 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0109 19:32:14.430007  3069 caffe_interface.cpp:125] Batch 119, loss = 0.301349
I0109 19:32:14.430032  3069 caffe_interface.cpp:125] Batch 119, top-1 = 0.88
I0109 19:32:14.430037  3069 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 19:32:14.434631  3069 caffe_interface.cpp:125] Batch 120, loss = 0.593289
I0109 19:32:14.434653  3069 caffe_interface.cpp:125] Batch 120, top-1 = 0.76
I0109 19:32:14.434659  3069 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0109 19:32:14.439245  3069 caffe_interface.cpp:125] Batch 121, loss = 0.482738
I0109 19:32:14.439267  3069 caffe_interface.cpp:125] Batch 121, top-1 = 0.86
I0109 19:32:14.439272  3069 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 19:32:14.443866  3069 caffe_interface.cpp:125] Batch 122, loss = 0.597999
I0109 19:32:14.443889  3069 caffe_interface.cpp:125] Batch 122, top-1 = 0.82
I0109 19:32:14.443895  3069 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 19:32:14.448509  3069 caffe_interface.cpp:125] Batch 123, loss = 0.407164
I0109 19:32:14.448534  3069 caffe_interface.cpp:125] Batch 123, top-1 = 0.86
I0109 19:32:14.448539  3069 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0109 19:32:14.453130  3069 caffe_interface.cpp:125] Batch 124, loss = 0.310717
I0109 19:32:14.453152  3069 caffe_interface.cpp:125] Batch 124, top-1 = 0.84
I0109 19:32:14.453160  3069 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 19:32:14.457790  3069 caffe_interface.cpp:125] Batch 125, loss = 0.59526
I0109 19:32:14.457818  3069 caffe_interface.cpp:125] Batch 125, top-1 = 0.84
I0109 19:32:14.457823  3069 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0109 19:32:14.462419  3069 caffe_interface.cpp:125] Batch 126, loss = 0.491401
I0109 19:32:14.462445  3069 caffe_interface.cpp:125] Batch 126, top-1 = 0.84
I0109 19:32:14.462450  3069 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 19:32:14.467056  3069 caffe_interface.cpp:125] Batch 127, loss = 0.332715
I0109 19:32:14.467080  3069 caffe_interface.cpp:125] Batch 127, top-1 = 0.82
I0109 19:32:14.467087  3069 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 19:32:14.471704  3069 caffe_interface.cpp:125] Batch 128, loss = 0.81391
I0109 19:32:14.471727  3069 caffe_interface.cpp:125] Batch 128, top-1 = 0.78
I0109 19:32:14.471734  3069 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0109 19:32:14.476351  3069 caffe_interface.cpp:125] Batch 129, loss = 0.335942
I0109 19:32:14.476373  3069 caffe_interface.cpp:125] Batch 129, top-1 = 0.88
I0109 19:32:14.476378  3069 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 19:32:14.480975  3069 caffe_interface.cpp:125] Batch 130, loss = 0.403207
I0109 19:32:14.481000  3069 caffe_interface.cpp:125] Batch 130, top-1 = 0.88
I0109 19:32:14.481005  3069 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 19:32:14.485616  3069 caffe_interface.cpp:125] Batch 131, loss = 0.632645
I0109 19:32:14.485640  3069 caffe_interface.cpp:125] Batch 131, top-1 = 0.74
I0109 19:32:14.485644  3069 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 19:32:14.490259  3069 caffe_interface.cpp:125] Batch 132, loss = 0.379126
I0109 19:32:14.490284  3069 caffe_interface.cpp:125] Batch 132, top-1 = 0.86
I0109 19:32:14.490290  3069 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 19:32:14.494901  3069 caffe_interface.cpp:125] Batch 133, loss = 0.269347
I0109 19:32:14.494926  3069 caffe_interface.cpp:125] Batch 133, top-1 = 0.92
I0109 19:32:14.494931  3069 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0109 19:32:14.499528  3069 caffe_interface.cpp:125] Batch 134, loss = 0.566187
I0109 19:32:14.499554  3069 caffe_interface.cpp:125] Batch 134, top-1 = 0.82
I0109 19:32:14.499560  3069 caffe_interface.cpp:125] Batch 134, top-5 = 0.98
I0109 19:32:14.504151  3069 caffe_interface.cpp:125] Batch 135, loss = 0.358924
I0109 19:32:14.504175  3069 caffe_interface.cpp:125] Batch 135, top-1 = 0.86
I0109 19:32:14.504181  3069 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0109 19:32:14.508782  3069 caffe_interface.cpp:125] Batch 136, loss = 0.198548
I0109 19:32:14.508810  3069 caffe_interface.cpp:125] Batch 136, top-1 = 0.86
I0109 19:32:14.508817  3069 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 19:32:14.513407  3069 caffe_interface.cpp:125] Batch 137, loss = 0.450642
I0109 19:32:14.513437  3069 caffe_interface.cpp:125] Batch 137, top-1 = 0.9
I0109 19:32:14.513443  3069 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 19:32:14.518087  3069 caffe_interface.cpp:125] Batch 138, loss = 0.691741
I0109 19:32:14.518115  3069 caffe_interface.cpp:125] Batch 138, top-1 = 0.82
I0109 19:32:14.518121  3069 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0109 19:32:14.522739  3069 caffe_interface.cpp:125] Batch 139, loss = 0.457685
I0109 19:32:14.522765  3069 caffe_interface.cpp:125] Batch 139, top-1 = 0.82
I0109 19:32:14.522771  3069 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 19:32:14.527388  3069 caffe_interface.cpp:125] Batch 140, loss = 0.620247
I0109 19:32:14.527412  3069 caffe_interface.cpp:125] Batch 140, top-1 = 0.82
I0109 19:32:14.527420  3069 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0109 19:32:14.532048  3069 caffe_interface.cpp:125] Batch 141, loss = 0.453661
I0109 19:32:14.532071  3069 caffe_interface.cpp:125] Batch 141, top-1 = 0.88
I0109 19:32:14.532078  3069 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0109 19:32:14.536700  3069 caffe_interface.cpp:125] Batch 142, loss = 0.375464
I0109 19:32:14.536725  3069 caffe_interface.cpp:125] Batch 142, top-1 = 0.86
I0109 19:32:14.536731  3069 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 19:32:14.541337  3069 caffe_interface.cpp:125] Batch 143, loss = 0.234686
I0109 19:32:14.541363  3069 caffe_interface.cpp:125] Batch 143, top-1 = 0.9
I0109 19:32:14.541368  3069 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 19:32:14.545970  3069 caffe_interface.cpp:125] Batch 144, loss = 0.343598
I0109 19:32:14.545994  3069 caffe_interface.cpp:125] Batch 144, top-1 = 0.84
I0109 19:32:14.546000  3069 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0109 19:32:14.550607  3069 caffe_interface.cpp:125] Batch 145, loss = 0.598908
I0109 19:32:14.550632  3069 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0109 19:32:14.550662  3069 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0109 19:32:14.555284  3069 caffe_interface.cpp:125] Batch 146, loss = 0.50675
I0109 19:32:14.555311  3069 caffe_interface.cpp:125] Batch 146, top-1 = 0.82
I0109 19:32:14.555316  3069 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0109 19:32:14.559813  3069 caffe_interface.cpp:125] Batch 147, loss = 0.472282
I0109 19:32:14.559839  3069 caffe_interface.cpp:125] Batch 147, top-1 = 0.84
I0109 19:32:14.559844  3069 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0109 19:32:14.564347  3069 caffe_interface.cpp:125] Batch 148, loss = 0.6524
I0109 19:32:14.564371  3069 caffe_interface.cpp:125] Batch 148, top-1 = 0.8
I0109 19:32:14.564378  3069 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0109 19:32:14.568878  3069 caffe_interface.cpp:125] Batch 149, loss = 0.57092
I0109 19:32:14.568902  3069 caffe_interface.cpp:125] Batch 149, top-1 = 0.82
I0109 19:32:14.568908  3069 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0109 19:32:14.573418  3069 caffe_interface.cpp:125] Batch 150, loss = 0.395593
I0109 19:32:14.573444  3069 caffe_interface.cpp:125] Batch 150, top-1 = 0.88
I0109 19:32:14.573451  3069 caffe_interface.cpp:125] Batch 150, top-5 = 0.98
I0109 19:32:14.577941  3069 caffe_interface.cpp:125] Batch 151, loss = 0.219902
I0109 19:32:14.577966  3069 caffe_interface.cpp:125] Batch 151, top-1 = 0.9
I0109 19:32:14.577972  3069 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 19:32:14.582479  3069 caffe_interface.cpp:125] Batch 152, loss = 0.492894
I0109 19:32:14.582501  3069 caffe_interface.cpp:125] Batch 152, top-1 = 0.82
I0109 19:32:14.582509  3069 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0109 19:32:14.587014  3069 caffe_interface.cpp:125] Batch 153, loss = 0.335453
I0109 19:32:14.587038  3069 caffe_interface.cpp:125] Batch 153, top-1 = 0.86
I0109 19:32:14.587044  3069 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 19:32:14.591544  3069 caffe_interface.cpp:125] Batch 154, loss = 0.233623
I0109 19:32:14.591568  3069 caffe_interface.cpp:125] Batch 154, top-1 = 0.92
I0109 19:32:14.591574  3069 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 19:32:14.596077  3069 caffe_interface.cpp:125] Batch 155, loss = 0.1386
I0109 19:32:14.596102  3069 caffe_interface.cpp:125] Batch 155, top-1 = 0.9
I0109 19:32:14.596108  3069 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 19:32:14.600592  3069 caffe_interface.cpp:125] Batch 156, loss = 0.321404
I0109 19:32:14.600616  3069 caffe_interface.cpp:125] Batch 156, top-1 = 0.92
I0109 19:32:14.600623  3069 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 19:32:14.605137  3069 caffe_interface.cpp:125] Batch 157, loss = 0.541961
I0109 19:32:14.605161  3069 caffe_interface.cpp:125] Batch 157, top-1 = 0.76
I0109 19:32:14.605167  3069 caffe_interface.cpp:125] Batch 157, top-5 = 0.98
I0109 19:32:14.609673  3069 caffe_interface.cpp:125] Batch 158, loss = 0.726789
I0109 19:32:14.609695  3069 caffe_interface.cpp:125] Batch 158, top-1 = 0.8
I0109 19:32:14.609701  3069 caffe_interface.cpp:125] Batch 158, top-5 = 0.96
I0109 19:32:14.614162  3069 caffe_interface.cpp:125] Batch 159, loss = 0.339049
I0109 19:32:14.614187  3069 caffe_interface.cpp:125] Batch 159, top-1 = 0.9
I0109 19:32:14.614192  3069 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0109 19:32:14.618697  3069 caffe_interface.cpp:125] Batch 160, loss = 0.859914
I0109 19:32:14.618722  3069 caffe_interface.cpp:125] Batch 160, top-1 = 0.74
I0109 19:32:14.618728  3069 caffe_interface.cpp:125] Batch 160, top-5 = 0.98
I0109 19:32:14.623237  3069 caffe_interface.cpp:125] Batch 161, loss = 0.349387
I0109 19:32:14.623261  3069 caffe_interface.cpp:125] Batch 161, top-1 = 0.92
I0109 19:32:14.623267  3069 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0109 19:32:14.627776  3069 caffe_interface.cpp:125] Batch 162, loss = 0.678159
I0109 19:32:14.627800  3069 caffe_interface.cpp:125] Batch 162, top-1 = 0.82
I0109 19:32:14.627806  3069 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0109 19:32:14.632287  3069 caffe_interface.cpp:125] Batch 163, loss = 0.375402
I0109 19:32:14.632330  3069 caffe_interface.cpp:125] Batch 163, top-1 = 0.86
I0109 19:32:14.632338  3069 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0109 19:32:14.636838  3069 caffe_interface.cpp:125] Batch 164, loss = 0.485426
I0109 19:32:14.636863  3069 caffe_interface.cpp:125] Batch 164, top-1 = 0.82
I0109 19:32:14.636869  3069 caffe_interface.cpp:125] Batch 164, top-5 = 1
I0109 19:32:14.641372  3069 caffe_interface.cpp:125] Batch 165, loss = 0.572184
I0109 19:32:14.641396  3069 caffe_interface.cpp:125] Batch 165, top-1 = 0.84
I0109 19:32:14.641402  3069 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 19:32:14.645891  3069 caffe_interface.cpp:125] Batch 166, loss = 0.393628
I0109 19:32:14.645915  3069 caffe_interface.cpp:125] Batch 166, top-1 = 0.9
I0109 19:32:14.645920  3069 caffe_interface.cpp:125] Batch 166, top-5 = 0.96
I0109 19:32:14.650408  3069 caffe_interface.cpp:125] Batch 167, loss = 0.602354
I0109 19:32:14.650431  3069 caffe_interface.cpp:125] Batch 167, top-1 = 0.82
I0109 19:32:14.650437  3069 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0109 19:32:14.654934  3069 caffe_interface.cpp:125] Batch 168, loss = 0.702617
I0109 19:32:14.654959  3069 caffe_interface.cpp:125] Batch 168, top-1 = 0.8
I0109 19:32:14.654966  3069 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0109 19:32:14.659458  3069 caffe_interface.cpp:125] Batch 169, loss = 0.751477
I0109 19:32:14.659482  3069 caffe_interface.cpp:125] Batch 169, top-1 = 0.82
I0109 19:32:14.659488  3069 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0109 19:32:14.663992  3069 caffe_interface.cpp:125] Batch 170, loss = 0.195199
I0109 19:32:14.664016  3069 caffe_interface.cpp:125] Batch 170, top-1 = 0.88
I0109 19:32:14.664021  3069 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0109 19:32:14.668540  3069 caffe_interface.cpp:125] Batch 171, loss = 0.347747
I0109 19:32:14.668565  3069 caffe_interface.cpp:125] Batch 171, top-1 = 0.84
I0109 19:32:14.668570  3069 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 19:32:14.673094  3069 caffe_interface.cpp:125] Batch 172, loss = 0.345972
I0109 19:32:14.673118  3069 caffe_interface.cpp:125] Batch 172, top-1 = 0.86
I0109 19:32:14.673125  3069 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 19:32:14.677649  3069 caffe_interface.cpp:125] Batch 173, loss = 0.815929
I0109 19:32:14.677670  3069 caffe_interface.cpp:125] Batch 173, top-1 = 0.76
I0109 19:32:14.677675  3069 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0109 19:32:14.682196  3069 caffe_interface.cpp:125] Batch 174, loss = 0.795243
I0109 19:32:14.682220  3069 caffe_interface.cpp:125] Batch 174, top-1 = 0.76
I0109 19:32:14.682225  3069 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0109 19:32:14.686731  3069 caffe_interface.cpp:125] Batch 175, loss = 0.469707
I0109 19:32:14.686758  3069 caffe_interface.cpp:125] Batch 175, top-1 = 0.86
I0109 19:32:14.686764  3069 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0109 19:32:14.691287  3069 caffe_interface.cpp:125] Batch 176, loss = 0.739602
I0109 19:32:14.691310  3069 caffe_interface.cpp:125] Batch 176, top-1 = 0.82
I0109 19:32:14.691316  3069 caffe_interface.cpp:125] Batch 176, top-5 = 1
I0109 19:32:14.695827  3069 caffe_interface.cpp:125] Batch 177, loss = 0.488727
I0109 19:32:14.695852  3069 caffe_interface.cpp:125] Batch 177, top-1 = 0.88
I0109 19:32:14.695858  3069 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 19:32:14.700345  3069 caffe_interface.cpp:125] Batch 178, loss = 0.205353
I0109 19:32:14.700368  3069 caffe_interface.cpp:125] Batch 178, top-1 = 0.92
I0109 19:32:14.700374  3069 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 19:32:14.704877  3069 caffe_interface.cpp:125] Batch 179, loss = 0.326025
I0109 19:32:14.704902  3069 caffe_interface.cpp:125] Batch 179, top-1 = 0.88
I0109 19:32:14.704908  3069 caffe_interface.cpp:125] Batch 179, top-5 = 0.98
I0109 19:32:14.704912  3069 caffe_interface.cpp:130] Loss: 0.480206
I0109 19:32:14.704921  3069 caffe_interface.cpp:142] loss = 0.480206 (* 1 = 0.480206 loss)
I0109 19:32:14.704929  3069 caffe_interface.cpp:142] top-1 = 0.847889
I0109 19:32:14.704968  3069 caffe_interface.cpp:142] top-5 = 0.992778
I0109 19:32:14.719506  3069 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/sparse.caffemodel
I0109 19:32:14.719549  3069 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.2:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.847888887    | -0.0171109438  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 60749          | -11.1713886%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 40636928       | -17.1582756%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config2.prototxt
I0109 19:32:14.838080  3374 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0109 19:32:14.950600  3374 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 19:32:14.951148  3374 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 19:32:14.951174  3374 caffe_interface.cpp:493] Using GPUs 0
I0109 19:32:14.951443  3374 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 19:32:15.614282  3374 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt"
type: "SGD"
I0109 19:32:15.614460  3374 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0109 19:32:15.614821  3374 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 19:32:15.614850  3374 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 19:32:15.614856  3374 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 19:32:15.615062  3374 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 19:32:15.615167  3374 layer_factory.hpp:77] Creating layer data
I0109 19:32:15.615440  3374 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:15.616310  3374 net.cpp:94] Creating Layer data
I0109 19:32:15.616346  3374 net.cpp:409] data -> data
I0109 19:32:15.616369  3374 net.cpp:409] data -> label
I0109 19:32:15.616719  3385 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 19:32:15.616767  3385 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 19:32:15.616873  3374 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 19:32:15.616987  3374 data_layer.cpp:83] output data size: 128,3,32,32
I0109 19:32:15.627791  3374 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:15.628499  3374 net.cpp:144] Setting up data
I0109 19:32:15.628520  3374 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 19:32:15.628530  3374 net.cpp:151] Top shape: 128 (128)
I0109 19:32:15.628536  3374 net.cpp:159] Memory required for data: 1573376
I0109 19:32:15.628543  3374 layer_factory.hpp:77] Creating layer conv1
I0109 19:32:15.628564  3374 net.cpp:94] Creating Layer conv1
I0109 19:32:15.628594  3374 net.cpp:435] conv1 <- data
I0109 19:32:15.628638  3374 net.cpp:409] conv1 -> conv1
I0109 19:32:15.630070  3374 net.cpp:144] Setting up conv1
I0109 19:32:15.630095  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.630105  3374 net.cpp:159] Memory required for data: 18350592
I0109 19:32:15.630131  3374 layer_factory.hpp:77] Creating layer bn1
I0109 19:32:15.630151  3374 net.cpp:94] Creating Layer bn1
I0109 19:32:15.630182  3374 net.cpp:435] bn1 <- conv1
I0109 19:32:15.630211  3374 net.cpp:409] bn1 -> scale1
I0109 19:32:15.631304  3374 net.cpp:144] Setting up bn1
I0109 19:32:15.631350  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.631371  3374 net.cpp:159] Memory required for data: 35127808
I0109 19:32:15.631429  3374 layer_factory.hpp:77] Creating layer relu1
I0109 19:32:15.631477  3374 net.cpp:94] Creating Layer relu1
I0109 19:32:15.631516  3374 net.cpp:435] relu1 <- scale1
I0109 19:32:15.631559  3374 net.cpp:409] relu1 -> relu1
I0109 19:32:15.631639  3374 net.cpp:144] Setting up relu1
I0109 19:32:15.631664  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.631672  3374 net.cpp:159] Memory required for data: 51905024
I0109 19:32:15.631760  3374 layer_factory.hpp:77] Creating layer conv2
I0109 19:32:15.631822  3374 net.cpp:94] Creating Layer conv2
I0109 19:32:15.631847  3374 net.cpp:435] conv2 <- relu1
I0109 19:32:15.631896  3374 net.cpp:409] conv2 -> conv2
I0109 19:32:15.633486  3374 net.cpp:144] Setting up conv2
I0109 19:32:15.633505  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.633510  3374 net.cpp:159] Memory required for data: 68682240
I0109 19:32:15.633520  3374 layer_factory.hpp:77] Creating layer bn2
I0109 19:32:15.633535  3374 net.cpp:94] Creating Layer bn2
I0109 19:32:15.633553  3374 net.cpp:435] bn2 <- conv2
I0109 19:32:15.633644  3374 net.cpp:409] bn2 -> scale2
I0109 19:32:15.634661  3374 net.cpp:144] Setting up bn2
I0109 19:32:15.634753  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.634827  3374 net.cpp:159] Memory required for data: 85459456
I0109 19:32:15.634905  3374 layer_factory.hpp:77] Creating layer relu2
I0109 19:32:15.634984  3374 net.cpp:94] Creating Layer relu2
I0109 19:32:15.635071  3374 net.cpp:435] relu2 <- scale2
I0109 19:32:15.635160  3374 net.cpp:409] relu2 -> relu2
I0109 19:32:15.635363  3374 net.cpp:144] Setting up relu2
I0109 19:32:15.635447  3374 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:32:15.635519  3374 net.cpp:159] Memory required for data: 102236672
I0109 19:32:15.635587  3374 layer_factory.hpp:77] Creating layer pool1
I0109 19:32:15.635669  3374 net.cpp:94] Creating Layer pool1
I0109 19:32:15.635743  3374 net.cpp:435] pool1 <- relu2
I0109 19:32:15.635818  3374 net.cpp:409] pool1 -> pool1
I0109 19:32:15.635960  3374 net.cpp:144] Setting up pool1
I0109 19:32:15.636047  3374 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:32:15.636122  3374 net.cpp:159] Memory required for data: 106430976
I0109 19:32:15.636206  3374 layer_factory.hpp:77] Creating layer drop1
I0109 19:32:15.636282  3374 net.cpp:94] Creating Layer drop1
I0109 19:32:15.636368  3374 net.cpp:435] drop1 <- pool1
I0109 19:32:15.636436  3374 net.cpp:409] drop1 -> drop1
I0109 19:32:15.636559  3374 net.cpp:144] Setting up drop1
I0109 19:32:15.636653  3374 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:32:15.636739  3374 net.cpp:159] Memory required for data: 110625280
I0109 19:32:15.636759  3374 layer_factory.hpp:77] Creating layer conv3
I0109 19:32:15.636804  3374 net.cpp:94] Creating Layer conv3
I0109 19:32:15.636850  3374 net.cpp:435] conv3 <- drop1
I0109 19:32:15.636868  3374 net.cpp:409] conv3 -> conv3
I0109 19:32:15.638617  3374 net.cpp:144] Setting up conv3
I0109 19:32:15.638723  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.638808  3374 net.cpp:159] Memory required for data: 119013888
I0109 19:32:15.638897  3374 layer_factory.hpp:77] Creating layer bn3
I0109 19:32:15.638973  3374 net.cpp:94] Creating Layer bn3
I0109 19:32:15.639040  3374 net.cpp:435] bn3 <- conv3
I0109 19:32:15.639113  3374 net.cpp:409] bn3 -> scale3
I0109 19:32:15.640262  3374 net.cpp:144] Setting up bn3
I0109 19:32:15.640285  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.640291  3374 net.cpp:159] Memory required for data: 127402496
I0109 19:32:15.640373  3374 layer_factory.hpp:77] Creating layer relu3
I0109 19:32:15.640399  3374 net.cpp:94] Creating Layer relu3
I0109 19:32:15.640408  3374 net.cpp:435] relu3 <- scale3
I0109 19:32:15.640491  3374 net.cpp:409] relu3 -> relu3
I0109 19:32:15.640622  3374 net.cpp:144] Setting up relu3
I0109 19:32:15.640702  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.640722  3374 net.cpp:159] Memory required for data: 135791104
I0109 19:32:15.640730  3374 layer_factory.hpp:77] Creating layer conv4
I0109 19:32:15.640822  3374 net.cpp:94] Creating Layer conv4
I0109 19:32:15.640908  3374 net.cpp:435] conv4 <- relu3
I0109 19:32:15.640993  3374 net.cpp:409] conv4 -> conv4
I0109 19:32:15.641969  3374 net.cpp:144] Setting up conv4
I0109 19:32:15.641991  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.642000  3374 net.cpp:159] Memory required for data: 144179712
I0109 19:32:15.642011  3374 layer_factory.hpp:77] Creating layer bn4
I0109 19:32:15.642102  3374 net.cpp:94] Creating Layer bn4
I0109 19:32:15.642180  3374 net.cpp:435] bn4 <- conv4
I0109 19:32:15.642240  3374 net.cpp:409] bn4 -> scale4
I0109 19:32:15.643236  3374 net.cpp:144] Setting up bn4
I0109 19:32:15.643328  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.643375  3374 net.cpp:159] Memory required for data: 152568320
I0109 19:32:15.643532  3374 layer_factory.hpp:77] Creating layer relu4
I0109 19:32:15.643566  3374 net.cpp:94] Creating Layer relu4
I0109 19:32:15.643575  3374 net.cpp:435] relu4 <- scale4
I0109 19:32:15.643589  3374 net.cpp:409] relu4 -> relu4
I0109 19:32:15.643642  3374 net.cpp:144] Setting up relu4
I0109 19:32:15.643663  3374 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:32:15.643671  3374 net.cpp:159] Memory required for data: 160956928
I0109 19:32:15.643676  3374 layer_factory.hpp:77] Creating layer pool2
I0109 19:32:15.643689  3374 net.cpp:94] Creating Layer pool2
I0109 19:32:15.643697  3374 net.cpp:435] pool2 <- relu4
I0109 19:32:15.643710  3374 net.cpp:409] pool2 -> pool2
I0109 19:32:15.643771  3374 net.cpp:144] Setting up pool2
I0109 19:32:15.643787  3374 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:32:15.643791  3374 net.cpp:159] Memory required for data: 163054080
I0109 19:32:15.643795  3374 layer_factory.hpp:77] Creating layer drop2
I0109 19:32:15.643807  3374 net.cpp:94] Creating Layer drop2
I0109 19:32:15.643822  3374 net.cpp:435] drop2 <- pool2
I0109 19:32:15.643836  3374 net.cpp:409] drop2 -> drop2
I0109 19:32:15.643895  3374 net.cpp:144] Setting up drop2
I0109 19:32:15.643910  3374 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:32:15.643915  3374 net.cpp:159] Memory required for data: 165151232
I0109 19:32:15.643919  3374 layer_factory.hpp:77] Creating layer fc1
I0109 19:32:15.643934  3374 net.cpp:94] Creating Layer fc1
I0109 19:32:15.643947  3374 net.cpp:435] fc1 <- drop2
I0109 19:32:15.643985  3374 net.cpp:409] fc1 -> fc1
I0109 19:32:15.665226  3374 net.cpp:144] Setting up fc1
I0109 19:32:15.665251  3374 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:32:15.665256  3374 net.cpp:159] Memory required for data: 165413376
I0109 19:32:15.665266  3374 layer_factory.hpp:77] Creating layer bn5
I0109 19:32:15.665277  3374 net.cpp:94] Creating Layer bn5
I0109 19:32:15.665293  3374 net.cpp:435] bn5 <- fc1
I0109 19:32:15.665308  3374 net.cpp:409] bn5 -> scale5
I0109 19:32:15.665938  3374 net.cpp:144] Setting up bn5
I0109 19:32:15.665958  3374 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:32:15.665962  3374 net.cpp:159] Memory required for data: 165675520
I0109 19:32:15.665985  3374 layer_factory.hpp:77] Creating layer relu5
I0109 19:32:15.666005  3374 net.cpp:94] Creating Layer relu5
I0109 19:32:15.666013  3374 net.cpp:435] relu5 <- scale5
I0109 19:32:15.666026  3374 net.cpp:409] relu5 -> relu5
I0109 19:32:15.666062  3374 net.cpp:144] Setting up relu5
I0109 19:32:15.666082  3374 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:32:15.666090  3374 net.cpp:159] Memory required for data: 165937664
I0109 19:32:15.666095  3374 layer_factory.hpp:77] Creating layer drop3
I0109 19:32:15.666110  3374 net.cpp:94] Creating Layer drop3
I0109 19:32:15.666126  3374 net.cpp:435] drop3 <- relu5
I0109 19:32:15.666138  3374 net.cpp:409] drop3 -> drop3
I0109 19:32:15.666193  3374 net.cpp:144] Setting up drop3
I0109 19:32:15.666213  3374 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:32:15.666219  3374 net.cpp:159] Memory required for data: 166199808
I0109 19:32:15.666224  3374 layer_factory.hpp:77] Creating layer fc2
I0109 19:32:15.666235  3374 net.cpp:94] Creating Layer fc2
I0109 19:32:15.666249  3374 net.cpp:435] fc2 <- drop3
I0109 19:32:15.666263  3374 net.cpp:409] fc2 -> fc2
I0109 19:32:15.666450  3374 net.cpp:144] Setting up fc2
I0109 19:32:15.666465  3374 net.cpp:151] Top shape: 128 10 (1280)
I0109 19:32:15.666468  3374 net.cpp:159] Memory required for data: 166204928
I0109 19:32:15.666476  3374 layer_factory.hpp:77] Creating layer loss
I0109 19:32:15.666486  3374 net.cpp:94] Creating Layer loss
I0109 19:32:15.666501  3374 net.cpp:435] loss <- fc2
I0109 19:32:15.666508  3374 net.cpp:435] loss <- label
I0109 19:32:15.666524  3374 net.cpp:409] loss -> loss
I0109 19:32:15.666538  3374 layer_factory.hpp:77] Creating layer loss
I0109 19:32:15.667340  3374 net.cpp:144] Setting up loss
I0109 19:32:15.667361  3374 net.cpp:151] Top shape: (1)
I0109 19:32:15.667367  3374 net.cpp:154]     with loss weight 1
I0109 19:32:15.667394  3374 net.cpp:159] Memory required for data: 166204932
I0109 19:32:15.667402  3374 net.cpp:220] loss needs backward computation.
I0109 19:32:15.667426  3374 net.cpp:220] fc2 needs backward computation.
I0109 19:32:15.667435  3374 net.cpp:220] drop3 needs backward computation.
I0109 19:32:15.667441  3374 net.cpp:220] relu5 needs backward computation.
I0109 19:32:15.667450  3374 net.cpp:220] bn5 needs backward computation.
I0109 19:32:15.667456  3374 net.cpp:220] fc1 needs backward computation.
I0109 19:32:15.667465  3374 net.cpp:220] drop2 needs backward computation.
I0109 19:32:15.667472  3374 net.cpp:220] pool2 needs backward computation.
I0109 19:32:15.667477  3374 net.cpp:220] relu4 needs backward computation.
I0109 19:32:15.667486  3374 net.cpp:220] bn4 needs backward computation.
I0109 19:32:15.667493  3374 net.cpp:220] conv4 needs backward computation.
I0109 19:32:15.667502  3374 net.cpp:220] relu3 needs backward computation.
I0109 19:32:15.667510  3374 net.cpp:220] bn3 needs backward computation.
I0109 19:32:15.667518  3374 net.cpp:220] conv3 needs backward computation.
I0109 19:32:15.667526  3374 net.cpp:220] drop1 needs backward computation.
I0109 19:32:15.667534  3374 net.cpp:220] pool1 needs backward computation.
I0109 19:32:15.667541  3374 net.cpp:220] relu2 needs backward computation.
I0109 19:32:15.667549  3374 net.cpp:220] bn2 needs backward computation.
I0109 19:32:15.667557  3374 net.cpp:220] conv2 needs backward computation.
I0109 19:32:15.667562  3374 net.cpp:220] relu1 needs backward computation.
I0109 19:32:15.667593  3374 net.cpp:220] bn1 needs backward computation.
I0109 19:32:15.667599  3374 net.cpp:220] conv1 needs backward computation.
I0109 19:32:15.667608  3374 net.cpp:222] data does not need backward computation.
I0109 19:32:15.667614  3374 net.cpp:264] This network produces output loss
I0109 19:32:15.667654  3374 net.cpp:284] Network initialization done.
I0109 19:32:15.668010  3374 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0109 19:32:15.668078  3374 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:32:15.668313  3374 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:32:15.668491  3374 layer_factory.hpp:77] Creating layer data
I0109 19:32:15.668591  3374 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:15.668769  3374 net.cpp:94] Creating Layer data
I0109 19:32:15.668804  3374 net.cpp:409] data -> data
I0109 19:32:15.668834  3374 net.cpp:409] data -> label
I0109 19:32:15.670115  3391 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:32:15.670153  3391 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:32:15.670285  3374 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:32:15.670459  3374 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:32:15.678601  3374 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:32:15.678673  3374 net.cpp:144] Setting up data
I0109 19:32:15.678689  3374 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:32:15.678700  3374 net.cpp:151] Top shape: 50 (50)
I0109 19:32:15.678706  3374 net.cpp:159] Memory required for data: 614600
I0109 19:32:15.678714  3374 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:32:15.678733  3374 net.cpp:94] Creating Layer label_data_1_split
I0109 19:32:15.678740  3374 net.cpp:435] label_data_1_split <- label
I0109 19:32:15.678750  3374 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:32:15.678776  3374 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:32:15.678793  3374 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:32:15.678918  3374 net.cpp:144] Setting up label_data_1_split
I0109 19:32:15.678936  3374 net.cpp:151] Top shape: 50 (50)
I0109 19:32:15.678946  3374 net.cpp:151] Top shape: 50 (50)
I0109 19:32:15.678952  3374 net.cpp:151] Top shape: 50 (50)
I0109 19:32:15.678958  3374 net.cpp:159] Memory required for data: 615200
I0109 19:32:15.678964  3374 layer_factory.hpp:77] Creating layer conv1
I0109 19:32:15.678990  3374 net.cpp:94] Creating Layer conv1
I0109 19:32:15.679002  3374 net.cpp:435] conv1 <- data
I0109 19:32:15.679015  3374 net.cpp:409] conv1 -> conv1
I0109 19:32:15.679337  3374 net.cpp:144] Setting up conv1
I0109 19:32:15.679359  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.679368  3374 net.cpp:159] Memory required for data: 7168800
I0109 19:32:15.679385  3374 layer_factory.hpp:77] Creating layer bn1
I0109 19:32:15.679406  3374 net.cpp:94] Creating Layer bn1
I0109 19:32:15.679416  3374 net.cpp:435] bn1 <- conv1
I0109 19:32:15.679431  3374 net.cpp:409] bn1 -> scale1
I0109 19:32:15.680490  3374 net.cpp:144] Setting up bn1
I0109 19:32:15.680508  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.680517  3374 net.cpp:159] Memory required for data: 13722400
I0109 19:32:15.680536  3374 layer_factory.hpp:77] Creating layer relu1
I0109 19:32:15.680557  3374 net.cpp:94] Creating Layer relu1
I0109 19:32:15.680565  3374 net.cpp:435] relu1 <- scale1
I0109 19:32:15.680578  3374 net.cpp:409] relu1 -> relu1
I0109 19:32:15.680780  3374 net.cpp:144] Setting up relu1
I0109 19:32:15.680799  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.680806  3374 net.cpp:159] Memory required for data: 20276000
I0109 19:32:15.680814  3374 layer_factory.hpp:77] Creating layer conv2
I0109 19:32:15.680831  3374 net.cpp:94] Creating Layer conv2
I0109 19:32:15.680842  3374 net.cpp:435] conv2 <- relu1
I0109 19:32:15.680858  3374 net.cpp:409] conv2 -> conv2
I0109 19:32:15.681401  3374 net.cpp:144] Setting up conv2
I0109 19:32:15.681421  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.681427  3374 net.cpp:159] Memory required for data: 26829600
I0109 19:32:15.681445  3374 layer_factory.hpp:77] Creating layer bn2
I0109 19:32:15.681468  3374 net.cpp:94] Creating Layer bn2
I0109 19:32:15.681480  3374 net.cpp:435] bn2 <- conv2
I0109 19:32:15.681496  3374 net.cpp:409] bn2 -> scale2
I0109 19:32:15.682262  3374 net.cpp:144] Setting up bn2
I0109 19:32:15.682286  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.682296  3374 net.cpp:159] Memory required for data: 33383200
I0109 19:32:15.682312  3374 layer_factory.hpp:77] Creating layer relu2
I0109 19:32:15.682323  3374 net.cpp:94] Creating Layer relu2
I0109 19:32:15.682332  3374 net.cpp:435] relu2 <- scale2
I0109 19:32:15.682343  3374 net.cpp:409] relu2 -> relu2
I0109 19:32:15.682386  3374 net.cpp:144] Setting up relu2
I0109 19:32:15.682399  3374 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:32:15.682406  3374 net.cpp:159] Memory required for data: 39936800
I0109 19:32:15.682412  3374 layer_factory.hpp:77] Creating layer pool1
I0109 19:32:15.682422  3374 net.cpp:94] Creating Layer pool1
I0109 19:32:15.682430  3374 net.cpp:435] pool1 <- relu2
I0109 19:32:15.682443  3374 net.cpp:409] pool1 -> pool1
I0109 19:32:15.682538  3374 net.cpp:144] Setting up pool1
I0109 19:32:15.682554  3374 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:32:15.682561  3374 net.cpp:159] Memory required for data: 41575200
I0109 19:32:15.682567  3374 layer_factory.hpp:77] Creating layer drop1
I0109 19:32:15.682576  3374 net.cpp:94] Creating Layer drop1
I0109 19:32:15.682585  3374 net.cpp:435] drop1 <- pool1
I0109 19:32:15.682597  3374 net.cpp:409] drop1 -> drop1
I0109 19:32:15.682699  3374 net.cpp:144] Setting up drop1
I0109 19:32:15.682725  3374 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:32:15.682732  3374 net.cpp:159] Memory required for data: 43213600
I0109 19:32:15.682740  3374 layer_factory.hpp:77] Creating layer conv3
I0109 19:32:15.682759  3374 net.cpp:94] Creating Layer conv3
I0109 19:32:15.682765  3374 net.cpp:435] conv3 <- drop1
I0109 19:32:15.682782  3374 net.cpp:409] conv3 -> conv3
I0109 19:32:15.683534  3374 net.cpp:144] Setting up conv3
I0109 19:32:15.683559  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.683568  3374 net.cpp:159] Memory required for data: 46490400
I0109 19:32:15.683578  3374 layer_factory.hpp:77] Creating layer bn3
I0109 19:32:15.683595  3374 net.cpp:94] Creating Layer bn3
I0109 19:32:15.683612  3374 net.cpp:435] bn3 <- conv3
I0109 19:32:15.683624  3374 net.cpp:409] bn3 -> scale3
I0109 19:32:15.685142  3374 net.cpp:144] Setting up bn3
I0109 19:32:15.685166  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.685175  3374 net.cpp:159] Memory required for data: 49767200
I0109 19:32:15.685199  3374 layer_factory.hpp:77] Creating layer relu3
I0109 19:32:15.685217  3374 net.cpp:94] Creating Layer relu3
I0109 19:32:15.685225  3374 net.cpp:435] relu3 <- scale3
I0109 19:32:15.685240  3374 net.cpp:409] relu3 -> relu3
I0109 19:32:15.685305  3374 net.cpp:144] Setting up relu3
I0109 19:32:15.685339  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.685366  3374 net.cpp:159] Memory required for data: 53044000
I0109 19:32:15.685394  3374 layer_factory.hpp:77] Creating layer conv4
I0109 19:32:15.685431  3374 net.cpp:94] Creating Layer conv4
I0109 19:32:15.685461  3374 net.cpp:435] conv4 <- relu3
I0109 19:32:15.685495  3374 net.cpp:409] conv4 -> conv4
I0109 19:32:15.686522  3374 net.cpp:144] Setting up conv4
I0109 19:32:15.686568  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.686599  3374 net.cpp:159] Memory required for data: 56320800
I0109 19:32:15.686630  3374 layer_factory.hpp:77] Creating layer bn4
I0109 19:32:15.686668  3374 net.cpp:94] Creating Layer bn4
I0109 19:32:15.686697  3374 net.cpp:435] bn4 <- conv4
I0109 19:32:15.686738  3374 net.cpp:409] bn4 -> scale4
I0109 19:32:15.688174  3374 net.cpp:144] Setting up bn4
I0109 19:32:15.688239  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.688247  3374 net.cpp:159] Memory required for data: 59597600
I0109 19:32:15.688263  3374 layer_factory.hpp:77] Creating layer relu4
I0109 19:32:15.688280  3374 net.cpp:94] Creating Layer relu4
I0109 19:32:15.688289  3374 net.cpp:435] relu4 <- scale4
I0109 19:32:15.688305  3374 net.cpp:409] relu4 -> relu4
I0109 19:32:15.688371  3374 net.cpp:144] Setting up relu4
I0109 19:32:15.688392  3374 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:32:15.688400  3374 net.cpp:159] Memory required for data: 62874400
I0109 19:32:15.688406  3374 layer_factory.hpp:77] Creating layer pool2
I0109 19:32:15.688433  3374 net.cpp:94] Creating Layer pool2
I0109 19:32:15.688441  3374 net.cpp:435] pool2 <- relu4
I0109 19:32:15.688454  3374 net.cpp:409] pool2 -> pool2
I0109 19:32:15.688568  3374 net.cpp:144] Setting up pool2
I0109 19:32:15.688591  3374 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:32:15.688596  3374 net.cpp:159] Memory required for data: 63693600
I0109 19:32:15.688603  3374 layer_factory.hpp:77] Creating layer drop2
I0109 19:32:15.688612  3374 net.cpp:94] Creating Layer drop2
I0109 19:32:15.688647  3374 net.cpp:435] drop2 <- pool2
I0109 19:32:15.688684  3374 net.cpp:409] drop2 -> drop2
I0109 19:32:15.688771  3374 net.cpp:144] Setting up drop2
I0109 19:32:15.688910  3374 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:32:15.688940  3374 net.cpp:159] Memory required for data: 64512800
I0109 19:32:15.688966  3374 layer_factory.hpp:77] Creating layer fc1
I0109 19:32:15.688997  3374 net.cpp:94] Creating Layer fc1
I0109 19:32:15.689024  3374 net.cpp:435] fc1 <- drop2
I0109 19:32:15.689059  3374 net.cpp:409] fc1 -> fc1
I0109 19:32:15.711603  3374 net.cpp:144] Setting up fc1
I0109 19:32:15.711632  3374 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:15.711637  3374 net.cpp:159] Memory required for data: 64615200
I0109 19:32:15.711644  3374 layer_factory.hpp:77] Creating layer bn5
I0109 19:32:15.711661  3374 net.cpp:94] Creating Layer bn5
I0109 19:32:15.711678  3374 net.cpp:435] bn5 <- fc1
I0109 19:32:15.711691  3374 net.cpp:409] bn5 -> scale5
I0109 19:32:15.712357  3374 net.cpp:144] Setting up bn5
I0109 19:32:15.712378  3374 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:15.712384  3374 net.cpp:159] Memory required for data: 64717600
I0109 19:32:15.712405  3374 layer_factory.hpp:77] Creating layer relu5
I0109 19:32:15.712427  3374 net.cpp:94] Creating Layer relu5
I0109 19:32:15.712435  3374 net.cpp:435] relu5 <- scale5
I0109 19:32:15.712446  3374 net.cpp:409] relu5 -> relu5
I0109 19:32:15.712489  3374 net.cpp:144] Setting up relu5
I0109 19:32:15.712507  3374 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:15.712512  3374 net.cpp:159] Memory required for data: 64820000
I0109 19:32:15.712519  3374 layer_factory.hpp:77] Creating layer drop3
I0109 19:32:15.712532  3374 net.cpp:94] Creating Layer drop3
I0109 19:32:15.712541  3374 net.cpp:435] drop3 <- relu5
I0109 19:32:15.712553  3374 net.cpp:409] drop3 -> drop3
I0109 19:32:15.712615  3374 net.cpp:144] Setting up drop3
I0109 19:32:15.712630  3374 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:32:15.712636  3374 net.cpp:159] Memory required for data: 64922400
I0109 19:32:15.712642  3374 layer_factory.hpp:77] Creating layer fc2
I0109 19:32:15.712658  3374 net.cpp:94] Creating Layer fc2
I0109 19:32:15.712671  3374 net.cpp:435] fc2 <- drop3
I0109 19:32:15.712685  3374 net.cpp:409] fc2 -> fc2
I0109 19:32:15.712867  3374 net.cpp:144] Setting up fc2
I0109 19:32:15.712885  3374 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:15.712893  3374 net.cpp:159] Memory required for data: 64924400
I0109 19:32:15.712903  3374 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:32:15.712913  3374 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:32:15.712921  3374 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:32:15.712934  3374 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:32:15.712954  3374 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:32:15.712965  3374 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:32:15.713026  3374 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:32:15.713040  3374 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:15.713048  3374 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:15.713054  3374 net.cpp:151] Top shape: 50 10 (500)
I0109 19:32:15.713060  3374 net.cpp:159] Memory required for data: 64930400
I0109 19:32:15.713066  3374 layer_factory.hpp:77] Creating layer loss
I0109 19:32:15.713081  3374 net.cpp:94] Creating Layer loss
I0109 19:32:15.713089  3374 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:32:15.713099  3374 net.cpp:435] loss <- label_data_1_split_0
I0109 19:32:15.713109  3374 net.cpp:409] loss -> loss
I0109 19:32:15.713122  3374 layer_factory.hpp:77] Creating layer loss
I0109 19:32:15.713239  3374 net.cpp:144] Setting up loss
I0109 19:32:15.713256  3374 net.cpp:151] Top shape: (1)
I0109 19:32:15.713263  3374 net.cpp:154]     with loss weight 1
I0109 19:32:15.713282  3374 net.cpp:159] Memory required for data: 64930404
I0109 19:32:15.713289  3374 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:32:15.713306  3374 net.cpp:94] Creating Layer accuracy-top1
I0109 19:32:15.713320  3374 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:32:15.713328  3374 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:32:15.713361  3374 net.cpp:409] accuracy-top1 -> top-1
I0109 19:32:15.713383  3374 net.cpp:144] Setting up accuracy-top1
I0109 19:32:15.713393  3374 net.cpp:151] Top shape: (1)
I0109 19:32:15.713398  3374 net.cpp:159] Memory required for data: 64930408
I0109 19:32:15.713413  3374 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:32:15.713431  3374 net.cpp:94] Creating Layer accuracy-top5
I0109 19:32:15.713438  3374 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:32:15.713449  3374 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:32:15.713457  3374 net.cpp:409] accuracy-top5 -> top-5
I0109 19:32:15.713472  3374 net.cpp:144] Setting up accuracy-top5
I0109 19:32:15.713484  3374 net.cpp:151] Top shape: (1)
I0109 19:32:15.713490  3374 net.cpp:159] Memory required for data: 64930412
I0109 19:32:15.713498  3374 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:32:15.713507  3374 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:32:15.713515  3374 net.cpp:220] loss needs backward computation.
I0109 19:32:15.713524  3374 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:32:15.713531  3374 net.cpp:220] fc2 needs backward computation.
I0109 19:32:15.713541  3374 net.cpp:220] drop3 needs backward computation.
I0109 19:32:15.713548  3374 net.cpp:220] relu5 needs backward computation.
I0109 19:32:15.713553  3374 net.cpp:220] bn5 needs backward computation.
I0109 19:32:15.713560  3374 net.cpp:220] fc1 needs backward computation.
I0109 19:32:15.713569  3374 net.cpp:220] drop2 needs backward computation.
I0109 19:32:15.713575  3374 net.cpp:220] pool2 needs backward computation.
I0109 19:32:15.713582  3374 net.cpp:220] relu4 needs backward computation.
I0109 19:32:15.713629  3374 net.cpp:220] bn4 needs backward computation.
I0109 19:32:15.713655  3374 net.cpp:220] conv4 needs backward computation.
I0109 19:32:15.713677  3374 net.cpp:220] relu3 needs backward computation.
I0109 19:32:15.713687  3374 net.cpp:220] bn3 needs backward computation.
I0109 19:32:15.713693  3374 net.cpp:220] conv3 needs backward computation.
I0109 19:32:15.713701  3374 net.cpp:220] drop1 needs backward computation.
I0109 19:32:15.713719  3374 net.cpp:220] pool1 needs backward computation.
I0109 19:32:15.713729  3374 net.cpp:220] relu2 needs backward computation.
I0109 19:32:15.713737  3374 net.cpp:220] bn2 needs backward computation.
I0109 19:32:15.713743  3374 net.cpp:220] conv2 needs backward computation.
I0109 19:32:15.713762  3374 net.cpp:220] relu1 needs backward computation.
I0109 19:32:15.713771  3374 net.cpp:220] bn1 needs backward computation.
I0109 19:32:15.713778  3374 net.cpp:220] conv1 needs backward computation.
I0109 19:32:15.713785  3374 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:32:15.713805  3374 net.cpp:222] data does not need backward computation.
I0109 19:32:15.713819  3374 net.cpp:264] This network produces output loss
I0109 19:32:15.713829  3374 net.cpp:264] This network produces output top-1
I0109 19:32:15.713836  3374 net.cpp:264] This network produces output top-5
I0109 19:32:15.713871  3374 net.cpp:284] Network initialization done.
I0109 19:32:15.713997  3374 solver.cpp:63] Solver scaffolding done.
I0109 19:32:15.715263  3374 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/sparse.caffemodel
I0109 19:32:15.780474  3374 caffe_interface.cpp:527] Starting Optimization
I0109 19:32:15.780526  3374 solver.cpp:335] Solving 
I0109 19:32:15.780535  3374 solver.cpp:336] Learning Rate Policy: poly
I0109 19:32:15.781879  3374 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 19:32:16.606021  3374 solver.cpp:517]     Test net output #0: loss = 0.480206 (* 1 = 0.480206 loss)
I0109 19:32:16.606076  3374 solver.cpp:517]     Test net output #1: top-1 = 0.847889
I0109 19:32:16.606088  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 19:32:16.653219  3374 solver.cpp:266] Iteration 0 (0 iter/s, 0.872621s/100 iter), loss = 0.0906281
I0109 19:32:16.653309  3374 solver.cpp:285]     Train net output #0: loss = 0.0906281 (* 1 = 0.0906281 loss)
I0109 19:32:16.653386  3374 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 19:32:19.947870  3374 solver.cpp:266] Iteration 100 (30.3527 iter/s, 3.2946s/100 iter), loss = 0.0887479
I0109 19:32:19.947938  3374 solver.cpp:285]     Train net output #0: loss = 0.0887479 (* 1 = 0.0887479 loss)
I0109 19:32:19.947953  3374 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 19:32:23.245370  3374 solver.cpp:266] Iteration 200 (30.3263 iter/s, 3.29746s/100 iter), loss = 0.143257
I0109 19:32:23.245432  3374 solver.cpp:285]     Train net output #0: loss = 0.143257 (* 1 = 0.143257 loss)
I0109 19:32:23.245445  3374 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 19:32:26.537118  3374 solver.cpp:266] Iteration 300 (30.3793 iter/s, 3.29172s/100 iter), loss = 0.129473
I0109 19:32:26.537184  3374 solver.cpp:285]     Train net output #0: loss = 0.129473 (* 1 = 0.129473 loss)
I0109 19:32:26.537196  3374 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 19:32:29.815384  3374 solver.cpp:266] Iteration 400 (30.5045 iter/s, 3.27821s/100 iter), loss = 0.214261
I0109 19:32:29.815449  3374 solver.cpp:285]     Train net output #0: loss = 0.214261 (* 1 = 0.214261 loss)
I0109 19:32:29.815461  3374 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 19:32:33.090883  3374 solver.cpp:266] Iteration 500 (30.53 iter/s, 3.27546s/100 iter), loss = 0.166988
I0109 19:32:33.090946  3374 solver.cpp:285]     Train net output #0: loss = 0.166988 (* 1 = 0.166988 loss)
I0109 19:32:33.090960  3374 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 19:32:36.367375  3374 solver.cpp:266] Iteration 600 (30.5208 iter/s, 3.27646s/100 iter), loss = 0.135123
I0109 19:32:36.367445  3374 solver.cpp:285]     Train net output #0: loss = 0.135123 (* 1 = 0.135123 loss)
I0109 19:32:36.367460  3374 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 19:32:39.641830  3374 solver.cpp:266] Iteration 700 (30.5401 iter/s, 3.27439s/100 iter), loss = 0.161566
I0109 19:32:39.641898  3374 solver.cpp:285]     Train net output #0: loss = 0.161566 (* 1 = 0.161566 loss)
I0109 19:32:39.641911  3374 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 19:32:42.914427  3374 solver.cpp:266] Iteration 800 (30.5571 iter/s, 3.27256s/100 iter), loss = 0.169837
I0109 19:32:42.914490  3374 solver.cpp:285]     Train net output #0: loss = 0.169837 (* 1 = 0.169837 loss)
I0109 19:32:42.914503  3374 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 19:32:46.208535  3374 solver.cpp:266] Iteration 900 (30.3576 iter/s, 3.29407s/100 iter), loss = 0.13044
I0109 19:32:46.208736  3374 solver.cpp:285]     Train net output #0: loss = 0.13044 (* 1 = 0.13044 loss)
I0109 19:32:46.208755  3374 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 19:32:49.477151  3374 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 19:32:50.294205  3374 solver.cpp:517]     Test net output #0: loss = 0.784391 (* 1 = 0.784391 loss)
I0109 19:32:50.294245  3374 solver.cpp:517]     Test net output #1: top-1 = 0.801
I0109 19:32:50.294253  3374 solver.cpp:517]     Test net output #2: top-5 = 0.982667
I0109 19:32:50.325088  3374 solver.cpp:266] Iteration 1000 (24.2931 iter/s, 4.11639s/100 iter), loss = 0.0939263
I0109 19:32:50.325171  3374 solver.cpp:285]     Train net output #0: loss = 0.0939263 (* 1 = 0.0939263 loss)
I0109 19:32:50.325189  3374 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 19:32:53.624573  3374 solver.cpp:266] Iteration 1100 (30.3085 iter/s, 3.29941s/100 iter), loss = 0.114219
I0109 19:32:53.624636  3374 solver.cpp:285]     Train net output #0: loss = 0.114219 (* 1 = 0.114219 loss)
I0109 19:32:53.624650  3374 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 19:32:56.932667  3374 solver.cpp:266] Iteration 1200 (30.2292 iter/s, 3.30806s/100 iter), loss = 0.177182
I0109 19:32:56.932735  3374 solver.cpp:285]     Train net output #0: loss = 0.177182 (* 1 = 0.177182 loss)
I0109 19:32:56.932749  3374 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 19:33:00.237327  3374 solver.cpp:266] Iteration 1300 (30.2606 iter/s, 3.30463s/100 iter), loss = 0.150306
I0109 19:33:00.237388  3374 solver.cpp:285]     Train net output #0: loss = 0.150306 (* 1 = 0.150306 loss)
I0109 19:33:00.237402  3374 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 19:33:03.556923  3374 solver.cpp:266] Iteration 1400 (30.1244 iter/s, 3.31956s/100 iter), loss = 0.182514
I0109 19:33:03.557005  3374 solver.cpp:285]     Train net output #0: loss = 0.182514 (* 1 = 0.182514 loss)
I0109 19:33:03.557021  3374 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 19:33:06.870375  3374 solver.cpp:266] Iteration 1500 (30.1807 iter/s, 3.31338s/100 iter), loss = 0.189118
I0109 19:33:06.870438  3374 solver.cpp:285]     Train net output #0: loss = 0.189118 (* 1 = 0.189118 loss)
I0109 19:33:06.870450  3374 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 19:33:10.187206  3374 solver.cpp:266] Iteration 1600 (30.1495 iter/s, 3.3168s/100 iter), loss = 0.238878
I0109 19:33:10.187270  3374 solver.cpp:285]     Train net output #0: loss = 0.238878 (* 1 = 0.238878 loss)
I0109 19:33:10.187283  3374 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 19:33:13.510336  3374 solver.cpp:266] Iteration 1700 (30.0924 iter/s, 3.3231s/100 iter), loss = 0.17474
I0109 19:33:13.510426  3374 solver.cpp:285]     Train net output #0: loss = 0.17474 (* 1 = 0.17474 loss)
I0109 19:33:13.510448  3374 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 19:33:16.847177  3374 solver.cpp:266] Iteration 1800 (29.9692 iter/s, 3.33676s/100 iter), loss = 0.278126
I0109 19:33:16.847373  3374 solver.cpp:285]     Train net output #0: loss = 0.278126 (* 1 = 0.278126 loss)
I0109 19:33:16.847393  3374 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 19:33:20.182335  3374 solver.cpp:266] Iteration 1900 (29.985 iter/s, 3.335s/100 iter), loss = 0.205367
I0109 19:33:20.182401  3374 solver.cpp:285]     Train net output #0: loss = 0.205367 (* 1 = 0.205367 loss)
I0109 19:33:20.182413  3374 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 19:33:23.485237  3374 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 19:33:24.310019  3374 solver.cpp:517]     Test net output #0: loss = 0.938347 (* 1 = 0.938347 loss)
I0109 19:33:24.310055  3374 solver.cpp:517]     Test net output #1: top-1 = 0.776
I0109 19:33:24.310062  3374 solver.cpp:517]     Test net output #2: top-5 = 0.980778
I0109 19:33:24.341447  3374 solver.cpp:266] Iteration 2000 (24.0437 iter/s, 4.15909s/100 iter), loss = 0.112629
I0109 19:33:24.341500  3374 solver.cpp:285]     Train net output #0: loss = 0.112629 (* 1 = 0.112629 loss)
I0109 19:33:24.341516  3374 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 19:33:27.667701  3374 solver.cpp:266] Iteration 2100 (30.0643 iter/s, 3.32621s/100 iter), loss = 0.173449
I0109 19:33:27.667765  3374 solver.cpp:285]     Train net output #0: loss = 0.173449 (* 1 = 0.173449 loss)
I0109 19:33:27.667778  3374 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 19:33:31.000138  3374 solver.cpp:266] Iteration 2200 (30.0084 iter/s, 3.3324s/100 iter), loss = 0.209606
I0109 19:33:31.000202  3374 solver.cpp:285]     Train net output #0: loss = 0.209606 (* 1 = 0.209606 loss)
I0109 19:33:31.000214  3374 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 19:33:34.328131  3374 solver.cpp:266] Iteration 2300 (30.0484 iter/s, 3.32796s/100 iter), loss = 0.126781
I0109 19:33:34.328205  3374 solver.cpp:285]     Train net output #0: loss = 0.126781 (* 1 = 0.126781 loss)
I0109 19:33:34.328220  3374 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 19:33:37.674186  3374 solver.cpp:266] Iteration 2400 (29.8865 iter/s, 3.34599s/100 iter), loss = 0.0910297
I0109 19:33:37.674268  3374 solver.cpp:285]     Train net output #0: loss = 0.0910297 (* 1 = 0.0910297 loss)
I0109 19:33:37.674283  3374 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 19:33:41.027088  3374 solver.cpp:266] Iteration 2500 (29.8253 iter/s, 3.35285s/100 iter), loss = 0.210442
I0109 19:33:41.027169  3374 solver.cpp:285]     Train net output #0: loss = 0.210442 (* 1 = 0.210442 loss)
I0109 19:33:41.027186  3374 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 19:33:44.360864  3374 solver.cpp:266] Iteration 2600 (29.9964 iter/s, 3.33373s/100 iter), loss = 0.213826
I0109 19:33:44.360932  3374 solver.cpp:285]     Train net output #0: loss = 0.213826 (* 1 = 0.213826 loss)
I0109 19:33:44.360946  3374 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 19:33:47.711661  3374 solver.cpp:266] Iteration 2700 (29.8442 iter/s, 3.35073s/100 iter), loss = 0.241707
I0109 19:33:47.711879  3374 solver.cpp:285]     Train net output #0: loss = 0.241707 (* 1 = 0.241707 loss)
I0109 19:33:47.711901  3374 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 19:33:51.057525  3374 solver.cpp:266] Iteration 2800 (29.8893 iter/s, 3.34568s/100 iter), loss = 0.263333
I0109 19:33:51.057610  3374 solver.cpp:285]     Train net output #0: loss = 0.263333 (* 1 = 0.263333 loss)
I0109 19:33:51.057631  3374 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 19:33:54.391696  3374 solver.cpp:266] Iteration 2900 (29.9928 iter/s, 3.33413s/100 iter), loss = 0.148682
I0109 19:33:54.391784  3374 solver.cpp:285]     Train net output #0: loss = 0.148683 (* 1 = 0.148683 loss)
I0109 19:33:54.391803  3374 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 19:33:57.703099  3374 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 19:33:58.530643  3374 solver.cpp:517]     Test net output #0: loss = 0.639948 (* 1 = 0.639948 loss)
I0109 19:33:58.530681  3374 solver.cpp:517]     Test net output #1: top-1 = 0.817111
I0109 19:33:58.530690  3374 solver.cpp:517]     Test net output #2: top-5 = 0.985
I0109 19:33:58.561952  3374 solver.cpp:266] Iteration 3000 (23.9796 iter/s, 4.17022s/100 iter), loss = 0.19871
I0109 19:33:58.561991  3374 solver.cpp:285]     Train net output #0: loss = 0.19871 (* 1 = 0.19871 loss)
I0109 19:33:58.562006  3374 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 19:34:01.897681  3374 solver.cpp:266] Iteration 3100 (29.9788 iter/s, 3.33569s/100 iter), loss = 0.207476
I0109 19:34:01.897763  3374 solver.cpp:285]     Train net output #0: loss = 0.207476 (* 1 = 0.207476 loss)
I0109 19:34:01.897778  3374 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 19:34:05.224167  3374 solver.cpp:266] Iteration 3200 (30.0622 iter/s, 3.32644s/100 iter), loss = 0.178443
I0109 19:34:05.224231  3374 solver.cpp:285]     Train net output #0: loss = 0.178443 (* 1 = 0.178443 loss)
I0109 19:34:05.224244  3374 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 19:34:08.560995  3374 solver.cpp:266] Iteration 3300 (29.9689 iter/s, 3.3368s/100 iter), loss = 0.188442
I0109 19:34:08.561071  3374 solver.cpp:285]     Train net output #0: loss = 0.188442 (* 1 = 0.188442 loss)
I0109 19:34:08.561086  3374 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 19:34:11.890260  3374 solver.cpp:266] Iteration 3400 (30.0373 iter/s, 3.3292s/100 iter), loss = 0.1608
I0109 19:34:11.890341  3374 solver.cpp:285]     Train net output #0: loss = 0.160801 (* 1 = 0.160801 loss)
I0109 19:34:11.890358  3374 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 19:34:15.231115  3374 solver.cpp:266] Iteration 3500 (29.9329 iter/s, 3.34081s/100 iter), loss = 0.242523
I0109 19:34:15.231181  3374 solver.cpp:285]     Train net output #0: loss = 0.242523 (* 1 = 0.242523 loss)
I0109 19:34:15.231195  3374 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 19:34:18.584743  3374 solver.cpp:266] Iteration 3600 (29.8188 iter/s, 3.35359s/100 iter), loss = 0.125146
I0109 19:34:18.584954  3374 solver.cpp:285]     Train net output #0: loss = 0.125146 (* 1 = 0.125146 loss)
I0109 19:34:18.584975  3374 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 19:34:21.937450  3374 solver.cpp:266] Iteration 3700 (29.8284 iter/s, 3.35251s/100 iter), loss = 0.122064
I0109 19:34:21.937531  3374 solver.cpp:285]     Train net output #0: loss = 0.122064 (* 1 = 0.122064 loss)
I0109 19:34:21.937547  3374 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 19:34:25.278245  3374 solver.cpp:266] Iteration 3800 (29.9334 iter/s, 3.34075s/100 iter), loss = 0.211301
I0109 19:34:25.278308  3374 solver.cpp:285]     Train net output #0: loss = 0.211301 (* 1 = 0.211301 loss)
I0109 19:34:25.278321  3374 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 19:34:28.625849  3374 solver.cpp:266] Iteration 3900 (29.8726 iter/s, 3.34755s/100 iter), loss = 0.146254
I0109 19:34:28.625931  3374 solver.cpp:285]     Train net output #0: loss = 0.146254 (* 1 = 0.146254 loss)
I0109 19:34:28.625946  3374 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 19:34:31.951253  3374 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 19:34:32.778635  3374 solver.cpp:517]     Test net output #0: loss = 0.574919 (* 1 = 0.574919 loss)
I0109 19:34:32.778676  3374 solver.cpp:517]     Test net output #1: top-1 = 0.831778
I0109 19:34:32.778683  3374 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0109 19:34:32.809985  3374 solver.cpp:266] Iteration 4000 (23.9 iter/s, 4.1841s/100 iter), loss = 0.119585
I0109 19:34:32.810066  3374 solver.cpp:285]     Train net output #0: loss = 0.119586 (* 1 = 0.119586 loss)
I0109 19:34:32.810083  3374 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 19:34:36.166656  3374 solver.cpp:266] Iteration 4100 (29.7918 iter/s, 3.35663s/100 iter), loss = 0.1336
I0109 19:34:36.166728  3374 solver.cpp:285]     Train net output #0: loss = 0.1336 (* 1 = 0.1336 loss)
I0109 19:34:36.166743  3374 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 19:34:39.514468  3374 solver.cpp:266] Iteration 4200 (29.8705 iter/s, 3.34778s/100 iter), loss = 0.188805
I0109 19:34:39.514537  3374 solver.cpp:285]     Train net output #0: loss = 0.188805 (* 1 = 0.188805 loss)
I0109 19:34:39.514551  3374 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 19:34:42.856632  3374 solver.cpp:266] Iteration 4300 (29.9212 iter/s, 3.34211s/100 iter), loss = 0.189185
I0109 19:34:42.856704  3374 solver.cpp:285]     Train net output #0: loss = 0.189186 (* 1 = 0.189186 loss)
I0109 19:34:42.856724  3374 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 19:34:46.212550  3374 solver.cpp:266] Iteration 4400 (29.7983 iter/s, 3.35589s/100 iter), loss = 0.167556
I0109 19:34:46.212633  3374 solver.cpp:285]     Train net output #0: loss = 0.167556 (* 1 = 0.167556 loss)
I0109 19:34:46.212653  3374 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 19:34:49.560364  3374 solver.cpp:266] Iteration 4500 (29.8705 iter/s, 3.34778s/100 iter), loss = 0.176505
I0109 19:34:49.560550  3374 solver.cpp:285]     Train net output #0: loss = 0.176505 (* 1 = 0.176505 loss)
I0109 19:34:49.560572  3374 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 19:34:52.908886  3374 solver.cpp:266] Iteration 4600 (29.8654 iter/s, 3.34836s/100 iter), loss = 0.254374
I0109 19:34:52.908958  3374 solver.cpp:285]     Train net output #0: loss = 0.254374 (* 1 = 0.254374 loss)
I0109 19:34:52.908977  3374 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 19:34:56.251243  3374 solver.cpp:266] Iteration 4700 (29.9192 iter/s, 3.34234s/100 iter), loss = 0.12908
I0109 19:34:56.251322  3374 solver.cpp:285]     Train net output #0: loss = 0.12908 (* 1 = 0.12908 loss)
I0109 19:34:56.251341  3374 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 19:34:59.600610  3374 solver.cpp:266] Iteration 4800 (29.8569 iter/s, 3.34931s/100 iter), loss = 0.274119
I0109 19:34:59.600698  3374 solver.cpp:285]     Train net output #0: loss = 0.274119 (* 1 = 0.274119 loss)
I0109 19:34:59.600718  3374 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 19:35:02.949335  3374 solver.cpp:266] Iteration 4900 (29.8625 iter/s, 3.34868s/100 iter), loss = 0.134201
I0109 19:35:02.949434  3374 solver.cpp:285]     Train net output #0: loss = 0.134201 (* 1 = 0.134201 loss)
I0109 19:35:02.949460  3374 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 19:35:06.265866  3374 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 19:35:07.092196  3374 solver.cpp:517]     Test net output #0: loss = 0.58552 (* 1 = 0.58552 loss)
I0109 19:35:07.092239  3374 solver.cpp:517]     Test net output #1: top-1 = 0.834889
I0109 19:35:07.092253  3374 solver.cpp:517]     Test net output #2: top-5 = 0.989667
I0109 19:35:07.123576  3374 solver.cpp:266] Iteration 5000 (23.9567 iter/s, 4.1742s/100 iter), loss = 0.129086
I0109 19:35:07.123658  3374 solver.cpp:285]     Train net output #0: loss = 0.129086 (* 1 = 0.129086 loss)
I0109 19:35:07.123679  3374 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 19:35:10.470329  3374 solver.cpp:266] Iteration 5100 (29.88 iter/s, 3.34672s/100 iter), loss = 0.159709
I0109 19:35:10.470396  3374 solver.cpp:285]     Train net output #0: loss = 0.159709 (* 1 = 0.159709 loss)
I0109 19:35:10.470413  3374 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 19:35:13.821631  3374 solver.cpp:266] Iteration 5200 (29.8396 iter/s, 3.35126s/100 iter), loss = 0.141825
I0109 19:35:13.821717  3374 solver.cpp:285]     Train net output #0: loss = 0.141825 (* 1 = 0.141825 loss)
I0109 19:35:13.821739  3374 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 19:35:17.169359  3374 solver.cpp:266] Iteration 5300 (29.8713 iter/s, 3.34769s/100 iter), loss = 0.212253
I0109 19:35:17.169423  3374 solver.cpp:285]     Train net output #0: loss = 0.212253 (* 1 = 0.212253 loss)
I0109 19:35:17.169437  3374 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 19:35:20.515218  3374 solver.cpp:266] Iteration 5400 (29.8879 iter/s, 3.34584s/100 iter), loss = 0.156527
I0109 19:35:20.515430  3374 solver.cpp:285]     Train net output #0: loss = 0.156527 (* 1 = 0.156527 loss)
I0109 19:35:20.515447  3374 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 19:35:23.869568  3374 solver.cpp:266] Iteration 5500 (29.8137 iter/s, 3.35416s/100 iter), loss = 0.174097
I0109 19:35:23.869670  3374 solver.cpp:285]     Train net output #0: loss = 0.174098 (* 1 = 0.174098 loss)
I0109 19:35:23.869686  3374 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 19:35:27.216912  3374 solver.cpp:266] Iteration 5600 (29.8749 iter/s, 3.34729s/100 iter), loss = 0.214025
I0109 19:35:27.216975  3374 solver.cpp:285]     Train net output #0: loss = 0.214025 (* 1 = 0.214025 loss)
I0109 19:35:27.216987  3374 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 19:35:30.567579  3374 solver.cpp:266] Iteration 5700 (29.845 iter/s, 3.35065s/100 iter), loss = 0.168314
I0109 19:35:30.567651  3374 solver.cpp:285]     Train net output #0: loss = 0.168314 (* 1 = 0.168314 loss)
I0109 19:35:30.567667  3374 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 19:35:33.912070  3374 solver.cpp:266] Iteration 5800 (29.9004 iter/s, 3.34444s/100 iter), loss = 0.135203
I0109 19:35:33.912135  3374 solver.cpp:285]     Train net output #0: loss = 0.135203 (* 1 = 0.135203 loss)
I0109 19:35:33.912149  3374 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 19:35:37.249970  3374 solver.cpp:266] Iteration 5900 (29.9591 iter/s, 3.33788s/100 iter), loss = 0.139464
I0109 19:35:37.250035  3374 solver.cpp:285]     Train net output #0: loss = 0.139464 (* 1 = 0.139464 loss)
I0109 19:35:37.250046  3374 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 19:35:40.560745  3374 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 19:35:41.385648  3374 solver.cpp:517]     Test net output #0: loss = 0.500516 (* 1 = 0.500516 loss)
I0109 19:35:41.385681  3374 solver.cpp:517]     Test net output #1: top-1 = 0.844666
I0109 19:35:41.385689  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:35:41.416996  3374 solver.cpp:266] Iteration 6000 (23.9979 iter/s, 4.16702s/100 iter), loss = 0.187167
I0109 19:35:41.417034  3374 solver.cpp:285]     Train net output #0: loss = 0.187167 (* 1 = 0.187167 loss)
I0109 19:35:41.417049  3374 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 19:35:44.741681  3374 solver.cpp:266] Iteration 6100 (30.0783 iter/s, 3.32466s/100 iter), loss = 0.202515
I0109 19:35:44.741749  3374 solver.cpp:285]     Train net output #0: loss = 0.202515 (* 1 = 0.202515 loss)
I0109 19:35:44.741763  3374 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 19:35:48.065501  3374 solver.cpp:266] Iteration 6200 (30.0861 iter/s, 3.3238s/100 iter), loss = 0.160858
I0109 19:35:48.065564  3374 solver.cpp:285]     Train net output #0: loss = 0.160858 (* 1 = 0.160858 loss)
I0109 19:35:48.065577  3374 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 19:35:51.377879  3374 solver.cpp:266] Iteration 6300 (30.1901 iter/s, 3.31235s/100 iter), loss = 0.124982
I0109 19:35:51.378108  3374 solver.cpp:285]     Train net output #0: loss = 0.124982 (* 1 = 0.124982 loss)
I0109 19:35:51.378125  3374 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 19:35:54.690610  3374 solver.cpp:266] Iteration 6400 (30.1885 iter/s, 3.31252s/100 iter), loss = 0.1845
I0109 19:35:54.690672  3374 solver.cpp:285]     Train net output #0: loss = 0.1845 (* 1 = 0.1845 loss)
I0109 19:35:54.690685  3374 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 19:35:58.007179  3374 solver.cpp:266] Iteration 6500 (30.1519 iter/s, 3.31654s/100 iter), loss = 0.238446
I0109 19:35:58.007242  3374 solver.cpp:285]     Train net output #0: loss = 0.238446 (* 1 = 0.238446 loss)
I0109 19:35:58.007257  3374 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 19:36:01.306442  3374 solver.cpp:266] Iteration 6600 (30.3101 iter/s, 3.29923s/100 iter), loss = 0.12051
I0109 19:36:01.306501  3374 solver.cpp:285]     Train net output #0: loss = 0.12051 (* 1 = 0.12051 loss)
I0109 19:36:01.306514  3374 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 19:36:04.614986  3374 solver.cpp:266] Iteration 6700 (30.2252 iter/s, 3.30849s/100 iter), loss = 0.178896
I0109 19:36:04.615047  3374 solver.cpp:285]     Train net output #0: loss = 0.178896 (* 1 = 0.178896 loss)
I0109 19:36:04.615061  3374 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 19:36:07.919322  3374 solver.cpp:266] Iteration 6800 (30.2636 iter/s, 3.3043s/100 iter), loss = 0.233042
I0109 19:36:07.919390  3374 solver.cpp:285]     Train net output #0: loss = 0.233042 (* 1 = 0.233042 loss)
I0109 19:36:07.919404  3374 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 19:36:11.217093  3374 solver.cpp:266] Iteration 6900 (30.3238 iter/s, 3.29774s/100 iter), loss = 0.0948199
I0109 19:36:11.217159  3374 solver.cpp:285]     Train net output #0: loss = 0.0948199 (* 1 = 0.0948199 loss)
I0109 19:36:11.217171  3374 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 19:36:14.496068  3374 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 19:36:15.317497  3374 solver.cpp:517]     Test net output #0: loss = 0.530794 (* 1 = 0.530794 loss)
I0109 19:36:15.317534  3374 solver.cpp:517]     Test net output #1: top-1 = 0.838333
I0109 19:36:15.317543  3374 solver.cpp:517]     Test net output #2: top-5 = 0.990889
I0109 19:36:15.348603  3374 solver.cpp:266] Iteration 7000 (24.2044 iter/s, 4.13148s/100 iter), loss = 0.131182
I0109 19:36:15.348659  3374 solver.cpp:285]     Train net output #0: loss = 0.131182 (* 1 = 0.131182 loss)
I0109 19:36:15.348675  3374 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 19:36:18.644793  3374 solver.cpp:266] Iteration 7100 (30.3385 iter/s, 3.29614s/100 iter), loss = 0.17122
I0109 19:36:18.644855  3374 solver.cpp:285]     Train net output #0: loss = 0.17122 (* 1 = 0.17122 loss)
I0109 19:36:18.644868  3374 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 19:36:21.952775  3374 solver.cpp:266] Iteration 7200 (30.2302 iter/s, 3.30795s/100 iter), loss = 0.12729
I0109 19:36:21.952924  3374 solver.cpp:285]     Train net output #0: loss = 0.12729 (* 1 = 0.12729 loss)
I0109 19:36:21.952939  3374 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 19:36:25.259027  3374 solver.cpp:266] Iteration 7300 (30.2468 iter/s, 3.30614s/100 iter), loss = 0.161572
I0109 19:36:25.259088  3374 solver.cpp:285]     Train net output #0: loss = 0.161572 (* 1 = 0.161572 loss)
I0109 19:36:25.259101  3374 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 19:36:28.560343  3374 solver.cpp:266] Iteration 7400 (30.2912 iter/s, 3.30129s/100 iter), loss = 0.130785
I0109 19:36:28.560402  3374 solver.cpp:285]     Train net output #0: loss = 0.130785 (* 1 = 0.130785 loss)
I0109 19:36:28.560415  3374 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 19:36:31.845744  3374 solver.cpp:266] Iteration 7500 (30.4382 iter/s, 3.28535s/100 iter), loss = 0.118136
I0109 19:36:31.845813  3374 solver.cpp:285]     Train net output #0: loss = 0.118136 (* 1 = 0.118136 loss)
I0109 19:36:31.845826  3374 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 19:36:35.144039  3374 solver.cpp:266] Iteration 7600 (30.319 iter/s, 3.29826s/100 iter), loss = 0.0921528
I0109 19:36:35.144102  3374 solver.cpp:285]     Train net output #0: loss = 0.0921528 (* 1 = 0.0921528 loss)
I0109 19:36:35.144115  3374 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 19:36:38.452893  3374 solver.cpp:266] Iteration 7700 (30.2222 iter/s, 3.30882s/100 iter), loss = 0.15581
I0109 19:36:38.452963  3374 solver.cpp:285]     Train net output #0: loss = 0.15581 (* 1 = 0.15581 loss)
I0109 19:36:38.452978  3374 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 19:36:41.754626  3374 solver.cpp:266] Iteration 7800 (30.2877 iter/s, 3.30167s/100 iter), loss = 0.114738
I0109 19:36:41.754688  3374 solver.cpp:285]     Train net output #0: loss = 0.114738 (* 1 = 0.114738 loss)
I0109 19:36:41.754700  3374 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 19:36:45.059391  3374 solver.cpp:266] Iteration 7900 (30.2596 iter/s, 3.30473s/100 iter), loss = 0.134838
I0109 19:36:45.059463  3374 solver.cpp:285]     Train net output #0: loss = 0.134838 (* 1 = 0.134838 loss)
I0109 19:36:45.059478  3374 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 19:36:48.333216  3374 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 19:36:49.154722  3374 solver.cpp:517]     Test net output #0: loss = 0.511669 (* 1 = 0.511669 loss)
I0109 19:36:49.154762  3374 solver.cpp:517]     Test net output #1: top-1 = 0.841778
I0109 19:36:49.154770  3374 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0109 19:36:49.185904  3374 solver.cpp:266] Iteration 8000 (24.2337 iter/s, 4.12648s/100 iter), loss = 0.219014
I0109 19:36:49.185947  3374 solver.cpp:285]     Train net output #0: loss = 0.219014 (* 1 = 0.219014 loss)
I0109 19:36:49.185961  3374 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 19:36:52.494076  3374 solver.cpp:266] Iteration 8100 (30.2283 iter/s, 3.30816s/100 iter), loss = 0.185011
I0109 19:36:52.494258  3374 solver.cpp:285]     Train net output #0: loss = 0.185011 (* 1 = 0.185011 loss)
I0109 19:36:52.494274  3374 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 19:36:55.800979  3374 solver.cpp:266] Iteration 8200 (30.2414 iter/s, 3.30673s/100 iter), loss = 0.132434
I0109 19:36:55.801050  3374 solver.cpp:285]     Train net output #0: loss = 0.132434 (* 1 = 0.132434 loss)
I0109 19:36:55.801065  3374 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 19:36:59.108145  3374 solver.cpp:266] Iteration 8300 (30.2377 iter/s, 3.30713s/100 iter), loss = 0.145573
I0109 19:36:59.108206  3374 solver.cpp:285]     Train net output #0: loss = 0.145573 (* 1 = 0.145573 loss)
I0109 19:36:59.108219  3374 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 19:37:02.423545  3374 solver.cpp:266] Iteration 8400 (30.1625 iter/s, 3.31537s/100 iter), loss = 0.129538
I0109 19:37:02.423614  3374 solver.cpp:285]     Train net output #0: loss = 0.129538 (* 1 = 0.129538 loss)
I0109 19:37:02.423625  3374 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 19:37:05.738689  3374 solver.cpp:266] Iteration 8500 (30.1652 iter/s, 3.31508s/100 iter), loss = 0.151228
I0109 19:37:05.738749  3374 solver.cpp:285]     Train net output #0: loss = 0.151228 (* 1 = 0.151228 loss)
I0109 19:37:05.738762  3374 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 19:37:09.061786  3374 solver.cpp:266] Iteration 8600 (30.0927 iter/s, 3.32307s/100 iter), loss = 0.231012
I0109 19:37:09.061846  3374 solver.cpp:285]     Train net output #0: loss = 0.231012 (* 1 = 0.231012 loss)
I0109 19:37:09.061858  3374 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 19:37:12.369659  3374 solver.cpp:266] Iteration 8700 (30.2312 iter/s, 3.30785s/100 iter), loss = 0.15258
I0109 19:37:12.369720  3374 solver.cpp:285]     Train net output #0: loss = 0.15258 (* 1 = 0.15258 loss)
I0109 19:37:12.369732  3374 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 19:37:15.690266  3374 solver.cpp:266] Iteration 8800 (30.1155 iter/s, 3.32055s/100 iter), loss = 0.136526
I0109 19:37:15.690343  3374 solver.cpp:285]     Train net output #0: loss = 0.136526 (* 1 = 0.136526 loss)
I0109 19:37:15.690358  3374 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 19:37:19.012825  3374 solver.cpp:266] Iteration 8900 (30.0977 iter/s, 3.32252s/100 iter), loss = 0.17842
I0109 19:37:19.012889  3374 solver.cpp:285]     Train net output #0: loss = 0.17842 (* 1 = 0.17842 loss)
I0109 19:37:19.012903  3374 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 19:37:22.311990  3374 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 19:37:23.141754  3374 solver.cpp:517]     Test net output #0: loss = 0.499107 (* 1 = 0.499107 loss)
I0109 19:37:23.141922  3374 solver.cpp:517]     Test net output #1: top-1 = 0.849555
I0109 19:37:23.141933  3374 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0109 19:37:23.173167  3374 solver.cpp:266] Iteration 9000 (24.0366 iter/s, 4.16033s/100 iter), loss = 0.112849
I0109 19:37:23.173202  3374 solver.cpp:285]     Train net output #0: loss = 0.112849 (* 1 = 0.112849 loss)
I0109 19:37:23.173218  3374 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 19:37:26.508450  3374 solver.cpp:266] Iteration 9100 (29.9826 iter/s, 3.33527s/100 iter), loss = 0.144805
I0109 19:37:26.508555  3374 solver.cpp:285]     Train net output #0: loss = 0.144805 (* 1 = 0.144805 loss)
I0109 19:37:26.508580  3374 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 19:37:29.848819  3374 solver.cpp:266] Iteration 9200 (29.9376 iter/s, 3.34028s/100 iter), loss = 0.165001
I0109 19:37:29.848892  3374 solver.cpp:285]     Train net output #0: loss = 0.165001 (* 1 = 0.165001 loss)
I0109 19:37:29.848911  3374 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 19:37:33.168411  3374 solver.cpp:266] Iteration 9300 (30.1245 iter/s, 3.31955s/100 iter), loss = 0.12912
I0109 19:37:33.168494  3374 solver.cpp:285]     Train net output #0: loss = 0.12912 (* 1 = 0.12912 loss)
I0109 19:37:33.168517  3374 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 19:37:36.513902  3374 solver.cpp:266] Iteration 9400 (29.8914 iter/s, 3.34544s/100 iter), loss = 0.139847
I0109 19:37:36.513990  3374 solver.cpp:285]     Train net output #0: loss = 0.139847 (* 1 = 0.139847 loss)
I0109 19:37:36.514010  3374 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 19:37:39.855485  3374 solver.cpp:266] Iteration 9500 (29.9266 iter/s, 3.34151s/100 iter), loss = 0.155795
I0109 19:37:39.855548  3374 solver.cpp:285]     Train net output #0: loss = 0.155795 (* 1 = 0.155795 loss)
I0109 19:37:39.855561  3374 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 19:37:43.200071  3374 solver.cpp:266] Iteration 9600 (29.8993 iter/s, 3.34456s/100 iter), loss = 0.172871
I0109 19:37:43.200134  3374 solver.cpp:285]     Train net output #0: loss = 0.172871 (* 1 = 0.172871 loss)
I0109 19:37:43.200146  3374 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 19:37:46.544126  3374 solver.cpp:266] Iteration 9700 (29.9041 iter/s, 3.34403s/100 iter), loss = 0.153741
I0109 19:37:46.544188  3374 solver.cpp:285]     Train net output #0: loss = 0.153741 (* 1 = 0.153741 loss)
I0109 19:37:46.544200  3374 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 19:37:49.870677  3374 solver.cpp:266] Iteration 9800 (30.0617 iter/s, 3.32649s/100 iter), loss = 0.12488
I0109 19:37:49.870756  3374 solver.cpp:285]     Train net output #0: loss = 0.12488 (* 1 = 0.12488 loss)
I0109 19:37:49.870771  3374 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 19:37:53.188722  3374 solver.cpp:266] Iteration 9900 (30.1387 iter/s, 3.318s/100 iter), loss = 0.106954
I0109 19:37:53.188836  3374 solver.cpp:285]     Train net output #0: loss = 0.106954 (* 1 = 0.106954 loss)
I0109 19:37:53.188853  3374 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 19:37:56.488714  3374 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 19:37:57.311007  3374 solver.cpp:517]     Test net output #0: loss = 0.491558 (* 1 = 0.491558 loss)
I0109 19:37:57.311045  3374 solver.cpp:517]     Test net output #1: top-1 = 0.852
I0109 19:37:57.311053  3374 solver.cpp:517]     Test net output #2: top-5 = 0.991555
I0109 19:37:57.342301  3374 solver.cpp:266] Iteration 10000 (24.076 iter/s, 4.15351s/100 iter), loss = 0.12443
I0109 19:37:57.342337  3374 solver.cpp:285]     Train net output #0: loss = 0.12443 (* 1 = 0.12443 loss)
I0109 19:37:57.342351  3374 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 19:38:00.657896  3374 solver.cpp:266] Iteration 10100 (30.1608 iter/s, 3.31556s/100 iter), loss = 0.0774559
I0109 19:38:00.657959  3374 solver.cpp:285]     Train net output #0: loss = 0.0774559 (* 1 = 0.0774559 loss)
I0109 19:38:00.657972  3374 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 19:38:03.970013  3374 solver.cpp:266] Iteration 10200 (30.1925 iter/s, 3.31208s/100 iter), loss = 0.153164
I0109 19:38:03.970093  3374 solver.cpp:285]     Train net output #0: loss = 0.153164 (* 1 = 0.153164 loss)
I0109 19:38:03.970108  3374 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 19:38:07.286048  3374 solver.cpp:266] Iteration 10300 (30.1569 iter/s, 3.31599s/100 iter), loss = 0.112607
I0109 19:38:07.286128  3374 solver.cpp:285]     Train net output #0: loss = 0.112607 (* 1 = 0.112607 loss)
I0109 19:38:07.286144  3374 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 19:38:10.597728  3374 solver.cpp:266] Iteration 10400 (30.1966 iter/s, 3.31163s/100 iter), loss = 0.133558
I0109 19:38:10.597792  3374 solver.cpp:285]     Train net output #0: loss = 0.133558 (* 1 = 0.133558 loss)
I0109 19:38:10.597805  3374 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 19:38:13.905537  3374 solver.cpp:266] Iteration 10500 (30.232 iter/s, 3.30775s/100 iter), loss = 0.109541
I0109 19:38:13.905612  3374 solver.cpp:285]     Train net output #0: loss = 0.109541 (* 1 = 0.109541 loss)
I0109 19:38:13.905627  3374 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 19:38:17.212581  3374 solver.cpp:266] Iteration 10600 (30.2389 iter/s, 3.307s/100 iter), loss = 0.121799
I0109 19:38:17.212658  3374 solver.cpp:285]     Train net output #0: loss = 0.121799 (* 1 = 0.121799 loss)
I0109 19:38:17.212672  3374 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 19:38:20.532028  3374 solver.cpp:266] Iteration 10700 (30.1259 iter/s, 3.31941s/100 iter), loss = 0.126423
I0109 19:38:20.532093  3374 solver.cpp:285]     Train net output #0: loss = 0.126423 (* 1 = 0.126423 loss)
I0109 19:38:20.532104  3374 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 19:38:23.842555  3374 solver.cpp:266] Iteration 10800 (30.2072 iter/s, 3.31047s/100 iter), loss = 0.12124
I0109 19:38:23.842728  3374 solver.cpp:285]     Train net output #0: loss = 0.12124 (* 1 = 0.12124 loss)
I0109 19:38:23.842743  3374 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 19:38:27.151665  3374 solver.cpp:266] Iteration 10900 (30.2209 iter/s, 3.30897s/100 iter), loss = 0.115694
I0109 19:38:27.151731  3374 solver.cpp:285]     Train net output #0: loss = 0.115694 (* 1 = 0.115694 loss)
I0109 19:38:27.151743  3374 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 19:38:30.432286  3374 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 19:38:31.252411  3374 solver.cpp:517]     Test net output #0: loss = 0.476257 (* 1 = 0.476257 loss)
I0109 19:38:31.252449  3374 solver.cpp:517]     Test net output #1: top-1 = 0.851222
I0109 19:38:31.252455  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:38:31.283710  3374 solver.cpp:266] Iteration 11000 (24.2013 iter/s, 4.13201s/100 iter), loss = 0.0986109
I0109 19:38:31.283790  3374 solver.cpp:285]     Train net output #0: loss = 0.0986109 (* 1 = 0.0986109 loss)
I0109 19:38:31.283807  3374 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 19:38:34.615356  3374 solver.cpp:266] Iteration 11100 (30.0156 iter/s, 3.3316s/100 iter), loss = 0.0928494
I0109 19:38:34.615422  3374 solver.cpp:285]     Train net output #0: loss = 0.0928493 (* 1 = 0.0928493 loss)
I0109 19:38:34.615434  3374 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 19:38:37.928488  3374 solver.cpp:266] Iteration 11200 (30.1835 iter/s, 3.31307s/100 iter), loss = 0.161875
I0109 19:38:37.928553  3374 solver.cpp:285]     Train net output #0: loss = 0.161875 (* 1 = 0.161875 loss)
I0109 19:38:37.928571  3374 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 19:38:41.251025  3374 solver.cpp:266] Iteration 11300 (30.0978 iter/s, 3.3225s/100 iter), loss = 0.0864644
I0109 19:38:41.251111  3374 solver.cpp:285]     Train net output #0: loss = 0.0864644 (* 1 = 0.0864644 loss)
I0109 19:38:41.251127  3374 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 19:38:44.571995  3374 solver.cpp:266] Iteration 11400 (30.1121 iter/s, 3.32092s/100 iter), loss = 0.125725
I0109 19:38:44.572057  3374 solver.cpp:285]     Train net output #0: loss = 0.125725 (* 1 = 0.125725 loss)
I0109 19:38:44.572068  3374 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 19:38:47.908092  3374 solver.cpp:266] Iteration 11500 (29.9756 iter/s, 3.33604s/100 iter), loss = 0.124087
I0109 19:38:47.908155  3374 solver.cpp:285]     Train net output #0: loss = 0.124087 (* 1 = 0.124087 loss)
I0109 19:38:47.908167  3374 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 19:38:51.235476  3374 solver.cpp:266] Iteration 11600 (30.0539 iter/s, 3.32735s/100 iter), loss = 0.152333
I0109 19:38:51.235538  3374 solver.cpp:285]     Train net output #0: loss = 0.152333 (* 1 = 0.152333 loss)
I0109 19:38:51.235550  3374 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 19:38:54.573621  3374 solver.cpp:266] Iteration 11700 (29.957 iter/s, 3.33812s/100 iter), loss = 0.116577
I0109 19:38:54.573808  3374 solver.cpp:285]     Train net output #0: loss = 0.116577 (* 1 = 0.116577 loss)
I0109 19:38:54.573825  3374 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 19:38:57.916939  3374 solver.cpp:266] Iteration 11800 (29.912 iter/s, 3.34314s/100 iter), loss = 0.0982303
I0109 19:38:57.917001  3374 solver.cpp:285]     Train net output #0: loss = 0.0982302 (* 1 = 0.0982302 loss)
I0109 19:38:57.917013  3374 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 19:39:01.244737  3374 solver.cpp:266] Iteration 11900 (30.0502 iter/s, 3.32777s/100 iter), loss = 0.131003
I0109 19:39:01.244801  3374 solver.cpp:285]     Train net output #0: loss = 0.131003 (* 1 = 0.131003 loss)
I0109 19:39:01.244813  3374 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 19:39:04.556284  3374 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 19:39:05.380689  3374 solver.cpp:517]     Test net output #0: loss = 0.464534 (* 1 = 0.464534 loss)
I0109 19:39:05.380726  3374 solver.cpp:517]     Test net output #1: top-1 = 0.860111
I0109 19:39:05.380733  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:39:05.412029  3374 solver.cpp:266] Iteration 12000 (23.9965 iter/s, 4.16728s/100 iter), loss = 0.137394
I0109 19:39:05.412065  3374 solver.cpp:285]     Train net output #0: loss = 0.137394 (* 1 = 0.137394 loss)
I0109 19:39:05.412079  3374 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 19:39:08.740274  3374 solver.cpp:266] Iteration 12100 (30.0462 iter/s, 3.32821s/100 iter), loss = 0.151981
I0109 19:39:08.740356  3374 solver.cpp:285]     Train net output #0: loss = 0.151981 (* 1 = 0.151981 loss)
I0109 19:39:08.740372  3374 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 19:39:12.089431  3374 solver.cpp:266] Iteration 12200 (29.8587 iter/s, 3.34911s/100 iter), loss = 0.0782052
I0109 19:39:12.089490  3374 solver.cpp:285]     Train net output #0: loss = 0.0782052 (* 1 = 0.0782052 loss)
I0109 19:39:12.089502  3374 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 19:39:15.431977  3374 solver.cpp:266] Iteration 12300 (29.9176 iter/s, 3.34252s/100 iter), loss = 0.143326
I0109 19:39:15.432055  3374 solver.cpp:285]     Train net output #0: loss = 0.143326 (* 1 = 0.143326 loss)
I0109 19:39:15.432070  3374 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 19:39:18.783463  3374 solver.cpp:266] Iteration 12400 (29.8381 iter/s, 3.35142s/100 iter), loss = 0.0927338
I0109 19:39:18.783527  3374 solver.cpp:285]     Train net output #0: loss = 0.0927338 (* 1 = 0.0927338 loss)
I0109 19:39:18.783540  3374 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 19:39:22.130756  3374 solver.cpp:266] Iteration 12500 (29.8751 iter/s, 3.34727s/100 iter), loss = 0.144669
I0109 19:39:22.130815  3374 solver.cpp:285]     Train net output #0: loss = 0.144669 (* 1 = 0.144669 loss)
I0109 19:39:22.130828  3374 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 19:39:25.476100  3374 solver.cpp:266] Iteration 12600 (29.8925 iter/s, 3.34532s/100 iter), loss = 0.146599
I0109 19:39:25.476289  3374 solver.cpp:285]     Train net output #0: loss = 0.146599 (* 1 = 0.146599 loss)
I0109 19:39:25.476305  3374 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 19:39:28.827059  3374 solver.cpp:266] Iteration 12700 (29.8438 iter/s, 3.35078s/100 iter), loss = 0.092712
I0109 19:39:28.827119  3374 solver.cpp:285]     Train net output #0: loss = 0.092712 (* 1 = 0.092712 loss)
I0109 19:39:28.827132  3374 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 19:39:32.165968  3374 solver.cpp:266] Iteration 12800 (29.9502 iter/s, 3.33888s/100 iter), loss = 0.121721
I0109 19:39:32.166044  3374 solver.cpp:285]     Train net output #0: loss = 0.121721 (* 1 = 0.121721 loss)
I0109 19:39:32.166059  3374 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 19:39:35.510813  3374 solver.cpp:266] Iteration 12900 (29.8971 iter/s, 3.34481s/100 iter), loss = 0.145396
I0109 19:39:35.510874  3374 solver.cpp:285]     Train net output #0: loss = 0.145396 (* 1 = 0.145396 loss)
I0109 19:39:35.510886  3374 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 19:39:38.818302  3374 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 19:39:39.644261  3374 solver.cpp:517]     Test net output #0: loss = 0.498777 (* 1 = 0.498777 loss)
I0109 19:39:39.644300  3374 solver.cpp:517]     Test net output #1: top-1 = 0.858
I0109 19:39:39.644313  3374 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0109 19:39:39.675917  3374 solver.cpp:266] Iteration 13000 (24.0092 iter/s, 4.16507s/100 iter), loss = 0.176702
I0109 19:39:39.675958  3374 solver.cpp:285]     Train net output #0: loss = 0.176702 (* 1 = 0.176702 loss)
I0109 19:39:39.675981  3374 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 19:39:43.021229  3374 solver.cpp:266] Iteration 13100 (29.8926 iter/s, 3.3453s/100 iter), loss = 0.13204
I0109 19:39:43.021302  3374 solver.cpp:285]     Train net output #0: loss = 0.13204 (* 1 = 0.13204 loss)
I0109 19:39:43.021317  3374 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 19:39:46.366899  3374 solver.cpp:266] Iteration 13200 (29.8897 iter/s, 3.34563s/100 iter), loss = 0.161972
I0109 19:39:46.366962  3374 solver.cpp:285]     Train net output #0: loss = 0.161972 (* 1 = 0.161972 loss)
I0109 19:39:46.366974  3374 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 19:39:49.708791  3374 solver.cpp:266] Iteration 13300 (29.9236 iter/s, 3.34184s/100 iter), loss = 0.123635
I0109 19:39:49.708871  3374 solver.cpp:285]     Train net output #0: loss = 0.123635 (* 1 = 0.123635 loss)
I0109 19:39:49.708887  3374 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 19:39:53.048820  3374 solver.cpp:266] Iteration 13400 (29.9404 iter/s, 3.33997s/100 iter), loss = 0.110178
I0109 19:39:53.048903  3374 solver.cpp:285]     Train net output #0: loss = 0.110178 (* 1 = 0.110178 loss)
I0109 19:39:53.048918  3374 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 19:39:56.396867  3374 solver.cpp:266] Iteration 13500 (29.8686 iter/s, 3.348s/100 iter), loss = 0.16651
I0109 19:39:56.396984  3374 solver.cpp:285]     Train net output #0: loss = 0.16651 (* 1 = 0.16651 loss)
I0109 19:39:56.396998  3374 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 19:39:59.744427  3374 solver.cpp:266] Iteration 13600 (29.8734 iter/s, 3.34745s/100 iter), loss = 0.0769897
I0109 19:39:59.744491  3374 solver.cpp:285]     Train net output #0: loss = 0.0769897 (* 1 = 0.0769897 loss)
I0109 19:39:59.744504  3374 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 19:40:03.073910  3374 solver.cpp:266] Iteration 13700 (30.0349 iter/s, 3.32945s/100 iter), loss = 0.0990108
I0109 19:40:03.073977  3374 solver.cpp:285]     Train net output #0: loss = 0.0990108 (* 1 = 0.0990108 loss)
I0109 19:40:03.073992  3374 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 19:40:06.418048  3374 solver.cpp:266] Iteration 13800 (29.9034 iter/s, 3.34411s/100 iter), loss = 0.121245
I0109 19:40:06.418128  3374 solver.cpp:285]     Train net output #0: loss = 0.121245 (* 1 = 0.121245 loss)
I0109 19:40:06.418143  3374 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 19:40:09.764016  3374 solver.cpp:266] Iteration 13900 (29.8873 iter/s, 3.3459s/100 iter), loss = 0.141481
I0109 19:40:09.764082  3374 solver.cpp:285]     Train net output #0: loss = 0.141481 (* 1 = 0.141481 loss)
I0109 19:40:09.764096  3374 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 19:40:13.078383  3374 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 19:40:13.900332  3374 solver.cpp:517]     Test net output #0: loss = 0.48118 (* 1 = 0.48118 loss)
I0109 19:40:13.900372  3374 solver.cpp:517]     Test net output #1: top-1 = 0.856778
I0109 19:40:13.900379  3374 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0109 19:40:13.931557  3374 solver.cpp:266] Iteration 14000 (23.9951 iter/s, 4.16752s/100 iter), loss = 0.118013
I0109 19:40:13.931615  3374 solver.cpp:285]     Train net output #0: loss = 0.118013 (* 1 = 0.118013 loss)
I0109 19:40:13.931632  3374 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 19:40:17.263206  3374 solver.cpp:266] Iteration 14100 (30.0154 iter/s, 3.33163s/100 iter), loss = 0.121516
I0109 19:40:17.263288  3374 solver.cpp:285]     Train net output #0: loss = 0.121516 (* 1 = 0.121516 loss)
I0109 19:40:17.263303  3374 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 19:40:20.609287  3374 solver.cpp:266] Iteration 14200 (29.8861 iter/s, 3.34604s/100 iter), loss = 0.13373
I0109 19:40:20.609351  3374 solver.cpp:285]     Train net output #0: loss = 0.13373 (* 1 = 0.13373 loss)
I0109 19:40:20.609364  3374 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 19:40:23.958938  3374 solver.cpp:266] Iteration 14300 (29.8544 iter/s, 3.34959s/100 iter), loss = 0.148741
I0109 19:40:23.959028  3374 solver.cpp:285]     Train net output #0: loss = 0.148741 (* 1 = 0.148741 loss)
I0109 19:40:23.959043  3374 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 19:40:27.308307  3374 solver.cpp:266] Iteration 14400 (29.8568 iter/s, 3.34932s/100 iter), loss = 0.0696976
I0109 19:40:27.308501  3374 solver.cpp:285]     Train net output #0: loss = 0.0696976 (* 1 = 0.0696976 loss)
I0109 19:40:27.308518  3374 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 19:40:30.647197  3374 solver.cpp:266] Iteration 14500 (29.9517 iter/s, 3.33871s/100 iter), loss = 0.086588
I0109 19:40:30.647270  3374 solver.cpp:285]     Train net output #0: loss = 0.086588 (* 1 = 0.086588 loss)
I0109 19:40:30.647285  3374 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 19:40:34.000089  3374 solver.cpp:266] Iteration 14600 (29.8253 iter/s, 3.35286s/100 iter), loss = 0.149889
I0109 19:40:34.000149  3374 solver.cpp:285]     Train net output #0: loss = 0.149889 (* 1 = 0.149889 loss)
I0109 19:40:34.000160  3374 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 19:40:37.338961  3374 solver.cpp:266] Iteration 14700 (29.9504 iter/s, 3.33885s/100 iter), loss = 0.121456
I0109 19:40:37.339023  3374 solver.cpp:285]     Train net output #0: loss = 0.121456 (* 1 = 0.121456 loss)
I0109 19:40:37.339036  3374 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 19:40:40.687162  3374 solver.cpp:266] Iteration 14800 (29.8672 iter/s, 3.34815s/100 iter), loss = 0.135855
I0109 19:40:40.687227  3374 solver.cpp:285]     Train net output #0: loss = 0.135855 (* 1 = 0.135855 loss)
I0109 19:40:40.687239  3374 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 19:40:44.038499  3374 solver.cpp:266] Iteration 14900 (29.8391 iter/s, 3.35131s/100 iter), loss = 0.11877
I0109 19:40:44.038571  3374 solver.cpp:285]     Train net output #0: loss = 0.11877 (* 1 = 0.11877 loss)
I0109 19:40:44.038586  3374 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 19:40:47.351863  3374 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 19:40:48.179574  3374 solver.cpp:517]     Test net output #0: loss = 0.472535 (* 1 = 0.472535 loss)
I0109 19:40:48.179612  3374 solver.cpp:517]     Test net output #1: top-1 = 0.858778
I0109 19:40:48.179622  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992555
I0109 19:40:48.210865  3374 solver.cpp:266] Iteration 15000 (23.9673 iter/s, 4.17235s/100 iter), loss = 0.139688
I0109 19:40:48.210906  3374 solver.cpp:285]     Train net output #0: loss = 0.139688 (* 1 = 0.139688 loss)
I0109 19:40:48.210922  3374 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 19:40:51.543118  3374 solver.cpp:266] Iteration 15100 (30.0098 iter/s, 3.33225s/100 iter), loss = 0.0923556
I0109 19:40:51.543179  3374 solver.cpp:285]     Train net output #0: loss = 0.0923556 (* 1 = 0.0923556 loss)
I0109 19:40:51.543190  3374 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 19:40:54.867625  3374 solver.cpp:266] Iteration 15200 (30.0801 iter/s, 3.32446s/100 iter), loss = 0.161816
I0109 19:40:54.867688  3374 solver.cpp:285]     Train net output #0: loss = 0.161816 (* 1 = 0.161816 loss)
I0109 19:40:54.867700  3374 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 19:40:58.200217  3374 solver.cpp:266] Iteration 15300 (30.0069 iter/s, 3.33257s/100 iter), loss = 0.137688
I0109 19:40:58.200397  3374 solver.cpp:285]     Train net output #0: loss = 0.137688 (* 1 = 0.137688 loss)
I0109 19:40:58.200413  3374 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 19:41:01.519187  3374 solver.cpp:266] Iteration 15400 (30.1311 iter/s, 3.31883s/100 iter), loss = 0.10085
I0109 19:41:01.519246  3374 solver.cpp:285]     Train net output #0: loss = 0.10085 (* 1 = 0.10085 loss)
I0109 19:41:01.519258  3374 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 19:41:04.844880  3374 solver.cpp:266] Iteration 15500 (30.0694 iter/s, 3.32564s/100 iter), loss = 0.113577
I0109 19:41:04.844944  3374 solver.cpp:285]     Train net output #0: loss = 0.113577 (* 1 = 0.113577 loss)
I0109 19:41:04.844956  3374 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 19:41:08.175622  3374 solver.cpp:266] Iteration 15600 (30.0236 iter/s, 3.33071s/100 iter), loss = 0.0697184
I0109 19:41:08.175684  3374 solver.cpp:285]     Train net output #0: loss = 0.0697184 (* 1 = 0.0697184 loss)
I0109 19:41:08.175696  3374 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 19:41:11.493806  3374 solver.cpp:266] Iteration 15700 (30.1372 iter/s, 3.31816s/100 iter), loss = 0.111957
I0109 19:41:11.493868  3374 solver.cpp:285]     Train net output #0: loss = 0.111957 (* 1 = 0.111957 loss)
I0109 19:41:11.493881  3374 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 19:41:14.811686  3374 solver.cpp:266] Iteration 15800 (30.1402 iter/s, 3.31783s/100 iter), loss = 0.117202
I0109 19:41:14.811748  3374 solver.cpp:285]     Train net output #0: loss = 0.117202 (* 1 = 0.117202 loss)
I0109 19:41:14.811759  3374 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 19:41:18.147580  3374 solver.cpp:266] Iteration 15900 (29.9772 iter/s, 3.33587s/100 iter), loss = 0.158571
I0109 19:41:18.147639  3374 solver.cpp:285]     Train net output #0: loss = 0.158571 (* 1 = 0.158571 loss)
I0109 19:41:18.147651  3374 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 19:41:21.439270  3374 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 19:41:22.267377  3374 solver.cpp:517]     Test net output #0: loss = 0.460922 (* 1 = 0.460922 loss)
I0109 19:41:22.267413  3374 solver.cpp:517]     Test net output #1: top-1 = 0.861333
I0109 19:41:22.267422  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:41:22.298779  3374 solver.cpp:266] Iteration 16000 (24.0895 iter/s, 4.15119s/100 iter), loss = 0.132659
I0109 19:41:22.298815  3374 solver.cpp:285]     Train net output #0: loss = 0.132659 (* 1 = 0.132659 loss)
I0109 19:41:22.298830  3374 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 19:41:25.624732  3374 solver.cpp:266] Iteration 16100 (30.0666 iter/s, 3.32595s/100 iter), loss = 0.137742
I0109 19:41:25.624792  3374 solver.cpp:285]     Train net output #0: loss = 0.137742 (* 1 = 0.137742 loss)
I0109 19:41:25.624805  3374 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 19:41:28.974540  3374 solver.cpp:266] Iteration 16200 (29.8529 iter/s, 3.34976s/100 iter), loss = 0.100993
I0109 19:41:28.974751  3374 solver.cpp:285]     Train net output #0: loss = 0.100993 (* 1 = 0.100993 loss)
I0109 19:41:28.974769  3374 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 19:41:32.319818  3374 solver.cpp:266] Iteration 16300 (29.8944 iter/s, 3.34511s/100 iter), loss = 0.127798
I0109 19:41:32.319878  3374 solver.cpp:285]     Train net output #0: loss = 0.127798 (* 1 = 0.127798 loss)
I0109 19:41:32.319890  3374 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 19:41:35.670122  3374 solver.cpp:266] Iteration 16400 (29.8485 iter/s, 3.35026s/100 iter), loss = 0.144074
I0109 19:41:35.670188  3374 solver.cpp:285]     Train net output #0: loss = 0.144074 (* 1 = 0.144074 loss)
I0109 19:41:35.670202  3374 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 19:41:39.020172  3374 solver.cpp:266] Iteration 16500 (29.8505 iter/s, 3.35003s/100 iter), loss = 0.112137
I0109 19:41:39.020241  3374 solver.cpp:285]     Train net output #0: loss = 0.112137 (* 1 = 0.112137 loss)
I0109 19:41:39.020253  3374 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 19:41:42.370556  3374 solver.cpp:266] Iteration 16600 (29.8476 iter/s, 3.35036s/100 iter), loss = 0.0993763
I0109 19:41:42.370638  3374 solver.cpp:285]     Train net output #0: loss = 0.0993762 (* 1 = 0.0993762 loss)
I0109 19:41:42.370653  3374 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 19:41:45.725522  3374 solver.cpp:266] Iteration 16700 (29.8071 iter/s, 3.3549s/100 iter), loss = 0.0770354
I0109 19:41:45.725605  3374 solver.cpp:285]     Train net output #0: loss = 0.0770354 (* 1 = 0.0770354 loss)
I0109 19:41:45.725620  3374 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 19:41:49.076997  3374 solver.cpp:266] Iteration 16800 (29.8378 iter/s, 3.35145s/100 iter), loss = 0.101542
I0109 19:41:49.077059  3374 solver.cpp:285]     Train net output #0: loss = 0.101542 (* 1 = 0.101542 loss)
I0109 19:41:49.077070  3374 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 19:41:52.435997  3374 solver.cpp:266] Iteration 16900 (29.7709 iter/s, 3.35898s/100 iter), loss = 0.0874587
I0109 19:41:52.436071  3374 solver.cpp:285]     Train net output #0: loss = 0.0874587 (* 1 = 0.0874587 loss)
I0109 19:41:52.436085  3374 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 19:41:55.757781  3374 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 19:41:56.586307  3374 solver.cpp:517]     Test net output #0: loss = 0.453838 (* 1 = 0.453838 loss)
I0109 19:41:56.586342  3374 solver.cpp:517]     Test net output #1: top-1 = 0.863334
I0109 19:41:56.586350  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:41:56.617625  3374 solver.cpp:266] Iteration 17000 (23.9142 iter/s, 4.18162s/100 iter), loss = 0.142947
I0109 19:41:56.617666  3374 solver.cpp:285]     Train net output #0: loss = 0.142947 (* 1 = 0.142947 loss)
I0109 19:41:56.617681  3374 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 19:41:59.972169  3374 solver.cpp:266] Iteration 17100 (29.8105 iter/s, 3.35452s/100 iter), loss = 0.0804057
I0109 19:41:59.972270  3374 solver.cpp:285]     Train net output #0: loss = 0.0804057 (* 1 = 0.0804057 loss)
I0109 19:41:59.972283  3374 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 19:42:03.333911  3374 solver.cpp:266] Iteration 17200 (29.747 iter/s, 3.36169s/100 iter), loss = 0.117782
I0109 19:42:03.333993  3374 solver.cpp:285]     Train net output #0: loss = 0.117782 (* 1 = 0.117782 loss)
I0109 19:42:03.334009  3374 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 19:42:06.681944  3374 solver.cpp:266] Iteration 17300 (29.8688 iter/s, 3.34797s/100 iter), loss = 0.0955581
I0109 19:42:06.682005  3374 solver.cpp:285]     Train net output #0: loss = 0.0955581 (* 1 = 0.0955581 loss)
I0109 19:42:06.682018  3374 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 19:42:10.027112  3374 solver.cpp:266] Iteration 17400 (29.894 iter/s, 3.34515s/100 iter), loss = 0.141674
I0109 19:42:10.027175  3374 solver.cpp:285]     Train net output #0: loss = 0.141674 (* 1 = 0.141674 loss)
I0109 19:42:10.027189  3374 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 19:42:13.373456  3374 solver.cpp:266] Iteration 17500 (29.8835 iter/s, 3.34633s/100 iter), loss = 0.064057
I0109 19:42:13.373519  3374 solver.cpp:285]     Train net output #0: loss = 0.0640571 (* 1 = 0.0640571 loss)
I0109 19:42:13.373531  3374 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 19:42:16.715178  3374 solver.cpp:266] Iteration 17600 (29.9251 iter/s, 3.34168s/100 iter), loss = 0.0828715
I0109 19:42:16.715241  3374 solver.cpp:285]     Train net output #0: loss = 0.0828715 (* 1 = 0.0828715 loss)
I0109 19:42:16.715253  3374 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 19:42:20.057335  3374 solver.cpp:266] Iteration 17700 (29.921 iter/s, 3.34214s/100 iter), loss = 0.0874304
I0109 19:42:20.057415  3374 solver.cpp:285]     Train net output #0: loss = 0.0874304 (* 1 = 0.0874304 loss)
I0109 19:42:20.057430  3374 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 19:42:23.405127  3374 solver.cpp:266] Iteration 17800 (29.8707 iter/s, 3.34776s/100 iter), loss = 0.114238
I0109 19:42:23.405194  3374 solver.cpp:285]     Train net output #0: loss = 0.114238 (* 1 = 0.114238 loss)
I0109 19:42:23.405205  3374 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 19:42:26.746325  3374 solver.cpp:266] Iteration 17900 (29.9298 iter/s, 3.34115s/100 iter), loss = 0.127028
I0109 19:42:26.746390  3374 solver.cpp:285]     Train net output #0: loss = 0.127028 (* 1 = 0.127028 loss)
I0109 19:42:26.746403  3374 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 19:42:30.044533  3374 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 19:42:30.871202  3374 solver.cpp:517]     Test net output #0: loss = 0.447825 (* 1 = 0.447825 loss)
I0109 19:42:30.871239  3374 solver.cpp:517]     Test net output #1: top-1 = 0.864667
I0109 19:42:30.871246  3374 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 19:42:30.902555  3374 solver.cpp:266] Iteration 18000 (24.0603 iter/s, 4.15623s/100 iter), loss = 0.104996
I0109 19:42:30.902590  3374 solver.cpp:285]     Train net output #0: loss = 0.104996 (* 1 = 0.104996 loss)
I0109 19:42:30.902606  3374 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 19:42:34.225910  3374 solver.cpp:266] Iteration 18100 (30.09 iter/s, 3.32336s/100 iter), loss = 0.15303
I0109 19:42:34.225968  3374 solver.cpp:285]     Train net output #0: loss = 0.15303 (* 1 = 0.15303 loss)
I0109 19:42:34.225981  3374 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 19:42:37.555721  3374 solver.cpp:266] Iteration 18200 (30.0319 iter/s, 3.3298s/100 iter), loss = 0.142488
I0109 19:42:37.555781  3374 solver.cpp:285]     Train net output #0: loss = 0.142489 (* 1 = 0.142489 loss)
I0109 19:42:37.555794  3374 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 19:42:40.879743  3374 solver.cpp:266] Iteration 18300 (30.0844 iter/s, 3.32398s/100 iter), loss = 0.113573
I0109 19:42:40.879807  3374 solver.cpp:285]     Train net output #0: loss = 0.113573 (* 1 = 0.113573 loss)
I0109 19:42:40.879820  3374 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 19:42:44.221565  3374 solver.cpp:266] Iteration 18400 (29.924 iter/s, 3.3418s/100 iter), loss = 0.12421
I0109 19:42:44.221634  3374 solver.cpp:285]     Train net output #0: loss = 0.12421 (* 1 = 0.12421 loss)
I0109 19:42:44.221647  3374 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 19:42:47.562769  3374 solver.cpp:266] Iteration 18500 (29.9296 iter/s, 3.34118s/100 iter), loss = 0.184151
I0109 19:42:47.562835  3374 solver.cpp:285]     Train net output #0: loss = 0.184151 (* 1 = 0.184151 loss)
I0109 19:42:47.562850  3374 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 19:42:50.914382  3374 solver.cpp:266] Iteration 18600 (29.8368 iter/s, 3.35157s/100 iter), loss = 0.0650499
I0109 19:42:50.914445  3374 solver.cpp:285]     Train net output #0: loss = 0.06505 (* 1 = 0.06505 loss)
I0109 19:42:50.914458  3374 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 19:42:54.260843  3374 solver.cpp:266] Iteration 18700 (29.8825 iter/s, 3.34644s/100 iter), loss = 0.122362
I0109 19:42:54.260924  3374 solver.cpp:285]     Train net output #0: loss = 0.122362 (* 1 = 0.122362 loss)
I0109 19:42:54.260939  3374 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 19:42:57.596146  3374 solver.cpp:266] Iteration 18800 (29.9826 iter/s, 3.33526s/100 iter), loss = 0.0974854
I0109 19:42:57.596210  3374 solver.cpp:285]     Train net output #0: loss = 0.0974854 (* 1 = 0.0974854 loss)
I0109 19:42:57.596222  3374 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 19:43:00.938349  3374 solver.cpp:266] Iteration 18900 (29.9208 iter/s, 3.34215s/100 iter), loss = 0.0898478
I0109 19:43:00.938532  3374 solver.cpp:285]     Train net output #0: loss = 0.0898478 (* 1 = 0.0898478 loss)
I0109 19:43:00.938552  3374 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 19:43:04.243060  3374 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 19:43:05.070024  3374 solver.cpp:517]     Test net output #0: loss = 0.43446 (* 1 = 0.43446 loss)
I0109 19:43:05.070060  3374 solver.cpp:517]     Test net output #1: top-1 = 0.869889
I0109 19:43:05.070070  3374 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 19:43:05.101361  3374 solver.cpp:266] Iteration 19000 (24.0218 iter/s, 4.16288s/100 iter), loss = 0.1212
I0109 19:43:05.101414  3374 solver.cpp:285]     Train net output #0: loss = 0.121201 (* 1 = 0.121201 loss)
I0109 19:43:05.101430  3374 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 19:43:08.430524  3374 solver.cpp:266] Iteration 19100 (30.0377 iter/s, 3.32915s/100 iter), loss = 0.098083
I0109 19:43:08.430594  3374 solver.cpp:285]     Train net output #0: loss = 0.098083 (* 1 = 0.098083 loss)
I0109 19:43:08.430608  3374 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 19:43:11.772981  3374 solver.cpp:266] Iteration 19200 (29.9186 iter/s, 3.3424s/100 iter), loss = 0.171865
I0109 19:43:11.773054  3374 solver.cpp:285]     Train net output #0: loss = 0.171865 (* 1 = 0.171865 loss)
I0109 19:43:11.773068  3374 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 19:43:15.114344  3374 solver.cpp:266] Iteration 19300 (29.9282 iter/s, 3.34133s/100 iter), loss = 0.107661
I0109 19:43:15.114408  3374 solver.cpp:285]     Train net output #0: loss = 0.107661 (* 1 = 0.107661 loss)
I0109 19:43:15.114423  3374 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 19:43:18.461863  3374 solver.cpp:266] Iteration 19400 (29.8731 iter/s, 3.34749s/100 iter), loss = 0.128074
I0109 19:43:18.461947  3374 solver.cpp:285]     Train net output #0: loss = 0.128074 (* 1 = 0.128074 loss)
I0109 19:43:18.461962  3374 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 19:43:21.810111  3374 solver.cpp:266] Iteration 19500 (29.867 iter/s, 3.34818s/100 iter), loss = 0.0789816
I0109 19:43:21.810173  3374 solver.cpp:285]     Train net output #0: loss = 0.0789816 (* 1 = 0.0789816 loss)
I0109 19:43:21.810185  3374 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 19:43:25.158167  3374 solver.cpp:266] Iteration 19600 (29.8683 iter/s, 3.34803s/100 iter), loss = 0.101114
I0109 19:43:25.158242  3374 solver.cpp:285]     Train net output #0: loss = 0.101114 (* 1 = 0.101114 loss)
I0109 19:43:25.158257  3374 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 19:43:28.512120  3374 solver.cpp:266] Iteration 19700 (29.8159 iter/s, 3.35392s/100 iter), loss = 0.0901356
I0109 19:43:28.512199  3374 solver.cpp:285]     Train net output #0: loss = 0.0901356 (* 1 = 0.0901356 loss)
I0109 19:43:28.512215  3374 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 19:43:31.861630  3374 solver.cpp:266] Iteration 19800 (29.8557 iter/s, 3.34945s/100 iter), loss = 0.124744
I0109 19:43:31.861848  3374 solver.cpp:285]     Train net output #0: loss = 0.124744 (* 1 = 0.124744 loss)
I0109 19:43:31.861871  3374 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 19:43:35.212926  3374 solver.cpp:266] Iteration 19900 (29.8407 iter/s, 3.35113s/100 iter), loss = 0.129698
I0109 19:43:35.212990  3374 solver.cpp:285]     Train net output #0: loss = 0.129698 (* 1 = 0.129698 loss)
I0109 19:43:35.213003  3374 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 19:43:38.526144  3374 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/_iter_20000.caffemodel
I0109 19:43:38.628530  3374 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/_iter_20000.solverstate
I0109 19:43:38.654862  3374 solver.cpp:378] Iteration 20000, loss = 0.00834794
I0109 19:43:38.654898  3374 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 19:43:39.477381  3374 solver.cpp:517]     Test net output #0: loss = 0.437919 (* 1 = 0.437919 loss)
I0109 19:43:39.477425  3374 solver.cpp:517]     Test net output #1: top-1 = 0.869444
I0109 19:43:39.477432  3374 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 19:43:39.477438  3374 solver.cpp:386] Optimization Done (29.4368 iter/s).
I0109 19:43:39.477445  3374 caffe_interface.cpp:530] Optimization Done.
I0109 19:43:39.643548  3402 pruning_runner.cpp:190] Sens info found, use it.
I0109 19:43:39.689424  3402 pruning_runner.cpp:217] Start compressing, please wait...
I0109 19:43:40.731721  3402 pruning_runner.cpp:264] Compression complete 0.00115385%
I0109 19:43:41.039705  3402 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 19:43:41.040030  3402 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 19:43:41.040406  3402 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:43:41.040637  3402 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:43:41.040815  3402 layer_factory.hpp:77] Creating layer data
I0109 19:43:41.040880  3402 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:41.040994  3402 net.cpp:94] Creating Layer data
I0109 19:43:41.041029  3402 net.cpp:409] data -> data
I0109 19:43:41.041057  3402 net.cpp:409] data -> label
I0109 19:43:41.041939  3447 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:43:41.041972  3447 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:43:41.042059  3402 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:43:41.042182  3402 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:43:41.048004  3402 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:41.048069  3402 net.cpp:144] Setting up data
I0109 19:43:41.048108  3402 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:43:41.048149  3402 net.cpp:151] Top shape: 50 (50)
I0109 19:43:41.048178  3402 net.cpp:159] Memory required for data: 614600
I0109 19:43:41.048211  3402 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:43:41.048267  3402 net.cpp:94] Creating Layer label_data_1_split
I0109 19:43:41.048310  3402 net.cpp:435] label_data_1_split <- label
I0109 19:43:41.048358  3402 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:43:41.048408  3402 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:43:41.048449  3402 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:43:41.048563  3402 net.cpp:144] Setting up label_data_1_split
I0109 19:43:41.048602  3402 net.cpp:151] Top shape: 50 (50)
I0109 19:43:41.048637  3402 net.cpp:151] Top shape: 50 (50)
I0109 19:43:41.048679  3402 net.cpp:151] Top shape: 50 (50)
I0109 19:43:41.048714  3402 net.cpp:159] Memory required for data: 615200
I0109 19:43:41.048751  3402 layer_factory.hpp:77] Creating layer conv1
I0109 19:43:41.048796  3402 net.cpp:94] Creating Layer conv1
I0109 19:43:41.048833  3402 net.cpp:435] conv1 <- data
I0109 19:43:41.048873  3402 net.cpp:409] conv1 -> conv1
I0109 19:43:41.050047  3402 net.cpp:144] Setting up conv1
I0109 19:43:41.050074  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.050129  3402 net.cpp:159] Memory required for data: 7168800
I0109 19:43:41.050179  3402 layer_factory.hpp:77] Creating layer bn1
I0109 19:43:41.050237  3402 net.cpp:94] Creating Layer bn1
I0109 19:43:41.050274  3402 net.cpp:435] bn1 <- conv1
I0109 19:43:41.050325  3402 net.cpp:409] bn1 -> scale1
I0109 19:43:41.051321  3402 net.cpp:144] Setting up bn1
I0109 19:43:41.051342  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.051349  3402 net.cpp:159] Memory required for data: 13722400
I0109 19:43:41.051398  3402 layer_factory.hpp:77] Creating layer relu1
I0109 19:43:41.051419  3402 net.cpp:94] Creating Layer relu1
I0109 19:43:41.051460  3402 net.cpp:435] relu1 <- scale1
I0109 19:43:41.051502  3402 net.cpp:409] relu1 -> relu1
I0109 19:43:41.051578  3402 net.cpp:144] Setting up relu1
I0109 19:43:41.051604  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.051611  3402 net.cpp:159] Memory required for data: 20276000
I0109 19:43:41.051620  3402 layer_factory.hpp:77] Creating layer conv2
I0109 19:43:41.051635  3402 net.cpp:94] Creating Layer conv2
I0109 19:43:41.051643  3402 net.cpp:435] conv2 <- relu1
I0109 19:43:41.051656  3402 net.cpp:409] conv2 -> conv2
I0109 19:43:41.052769  3402 net.cpp:144] Setting up conv2
I0109 19:43:41.052794  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.052803  3402 net.cpp:159] Memory required for data: 26829600
I0109 19:43:41.052819  3402 layer_factory.hpp:77] Creating layer bn2
I0109 19:43:41.052848  3402 net.cpp:94] Creating Layer bn2
I0109 19:43:41.052896  3402 net.cpp:435] bn2 <- conv2
I0109 19:43:41.052942  3402 net.cpp:409] bn2 -> scale2
I0109 19:43:41.054096  3402 net.cpp:144] Setting up bn2
I0109 19:43:41.054118  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.054126  3402 net.cpp:159] Memory required for data: 33383200
I0109 19:43:41.054143  3402 layer_factory.hpp:77] Creating layer relu2
I0109 19:43:41.054172  3402 net.cpp:94] Creating Layer relu2
I0109 19:43:41.054188  3402 net.cpp:435] relu2 <- scale2
I0109 19:43:41.054199  3402 net.cpp:409] relu2 -> relu2
I0109 19:43:41.054256  3402 net.cpp:144] Setting up relu2
I0109 19:43:41.054306  3402 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:41.054347  3402 net.cpp:159] Memory required for data: 39936800
I0109 19:43:41.054396  3402 layer_factory.hpp:77] Creating layer pool1
I0109 19:43:41.054443  3402 net.cpp:94] Creating Layer pool1
I0109 19:43:41.054486  3402 net.cpp:435] pool1 <- relu2
I0109 19:43:41.054527  3402 net.cpp:409] pool1 -> pool1
I0109 19:43:41.054618  3402 net.cpp:144] Setting up pool1
I0109 19:43:41.054664  3402 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:43:41.054704  3402 net.cpp:159] Memory required for data: 41575200
I0109 19:43:41.054742  3402 layer_factory.hpp:77] Creating layer drop1
I0109 19:43:41.054783  3402 net.cpp:94] Creating Layer drop1
I0109 19:43:41.054824  3402 net.cpp:435] drop1 <- pool1
I0109 19:43:41.054874  3402 net.cpp:409] drop1 -> drop1
I0109 19:43:41.054998  3402 net.cpp:144] Setting up drop1
I0109 19:43:41.055052  3402 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:43:41.055089  3402 net.cpp:159] Memory required for data: 43213600
I0109 19:43:41.055127  3402 layer_factory.hpp:77] Creating layer conv3
I0109 19:43:41.055173  3402 net.cpp:94] Creating Layer conv3
I0109 19:43:41.055222  3402 net.cpp:435] conv3 <- drop1
I0109 19:43:41.055266  3402 net.cpp:409] conv3 -> conv3
I0109 19:43:41.056643  3402 net.cpp:144] Setting up conv3
I0109 19:43:41.056700  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.056740  3402 net.cpp:159] Memory required for data: 46490400
I0109 19:43:41.056783  3402 layer_factory.hpp:77] Creating layer bn3
I0109 19:43:41.056833  3402 net.cpp:94] Creating Layer bn3
I0109 19:43:41.056869  3402 net.cpp:435] bn3 <- conv3
I0109 19:43:41.056898  3402 net.cpp:409] bn3 -> scale3
I0109 19:43:41.057978  3402 net.cpp:144] Setting up bn3
I0109 19:43:41.057999  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.058007  3402 net.cpp:159] Memory required for data: 49767200
I0109 19:43:41.058028  3402 layer_factory.hpp:77] Creating layer relu3
I0109 19:43:41.058056  3402 net.cpp:94] Creating Layer relu3
I0109 19:43:41.058097  3402 net.cpp:435] relu3 <- scale3
I0109 19:43:41.058169  3402 net.cpp:409] relu3 -> relu3
I0109 19:43:41.058280  3402 net.cpp:144] Setting up relu3
I0109 19:43:41.058354  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.058418  3402 net.cpp:159] Memory required for data: 53044000
I0109 19:43:41.058480  3402 layer_factory.hpp:77] Creating layer conv4
I0109 19:43:41.058552  3402 net.cpp:94] Creating Layer conv4
I0109 19:43:41.058569  3402 net.cpp:435] conv4 <- relu3
I0109 19:43:41.058586  3402 net.cpp:409] conv4 -> conv4
I0109 19:43:41.059360  3402 net.cpp:144] Setting up conv4
I0109 19:43:41.059381  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.059386  3402 net.cpp:159] Memory required for data: 56320800
I0109 19:43:41.059394  3402 layer_factory.hpp:77] Creating layer bn4
I0109 19:43:41.059406  3402 net.cpp:94] Creating Layer bn4
I0109 19:43:41.059471  3402 net.cpp:435] bn4 <- conv4
I0109 19:43:41.059491  3402 net.cpp:409] bn4 -> scale4
I0109 19:43:41.060175  3402 net.cpp:144] Setting up bn4
I0109 19:43:41.060194  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.060199  3402 net.cpp:159] Memory required for data: 59597600
I0109 19:43:41.060214  3402 layer_factory.hpp:77] Creating layer relu4
I0109 19:43:41.060235  3402 net.cpp:94] Creating Layer relu4
I0109 19:43:41.060242  3402 net.cpp:435] relu4 <- scale4
I0109 19:43:41.060252  3402 net.cpp:409] relu4 -> relu4
I0109 19:43:41.060297  3402 net.cpp:144] Setting up relu4
I0109 19:43:41.060320  3402 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:41.060328  3402 net.cpp:159] Memory required for data: 62874400
I0109 19:43:41.060334  3402 layer_factory.hpp:77] Creating layer pool2
I0109 19:43:41.060348  3402 net.cpp:94] Creating Layer pool2
I0109 19:43:41.060362  3402 net.cpp:435] pool2 <- relu4
I0109 19:43:41.060374  3402 net.cpp:409] pool2 -> pool2
I0109 19:43:41.060429  3402 net.cpp:144] Setting up pool2
I0109 19:43:41.060447  3402 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:43:41.060456  3402 net.cpp:159] Memory required for data: 63693600
I0109 19:43:41.060461  3402 layer_factory.hpp:77] Creating layer drop2
I0109 19:43:41.060472  3402 net.cpp:94] Creating Layer drop2
I0109 19:43:41.060484  3402 net.cpp:435] drop2 <- pool2
I0109 19:43:41.060497  3402 net.cpp:409] drop2 -> drop2
I0109 19:43:41.060551  3402 net.cpp:144] Setting up drop2
I0109 19:43:41.060567  3402 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:43:41.060575  3402 net.cpp:159] Memory required for data: 64512800
I0109 19:43:41.060581  3402 layer_factory.hpp:77] Creating layer fc1
I0109 19:43:41.060602  3402 net.cpp:94] Creating Layer fc1
I0109 19:43:41.060611  3402 net.cpp:435] fc1 <- drop2
I0109 19:43:41.060621  3402 net.cpp:409] fc1 -> fc1
I0109 19:43:41.081647  3402 net.cpp:144] Setting up fc1
I0109 19:43:41.081671  3402 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:41.081676  3402 net.cpp:159] Memory required for data: 64615200
I0109 19:43:41.081684  3402 layer_factory.hpp:77] Creating layer bn5
I0109 19:43:41.081703  3402 net.cpp:94] Creating Layer bn5
I0109 19:43:41.081719  3402 net.cpp:435] bn5 <- fc1
I0109 19:43:41.081733  3402 net.cpp:409] bn5 -> scale5
I0109 19:43:41.082332  3402 net.cpp:144] Setting up bn5
I0109 19:43:41.082366  3402 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:41.082371  3402 net.cpp:159] Memory required for data: 64717600
I0109 19:43:41.082393  3402 layer_factory.hpp:77] Creating layer relu5
I0109 19:43:41.082414  3402 net.cpp:94] Creating Layer relu5
I0109 19:43:41.082423  3402 net.cpp:435] relu5 <- scale5
I0109 19:43:41.082435  3402 net.cpp:409] relu5 -> relu5
I0109 19:43:41.082506  3402 net.cpp:144] Setting up relu5
I0109 19:43:41.082525  3402 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:41.082533  3402 net.cpp:159] Memory required for data: 64820000
I0109 19:43:41.082540  3402 layer_factory.hpp:77] Creating layer drop3
I0109 19:43:41.082551  3402 net.cpp:94] Creating Layer drop3
I0109 19:43:41.082563  3402 net.cpp:435] drop3 <- relu5
I0109 19:43:41.082576  3402 net.cpp:409] drop3 -> drop3
I0109 19:43:41.082633  3402 net.cpp:144] Setting up drop3
I0109 19:43:41.082648  3402 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:41.082654  3402 net.cpp:159] Memory required for data: 64922400
I0109 19:43:41.082661  3402 layer_factory.hpp:77] Creating layer fc2
I0109 19:43:41.082691  3402 net.cpp:94] Creating Layer fc2
I0109 19:43:41.082706  3402 net.cpp:435] fc2 <- drop3
I0109 19:43:41.082721  3402 net.cpp:409] fc2 -> fc2
I0109 19:43:41.082912  3402 net.cpp:144] Setting up fc2
I0109 19:43:41.082928  3402 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:41.082932  3402 net.cpp:159] Memory required for data: 64924400
I0109 19:43:41.082940  3402 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:43:41.082953  3402 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:43:41.082967  3402 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:43:41.082978  3402 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:43:41.082990  3402 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:43:41.083011  3402 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:43:41.083079  3402 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:43:41.083093  3402 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:41.083098  3402 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:41.083103  3402 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:41.083107  3402 net.cpp:159] Memory required for data: 64930400
I0109 19:43:41.083112  3402 layer_factory.hpp:77] Creating layer loss
I0109 19:43:41.083132  3402 net.cpp:94] Creating Layer loss
I0109 19:43:41.083140  3402 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:43:41.083174  3402 net.cpp:435] loss <- label_data_1_split_0
I0109 19:43:41.083211  3402 net.cpp:409] loss -> loss
I0109 19:43:41.083237  3402 layer_factory.hpp:77] Creating layer loss
I0109 19:43:41.083348  3402 net.cpp:144] Setting up loss
I0109 19:43:41.083364  3402 net.cpp:151] Top shape: (1)
I0109 19:43:41.083370  3402 net.cpp:154]     with loss weight 1
I0109 19:43:41.083396  3402 net.cpp:159] Memory required for data: 64930404
I0109 19:43:41.083406  3402 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:43:41.083425  3402 net.cpp:94] Creating Layer accuracy-top1
I0109 19:43:41.083433  3402 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:43:41.083441  3402 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:43:41.083459  3402 net.cpp:409] accuracy-top1 -> top-1
I0109 19:43:41.083483  3402 net.cpp:144] Setting up accuracy-top1
I0109 19:43:41.083493  3402 net.cpp:151] Top shape: (1)
I0109 19:43:41.083498  3402 net.cpp:159] Memory required for data: 64930408
I0109 19:43:41.083513  3402 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:43:41.083525  3402 net.cpp:94] Creating Layer accuracy-top5
I0109 19:43:41.083533  3402 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:43:41.083539  3402 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:43:41.083557  3402 net.cpp:409] accuracy-top5 -> top-5
I0109 19:43:41.083571  3402 net.cpp:144] Setting up accuracy-top5
I0109 19:43:41.083595  3402 net.cpp:151] Top shape: (1)
I0109 19:43:41.083609  3402 net.cpp:159] Memory required for data: 64930412
I0109 19:43:41.083618  3402 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:43:41.083647  3402 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:43:41.083658  3402 net.cpp:220] loss needs backward computation.
I0109 19:43:41.083664  3402 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:43:41.083673  3402 net.cpp:220] fc2 needs backward computation.
I0109 19:43:41.083681  3402 net.cpp:220] drop3 needs backward computation.
I0109 19:43:41.083688  3402 net.cpp:220] relu5 needs backward computation.
I0109 19:43:41.083694  3402 net.cpp:220] bn5 needs backward computation.
I0109 19:43:41.083704  3402 net.cpp:220] fc1 needs backward computation.
I0109 19:43:41.083711  3402 net.cpp:220] drop2 needs backward computation.
I0109 19:43:41.083719  3402 net.cpp:220] pool2 needs backward computation.
I0109 19:43:41.083726  3402 net.cpp:220] relu4 needs backward computation.
I0109 19:43:41.083734  3402 net.cpp:220] bn4 needs backward computation.
I0109 19:43:41.083740  3402 net.cpp:220] conv4 needs backward computation.
I0109 19:43:41.083750  3402 net.cpp:220] relu3 needs backward computation.
I0109 19:43:41.083756  3402 net.cpp:220] bn3 needs backward computation.
I0109 19:43:41.083766  3402 net.cpp:220] conv3 needs backward computation.
I0109 19:43:41.083773  3402 net.cpp:220] drop1 needs backward computation.
I0109 19:43:41.083779  3402 net.cpp:220] pool1 needs backward computation.
I0109 19:43:41.083787  3402 net.cpp:220] relu2 needs backward computation.
I0109 19:43:41.083794  3402 net.cpp:220] bn2 needs backward computation.
I0109 19:43:41.083801  3402 net.cpp:220] conv2 needs backward computation.
I0109 19:43:41.083807  3402 net.cpp:220] relu1 needs backward computation.
I0109 19:43:41.083817  3402 net.cpp:220] bn1 needs backward computation.
I0109 19:43:41.083824  3402 net.cpp:220] conv1 needs backward computation.
I0109 19:43:41.083833  3402 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:43:41.083842  3402 net.cpp:222] data does not need backward computation.
I0109 19:43:41.083849  3402 net.cpp:264] This network produces output loss
I0109 19:43:41.083858  3402 net.cpp:264] This network produces output top-1
I0109 19:43:41.083864  3402 net.cpp:264] This network produces output top-5
I0109 19:43:41.083900  3402 net.cpp:284] Network initialization done.
I0109 19:43:41.087733  3402 caffe_interface.cpp:363] Running for 180 iterations.
I0109 19:43:41.097503  3402 caffe_interface.cpp:125] Batch 0, loss = 0.639483
I0109 19:43:41.097532  3402 caffe_interface.cpp:125] Batch 0, top-1 = 0.82
I0109 19:43:41.097539  3402 caffe_interface.cpp:125] Batch 0, top-5 = 0.98
I0109 19:43:41.102089  3402 caffe_interface.cpp:125] Batch 1, loss = 0.357716
I0109 19:43:41.102114  3402 caffe_interface.cpp:125] Batch 1, top-1 = 0.82
I0109 19:43:41.102120  3402 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 19:43:41.106628  3402 caffe_interface.cpp:125] Batch 2, loss = 1.26617
I0109 19:43:41.106652  3402 caffe_interface.cpp:125] Batch 2, top-1 = 0.68
I0109 19:43:41.106658  3402 caffe_interface.cpp:125] Batch 2, top-5 = 0.94
I0109 19:43:41.111160  3402 caffe_interface.cpp:125] Batch 3, loss = 0.417312
I0109 19:43:41.111183  3402 caffe_interface.cpp:125] Batch 3, top-1 = 0.86
I0109 19:43:41.111188  3402 caffe_interface.cpp:125] Batch 3, top-5 = 0.98
I0109 19:43:41.115706  3402 caffe_interface.cpp:125] Batch 4, loss = 0.443768
I0109 19:43:41.115730  3402 caffe_interface.cpp:125] Batch 4, top-1 = 0.9
I0109 19:43:41.115734  3402 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 19:43:41.120184  3402 caffe_interface.cpp:125] Batch 5, loss = 1.01441
I0109 19:43:41.120208  3402 caffe_interface.cpp:125] Batch 5, top-1 = 0.7
I0109 19:43:41.120214  3402 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 19:43:41.124727  3402 caffe_interface.cpp:125] Batch 6, loss = 1.06635
I0109 19:43:41.124750  3402 caffe_interface.cpp:125] Batch 6, top-1 = 0.8
I0109 19:43:41.124758  3402 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 19:43:41.129240  3402 caffe_interface.cpp:125] Batch 7, loss = 0.286368
I0109 19:43:41.129262  3402 caffe_interface.cpp:125] Batch 7, top-1 = 0.86
I0109 19:43:41.129283  3402 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 19:43:41.133824  3402 caffe_interface.cpp:125] Batch 8, loss = 1.25035
I0109 19:43:41.133846  3402 caffe_interface.cpp:125] Batch 8, top-1 = 0.7
I0109 19:43:41.133852  3402 caffe_interface.cpp:125] Batch 8, top-5 = 0.96
I0109 19:43:41.138350  3402 caffe_interface.cpp:125] Batch 9, loss = 0.520419
I0109 19:43:41.138373  3402 caffe_interface.cpp:125] Batch 9, top-1 = 0.76
I0109 19:43:41.138378  3402 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 19:43:41.142877  3402 caffe_interface.cpp:125] Batch 10, loss = 0.734401
I0109 19:43:41.142900  3402 caffe_interface.cpp:125] Batch 10, top-1 = 0.78
I0109 19:43:41.142906  3402 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 19:43:41.147418  3402 caffe_interface.cpp:125] Batch 11, loss = 0.438458
I0109 19:43:41.147441  3402 caffe_interface.cpp:125] Batch 11, top-1 = 0.86
I0109 19:43:41.147446  3402 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 19:43:41.151958  3402 caffe_interface.cpp:125] Batch 12, loss = 0.47109
I0109 19:43:41.151983  3402 caffe_interface.cpp:125] Batch 12, top-1 = 0.84
I0109 19:43:41.151988  3402 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0109 19:43:41.156474  3402 caffe_interface.cpp:125] Batch 13, loss = 0.541039
I0109 19:43:41.156498  3402 caffe_interface.cpp:125] Batch 13, top-1 = 0.86
I0109 19:43:41.156504  3402 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0109 19:43:41.160996  3402 caffe_interface.cpp:125] Batch 14, loss = 0.560496
I0109 19:43:41.161020  3402 caffe_interface.cpp:125] Batch 14, top-1 = 0.86
I0109 19:43:41.161026  3402 caffe_interface.cpp:125] Batch 14, top-5 = 0.98
I0109 19:43:41.165513  3402 caffe_interface.cpp:125] Batch 15, loss = 0.924244
I0109 19:43:41.165535  3402 caffe_interface.cpp:125] Batch 15, top-1 = 0.8
I0109 19:43:41.165541  3402 caffe_interface.cpp:125] Batch 15, top-5 = 0.94
I0109 19:43:41.170035  3402 caffe_interface.cpp:125] Batch 16, loss = 0.568428
I0109 19:43:41.170056  3402 caffe_interface.cpp:125] Batch 16, top-1 = 0.84
I0109 19:43:41.170061  3402 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0109 19:43:41.174553  3402 caffe_interface.cpp:125] Batch 17, loss = 0.767734
I0109 19:43:41.174576  3402 caffe_interface.cpp:125] Batch 17, top-1 = 0.82
I0109 19:43:41.174580  3402 caffe_interface.cpp:125] Batch 17, top-5 = 0.98
I0109 19:43:41.179061  3402 caffe_interface.cpp:125] Batch 18, loss = 0.529784
I0109 19:43:41.179082  3402 caffe_interface.cpp:125] Batch 18, top-1 = 0.84
I0109 19:43:41.179088  3402 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 19:43:41.183578  3402 caffe_interface.cpp:125] Batch 19, loss = 0.520517
I0109 19:43:41.183599  3402 caffe_interface.cpp:125] Batch 19, top-1 = 0.88
I0109 19:43:41.183604  3402 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0109 19:43:41.188113  3402 caffe_interface.cpp:125] Batch 20, loss = 0.39295
I0109 19:43:41.188135  3402 caffe_interface.cpp:125] Batch 20, top-1 = 0.88
I0109 19:43:41.188140  3402 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0109 19:43:41.192620  3402 caffe_interface.cpp:125] Batch 21, loss = 0.976103
I0109 19:43:41.192642  3402 caffe_interface.cpp:125] Batch 21, top-1 = 0.76
I0109 19:43:41.192647  3402 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0109 19:43:41.197134  3402 caffe_interface.cpp:125] Batch 22, loss = 0.914945
I0109 19:43:41.197155  3402 caffe_interface.cpp:125] Batch 22, top-1 = 0.78
I0109 19:43:41.197161  3402 caffe_interface.cpp:125] Batch 22, top-5 = 0.96
I0109 19:43:41.201671  3402 caffe_interface.cpp:125] Batch 23, loss = 0.304189
I0109 19:43:41.201691  3402 caffe_interface.cpp:125] Batch 23, top-1 = 0.9
I0109 19:43:41.201696  3402 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 19:43:41.206164  3402 caffe_interface.cpp:125] Batch 24, loss = 0.579622
I0109 19:43:41.206185  3402 caffe_interface.cpp:125] Batch 24, top-1 = 0.84
I0109 19:43:41.206190  3402 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0109 19:43:41.210700  3402 caffe_interface.cpp:125] Batch 25, loss = 1.10003
I0109 19:43:41.210721  3402 caffe_interface.cpp:125] Batch 25, top-1 = 0.78
I0109 19:43:41.210741  3402 caffe_interface.cpp:125] Batch 25, top-5 = 0.92
I0109 19:43:41.215234  3402 caffe_interface.cpp:125] Batch 26, loss = 0.846663
I0109 19:43:41.215255  3402 caffe_interface.cpp:125] Batch 26, top-1 = 0.78
I0109 19:43:41.215261  3402 caffe_interface.cpp:125] Batch 26, top-5 = 1
I0109 19:43:41.219740  3402 caffe_interface.cpp:125] Batch 27, loss = 0.447783
I0109 19:43:41.219763  3402 caffe_interface.cpp:125] Batch 27, top-1 = 0.84
I0109 19:43:41.219769  3402 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 19:43:41.224234  3402 caffe_interface.cpp:125] Batch 28, loss = 0.491363
I0109 19:43:41.224256  3402 caffe_interface.cpp:125] Batch 28, top-1 = 0.9
I0109 19:43:41.224262  3402 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0109 19:43:41.228751  3402 caffe_interface.cpp:125] Batch 29, loss = 0.542529
I0109 19:43:41.228773  3402 caffe_interface.cpp:125] Batch 29, top-1 = 0.8
I0109 19:43:41.228780  3402 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 19:43:41.233254  3402 caffe_interface.cpp:125] Batch 30, loss = 0.543026
I0109 19:43:41.233276  3402 caffe_interface.cpp:125] Batch 30, top-1 = 0.84
I0109 19:43:41.233281  3402 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0109 19:43:41.237783  3402 caffe_interface.cpp:125] Batch 31, loss = 0.458382
I0109 19:43:41.237804  3402 caffe_interface.cpp:125] Batch 31, top-1 = 0.84
I0109 19:43:41.237809  3402 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 19:43:41.242316  3402 caffe_interface.cpp:125] Batch 32, loss = 0.707571
I0109 19:43:41.242341  3402 caffe_interface.cpp:125] Batch 32, top-1 = 0.72
I0109 19:43:41.242347  3402 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 19:43:41.246821  3402 caffe_interface.cpp:125] Batch 33, loss = 0.642107
I0109 19:43:41.246843  3402 caffe_interface.cpp:125] Batch 33, top-1 = 0.82
I0109 19:43:41.246850  3402 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 19:43:41.251346  3402 caffe_interface.cpp:125] Batch 34, loss = 0.511347
I0109 19:43:41.251368  3402 caffe_interface.cpp:125] Batch 34, top-1 = 0.86
I0109 19:43:41.251374  3402 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0109 19:43:41.255864  3402 caffe_interface.cpp:125] Batch 35, loss = 0.574678
I0109 19:43:41.255887  3402 caffe_interface.cpp:125] Batch 35, top-1 = 0.86
I0109 19:43:41.255892  3402 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 19:43:41.260365  3402 caffe_interface.cpp:125] Batch 36, loss = 0.742726
I0109 19:43:41.260387  3402 caffe_interface.cpp:125] Batch 36, top-1 = 0.84
I0109 19:43:41.260392  3402 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0109 19:43:41.264879  3402 caffe_interface.cpp:125] Batch 37, loss = 0.489455
I0109 19:43:41.264902  3402 caffe_interface.cpp:125] Batch 37, top-1 = 0.86
I0109 19:43:41.264907  3402 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0109 19:43:41.269366  3402 caffe_interface.cpp:125] Batch 38, loss = 0.794639
I0109 19:43:41.269388  3402 caffe_interface.cpp:125] Batch 38, top-1 = 0.82
I0109 19:43:41.269393  3402 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0109 19:43:41.273810  3402 caffe_interface.cpp:125] Batch 39, loss = 0.360421
I0109 19:43:41.273833  3402 caffe_interface.cpp:125] Batch 39, top-1 = 0.9
I0109 19:43:41.273838  3402 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 19:43:41.278242  3402 caffe_interface.cpp:125] Batch 40, loss = 0.568648
I0109 19:43:41.278265  3402 caffe_interface.cpp:125] Batch 40, top-1 = 0.82
I0109 19:43:41.278270  3402 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0109 19:43:41.282678  3402 caffe_interface.cpp:125] Batch 41, loss = 0.420751
I0109 19:43:41.282701  3402 caffe_interface.cpp:125] Batch 41, top-1 = 0.9
I0109 19:43:41.282707  3402 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 19:43:41.287117  3402 caffe_interface.cpp:125] Batch 42, loss = 0.424285
I0109 19:43:41.287140  3402 caffe_interface.cpp:125] Batch 42, top-1 = 0.84
I0109 19:43:41.287147  3402 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0109 19:43:41.291571  3402 caffe_interface.cpp:125] Batch 43, loss = 0.63751
I0109 19:43:41.291594  3402 caffe_interface.cpp:125] Batch 43, top-1 = 0.84
I0109 19:43:41.291615  3402 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0109 19:43:41.296049  3402 caffe_interface.cpp:125] Batch 44, loss = 0.448572
I0109 19:43:41.296072  3402 caffe_interface.cpp:125] Batch 44, top-1 = 0.82
I0109 19:43:41.296077  3402 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0109 19:43:41.300487  3402 caffe_interface.cpp:125] Batch 45, loss = 0.752642
I0109 19:43:41.300508  3402 caffe_interface.cpp:125] Batch 45, top-1 = 0.82
I0109 19:43:41.300514  3402 caffe_interface.cpp:125] Batch 45, top-5 = 0.96
I0109 19:43:41.304893  3402 caffe_interface.cpp:125] Batch 46, loss = 0.86138
I0109 19:43:41.304915  3402 caffe_interface.cpp:125] Batch 46, top-1 = 0.74
I0109 19:43:41.304921  3402 caffe_interface.cpp:125] Batch 46, top-5 = 0.96
I0109 19:43:41.309324  3402 caffe_interface.cpp:125] Batch 47, loss = 0.632439
I0109 19:43:41.309345  3402 caffe_interface.cpp:125] Batch 47, top-1 = 0.8
I0109 19:43:41.309350  3402 caffe_interface.cpp:125] Batch 47, top-5 = 0.98
I0109 19:43:41.313783  3402 caffe_interface.cpp:125] Batch 48, loss = 0.604952
I0109 19:43:41.313807  3402 caffe_interface.cpp:125] Batch 48, top-1 = 0.82
I0109 19:43:41.313812  3402 caffe_interface.cpp:125] Batch 48, top-5 = 0.98
I0109 19:43:41.318188  3402 caffe_interface.cpp:125] Batch 49, loss = 0.598779
I0109 19:43:41.318210  3402 caffe_interface.cpp:125] Batch 49, top-1 = 0.8
I0109 19:43:41.318217  3402 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0109 19:43:41.322616  3402 caffe_interface.cpp:125] Batch 50, loss = 0.437886
I0109 19:43:41.322638  3402 caffe_interface.cpp:125] Batch 50, top-1 = 0.84
I0109 19:43:41.322644  3402 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0109 19:43:41.327050  3402 caffe_interface.cpp:125] Batch 51, loss = 0.625907
I0109 19:43:41.327072  3402 caffe_interface.cpp:125] Batch 51, top-1 = 0.86
I0109 19:43:41.327077  3402 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0109 19:43:41.331481  3402 caffe_interface.cpp:125] Batch 52, loss = 0.682015
I0109 19:43:41.331504  3402 caffe_interface.cpp:125] Batch 52, top-1 = 0.8
I0109 19:43:41.331509  3402 caffe_interface.cpp:125] Batch 52, top-5 = 0.96
I0109 19:43:41.335924  3402 caffe_interface.cpp:125] Batch 53, loss = 0.674883
I0109 19:43:41.335947  3402 caffe_interface.cpp:125] Batch 53, top-1 = 0.74
I0109 19:43:41.335952  3402 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0109 19:43:41.340363  3402 caffe_interface.cpp:125] Batch 54, loss = 0.586983
I0109 19:43:41.340384  3402 caffe_interface.cpp:125] Batch 54, top-1 = 0.84
I0109 19:43:41.340389  3402 caffe_interface.cpp:125] Batch 54, top-5 = 0.98
I0109 19:43:41.344796  3402 caffe_interface.cpp:125] Batch 55, loss = 0.464846
I0109 19:43:41.344817  3402 caffe_interface.cpp:125] Batch 55, top-1 = 0.82
I0109 19:43:41.344822  3402 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0109 19:43:41.349220  3402 caffe_interface.cpp:125] Batch 56, loss = 0.633221
I0109 19:43:41.349241  3402 caffe_interface.cpp:125] Batch 56, top-1 = 0.86
I0109 19:43:41.349246  3402 caffe_interface.cpp:125] Batch 56, top-5 = 0.98
I0109 19:43:41.353674  3402 caffe_interface.cpp:125] Batch 57, loss = 0.612695
I0109 19:43:41.353696  3402 caffe_interface.cpp:125] Batch 57, top-1 = 0.76
I0109 19:43:41.353703  3402 caffe_interface.cpp:125] Batch 57, top-5 = 0.98
I0109 19:43:41.358135  3402 caffe_interface.cpp:125] Batch 58, loss = 0.803078
I0109 19:43:41.358156  3402 caffe_interface.cpp:125] Batch 58, top-1 = 0.82
I0109 19:43:41.358162  3402 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0109 19:43:41.362581  3402 caffe_interface.cpp:125] Batch 59, loss = 0.365131
I0109 19:43:41.362602  3402 caffe_interface.cpp:125] Batch 59, top-1 = 0.86
I0109 19:43:41.362607  3402 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0109 19:43:41.367012  3402 caffe_interface.cpp:125] Batch 60, loss = 0.653238
I0109 19:43:41.367033  3402 caffe_interface.cpp:125] Batch 60, top-1 = 0.78
I0109 19:43:41.367038  3402 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0109 19:43:41.371454  3402 caffe_interface.cpp:125] Batch 61, loss = 0.335853
I0109 19:43:41.371490  3402 caffe_interface.cpp:125] Batch 61, top-1 = 0.88
I0109 19:43:41.371496  3402 caffe_interface.cpp:125] Batch 61, top-5 = 1
I0109 19:43:41.375908  3402 caffe_interface.cpp:125] Batch 62, loss = 0.387302
I0109 19:43:41.375929  3402 caffe_interface.cpp:125] Batch 62, top-1 = 0.88
I0109 19:43:41.375936  3402 caffe_interface.cpp:125] Batch 62, top-5 = 0.98
I0109 19:43:41.380350  3402 caffe_interface.cpp:125] Batch 63, loss = 0.637132
I0109 19:43:41.380372  3402 caffe_interface.cpp:125] Batch 63, top-1 = 0.78
I0109 19:43:41.380376  3402 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0109 19:43:41.384791  3402 caffe_interface.cpp:125] Batch 64, loss = 0.342741
I0109 19:43:41.384814  3402 caffe_interface.cpp:125] Batch 64, top-1 = 0.9
I0109 19:43:41.384819  3402 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 19:43:41.389223  3402 caffe_interface.cpp:125] Batch 65, loss = 0.473815
I0109 19:43:41.389246  3402 caffe_interface.cpp:125] Batch 65, top-1 = 0.82
I0109 19:43:41.389252  3402 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 19:43:41.393713  3402 caffe_interface.cpp:125] Batch 66, loss = 0.944718
I0109 19:43:41.393734  3402 caffe_interface.cpp:125] Batch 66, top-1 = 0.68
I0109 19:43:41.393740  3402 caffe_interface.cpp:125] Batch 66, top-5 = 0.92
I0109 19:43:41.398154  3402 caffe_interface.cpp:125] Batch 67, loss = 0.297751
I0109 19:43:41.398176  3402 caffe_interface.cpp:125] Batch 67, top-1 = 0.86
I0109 19:43:41.398181  3402 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 19:43:41.402604  3402 caffe_interface.cpp:125] Batch 68, loss = 0.769475
I0109 19:43:41.402627  3402 caffe_interface.cpp:125] Batch 68, top-1 = 0.74
I0109 19:43:41.402632  3402 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0109 19:43:41.407027  3402 caffe_interface.cpp:125] Batch 69, loss = 0.974539
I0109 19:43:41.407049  3402 caffe_interface.cpp:125] Batch 69, top-1 = 0.74
I0109 19:43:41.407055  3402 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 19:43:41.411445  3402 caffe_interface.cpp:125] Batch 70, loss = 0.829299
I0109 19:43:41.411466  3402 caffe_interface.cpp:125] Batch 70, top-1 = 0.8
I0109 19:43:41.411473  3402 caffe_interface.cpp:125] Batch 70, top-5 = 0.98
I0109 19:43:41.415869  3402 caffe_interface.cpp:125] Batch 71, loss = 0.558372
I0109 19:43:41.415891  3402 caffe_interface.cpp:125] Batch 71, top-1 = 0.78
I0109 19:43:41.415896  3402 caffe_interface.cpp:125] Batch 71, top-5 = 0.98
I0109 19:43:41.420264  3402 caffe_interface.cpp:125] Batch 72, loss = 0.420011
I0109 19:43:41.420286  3402 caffe_interface.cpp:125] Batch 72, top-1 = 0.88
I0109 19:43:41.420291  3402 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 19:43:41.424715  3402 caffe_interface.cpp:125] Batch 73, loss = 0.64506
I0109 19:43:41.424736  3402 caffe_interface.cpp:125] Batch 73, top-1 = 0.82
I0109 19:43:41.424742  3402 caffe_interface.cpp:125] Batch 73, top-5 = 1
I0109 19:43:41.429244  3402 caffe_interface.cpp:125] Batch 74, loss = 0.500126
I0109 19:43:41.429268  3402 caffe_interface.cpp:125] Batch 74, top-1 = 0.86
I0109 19:43:41.429275  3402 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 19:43:41.433789  3402 caffe_interface.cpp:125] Batch 75, loss = 0.401824
I0109 19:43:41.433811  3402 caffe_interface.cpp:125] Batch 75, top-1 = 0.88
I0109 19:43:41.433816  3402 caffe_interface.cpp:125] Batch 75, top-5 = 1
I0109 19:43:41.438302  3402 caffe_interface.cpp:125] Batch 76, loss = 0.433958
I0109 19:43:41.438325  3402 caffe_interface.cpp:125] Batch 76, top-1 = 0.86
I0109 19:43:41.438331  3402 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 19:43:41.442806  3402 caffe_interface.cpp:125] Batch 77, loss = 0.533052
I0109 19:43:41.442828  3402 caffe_interface.cpp:125] Batch 77, top-1 = 0.84
I0109 19:43:41.442834  3402 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0109 19:43:41.447331  3402 caffe_interface.cpp:125] Batch 78, loss = 0.576765
I0109 19:43:41.447355  3402 caffe_interface.cpp:125] Batch 78, top-1 = 0.82
I0109 19:43:41.447360  3402 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0109 19:43:41.451845  3402 caffe_interface.cpp:125] Batch 79, loss = 0.942122
I0109 19:43:41.451884  3402 caffe_interface.cpp:125] Batch 79, top-1 = 0.8
I0109 19:43:41.451891  3402 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0109 19:43:41.456372  3402 caffe_interface.cpp:125] Batch 80, loss = 0.696821
I0109 19:43:41.456396  3402 caffe_interface.cpp:125] Batch 80, top-1 = 0.8
I0109 19:43:41.456401  3402 caffe_interface.cpp:125] Batch 80, top-5 = 0.98
I0109 19:43:41.460902  3402 caffe_interface.cpp:125] Batch 81, loss = 0.879388
I0109 19:43:41.460927  3402 caffe_interface.cpp:125] Batch 81, top-1 = 0.8
I0109 19:43:41.460932  3402 caffe_interface.cpp:125] Batch 81, top-5 = 0.96
I0109 19:43:41.465433  3402 caffe_interface.cpp:125] Batch 82, loss = 0.386805
I0109 19:43:41.465457  3402 caffe_interface.cpp:125] Batch 82, top-1 = 0.9
I0109 19:43:41.465463  3402 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 19:43:41.469947  3402 caffe_interface.cpp:125] Batch 83, loss = 0.503249
I0109 19:43:41.469970  3402 caffe_interface.cpp:125] Batch 83, top-1 = 0.84
I0109 19:43:41.469975  3402 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0109 19:43:41.474473  3402 caffe_interface.cpp:125] Batch 84, loss = 0.720659
I0109 19:43:41.474495  3402 caffe_interface.cpp:125] Batch 84, top-1 = 0.7
I0109 19:43:41.474501  3402 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 19:43:41.478999  3402 caffe_interface.cpp:125] Batch 85, loss = 0.829466
I0109 19:43:41.479022  3402 caffe_interface.cpp:125] Batch 85, top-1 = 0.76
I0109 19:43:41.479028  3402 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 19:43:41.483525  3402 caffe_interface.cpp:125] Batch 86, loss = 0.651182
I0109 19:43:41.483547  3402 caffe_interface.cpp:125] Batch 86, top-1 = 0.72
I0109 19:43:41.483553  3402 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 19:43:41.488046  3402 caffe_interface.cpp:125] Batch 87, loss = 0.744845
I0109 19:43:41.488070  3402 caffe_interface.cpp:125] Batch 87, top-1 = 0.82
I0109 19:43:41.488075  3402 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0109 19:43:41.492590  3402 caffe_interface.cpp:125] Batch 88, loss = 0.925938
I0109 19:43:41.492614  3402 caffe_interface.cpp:125] Batch 88, top-1 = 0.76
I0109 19:43:41.492619  3402 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 19:43:41.497113  3402 caffe_interface.cpp:125] Batch 89, loss = 0.614912
I0109 19:43:41.497138  3402 caffe_interface.cpp:125] Batch 89, top-1 = 0.78
I0109 19:43:41.497143  3402 caffe_interface.cpp:125] Batch 89, top-5 = 0.98
I0109 19:43:41.501660  3402 caffe_interface.cpp:125] Batch 90, loss = 0.350801
I0109 19:43:41.501682  3402 caffe_interface.cpp:125] Batch 90, top-1 = 0.88
I0109 19:43:41.501686  3402 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 19:43:41.506194  3402 caffe_interface.cpp:125] Batch 91, loss = 0.406687
I0109 19:43:41.506217  3402 caffe_interface.cpp:125] Batch 91, top-1 = 0.82
I0109 19:43:41.506223  3402 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 19:43:41.510720  3402 caffe_interface.cpp:125] Batch 92, loss = 0.707433
I0109 19:43:41.510745  3402 caffe_interface.cpp:125] Batch 92, top-1 = 0.8
I0109 19:43:41.510749  3402 caffe_interface.cpp:125] Batch 92, top-5 = 0.98
I0109 19:43:41.515226  3402 caffe_interface.cpp:125] Batch 93, loss = 0.290275
I0109 19:43:41.515249  3402 caffe_interface.cpp:125] Batch 93, top-1 = 0.9
I0109 19:43:41.515255  3402 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 19:43:41.519706  3402 caffe_interface.cpp:125] Batch 94, loss = 0.754182
I0109 19:43:41.519729  3402 caffe_interface.cpp:125] Batch 94, top-1 = 0.76
I0109 19:43:41.519734  3402 caffe_interface.cpp:125] Batch 94, top-5 = 0.96
I0109 19:43:41.524195  3402 caffe_interface.cpp:125] Batch 95, loss = 0.496773
I0109 19:43:41.524219  3402 caffe_interface.cpp:125] Batch 95, top-1 = 0.82
I0109 19:43:41.524224  3402 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 19:43:41.528753  3402 caffe_interface.cpp:125] Batch 96, loss = 0.373761
I0109 19:43:41.528777  3402 caffe_interface.cpp:125] Batch 96, top-1 = 0.84
I0109 19:43:41.528784  3402 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 19:43:41.533352  3402 caffe_interface.cpp:125] Batch 97, loss = 0.795498
I0109 19:43:41.533391  3402 caffe_interface.cpp:125] Batch 97, top-1 = 0.76
I0109 19:43:41.533398  3402 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0109 19:43:41.537950  3402 caffe_interface.cpp:125] Batch 98, loss = 0.676133
I0109 19:43:41.537972  3402 caffe_interface.cpp:125] Batch 98, top-1 = 0.78
I0109 19:43:41.537978  3402 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0109 19:43:41.542531  3402 caffe_interface.cpp:125] Batch 99, loss = 0.476628
I0109 19:43:41.542553  3402 caffe_interface.cpp:125] Batch 99, top-1 = 0.82
I0109 19:43:41.542559  3402 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0109 19:43:41.547103  3402 caffe_interface.cpp:125] Batch 100, loss = 0.313859
I0109 19:43:41.547127  3402 caffe_interface.cpp:125] Batch 100, top-1 = 0.92
I0109 19:43:41.547133  3402 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 19:43:41.551672  3402 caffe_interface.cpp:125] Batch 101, loss = 0.630457
I0109 19:43:41.551697  3402 caffe_interface.cpp:125] Batch 101, top-1 = 0.78
I0109 19:43:41.551702  3402 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 19:43:41.556265  3402 caffe_interface.cpp:125] Batch 102, loss = 0.554901
I0109 19:43:41.556288  3402 caffe_interface.cpp:125] Batch 102, top-1 = 0.78
I0109 19:43:41.556294  3402 caffe_interface.cpp:125] Batch 102, top-5 = 1
I0109 19:43:41.560859  3402 caffe_interface.cpp:125] Batch 103, loss = 0.563797
I0109 19:43:41.560883  3402 caffe_interface.cpp:125] Batch 103, top-1 = 0.82
I0109 19:43:41.560889  3402 caffe_interface.cpp:125] Batch 103, top-5 = 0.98
I0109 19:43:41.565429  3402 caffe_interface.cpp:125] Batch 104, loss = 0.462878
I0109 19:43:41.565454  3402 caffe_interface.cpp:125] Batch 104, top-1 = 0.84
I0109 19:43:41.565459  3402 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 19:43:41.570024  3402 caffe_interface.cpp:125] Batch 105, loss = 0.344881
I0109 19:43:41.570047  3402 caffe_interface.cpp:125] Batch 105, top-1 = 0.86
I0109 19:43:41.570053  3402 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 19:43:41.574632  3402 caffe_interface.cpp:125] Batch 106, loss = 0.735457
I0109 19:43:41.574654  3402 caffe_interface.cpp:125] Batch 106, top-1 = 0.82
I0109 19:43:41.574661  3402 caffe_interface.cpp:125] Batch 106, top-5 = 0.96
I0109 19:43:41.579246  3402 caffe_interface.cpp:125] Batch 107, loss = 0.570131
I0109 19:43:41.579269  3402 caffe_interface.cpp:125] Batch 107, top-1 = 0.82
I0109 19:43:41.579274  3402 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0109 19:43:41.583833  3402 caffe_interface.cpp:125] Batch 108, loss = 0.678271
I0109 19:43:41.583856  3402 caffe_interface.cpp:125] Batch 108, top-1 = 0.74
I0109 19:43:41.583861  3402 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0109 19:43:41.588423  3402 caffe_interface.cpp:125] Batch 109, loss = 0.591409
I0109 19:43:41.588446  3402 caffe_interface.cpp:125] Batch 109, top-1 = 0.82
I0109 19:43:41.588452  3402 caffe_interface.cpp:125] Batch 109, top-5 = 0.98
I0109 19:43:41.592998  3402 caffe_interface.cpp:125] Batch 110, loss = 0.375956
I0109 19:43:41.593022  3402 caffe_interface.cpp:125] Batch 110, top-1 = 0.88
I0109 19:43:41.593027  3402 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 19:43:41.597612  3402 caffe_interface.cpp:125] Batch 111, loss = 0.334671
I0109 19:43:41.597633  3402 caffe_interface.cpp:125] Batch 111, top-1 = 0.84
I0109 19:43:41.597638  3402 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0109 19:43:41.602216  3402 caffe_interface.cpp:125] Batch 112, loss = 0.705436
I0109 19:43:41.602239  3402 caffe_interface.cpp:125] Batch 112, top-1 = 0.78
I0109 19:43:41.602246  3402 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0109 19:43:41.606786  3402 caffe_interface.cpp:125] Batch 113, loss = 0.739489
I0109 19:43:41.606808  3402 caffe_interface.cpp:125] Batch 113, top-1 = 0.78
I0109 19:43:41.606814  3402 caffe_interface.cpp:125] Batch 113, top-5 = 0.98
I0109 19:43:41.611367  3402 caffe_interface.cpp:125] Batch 114, loss = 0.588104
I0109 19:43:41.611390  3402 caffe_interface.cpp:125] Batch 114, top-1 = 0.82
I0109 19:43:41.611397  3402 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0109 19:43:41.615972  3402 caffe_interface.cpp:125] Batch 115, loss = 0.480012
I0109 19:43:41.615995  3402 caffe_interface.cpp:125] Batch 115, top-1 = 0.86
I0109 19:43:41.616000  3402 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 19:43:41.620524  3402 caffe_interface.cpp:125] Batch 116, loss = 0.82857
I0109 19:43:41.620548  3402 caffe_interface.cpp:125] Batch 116, top-1 = 0.8
I0109 19:43:41.620553  3402 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0109 19:43:41.625057  3402 caffe_interface.cpp:125] Batch 117, loss = 0.298886
I0109 19:43:41.625082  3402 caffe_interface.cpp:125] Batch 117, top-1 = 0.9
I0109 19:43:41.625087  3402 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0109 19:43:41.629709  3402 caffe_interface.cpp:125] Batch 118, loss = 1.00117
I0109 19:43:41.629730  3402 caffe_interface.cpp:125] Batch 118, top-1 = 0.68
I0109 19:43:41.629736  3402 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0109 19:43:41.634327  3402 caffe_interface.cpp:125] Batch 119, loss = 0.46689
I0109 19:43:41.634351  3402 caffe_interface.cpp:125] Batch 119, top-1 = 0.86
I0109 19:43:41.634356  3402 caffe_interface.cpp:125] Batch 119, top-5 = 0.98
I0109 19:43:41.638942  3402 caffe_interface.cpp:125] Batch 120, loss = 0.910939
I0109 19:43:41.638967  3402 caffe_interface.cpp:125] Batch 120, top-1 = 0.74
I0109 19:43:41.638972  3402 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0109 19:43:41.643576  3402 caffe_interface.cpp:125] Batch 121, loss = 0.823551
I0109 19:43:41.643600  3402 caffe_interface.cpp:125] Batch 121, top-1 = 0.78
I0109 19:43:41.643605  3402 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 19:43:41.648221  3402 caffe_interface.cpp:125] Batch 122, loss = 0.757737
I0109 19:43:41.648244  3402 caffe_interface.cpp:125] Batch 122, top-1 = 0.76
I0109 19:43:41.648250  3402 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 19:43:41.652851  3402 caffe_interface.cpp:125] Batch 123, loss = 0.557561
I0109 19:43:41.652874  3402 caffe_interface.cpp:125] Batch 123, top-1 = 0.8
I0109 19:43:41.652880  3402 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0109 19:43:41.657482  3402 caffe_interface.cpp:125] Batch 124, loss = 0.338592
I0109 19:43:41.657506  3402 caffe_interface.cpp:125] Batch 124, top-1 = 0.88
I0109 19:43:41.657511  3402 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 19:43:41.662132  3402 caffe_interface.cpp:125] Batch 125, loss = 0.62112
I0109 19:43:41.662155  3402 caffe_interface.cpp:125] Batch 125, top-1 = 0.8
I0109 19:43:41.662161  3402 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0109 19:43:41.666780  3402 caffe_interface.cpp:125] Batch 126, loss = 0.41635
I0109 19:43:41.666803  3402 caffe_interface.cpp:125] Batch 126, top-1 = 0.88
I0109 19:43:41.666810  3402 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 19:43:41.671412  3402 caffe_interface.cpp:125] Batch 127, loss = 0.628785
I0109 19:43:41.671435  3402 caffe_interface.cpp:125] Batch 127, top-1 = 0.7
I0109 19:43:41.671442  3402 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 19:43:41.676045  3402 caffe_interface.cpp:125] Batch 128, loss = 1.07983
I0109 19:43:41.676069  3402 caffe_interface.cpp:125] Batch 128, top-1 = 0.74
I0109 19:43:41.676074  3402 caffe_interface.cpp:125] Batch 128, top-5 = 0.96
I0109 19:43:41.680675  3402 caffe_interface.cpp:125] Batch 129, loss = 0.568281
I0109 19:43:41.680698  3402 caffe_interface.cpp:125] Batch 129, top-1 = 0.74
I0109 19:43:41.680704  3402 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 19:43:41.685293  3402 caffe_interface.cpp:125] Batch 130, loss = 0.543394
I0109 19:43:41.685317  3402 caffe_interface.cpp:125] Batch 130, top-1 = 0.88
I0109 19:43:41.685322  3402 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 19:43:41.689926  3402 caffe_interface.cpp:125] Batch 131, loss = 0.755688
I0109 19:43:41.689949  3402 caffe_interface.cpp:125] Batch 131, top-1 = 0.76
I0109 19:43:41.689954  3402 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 19:43:41.694576  3402 caffe_interface.cpp:125] Batch 132, loss = 0.545659
I0109 19:43:41.694599  3402 caffe_interface.cpp:125] Batch 132, top-1 = 0.82
I0109 19:43:41.694624  3402 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 19:43:41.699203  3402 caffe_interface.cpp:125] Batch 133, loss = 0.447459
I0109 19:43:41.699226  3402 caffe_interface.cpp:125] Batch 133, top-1 = 0.86
I0109 19:43:41.699232  3402 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0109 19:43:41.703837  3402 caffe_interface.cpp:125] Batch 134, loss = 0.596412
I0109 19:43:41.703860  3402 caffe_interface.cpp:125] Batch 134, top-1 = 0.82
I0109 19:43:41.703866  3402 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0109 19:43:41.708456  3402 caffe_interface.cpp:125] Batch 135, loss = 0.561538
I0109 19:43:41.708478  3402 caffe_interface.cpp:125] Batch 135, top-1 = 0.8
I0109 19:43:41.708483  3402 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0109 19:43:41.713089  3402 caffe_interface.cpp:125] Batch 136, loss = 0.214425
I0109 19:43:41.713114  3402 caffe_interface.cpp:125] Batch 136, top-1 = 0.94
I0109 19:43:41.713119  3402 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 19:43:41.717737  3402 caffe_interface.cpp:125] Batch 137, loss = 0.517873
I0109 19:43:41.717761  3402 caffe_interface.cpp:125] Batch 137, top-1 = 0.82
I0109 19:43:41.717767  3402 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 19:43:41.722342  3402 caffe_interface.cpp:125] Batch 138, loss = 0.558091
I0109 19:43:41.722364  3402 caffe_interface.cpp:125] Batch 138, top-1 = 0.86
I0109 19:43:41.722370  3402 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0109 19:43:41.726972  3402 caffe_interface.cpp:125] Batch 139, loss = 0.679912
I0109 19:43:41.726996  3402 caffe_interface.cpp:125] Batch 139, top-1 = 0.74
I0109 19:43:41.727001  3402 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 19:43:41.731586  3402 caffe_interface.cpp:125] Batch 140, loss = 0.883746
I0109 19:43:41.731611  3402 caffe_interface.cpp:125] Batch 140, top-1 = 0.76
I0109 19:43:41.731616  3402 caffe_interface.cpp:125] Batch 140, top-5 = 0.98
I0109 19:43:41.736219  3402 caffe_interface.cpp:125] Batch 141, loss = 0.560744
I0109 19:43:41.736244  3402 caffe_interface.cpp:125] Batch 141, top-1 = 0.8
I0109 19:43:41.736250  3402 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0109 19:43:41.740856  3402 caffe_interface.cpp:125] Batch 142, loss = 0.497179
I0109 19:43:41.740880  3402 caffe_interface.cpp:125] Batch 142, top-1 = 0.82
I0109 19:43:41.740885  3402 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 19:43:41.745477  3402 caffe_interface.cpp:125] Batch 143, loss = 0.521408
I0109 19:43:41.745503  3402 caffe_interface.cpp:125] Batch 143, top-1 = 0.84
I0109 19:43:41.745508  3402 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 19:43:41.750111  3402 caffe_interface.cpp:125] Batch 144, loss = 0.554389
I0109 19:43:41.750135  3402 caffe_interface.cpp:125] Batch 144, top-1 = 0.86
I0109 19:43:41.750140  3402 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0109 19:43:41.754725  3402 caffe_interface.cpp:125] Batch 145, loss = 0.787013
I0109 19:43:41.754747  3402 caffe_interface.cpp:125] Batch 145, top-1 = 0.78
I0109 19:43:41.754753  3402 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0109 19:43:41.759315  3402 caffe_interface.cpp:125] Batch 146, loss = 0.612999
I0109 19:43:41.759336  3402 caffe_interface.cpp:125] Batch 146, top-1 = 0.78
I0109 19:43:41.759342  3402 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0109 19:43:41.763947  3402 caffe_interface.cpp:125] Batch 147, loss = 0.631511
I0109 19:43:41.763969  3402 caffe_interface.cpp:125] Batch 147, top-1 = 0.82
I0109 19:43:41.763975  3402 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0109 19:43:41.768544  3402 caffe_interface.cpp:125] Batch 148, loss = 0.947294
I0109 19:43:41.768566  3402 caffe_interface.cpp:125] Batch 148, top-1 = 0.74
I0109 19:43:41.768573  3402 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0109 19:43:41.773176  3402 caffe_interface.cpp:125] Batch 149, loss = 0.784148
I0109 19:43:41.773198  3402 caffe_interface.cpp:125] Batch 149, top-1 = 0.76
I0109 19:43:41.773205  3402 caffe_interface.cpp:125] Batch 149, top-5 = 0.98
I0109 19:43:41.777786  3402 caffe_interface.cpp:125] Batch 150, loss = 0.712064
I0109 19:43:41.777825  3402 caffe_interface.cpp:125] Batch 150, top-1 = 0.78
I0109 19:43:41.777832  3402 caffe_interface.cpp:125] Batch 150, top-5 = 0.96
I0109 19:43:41.782418  3402 caffe_interface.cpp:125] Batch 151, loss = 0.428523
I0109 19:43:41.782439  3402 caffe_interface.cpp:125] Batch 151, top-1 = 0.82
I0109 19:43:41.782444  3402 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 19:43:41.787032  3402 caffe_interface.cpp:125] Batch 152, loss = 0.745607
I0109 19:43:41.787053  3402 caffe_interface.cpp:125] Batch 152, top-1 = 0.78
I0109 19:43:41.787058  3402 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0109 19:43:41.791647  3402 caffe_interface.cpp:125] Batch 153, loss = 0.310039
I0109 19:43:41.791669  3402 caffe_interface.cpp:125] Batch 153, top-1 = 0.9
I0109 19:43:41.791676  3402 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 19:43:41.796268  3402 caffe_interface.cpp:125] Batch 154, loss = 0.23678
I0109 19:43:41.796289  3402 caffe_interface.cpp:125] Batch 154, top-1 = 0.9
I0109 19:43:41.796295  3402 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 19:43:41.800884  3402 caffe_interface.cpp:125] Batch 155, loss = 0.325324
I0109 19:43:41.800905  3402 caffe_interface.cpp:125] Batch 155, top-1 = 0.92
I0109 19:43:41.800910  3402 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 19:43:41.805495  3402 caffe_interface.cpp:125] Batch 156, loss = 0.433394
I0109 19:43:41.805516  3402 caffe_interface.cpp:125] Batch 156, top-1 = 0.86
I0109 19:43:41.805521  3402 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 19:43:41.810132  3402 caffe_interface.cpp:125] Batch 157, loss = 0.826623
I0109 19:43:41.810153  3402 caffe_interface.cpp:125] Batch 157, top-1 = 0.66
I0109 19:43:41.810158  3402 caffe_interface.cpp:125] Batch 157, top-5 = 0.98
I0109 19:43:41.814757  3402 caffe_interface.cpp:125] Batch 158, loss = 0.723733
I0109 19:43:41.814779  3402 caffe_interface.cpp:125] Batch 158, top-1 = 0.74
I0109 19:43:41.814785  3402 caffe_interface.cpp:125] Batch 158, top-5 = 0.96
I0109 19:43:41.819380  3402 caffe_interface.cpp:125] Batch 159, loss = 0.476411
I0109 19:43:41.819402  3402 caffe_interface.cpp:125] Batch 159, top-1 = 0.84
I0109 19:43:41.819407  3402 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0109 19:43:41.823972  3402 caffe_interface.cpp:125] Batch 160, loss = 1.04123
I0109 19:43:41.823994  3402 caffe_interface.cpp:125] Batch 160, top-1 = 0.76
I0109 19:43:41.824000  3402 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0109 19:43:41.828568  3402 caffe_interface.cpp:125] Batch 161, loss = 0.516445
I0109 19:43:41.828590  3402 caffe_interface.cpp:125] Batch 161, top-1 = 0.88
I0109 19:43:41.828596  3402 caffe_interface.cpp:125] Batch 161, top-5 = 0.98
I0109 19:43:41.833176  3402 caffe_interface.cpp:125] Batch 162, loss = 0.60713
I0109 19:43:41.833199  3402 caffe_interface.cpp:125] Batch 162, top-1 = 0.84
I0109 19:43:41.833204  3402 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0109 19:43:41.837786  3402 caffe_interface.cpp:125] Batch 163, loss = 0.455282
I0109 19:43:41.837807  3402 caffe_interface.cpp:125] Batch 163, top-1 = 0.84
I0109 19:43:41.837812  3402 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0109 19:43:41.842411  3402 caffe_interface.cpp:125] Batch 164, loss = 0.810805
I0109 19:43:41.842433  3402 caffe_interface.cpp:125] Batch 164, top-1 = 0.72
I0109 19:43:41.842438  3402 caffe_interface.cpp:125] Batch 164, top-5 = 1
I0109 19:43:41.847025  3402 caffe_interface.cpp:125] Batch 165, loss = 0.500756
I0109 19:43:41.847048  3402 caffe_interface.cpp:125] Batch 165, top-1 = 0.8
I0109 19:43:41.847054  3402 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 19:43:41.851647  3402 caffe_interface.cpp:125] Batch 166, loss = 0.518928
I0109 19:43:41.851668  3402 caffe_interface.cpp:125] Batch 166, top-1 = 0.86
I0109 19:43:41.851675  3402 caffe_interface.cpp:125] Batch 166, top-5 = 0.96
I0109 19:43:41.856259  3402 caffe_interface.cpp:125] Batch 167, loss = 0.512417
I0109 19:43:41.856281  3402 caffe_interface.cpp:125] Batch 167, top-1 = 0.82
I0109 19:43:41.856287  3402 caffe_interface.cpp:125] Batch 167, top-5 = 1
I0109 19:43:41.860904  3402 caffe_interface.cpp:125] Batch 168, loss = 1.07644
I0109 19:43:41.860926  3402 caffe_interface.cpp:125] Batch 168, top-1 = 0.72
I0109 19:43:41.860931  3402 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0109 19:43:41.865520  3402 caffe_interface.cpp:125] Batch 169, loss = 0.895469
I0109 19:43:41.865541  3402 caffe_interface.cpp:125] Batch 169, top-1 = 0.78
I0109 19:43:41.865546  3402 caffe_interface.cpp:125] Batch 169, top-5 = 1
I0109 19:43:41.870164  3402 caffe_interface.cpp:125] Batch 170, loss = 0.390882
I0109 19:43:41.870187  3402 caffe_interface.cpp:125] Batch 170, top-1 = 0.82
I0109 19:43:41.870193  3402 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0109 19:43:41.874781  3402 caffe_interface.cpp:125] Batch 171, loss = 0.467868
I0109 19:43:41.874804  3402 caffe_interface.cpp:125] Batch 171, top-1 = 0.82
I0109 19:43:41.874809  3402 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 19:43:41.879396  3402 caffe_interface.cpp:125] Batch 172, loss = 0.462116
I0109 19:43:41.879418  3402 caffe_interface.cpp:125] Batch 172, top-1 = 0.88
I0109 19:43:41.879423  3402 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 19:43:41.884024  3402 caffe_interface.cpp:125] Batch 173, loss = 1.15928
I0109 19:43:41.884047  3402 caffe_interface.cpp:125] Batch 173, top-1 = 0.64
I0109 19:43:41.884052  3402 caffe_interface.cpp:125] Batch 173, top-5 = 0.94
I0109 19:43:41.888645  3402 caffe_interface.cpp:125] Batch 174, loss = 0.873864
I0109 19:43:41.888667  3402 caffe_interface.cpp:125] Batch 174, top-1 = 0.7
I0109 19:43:41.888674  3402 caffe_interface.cpp:125] Batch 174, top-5 = 0.98
I0109 19:43:41.893251  3402 caffe_interface.cpp:125] Batch 175, loss = 0.555177
I0109 19:43:41.893273  3402 caffe_interface.cpp:125] Batch 175, top-1 = 0.82
I0109 19:43:41.893278  3402 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0109 19:43:41.897855  3402 caffe_interface.cpp:125] Batch 176, loss = 0.756279
I0109 19:43:41.897877  3402 caffe_interface.cpp:125] Batch 176, top-1 = 0.8
I0109 19:43:41.897882  3402 caffe_interface.cpp:125] Batch 176, top-5 = 1
I0109 19:43:41.902472  3402 caffe_interface.cpp:125] Batch 177, loss = 0.824187
I0109 19:43:41.902493  3402 caffe_interface.cpp:125] Batch 177, top-1 = 0.86
I0109 19:43:41.902499  3402 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0109 19:43:41.907088  3402 caffe_interface.cpp:125] Batch 178, loss = 0.279827
I0109 19:43:41.907109  3402 caffe_interface.cpp:125] Batch 178, top-1 = 0.88
I0109 19:43:41.907115  3402 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 19:43:41.911700  3402 caffe_interface.cpp:125] Batch 179, loss = 0.585116
I0109 19:43:41.911720  3402 caffe_interface.cpp:125] Batch 179, top-1 = 0.82
I0109 19:43:41.911725  3402 caffe_interface.cpp:125] Batch 179, top-5 = 0.96
I0109 19:43:41.911731  3402 caffe_interface.cpp:130] Loss: 0.612173
I0109 19:43:41.911741  3402 caffe_interface.cpp:142] loss = 0.612173 (* 1 = 0.612173 loss)
I0109 19:43:41.911747  3402 caffe_interface.cpp:142] top-1 = 0.813889
I0109 19:43:41.911753  3402 caffe_interface.cpp:142] top-5 = 0.988778
I0109 19:43:41.926594  3402 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/sparse.caffemodel
I0109 19:43:41.926633  3402 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.3:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.813889146    | -0.0511106849  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 51597          | -24.5536575%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 34160128       | -30.3617649%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config3.prototxt
I0109 19:43:42.044816  3455 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0109 19:43:42.158644  3455 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 19:43:42.159221  3455 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 19:43:42.159252  3455 caffe_interface.cpp:493] Using GPUs 0
I0109 19:43:42.159539  3455 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 19:43:42.820951  3455 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt"
type: "SGD"
I0109 19:43:42.821142  3455 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0109 19:43:42.821534  3455 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 19:43:42.821576  3455 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 19:43:42.821604  3455 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 19:43:42.821858  3455 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 19:43:42.821990  3455 layer_factory.hpp:77] Creating layer data
I0109 19:43:42.822188  3455 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:42.823009  3455 net.cpp:94] Creating Layer data
I0109 19:43:42.823045  3455 net.cpp:409] data -> data
I0109 19:43:42.823068  3455 net.cpp:409] data -> label
I0109 19:43:42.824195  3466 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 19:43:42.824252  3466 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 19:43:42.824359  3455 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 19:43:42.824476  3455 data_layer.cpp:83] output data size: 128,3,32,32
I0109 19:43:42.836016  3455 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:42.836083  3455 net.cpp:144] Setting up data
I0109 19:43:42.836107  3455 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 19:43:42.836117  3455 net.cpp:151] Top shape: 128 (128)
I0109 19:43:42.836123  3455 net.cpp:159] Memory required for data: 1573376
I0109 19:43:42.836138  3455 layer_factory.hpp:77] Creating layer conv1
I0109 19:43:42.836159  3455 net.cpp:94] Creating Layer conv1
I0109 19:43:42.836174  3455 net.cpp:435] conv1 <- data
I0109 19:43:42.836205  3455 net.cpp:409] conv1 -> conv1
I0109 19:43:42.837425  3455 net.cpp:144] Setting up conv1
I0109 19:43:42.837447  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.837455  3455 net.cpp:159] Memory required for data: 18350592
I0109 19:43:42.837481  3455 layer_factory.hpp:77] Creating layer bn1
I0109 19:43:42.837502  3455 net.cpp:94] Creating Layer bn1
I0109 19:43:42.837512  3455 net.cpp:435] bn1 <- conv1
I0109 19:43:42.837527  3455 net.cpp:409] bn1 -> scale1
I0109 19:43:42.838642  3455 net.cpp:144] Setting up bn1
I0109 19:43:42.838665  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.838672  3455 net.cpp:159] Memory required for data: 35127808
I0109 19:43:42.838694  3455 layer_factory.hpp:77] Creating layer relu1
I0109 19:43:42.838727  3455 net.cpp:94] Creating Layer relu1
I0109 19:43:42.838742  3455 net.cpp:435] relu1 <- scale1
I0109 19:43:42.838754  3455 net.cpp:409] relu1 -> relu1
I0109 19:43:42.838815  3455 net.cpp:144] Setting up relu1
I0109 19:43:42.838855  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.838891  3455 net.cpp:159] Memory required for data: 51905024
I0109 19:43:42.838925  3455 layer_factory.hpp:77] Creating layer conv2
I0109 19:43:42.838989  3455 net.cpp:94] Creating Layer conv2
I0109 19:43:42.839012  3455 net.cpp:435] conv2 <- relu1
I0109 19:43:42.839025  3455 net.cpp:409] conv2 -> conv2
I0109 19:43:42.840667  3455 net.cpp:144] Setting up conv2
I0109 19:43:42.840690  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.840696  3455 net.cpp:159] Memory required for data: 68682240
I0109 19:43:42.840708  3455 layer_factory.hpp:77] Creating layer bn2
I0109 19:43:42.840719  3455 net.cpp:94] Creating Layer bn2
I0109 19:43:42.840726  3455 net.cpp:435] bn2 <- conv2
I0109 19:43:42.840740  3455 net.cpp:409] bn2 -> scale2
I0109 19:43:42.841784  3455 net.cpp:144] Setting up bn2
I0109 19:43:42.841804  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.841809  3455 net.cpp:159] Memory required for data: 85459456
I0109 19:43:42.841820  3455 layer_factory.hpp:77] Creating layer relu2
I0109 19:43:42.841828  3455 net.cpp:94] Creating Layer relu2
I0109 19:43:42.841832  3455 net.cpp:435] relu2 <- scale2
I0109 19:43:42.841841  3455 net.cpp:409] relu2 -> relu2
I0109 19:43:42.842010  3455 net.cpp:144] Setting up relu2
I0109 19:43:42.842034  3455 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:43:42.842041  3455 net.cpp:159] Memory required for data: 102236672
I0109 19:43:42.842046  3455 layer_factory.hpp:77] Creating layer pool1
I0109 19:43:42.842063  3455 net.cpp:94] Creating Layer pool1
I0109 19:43:42.842070  3455 net.cpp:435] pool1 <- relu2
I0109 19:43:42.842082  3455 net.cpp:409] pool1 -> pool1
I0109 19:43:42.842149  3455 net.cpp:144] Setting up pool1
I0109 19:43:42.842167  3455 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:43:42.842173  3455 net.cpp:159] Memory required for data: 106430976
I0109 19:43:42.842180  3455 layer_factory.hpp:77] Creating layer drop1
I0109 19:43:42.842191  3455 net.cpp:94] Creating Layer drop1
I0109 19:43:42.842216  3455 net.cpp:435] drop1 <- pool1
I0109 19:43:42.842236  3455 net.cpp:409] drop1 -> drop1
I0109 19:43:42.842288  3455 net.cpp:144] Setting up drop1
I0109 19:43:42.842298  3455 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:43:42.842305  3455 net.cpp:159] Memory required for data: 110625280
I0109 19:43:42.842311  3455 layer_factory.hpp:77] Creating layer conv3
I0109 19:43:42.842330  3455 net.cpp:94] Creating Layer conv3
I0109 19:43:42.842337  3455 net.cpp:435] conv3 <- drop1
I0109 19:43:42.842352  3455 net.cpp:409] conv3 -> conv3
I0109 19:43:42.843708  3455 net.cpp:144] Setting up conv3
I0109 19:43:42.843729  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.843734  3455 net.cpp:159] Memory required for data: 119013888
I0109 19:43:42.843746  3455 layer_factory.hpp:77] Creating layer bn3
I0109 19:43:42.843762  3455 net.cpp:94] Creating Layer bn3
I0109 19:43:42.843768  3455 net.cpp:435] bn3 <- conv3
I0109 19:43:42.843781  3455 net.cpp:409] bn3 -> scale3
I0109 19:43:42.844789  3455 net.cpp:144] Setting up bn3
I0109 19:43:42.844806  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.844810  3455 net.cpp:159] Memory required for data: 127402496
I0109 19:43:42.844828  3455 layer_factory.hpp:77] Creating layer relu3
I0109 19:43:42.844841  3455 net.cpp:94] Creating Layer relu3
I0109 19:43:42.844846  3455 net.cpp:435] relu3 <- scale3
I0109 19:43:42.844859  3455 net.cpp:409] relu3 -> relu3
I0109 19:43:42.844898  3455 net.cpp:144] Setting up relu3
I0109 19:43:42.844919  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.844926  3455 net.cpp:159] Memory required for data: 135791104
I0109 19:43:42.844933  3455 layer_factory.hpp:77] Creating layer conv4
I0109 19:43:42.844947  3455 net.cpp:94] Creating Layer conv4
I0109 19:43:42.844956  3455 net.cpp:435] conv4 <- relu3
I0109 19:43:42.844967  3455 net.cpp:409] conv4 -> conv4
I0109 19:43:42.845939  3455 net.cpp:144] Setting up conv4
I0109 19:43:42.845957  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.845964  3455 net.cpp:159] Memory required for data: 144179712
I0109 19:43:42.845974  3455 layer_factory.hpp:77] Creating layer bn4
I0109 19:43:42.845989  3455 net.cpp:94] Creating Layer bn4
I0109 19:43:42.845995  3455 net.cpp:435] bn4 <- conv4
I0109 19:43:42.846009  3455 net.cpp:409] bn4 -> scale4
I0109 19:43:42.847028  3455 net.cpp:144] Setting up bn4
I0109 19:43:42.847044  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.847048  3455 net.cpp:159] Memory required for data: 152568320
I0109 19:43:42.847060  3455 layer_factory.hpp:77] Creating layer relu4
I0109 19:43:42.847069  3455 net.cpp:94] Creating Layer relu4
I0109 19:43:42.847074  3455 net.cpp:435] relu4 <- scale4
I0109 19:43:42.847080  3455 net.cpp:409] relu4 -> relu4
I0109 19:43:42.847118  3455 net.cpp:144] Setting up relu4
I0109 19:43:42.847132  3455 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:43:42.847138  3455 net.cpp:159] Memory required for data: 160956928
I0109 19:43:42.847144  3455 layer_factory.hpp:77] Creating layer pool2
I0109 19:43:42.847160  3455 net.cpp:94] Creating Layer pool2
I0109 19:43:42.847168  3455 net.cpp:435] pool2 <- relu4
I0109 19:43:42.847177  3455 net.cpp:409] pool2 -> pool2
I0109 19:43:42.847255  3455 net.cpp:144] Setting up pool2
I0109 19:43:42.847272  3455 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:43:42.847276  3455 net.cpp:159] Memory required for data: 163054080
I0109 19:43:42.847280  3455 layer_factory.hpp:77] Creating layer drop2
I0109 19:43:42.847293  3455 net.cpp:94] Creating Layer drop2
I0109 19:43:42.847301  3455 net.cpp:435] drop2 <- pool2
I0109 19:43:42.847308  3455 net.cpp:409] drop2 -> drop2
I0109 19:43:42.847491  3455 net.cpp:144] Setting up drop2
I0109 19:43:42.847508  3455 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:43:42.847517  3455 net.cpp:159] Memory required for data: 165151232
I0109 19:43:42.847523  3455 layer_factory.hpp:77] Creating layer fc1
I0109 19:43:42.847581  3455 net.cpp:94] Creating Layer fc1
I0109 19:43:42.847604  3455 net.cpp:435] fc1 <- drop2
I0109 19:43:42.847690  3455 net.cpp:409] fc1 -> fc1
I0109 19:43:42.872210  3455 net.cpp:144] Setting up fc1
I0109 19:43:42.872238  3455 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:43:42.872243  3455 net.cpp:159] Memory required for data: 165413376
I0109 19:43:42.872253  3455 layer_factory.hpp:77] Creating layer bn5
I0109 19:43:42.872267  3455 net.cpp:94] Creating Layer bn5
I0109 19:43:42.872275  3455 net.cpp:435] bn5 <- fc1
I0109 19:43:42.872283  3455 net.cpp:409] bn5 -> scale5
I0109 19:43:42.872885  3455 net.cpp:144] Setting up bn5
I0109 19:43:42.872902  3455 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:43:42.872906  3455 net.cpp:159] Memory required for data: 165675520
I0109 19:43:42.872925  3455 layer_factory.hpp:77] Creating layer relu5
I0109 19:43:42.872938  3455 net.cpp:94] Creating Layer relu5
I0109 19:43:42.872944  3455 net.cpp:435] relu5 <- scale5
I0109 19:43:42.872952  3455 net.cpp:409] relu5 -> relu5
I0109 19:43:42.872978  3455 net.cpp:144] Setting up relu5
I0109 19:43:42.872993  3455 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:43:42.873000  3455 net.cpp:159] Memory required for data: 165937664
I0109 19:43:42.873006  3455 layer_factory.hpp:77] Creating layer drop3
I0109 19:43:42.873015  3455 net.cpp:94] Creating Layer drop3
I0109 19:43:42.873023  3455 net.cpp:435] drop3 <- relu5
I0109 19:43:42.873035  3455 net.cpp:409] drop3 -> drop3
I0109 19:43:42.873095  3455 net.cpp:144] Setting up drop3
I0109 19:43:42.873111  3455 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:43:42.873118  3455 net.cpp:159] Memory required for data: 166199808
I0109 19:43:42.873124  3455 layer_factory.hpp:77] Creating layer fc2
I0109 19:43:42.873139  3455 net.cpp:94] Creating Layer fc2
I0109 19:43:42.873152  3455 net.cpp:435] fc2 <- drop3
I0109 19:43:42.873164  3455 net.cpp:409] fc2 -> fc2
I0109 19:43:42.873356  3455 net.cpp:144] Setting up fc2
I0109 19:43:42.873373  3455 net.cpp:151] Top shape: 128 10 (1280)
I0109 19:43:42.873379  3455 net.cpp:159] Memory required for data: 166204928
I0109 19:43:42.873390  3455 layer_factory.hpp:77] Creating layer loss
I0109 19:43:42.873401  3455 net.cpp:94] Creating Layer loss
I0109 19:43:42.873414  3455 net.cpp:435] loss <- fc2
I0109 19:43:42.873422  3455 net.cpp:435] loss <- label
I0109 19:43:42.873432  3455 net.cpp:409] loss -> loss
I0109 19:43:42.873456  3455 layer_factory.hpp:77] Creating layer loss
I0109 19:43:42.874295  3455 net.cpp:144] Setting up loss
I0109 19:43:42.874320  3455 net.cpp:151] Top shape: (1)
I0109 19:43:42.874327  3455 net.cpp:154]     with loss weight 1
I0109 19:43:42.874352  3455 net.cpp:159] Memory required for data: 166204932
I0109 19:43:42.874366  3455 net.cpp:220] loss needs backward computation.
I0109 19:43:42.874387  3455 net.cpp:220] fc2 needs backward computation.
I0109 19:43:42.874404  3455 net.cpp:220] drop3 needs backward computation.
I0109 19:43:42.874413  3455 net.cpp:220] relu5 needs backward computation.
I0109 19:43:42.874418  3455 net.cpp:220] bn5 needs backward computation.
I0109 19:43:42.874424  3455 net.cpp:220] fc1 needs backward computation.
I0109 19:43:42.874431  3455 net.cpp:220] drop2 needs backward computation.
I0109 19:43:42.874444  3455 net.cpp:220] pool2 needs backward computation.
I0109 19:43:42.874450  3455 net.cpp:220] relu4 needs backward computation.
I0109 19:43:42.874457  3455 net.cpp:220] bn4 needs backward computation.
I0109 19:43:42.874471  3455 net.cpp:220] conv4 needs backward computation.
I0109 19:43:42.874478  3455 net.cpp:220] relu3 needs backward computation.
I0109 19:43:42.874485  3455 net.cpp:220] bn3 needs backward computation.
I0109 19:43:42.874491  3455 net.cpp:220] conv3 needs backward computation.
I0109 19:43:42.874503  3455 net.cpp:220] drop1 needs backward computation.
I0109 19:43:42.874511  3455 net.cpp:220] pool1 needs backward computation.
I0109 19:43:42.874517  3455 net.cpp:220] relu2 needs backward computation.
I0109 19:43:42.874531  3455 net.cpp:220] bn2 needs backward computation.
I0109 19:43:42.874538  3455 net.cpp:220] conv2 needs backward computation.
I0109 19:43:42.874548  3455 net.cpp:220] relu1 needs backward computation.
I0109 19:43:42.874581  3455 net.cpp:220] bn1 needs backward computation.
I0109 19:43:42.874589  3455 net.cpp:220] conv1 needs backward computation.
I0109 19:43:42.874598  3455 net.cpp:222] data does not need backward computation.
I0109 19:43:42.874604  3455 net.cpp:264] This network produces output loss
I0109 19:43:42.874645  3455 net.cpp:284] Network initialization done.
I0109 19:43:42.875016  3455 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0109 19:43:42.875079  3455 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:43:42.875308  3455 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:43:42.875452  3455 layer_factory.hpp:77] Creating layer data
I0109 19:43:42.875515  3455 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:42.875754  3455 net.cpp:94] Creating Layer data
I0109 19:43:42.875795  3455 net.cpp:409] data -> data
I0109 19:43:42.875830  3455 net.cpp:409] data -> label
I0109 19:43:42.876793  3472 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:43:42.876837  3472 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:43:42.876952  3455 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:43:42.877086  3455 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:43:42.885054  3455 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:43:42.885123  3455 net.cpp:144] Setting up data
I0109 19:43:42.885169  3455 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:43:42.885185  3455 net.cpp:151] Top shape: 50 (50)
I0109 19:43:42.885191  3455 net.cpp:159] Memory required for data: 614600
I0109 19:43:42.885200  3455 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:43:42.885219  3455 net.cpp:94] Creating Layer label_data_1_split
I0109 19:43:42.885228  3455 net.cpp:435] label_data_1_split <- label
I0109 19:43:42.885249  3455 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:43:42.885274  3455 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:43:42.885295  3455 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:43:42.885365  3455 net.cpp:144] Setting up label_data_1_split
I0109 19:43:42.885382  3455 net.cpp:151] Top shape: 50 (50)
I0109 19:43:42.885391  3455 net.cpp:151] Top shape: 50 (50)
I0109 19:43:42.885398  3455 net.cpp:151] Top shape: 50 (50)
I0109 19:43:42.885411  3455 net.cpp:159] Memory required for data: 615200
I0109 19:43:42.885417  3455 layer_factory.hpp:77] Creating layer conv1
I0109 19:43:42.885447  3455 net.cpp:94] Creating Layer conv1
I0109 19:43:42.885462  3455 net.cpp:435] conv1 <- data
I0109 19:43:42.885474  3455 net.cpp:409] conv1 -> conv1
I0109 19:43:42.885833  3455 net.cpp:144] Setting up conv1
I0109 19:43:42.885855  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.885862  3455 net.cpp:159] Memory required for data: 7168800
I0109 19:43:42.885877  3455 layer_factory.hpp:77] Creating layer bn1
I0109 19:43:42.885900  3455 net.cpp:94] Creating Layer bn1
I0109 19:43:42.885915  3455 net.cpp:435] bn1 <- conv1
I0109 19:43:42.885931  3455 net.cpp:409] bn1 -> scale1
I0109 19:43:42.887204  3455 net.cpp:144] Setting up bn1
I0109 19:43:42.887223  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.887231  3455 net.cpp:159] Memory required for data: 13722400
I0109 19:43:42.887251  3455 layer_factory.hpp:77] Creating layer relu1
I0109 19:43:42.887274  3455 net.cpp:94] Creating Layer relu1
I0109 19:43:42.887289  3455 net.cpp:435] relu1 <- scale1
I0109 19:43:42.887303  3455 net.cpp:409] relu1 -> relu1
I0109 19:43:42.887349  3455 net.cpp:144] Setting up relu1
I0109 19:43:42.887364  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.887372  3455 net.cpp:159] Memory required for data: 20276000
I0109 19:43:42.887378  3455 layer_factory.hpp:77] Creating layer conv2
I0109 19:43:42.887404  3455 net.cpp:94] Creating Layer conv2
I0109 19:43:42.887418  3455 net.cpp:435] conv2 <- relu1
I0109 19:43:42.887434  3455 net.cpp:409] conv2 -> conv2
I0109 19:43:42.888020  3455 net.cpp:144] Setting up conv2
I0109 19:43:42.888038  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.888046  3455 net.cpp:159] Memory required for data: 26829600
I0109 19:43:42.888059  3455 layer_factory.hpp:77] Creating layer bn2
I0109 19:43:42.888095  3455 net.cpp:94] Creating Layer bn2
I0109 19:43:42.888149  3455 net.cpp:435] bn2 <- conv2
I0109 19:43:42.888203  3455 net.cpp:409] bn2 -> scale2
I0109 19:43:42.889564  3455 net.cpp:144] Setting up bn2
I0109 19:43:42.889647  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.889696  3455 net.cpp:159] Memory required for data: 33383200
I0109 19:43:42.889747  3455 layer_factory.hpp:77] Creating layer relu2
I0109 19:43:42.889786  3455 net.cpp:94] Creating Layer relu2
I0109 19:43:42.889811  3455 net.cpp:435] relu2 <- scale2
I0109 19:43:42.889835  3455 net.cpp:409] relu2 -> relu2
I0109 19:43:42.889896  3455 net.cpp:144] Setting up relu2
I0109 19:43:42.889976  3455 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:43:42.890065  3455 net.cpp:159] Memory required for data: 39936800
I0109 19:43:42.890080  3455 layer_factory.hpp:77] Creating layer pool1
I0109 19:43:42.890182  3455 net.cpp:94] Creating Layer pool1
I0109 19:43:42.890326  3455 net.cpp:435] pool1 <- relu2
I0109 19:43:42.890398  3455 net.cpp:409] pool1 -> pool1
I0109 19:43:42.890523  3455 net.cpp:144] Setting up pool1
I0109 19:43:42.890542  3455 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:43:42.890549  3455 net.cpp:159] Memory required for data: 41575200
I0109 19:43:42.890594  3455 layer_factory.hpp:77] Creating layer drop1
I0109 19:43:42.890631  3455 net.cpp:94] Creating Layer drop1
I0109 19:43:42.890673  3455 net.cpp:435] drop1 <- pool1
I0109 19:43:42.890727  3455 net.cpp:409] drop1 -> drop1
I0109 19:43:42.890933  3455 net.cpp:144] Setting up drop1
I0109 19:43:42.890981  3455 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:43:42.891023  3455 net.cpp:159] Memory required for data: 43213600
I0109 19:43:42.891062  3455 layer_factory.hpp:77] Creating layer conv3
I0109 19:43:42.891117  3455 net.cpp:94] Creating Layer conv3
I0109 19:43:42.891157  3455 net.cpp:435] conv3 <- drop1
I0109 19:43:42.891203  3455 net.cpp:409] conv3 -> conv3
I0109 19:43:42.891901  3455 net.cpp:144] Setting up conv3
I0109 19:43:42.892015  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.892055  3455 net.cpp:159] Memory required for data: 46490400
I0109 19:43:42.892158  3455 layer_factory.hpp:77] Creating layer bn3
I0109 19:43:42.892202  3455 net.cpp:94] Creating Layer bn3
I0109 19:43:42.892287  3455 net.cpp:435] bn3 <- conv3
I0109 19:43:42.892313  3455 net.cpp:409] bn3 -> scale3
I0109 19:43:42.893630  3455 net.cpp:144] Setting up bn3
I0109 19:43:42.893653  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.893661  3455 net.cpp:159] Memory required for data: 49767200
I0109 19:43:42.893685  3455 layer_factory.hpp:77] Creating layer relu3
I0109 19:43:42.893748  3455 net.cpp:94] Creating Layer relu3
I0109 19:43:42.893759  3455 net.cpp:435] relu3 <- scale3
I0109 19:43:42.893769  3455 net.cpp:409] relu3 -> relu3
I0109 19:43:42.893865  3455 net.cpp:144] Setting up relu3
I0109 19:43:42.893949  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.894016  3455 net.cpp:159] Memory required for data: 53044000
I0109 19:43:42.894057  3455 layer_factory.hpp:77] Creating layer conv4
I0109 19:43:42.894156  3455 net.cpp:94] Creating Layer conv4
I0109 19:43:42.894222  3455 net.cpp:435] conv4 <- relu3
I0109 19:43:42.894294  3455 net.cpp:409] conv4 -> conv4
I0109 19:43:42.895156  3455 net.cpp:144] Setting up conv4
I0109 19:43:42.895233  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.895299  3455 net.cpp:159] Memory required for data: 56320800
I0109 19:43:42.895364  3455 layer_factory.hpp:77] Creating layer bn4
I0109 19:43:42.895393  3455 net.cpp:94] Creating Layer bn4
I0109 19:43:42.895491  3455 net.cpp:435] bn4 <- conv4
I0109 19:43:42.895539  3455 net.cpp:409] bn4 -> scale4
I0109 19:43:42.896770  3455 net.cpp:144] Setting up bn4
I0109 19:43:42.896791  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.896800  3455 net.cpp:159] Memory required for data: 59597600
I0109 19:43:42.896879  3455 layer_factory.hpp:77] Creating layer relu4
I0109 19:43:42.896900  3455 net.cpp:94] Creating Layer relu4
I0109 19:43:42.896910  3455 net.cpp:435] relu4 <- scale4
I0109 19:43:42.896970  3455 net.cpp:409] relu4 -> relu4
I0109 19:43:42.897019  3455 net.cpp:144] Setting up relu4
I0109 19:43:42.897035  3455 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:43:42.897043  3455 net.cpp:159] Memory required for data: 62874400
I0109 19:43:42.897049  3455 layer_factory.hpp:77] Creating layer pool2
I0109 19:43:42.897063  3455 net.cpp:94] Creating Layer pool2
I0109 19:43:42.897092  3455 net.cpp:435] pool2 <- relu4
I0109 19:43:42.897131  3455 net.cpp:409] pool2 -> pool2
I0109 19:43:42.897215  3455 net.cpp:144] Setting up pool2
I0109 19:43:42.897231  3455 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:43:42.897235  3455 net.cpp:159] Memory required for data: 63693600
I0109 19:43:42.897239  3455 layer_factory.hpp:77] Creating layer drop2
I0109 19:43:42.897249  3455 net.cpp:94] Creating Layer drop2
I0109 19:43:42.897253  3455 net.cpp:435] drop2 <- pool2
I0109 19:43:42.897259  3455 net.cpp:409] drop2 -> drop2
I0109 19:43:42.897313  3455 net.cpp:144] Setting up drop2
I0109 19:43:42.897338  3455 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:43:42.897342  3455 net.cpp:159] Memory required for data: 64512800
I0109 19:43:42.897346  3455 layer_factory.hpp:77] Creating layer fc1
I0109 19:43:42.897356  3455 net.cpp:94] Creating Layer fc1
I0109 19:43:42.897367  3455 net.cpp:435] fc1 <- drop2
I0109 19:43:42.897377  3455 net.cpp:409] fc1 -> fc1
I0109 19:43:42.918869  3455 net.cpp:144] Setting up fc1
I0109 19:43:42.918900  3455 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:42.918903  3455 net.cpp:159] Memory required for data: 64615200
I0109 19:43:42.918913  3455 layer_factory.hpp:77] Creating layer bn5
I0109 19:43:42.918927  3455 net.cpp:94] Creating Layer bn5
I0109 19:43:42.918933  3455 net.cpp:435] bn5 <- fc1
I0109 19:43:42.918946  3455 net.cpp:409] bn5 -> scale5
I0109 19:43:42.919638  3455 net.cpp:144] Setting up bn5
I0109 19:43:42.919657  3455 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:42.919662  3455 net.cpp:159] Memory required for data: 64717600
I0109 19:43:42.919682  3455 layer_factory.hpp:77] Creating layer relu5
I0109 19:43:42.919703  3455 net.cpp:94] Creating Layer relu5
I0109 19:43:42.919711  3455 net.cpp:435] relu5 <- scale5
I0109 19:43:42.919721  3455 net.cpp:409] relu5 -> relu5
I0109 19:43:42.919769  3455 net.cpp:144] Setting up relu5
I0109 19:43:42.919788  3455 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:42.919795  3455 net.cpp:159] Memory required for data: 64820000
I0109 19:43:42.919800  3455 layer_factory.hpp:77] Creating layer drop3
I0109 19:43:42.919807  3455 net.cpp:94] Creating Layer drop3
I0109 19:43:42.919813  3455 net.cpp:435] drop3 <- relu5
I0109 19:43:42.919822  3455 net.cpp:409] drop3 -> drop3
I0109 19:43:42.919920  3455 net.cpp:144] Setting up drop3
I0109 19:43:42.919937  3455 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:43:42.919941  3455 net.cpp:159] Memory required for data: 64922400
I0109 19:43:42.919945  3455 layer_factory.hpp:77] Creating layer fc2
I0109 19:43:42.919958  3455 net.cpp:94] Creating Layer fc2
I0109 19:43:42.919968  3455 net.cpp:435] fc2 <- drop3
I0109 19:43:42.919981  3455 net.cpp:409] fc2 -> fc2
I0109 19:43:42.920184  3455 net.cpp:144] Setting up fc2
I0109 19:43:42.920199  3455 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:42.920203  3455 net.cpp:159] Memory required for data: 64924400
I0109 19:43:42.920210  3455 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:43:42.920218  3455 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:43:42.920230  3455 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:43:42.920238  3455 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:43:42.920254  3455 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:43:42.920275  3455 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:43:42.920354  3455 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:43:42.920369  3455 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:42.920374  3455 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:42.920379  3455 net.cpp:151] Top shape: 50 10 (500)
I0109 19:43:42.920383  3455 net.cpp:159] Memory required for data: 64930400
I0109 19:43:42.920387  3455 layer_factory.hpp:77] Creating layer loss
I0109 19:43:42.920398  3455 net.cpp:94] Creating Layer loss
I0109 19:43:42.920403  3455 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:43:42.920415  3455 net.cpp:435] loss <- label_data_1_split_0
I0109 19:43:42.920425  3455 net.cpp:409] loss -> loss
I0109 19:43:42.920446  3455 layer_factory.hpp:77] Creating layer loss
I0109 19:43:42.920576  3455 net.cpp:144] Setting up loss
I0109 19:43:42.920593  3455 net.cpp:151] Top shape: (1)
I0109 19:43:42.920596  3455 net.cpp:154]     with loss weight 1
I0109 19:43:42.920611  3455 net.cpp:159] Memory required for data: 64930404
I0109 19:43:42.920616  3455 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:43:42.920632  3455 net.cpp:94] Creating Layer accuracy-top1
I0109 19:43:42.920648  3455 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:43:42.920656  3455 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:43:42.920691  3455 net.cpp:409] accuracy-top1 -> top-1
I0109 19:43:42.920733  3455 net.cpp:144] Setting up accuracy-top1
I0109 19:43:42.920755  3455 net.cpp:151] Top shape: (1)
I0109 19:43:42.920761  3455 net.cpp:159] Memory required for data: 64930408
I0109 19:43:42.920768  3455 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:43:42.920780  3455 net.cpp:94] Creating Layer accuracy-top5
I0109 19:43:42.920787  3455 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:43:42.920795  3455 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:43:42.920812  3455 net.cpp:409] accuracy-top5 -> top-5
I0109 19:43:42.920835  3455 net.cpp:144] Setting up accuracy-top5
I0109 19:43:42.920843  3455 net.cpp:151] Top shape: (1)
I0109 19:43:42.920850  3455 net.cpp:159] Memory required for data: 64930412
I0109 19:43:42.920857  3455 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:43:42.920866  3455 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:43:42.920876  3455 net.cpp:220] loss needs backward computation.
I0109 19:43:42.920883  3455 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:43:42.920889  3455 net.cpp:220] fc2 needs backward computation.
I0109 19:43:42.920902  3455 net.cpp:220] drop3 needs backward computation.
I0109 19:43:42.920907  3455 net.cpp:220] relu5 needs backward computation.
I0109 19:43:42.920914  3455 net.cpp:220] bn5 needs backward computation.
I0109 19:43:42.920923  3455 net.cpp:220] fc1 needs backward computation.
I0109 19:43:42.920930  3455 net.cpp:220] drop2 needs backward computation.
I0109 19:43:42.920938  3455 net.cpp:220] pool2 needs backward computation.
I0109 19:43:42.920943  3455 net.cpp:220] relu4 needs backward computation.
I0109 19:43:42.920951  3455 net.cpp:220] bn4 needs backward computation.
I0109 19:43:42.920959  3455 net.cpp:220] conv4 needs backward computation.
I0109 19:43:42.920965  3455 net.cpp:220] relu3 needs backward computation.
I0109 19:43:42.920974  3455 net.cpp:220] bn3 needs backward computation.
I0109 19:43:42.920980  3455 net.cpp:220] conv3 needs backward computation.
I0109 19:43:42.920990  3455 net.cpp:220] drop1 needs backward computation.
I0109 19:43:42.920996  3455 net.cpp:220] pool1 needs backward computation.
I0109 19:43:42.921005  3455 net.cpp:220] relu2 needs backward computation.
I0109 19:43:42.921011  3455 net.cpp:220] bn2 needs backward computation.
I0109 19:43:42.921020  3455 net.cpp:220] conv2 needs backward computation.
I0109 19:43:42.921033  3455 net.cpp:220] relu1 needs backward computation.
I0109 19:43:42.921041  3455 net.cpp:220] bn1 needs backward computation.
I0109 19:43:42.921047  3455 net.cpp:220] conv1 needs backward computation.
I0109 19:43:42.921056  3455 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:43:42.921066  3455 net.cpp:222] data does not need backward computation.
I0109 19:43:42.921070  3455 net.cpp:264] This network produces output loss
I0109 19:43:42.921080  3455 net.cpp:264] This network produces output top-1
I0109 19:43:42.921088  3455 net.cpp:264] This network produces output top-5
I0109 19:43:42.921128  3455 net.cpp:284] Network initialization done.
I0109 19:43:42.921267  3455 solver.cpp:63] Solver scaffolding done.
I0109 19:43:42.922665  3455 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/sparse.caffemodel
I0109 19:43:42.987679  3455 caffe_interface.cpp:527] Starting Optimization
I0109 19:43:42.987726  3455 solver.cpp:335] Solving 
I0109 19:43:42.987731  3455 solver.cpp:336] Learning Rate Policy: poly
I0109 19:43:42.989066  3455 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 19:43:43.809559  3455 solver.cpp:517]     Test net output #0: loss = 0.612173 (* 1 = 0.612173 loss)
I0109 19:43:43.809623  3455 solver.cpp:517]     Test net output #1: top-1 = 0.813889
I0109 19:43:43.809631  3455 solver.cpp:517]     Test net output #2: top-5 = 0.988778
I0109 19:43:43.857055  3455 solver.cpp:266] Iteration 0 (0 iter/s, 0.869281s/100 iter), loss = 0.249021
I0109 19:43:43.857095  3455 solver.cpp:285]     Train net output #0: loss = 0.249021 (* 1 = 0.249021 loss)
I0109 19:43:43.857143  3455 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 19:43:47.172969  3455 solver.cpp:266] Iteration 100 (30.1577 iter/s, 3.3159s/100 iter), loss = 0.109713
I0109 19:43:47.173053  3455 solver.cpp:285]     Train net output #0: loss = 0.109713 (* 1 = 0.109713 loss)
I0109 19:43:47.173074  3455 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 19:43:50.495977  3455 solver.cpp:266] Iteration 200 (30.0936 iter/s, 3.32297s/100 iter), loss = 0.206692
I0109 19:43:50.496047  3455 solver.cpp:285]     Train net output #0: loss = 0.206692 (* 1 = 0.206692 loss)
I0109 19:43:50.496060  3455 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 19:43:53.812856  3455 solver.cpp:266] Iteration 300 (30.1494 iter/s, 3.31682s/100 iter), loss = 0.245164
I0109 19:43:53.812922  3455 solver.cpp:285]     Train net output #0: loss = 0.245164 (* 1 = 0.245164 loss)
I0109 19:43:53.812934  3455 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 19:43:57.137177  3455 solver.cpp:266] Iteration 400 (30.0816 iter/s, 3.32429s/100 iter), loss = 0.330951
I0109 19:43:57.137249  3455 solver.cpp:285]     Train net output #0: loss = 0.330951 (* 1 = 0.330951 loss)
I0109 19:43:57.137262  3455 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 19:44:00.453163  3455 solver.cpp:266] Iteration 500 (30.1572 iter/s, 3.31595s/100 iter), loss = 0.244122
I0109 19:44:00.453230  3455 solver.cpp:285]     Train net output #0: loss = 0.244122 (* 1 = 0.244122 loss)
I0109 19:44:00.453243  3455 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 19:44:03.763062  3455 solver.cpp:266] Iteration 600 (30.2129 iter/s, 3.30984s/100 iter), loss = 0.292442
I0109 19:44:03.763128  3455 solver.cpp:285]     Train net output #0: loss = 0.292442 (* 1 = 0.292442 loss)
I0109 19:44:03.763140  3455 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 19:44:07.062248  3455 solver.cpp:266] Iteration 700 (30.3107 iter/s, 3.29916s/100 iter), loss = 0.260185
I0109 19:44:07.062314  3455 solver.cpp:285]     Train net output #0: loss = 0.260185 (* 1 = 0.260185 loss)
I0109 19:44:07.062325  3455 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 19:44:10.360282  3455 solver.cpp:266] Iteration 800 (30.3213 iter/s, 3.29801s/100 iter), loss = 0.182137
I0109 19:44:10.360350  3455 solver.cpp:285]     Train net output #0: loss = 0.182137 (* 1 = 0.182137 loss)
I0109 19:44:10.360363  3455 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 19:44:13.661147  3455 solver.cpp:266] Iteration 900 (30.2956 iter/s, 3.30081s/100 iter), loss = 0.199748
I0109 19:44:13.661286  3455 solver.cpp:285]     Train net output #0: loss = 0.199748 (* 1 = 0.199748 loss)
I0109 19:44:13.661301  3455 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 19:44:16.916350  3455 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 19:44:17.738047  3455 solver.cpp:517]     Test net output #0: loss = 1.03847 (* 1 = 1.03847 loss)
I0109 19:44:17.738083  3455 solver.cpp:517]     Test net output #1: top-1 = 0.762889
I0109 19:44:17.738092  3455 solver.cpp:517]     Test net output #2: top-5 = 0.974445
I0109 19:44:17.768936  3455 solver.cpp:266] Iteration 1000 (24.3445 iter/s, 4.10771s/100 iter), loss = 0.197773
I0109 19:44:17.768973  3455 solver.cpp:285]     Train net output #0: loss = 0.197773 (* 1 = 0.197773 loss)
I0109 19:44:17.768988  3455 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 19:44:21.063060  3455 solver.cpp:266] Iteration 1100 (30.3571 iter/s, 3.29412s/100 iter), loss = 0.229304
I0109 19:44:21.063127  3455 solver.cpp:285]     Train net output #0: loss = 0.229304 (* 1 = 0.229304 loss)
I0109 19:44:21.063138  3455 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 19:44:24.372421  3455 solver.cpp:266] Iteration 1200 (30.2176 iter/s, 3.30933s/100 iter), loss = 0.286409
I0109 19:44:24.372488  3455 solver.cpp:285]     Train net output #0: loss = 0.286409 (* 1 = 0.286409 loss)
I0109 19:44:24.372499  3455 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 19:44:27.686465  3455 solver.cpp:266] Iteration 1300 (30.1752 iter/s, 3.31398s/100 iter), loss = 0.201701
I0109 19:44:27.686550  3455 solver.cpp:285]     Train net output #0: loss = 0.201701 (* 1 = 0.201701 loss)
I0109 19:44:27.686568  3455 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 19:44:31.001180  3455 solver.cpp:266] Iteration 1400 (30.1689 iter/s, 3.31467s/100 iter), loss = 0.240767
I0109 19:44:31.001248  3455 solver.cpp:285]     Train net output #0: loss = 0.240767 (* 1 = 0.240767 loss)
I0109 19:44:31.001260  3455 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 19:44:34.319399  3455 solver.cpp:266] Iteration 1500 (30.1369 iter/s, 3.31819s/100 iter), loss = 0.166246
I0109 19:44:34.319466  3455 solver.cpp:285]     Train net output #0: loss = 0.166246 (* 1 = 0.166246 loss)
I0109 19:44:34.319478  3455 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 19:44:37.628563  3455 solver.cpp:266] Iteration 1600 (30.2194 iter/s, 3.30913s/100 iter), loss = 0.239128
I0109 19:44:37.628639  3455 solver.cpp:285]     Train net output #0: loss = 0.239128 (* 1 = 0.239128 loss)
I0109 19:44:37.628657  3455 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 19:44:40.960554  3455 solver.cpp:266] Iteration 1700 (30.0127 iter/s, 3.33193s/100 iter), loss = 0.243606
I0109 19:44:40.960641  3455 solver.cpp:285]     Train net output #0: loss = 0.243606 (* 1 = 0.243606 loss)
I0109 19:44:40.960657  3455 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 19:44:44.284771  3455 solver.cpp:266] Iteration 1800 (30.0827 iter/s, 3.32417s/100 iter), loss = 0.186119
I0109 19:44:44.284970  3455 solver.cpp:285]     Train net output #0: loss = 0.186119 (* 1 = 0.186119 loss)
I0109 19:44:44.284988  3455 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 19:44:47.604107  3455 solver.cpp:266] Iteration 1900 (30.1279 iter/s, 3.31918s/100 iter), loss = 0.27276
I0109 19:44:47.604174  3455 solver.cpp:285]     Train net output #0: loss = 0.27276 (* 1 = 0.27276 loss)
I0109 19:44:47.604188  3455 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 19:44:50.893463  3455 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 19:44:51.718932  3455 solver.cpp:517]     Test net output #0: loss = 0.865113 (* 1 = 0.865113 loss)
I0109 19:44:51.718971  3455 solver.cpp:517]     Test net output #1: top-1 = 0.781222
I0109 19:44:51.718978  3455 solver.cpp:517]     Test net output #2: top-5 = 0.983222
I0109 19:44:51.750283  3455 solver.cpp:266] Iteration 2000 (24.1188 iter/s, 4.14614s/100 iter), loss = 0.150003
I0109 19:44:51.750322  3455 solver.cpp:285]     Train net output #0: loss = 0.150003 (* 1 = 0.150003 loss)
I0109 19:44:51.750339  3455 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 19:44:55.071285  3455 solver.cpp:266] Iteration 2100 (30.1114 iter/s, 3.321s/100 iter), loss = 0.186767
I0109 19:44:55.071352  3455 solver.cpp:285]     Train net output #0: loss = 0.186767 (* 1 = 0.186767 loss)
I0109 19:44:55.071365  3455 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 19:44:58.392210  3455 solver.cpp:266] Iteration 2200 (30.1123 iter/s, 3.3209s/100 iter), loss = 0.201135
I0109 19:44:58.392274  3455 solver.cpp:285]     Train net output #0: loss = 0.201135 (* 1 = 0.201135 loss)
I0109 19:44:58.392287  3455 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 19:45:01.714901  3455 solver.cpp:266] Iteration 2300 (30.0965 iter/s, 3.32264s/100 iter), loss = 0.253951
I0109 19:45:01.714973  3455 solver.cpp:285]     Train net output #0: loss = 0.253951 (* 1 = 0.253951 loss)
I0109 19:45:01.714987  3455 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 19:45:05.047591  3455 solver.cpp:266] Iteration 2400 (30.0061 iter/s, 3.33266s/100 iter), loss = 0.157228
I0109 19:45:05.047662  3455 solver.cpp:285]     Train net output #0: loss = 0.157228 (* 1 = 0.157228 loss)
I0109 19:45:05.047675  3455 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 19:45:08.375607  3455 solver.cpp:266] Iteration 2500 (30.0482 iter/s, 3.32799s/100 iter), loss = 0.222348
I0109 19:45:08.375672  3455 solver.cpp:285]     Train net output #0: loss = 0.222348 (* 1 = 0.222348 loss)
I0109 19:45:08.375686  3455 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 19:45:11.707980  3455 solver.cpp:266] Iteration 2600 (30.0091 iter/s, 3.33232s/100 iter), loss = 0.249254
I0109 19:45:11.708047  3455 solver.cpp:285]     Train net output #0: loss = 0.249254 (* 1 = 0.249254 loss)
I0109 19:45:11.708060  3455 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 19:45:15.031204  3455 solver.cpp:266] Iteration 2700 (30.0915 iter/s, 3.3232s/100 iter), loss = 0.216246
I0109 19:45:15.031394  3455 solver.cpp:285]     Train net output #0: loss = 0.216246 (* 1 = 0.216246 loss)
I0109 19:45:15.031412  3455 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 19:45:18.347148  3455 solver.cpp:266] Iteration 2800 (30.1587 iter/s, 3.3158s/100 iter), loss = 0.31709
I0109 19:45:18.347218  3455 solver.cpp:285]     Train net output #0: loss = 0.31709 (* 1 = 0.31709 loss)
I0109 19:45:18.347231  3455 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 19:45:21.674288  3455 solver.cpp:266] Iteration 2900 (30.0563 iter/s, 3.32709s/100 iter), loss = 0.179046
I0109 19:45:21.674355  3455 solver.cpp:285]     Train net output #0: loss = 0.179046 (* 1 = 0.179046 loss)
I0109 19:45:21.674369  3455 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 19:45:24.968736  3455 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 19:45:25.795774  3455 solver.cpp:517]     Test net output #0: loss = 0.725967 (* 1 = 0.725967 loss)
I0109 19:45:25.795816  3455 solver.cpp:517]     Test net output #1: top-1 = 0.796445
I0109 19:45:25.795825  3455 solver.cpp:517]     Test net output #2: top-5 = 0.977445
I0109 19:45:25.827607  3455 solver.cpp:266] Iteration 3000 (24.0772 iter/s, 4.1533s/100 iter), loss = 0.260406
I0109 19:45:25.827687  3455 solver.cpp:285]     Train net output #0: loss = 0.260406 (* 1 = 0.260406 loss)
I0109 19:45:25.827703  3455 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 19:45:29.148234  3455 solver.cpp:266] Iteration 3100 (30.1151 iter/s, 3.32059s/100 iter), loss = 0.214509
I0109 19:45:29.148305  3455 solver.cpp:285]     Train net output #0: loss = 0.214509 (* 1 = 0.214509 loss)
I0109 19:45:29.148319  3455 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 19:45:32.473162  3455 solver.cpp:266] Iteration 3200 (30.0761 iter/s, 3.3249s/100 iter), loss = 0.230868
I0109 19:45:32.473265  3455 solver.cpp:285]     Train net output #0: loss = 0.230869 (* 1 = 0.230869 loss)
I0109 19:45:32.473289  3455 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 19:45:35.789194  3455 solver.cpp:266] Iteration 3300 (30.1572 iter/s, 3.31595s/100 iter), loss = 0.17638
I0109 19:45:35.789268  3455 solver.cpp:285]     Train net output #0: loss = 0.17638 (* 1 = 0.17638 loss)
I0109 19:45:35.789280  3455 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 19:45:39.098690  3455 solver.cpp:266] Iteration 3400 (30.2164 iter/s, 3.30947s/100 iter), loss = 0.199389
I0109 19:45:39.098762  3455 solver.cpp:285]     Train net output #0: loss = 0.199389 (* 1 = 0.199389 loss)
I0109 19:45:39.098775  3455 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 19:45:42.420743  3455 solver.cpp:266] Iteration 3500 (30.1021 iter/s, 3.32202s/100 iter), loss = 0.228033
I0109 19:45:42.420830  3455 solver.cpp:285]     Train net output #0: loss = 0.228034 (* 1 = 0.228034 loss)
I0109 19:45:42.420846  3455 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 19:45:45.746103  3455 solver.cpp:266] Iteration 3600 (30.0726 iter/s, 3.32529s/100 iter), loss = 0.188089
I0109 19:45:45.746294  3455 solver.cpp:285]     Train net output #0: loss = 0.188089 (* 1 = 0.188089 loss)
I0109 19:45:45.746310  3455 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 19:45:49.060941  3455 solver.cpp:266] Iteration 3700 (30.1687 iter/s, 3.31469s/100 iter), loss = 0.164423
I0109 19:45:49.061031  3455 solver.cpp:285]     Train net output #0: loss = 0.164423 (* 1 = 0.164423 loss)
I0109 19:45:49.061048  3455 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 19:45:52.382999  3455 solver.cpp:266] Iteration 3800 (30.1022 iter/s, 3.32201s/100 iter), loss = 0.199919
I0109 19:45:52.383072  3455 solver.cpp:285]     Train net output #0: loss = 0.199919 (* 1 = 0.199919 loss)
I0109 19:45:52.383085  3455 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 19:45:55.717108  3455 solver.cpp:266] Iteration 3900 (29.9936 iter/s, 3.33405s/100 iter), loss = 0.17984
I0109 19:45:55.717195  3455 solver.cpp:285]     Train net output #0: loss = 0.17984 (* 1 = 0.17984 loss)
I0109 19:45:55.717209  3455 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 19:45:59.010609  3455 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 19:45:59.836474  3455 solver.cpp:517]     Test net output #0: loss = 0.562565 (* 1 = 0.562565 loss)
I0109 19:45:59.836513  3455 solver.cpp:517]     Test net output #1: top-1 = 0.835222
I0109 19:45:59.836519  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0109 19:45:59.867744  3455 solver.cpp:266] Iteration 4000 (24.0928 iter/s, 4.15061s/100 iter), loss = 0.151167
I0109 19:45:59.867782  3455 solver.cpp:285]     Train net output #0: loss = 0.151167 (* 1 = 0.151167 loss)
I0109 19:45:59.867796  3455 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 19:46:03.195647  3455 solver.cpp:266] Iteration 4100 (30.0489 iter/s, 3.32791s/100 iter), loss = 0.252104
I0109 19:46:03.195716  3455 solver.cpp:285]     Train net output #0: loss = 0.252104 (* 1 = 0.252104 loss)
I0109 19:46:03.195731  3455 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 19:46:06.509625  3455 solver.cpp:266] Iteration 4200 (30.1756 iter/s, 3.31394s/100 iter), loss = 0.209427
I0109 19:46:06.509708  3455 solver.cpp:285]     Train net output #0: loss = 0.209427 (* 1 = 0.209427 loss)
I0109 19:46:06.509723  3455 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 19:46:09.832590  3455 solver.cpp:266] Iteration 4300 (30.0942 iter/s, 3.3229s/100 iter), loss = 0.217825
I0109 19:46:09.832656  3455 solver.cpp:285]     Train net output #0: loss = 0.217825 (* 1 = 0.217825 loss)
I0109 19:46:09.832669  3455 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 19:46:13.162797  3455 solver.cpp:266] Iteration 4400 (30.0284 iter/s, 3.33019s/100 iter), loss = 0.207423
I0109 19:46:13.162864  3455 solver.cpp:285]     Train net output #0: loss = 0.207423 (* 1 = 0.207423 loss)
I0109 19:46:13.162878  3455 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 19:46:16.481276  3455 solver.cpp:266] Iteration 4500 (30.1345 iter/s, 3.31846s/100 iter), loss = 0.229766
I0109 19:46:16.481475  3455 solver.cpp:285]     Train net output #0: loss = 0.229766 (* 1 = 0.229766 loss)
I0109 19:46:16.481492  3455 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 19:46:19.794627  3455 solver.cpp:266] Iteration 4600 (30.1826 iter/s, 3.31317s/100 iter), loss = 0.243657
I0109 19:46:19.794692  3455 solver.cpp:285]     Train net output #0: loss = 0.243657 (* 1 = 0.243657 loss)
I0109 19:46:19.794705  3455 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 19:46:23.092963  3455 solver.cpp:266] Iteration 4700 (30.3186 iter/s, 3.29831s/100 iter), loss = 0.147863
I0109 19:46:23.093027  3455 solver.cpp:285]     Train net output #0: loss = 0.147863 (* 1 = 0.147863 loss)
I0109 19:46:23.093039  3455 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 19:46:26.385998  3455 solver.cpp:266] Iteration 4800 (30.3674 iter/s, 3.29301s/100 iter), loss = 0.259069
I0109 19:46:26.386060  3455 solver.cpp:285]     Train net output #0: loss = 0.259069 (* 1 = 0.259069 loss)
I0109 19:46:26.386072  3455 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 19:46:29.658416  3455 solver.cpp:266] Iteration 4900 (30.5587 iter/s, 3.27239s/100 iter), loss = 0.199587
I0109 19:46:29.658483  3455 solver.cpp:285]     Train net output #0: loss = 0.199587 (* 1 = 0.199587 loss)
I0109 19:46:29.658495  3455 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 19:46:32.899248  3455 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 19:46:33.718386  3455 solver.cpp:517]     Test net output #0: loss = 0.57865 (* 1 = 0.57865 loss)
I0109 19:46:33.718425  3455 solver.cpp:517]     Test net output #1: top-1 = 0.828444
I0109 19:46:33.718435  3455 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0109 19:46:33.749351  3455 solver.cpp:266] Iteration 5000 (24.4446 iter/s, 4.09089s/100 iter), loss = 0.142003
I0109 19:46:33.749388  3455 solver.cpp:285]     Train net output #0: loss = 0.142003 (* 1 = 0.142003 loss)
I0109 19:46:33.749403  3455 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 19:46:37.018404  3455 solver.cpp:266] Iteration 5100 (30.59 iter/s, 3.26905s/100 iter), loss = 0.172023
I0109 19:46:37.018481  3455 solver.cpp:285]     Train net output #0: loss = 0.172023 (* 1 = 0.172023 loss)
I0109 19:46:37.018501  3455 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 19:46:40.285142  3455 solver.cpp:266] Iteration 5200 (30.612 iter/s, 3.2667s/100 iter), loss = 0.20147
I0109 19:46:40.285224  3455 solver.cpp:285]     Train net output #0: loss = 0.20147 (* 1 = 0.20147 loss)
I0109 19:46:40.285243  3455 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 19:46:43.553050  3455 solver.cpp:266] Iteration 5300 (30.6011 iter/s, 3.26786s/100 iter), loss = 0.200639
I0109 19:46:43.553123  3455 solver.cpp:285]     Train net output #0: loss = 0.200639 (* 1 = 0.200639 loss)
I0109 19:46:43.553143  3455 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 19:46:46.824231  3455 solver.cpp:266] Iteration 5400 (30.5707 iter/s, 3.27111s/100 iter), loss = 0.174519
I0109 19:46:46.824482  3455 solver.cpp:285]     Train net output #0: loss = 0.174519 (* 1 = 0.174519 loss)
I0109 19:46:46.824507  3455 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 19:46:50.093498  3455 solver.cpp:266] Iteration 5500 (30.5899 iter/s, 3.26905s/100 iter), loss = 0.166613
I0109 19:46:50.093571  3455 solver.cpp:285]     Train net output #0: loss = 0.166613 (* 1 = 0.166613 loss)
I0109 19:46:50.093602  3455 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 19:46:53.360388  3455 solver.cpp:266] Iteration 5600 (30.6106 iter/s, 3.26684s/100 iter), loss = 0.249797
I0109 19:46:53.360484  3455 solver.cpp:285]     Train net output #0: loss = 0.249797 (* 1 = 0.249797 loss)
I0109 19:46:53.360502  3455 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 19:46:56.626957  3455 solver.cpp:266] Iteration 5700 (30.6138 iter/s, 3.2665s/100 iter), loss = 0.18319
I0109 19:46:56.627050  3455 solver.cpp:285]     Train net output #0: loss = 0.18319 (* 1 = 0.18319 loss)
I0109 19:46:56.627074  3455 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 19:46:59.893163  3455 solver.cpp:266] Iteration 5800 (30.6174 iter/s, 3.26612s/100 iter), loss = 0.182329
I0109 19:46:59.893236  3455 solver.cpp:285]     Train net output #0: loss = 0.182329 (* 1 = 0.182329 loss)
I0109 19:46:59.893254  3455 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 19:47:03.161290  3455 solver.cpp:266] Iteration 5900 (30.599 iter/s, 3.26809s/100 iter), loss = 0.203801
I0109 19:47:03.161370  3455 solver.cpp:285]     Train net output #0: loss = 0.203801 (* 1 = 0.203801 loss)
I0109 19:47:03.161391  3455 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 19:47:06.400892  3455 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 19:47:07.213981  3455 solver.cpp:517]     Test net output #0: loss = 0.522498 (* 1 = 0.522498 loss)
I0109 19:47:07.214025  3455 solver.cpp:517]     Test net output #1: top-1 = 0.839666
I0109 19:47:07.214037  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991555
I0109 19:47:07.244992  3455 solver.cpp:266] Iteration 6000 (24.4878 iter/s, 4.08366s/100 iter), loss = 0.177735
I0109 19:47:07.245043  3455 solver.cpp:285]     Train net output #0: loss = 0.177735 (* 1 = 0.177735 loss)
I0109 19:47:07.245065  3455 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 19:47:10.509655  3455 solver.cpp:266] Iteration 6100 (30.6312 iter/s, 3.26464s/100 iter), loss = 0.230902
I0109 19:47:10.509729  3455 solver.cpp:285]     Train net output #0: loss = 0.230902 (* 1 = 0.230902 loss)
I0109 19:47:10.509749  3455 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 19:47:13.774838  3455 solver.cpp:266] Iteration 6200 (30.6268 iter/s, 3.26511s/100 iter), loss = 0.118935
I0109 19:47:13.774926  3455 solver.cpp:285]     Train net output #0: loss = 0.118935 (* 1 = 0.118935 loss)
I0109 19:47:13.774950  3455 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 19:47:17.041731  3455 solver.cpp:266] Iteration 6300 (30.6107 iter/s, 3.26684s/100 iter), loss = 0.149073
I0109 19:47:17.041981  3455 solver.cpp:285]     Train net output #0: loss = 0.149073 (* 1 = 0.149073 loss)
I0109 19:47:17.042006  3455 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 19:47:20.307165  3455 solver.cpp:266] Iteration 6400 (30.6258 iter/s, 3.26522s/100 iter), loss = 0.199365
I0109 19:47:20.307238  3455 solver.cpp:285]     Train net output #0: loss = 0.199365 (* 1 = 0.199365 loss)
I0109 19:47:20.307256  3455 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 19:47:23.578955  3455 solver.cpp:266] Iteration 6500 (30.5647 iter/s, 3.27174s/100 iter), loss = 0.242001
I0109 19:47:23.579048  3455 solver.cpp:285]     Train net output #0: loss = 0.242001 (* 1 = 0.242001 loss)
I0109 19:47:23.579073  3455 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 19:47:26.845151  3455 solver.cpp:266] Iteration 6600 (30.6175 iter/s, 3.26611s/100 iter), loss = 0.146373
I0109 19:47:26.845221  3455 solver.cpp:285]     Train net output #0: loss = 0.146373 (* 1 = 0.146373 loss)
I0109 19:47:26.845233  3455 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 19:47:30.113513  3455 solver.cpp:266] Iteration 6700 (30.5968 iter/s, 3.26832s/100 iter), loss = 0.177493
I0109 19:47:30.113581  3455 solver.cpp:285]     Train net output #0: loss = 0.177493 (* 1 = 0.177493 loss)
I0109 19:47:30.113610  3455 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 19:47:33.377749  3455 solver.cpp:266] Iteration 6800 (30.6354 iter/s, 3.26419s/100 iter), loss = 0.224287
I0109 19:47:33.377813  3455 solver.cpp:285]     Train net output #0: loss = 0.224287 (* 1 = 0.224287 loss)
I0109 19:47:33.377825  3455 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 19:47:36.642849  3455 solver.cpp:266] Iteration 6900 (30.6273 iter/s, 3.26506s/100 iter), loss = 0.135455
I0109 19:47:36.642913  3455 solver.cpp:285]     Train net output #0: loss = 0.135455 (* 1 = 0.135455 loss)
I0109 19:47:36.642926  3455 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 19:47:39.873715  3455 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 19:47:40.687139  3455 solver.cpp:517]     Test net output #0: loss = 0.541375 (* 1 = 0.541375 loss)
I0109 19:47:40.687178  3455 solver.cpp:517]     Test net output #1: top-1 = 0.834444
I0109 19:47:40.687188  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0109 19:47:40.718155  3455 solver.cpp:266] Iteration 7000 (24.5383 iter/s, 4.07525s/100 iter), loss = 0.194405
I0109 19:47:40.718190  3455 solver.cpp:285]     Train net output #0: loss = 0.194405 (* 1 = 0.194405 loss)
I0109 19:47:40.718205  3455 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 19:47:43.988034  3455 solver.cpp:266] Iteration 7100 (30.5823 iter/s, 3.26986s/100 iter), loss = 0.20403
I0109 19:47:43.988118  3455 solver.cpp:285]     Train net output #0: loss = 0.20403 (* 1 = 0.20403 loss)
I0109 19:47:43.988133  3455 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 19:47:47.258304  3455 solver.cpp:266] Iteration 7200 (30.5791 iter/s, 3.27021s/100 iter), loss = 0.190332
I0109 19:47:47.258451  3455 solver.cpp:285]     Train net output #0: loss = 0.190332 (* 1 = 0.190332 loss)
I0109 19:47:47.258466  3455 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 19:47:50.523299  3455 solver.cpp:266] Iteration 7300 (30.629 iter/s, 3.26488s/100 iter), loss = 0.132597
I0109 19:47:50.523366  3455 solver.cpp:285]     Train net output #0: loss = 0.132597 (* 1 = 0.132597 loss)
I0109 19:47:50.523380  3455 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 19:47:53.787333  3455 solver.cpp:266] Iteration 7400 (30.6377 iter/s, 3.26395s/100 iter), loss = 0.2183
I0109 19:47:53.787418  3455 solver.cpp:285]     Train net output #0: loss = 0.2183 (* 1 = 0.2183 loss)
I0109 19:47:53.787434  3455 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 19:47:57.055079  3455 solver.cpp:266] Iteration 7500 (30.6027 iter/s, 3.26768s/100 iter), loss = 0.110715
I0109 19:47:57.055174  3455 solver.cpp:285]     Train net output #0: loss = 0.110715 (* 1 = 0.110715 loss)
I0109 19:47:57.055191  3455 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 19:48:00.321285  3455 solver.cpp:266] Iteration 7600 (30.6172 iter/s, 3.26614s/100 iter), loss = 0.146812
I0109 19:48:00.321352  3455 solver.cpp:285]     Train net output #0: loss = 0.146812 (* 1 = 0.146812 loss)
I0109 19:48:00.321365  3455 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 19:48:03.588438  3455 solver.cpp:266] Iteration 7700 (30.6081 iter/s, 3.26711s/100 iter), loss = 0.178335
I0109 19:48:03.588505  3455 solver.cpp:285]     Train net output #0: loss = 0.178335 (* 1 = 0.178335 loss)
I0109 19:48:03.588518  3455 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 19:48:06.851050  3455 solver.cpp:266] Iteration 7800 (30.6509 iter/s, 3.26254s/100 iter), loss = 0.147826
I0109 19:48:06.851117  3455 solver.cpp:285]     Train net output #0: loss = 0.147826 (* 1 = 0.147826 loss)
I0109 19:48:06.851130  3455 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 19:48:10.121047  3455 solver.cpp:266] Iteration 7900 (30.5815 iter/s, 3.26995s/100 iter), loss = 0.178269
I0109 19:48:10.121120  3455 solver.cpp:285]     Train net output #0: loss = 0.178269 (* 1 = 0.178269 loss)
I0109 19:48:10.121135  3455 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 19:48:13.357568  3455 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 19:48:14.174549  3455 solver.cpp:517]     Test net output #0: loss = 0.503789 (* 1 = 0.503789 loss)
I0109 19:48:14.174588  3455 solver.cpp:517]     Test net output #1: top-1 = 0.844
I0109 19:48:14.174598  3455 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0109 19:48:14.205341  3455 solver.cpp:266] Iteration 8000 (24.4843 iter/s, 4.08425s/100 iter), loss = 0.169556
I0109 19:48:14.205404  3455 solver.cpp:285]     Train net output #0: loss = 0.169556 (* 1 = 0.169556 loss)
I0109 19:48:14.205420  3455 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 19:48:17.468259  3455 solver.cpp:266] Iteration 8100 (30.6478 iter/s, 3.26288s/100 iter), loss = 0.225525
I0109 19:48:17.468396  3455 solver.cpp:285]     Train net output #0: loss = 0.225525 (* 1 = 0.225525 loss)
I0109 19:48:17.468411  3455 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 19:48:20.736835  3455 solver.cpp:266] Iteration 8200 (30.5956 iter/s, 3.26844s/100 iter), loss = 0.154487
I0109 19:48:20.736902  3455 solver.cpp:285]     Train net output #0: loss = 0.154487 (* 1 = 0.154487 loss)
I0109 19:48:20.736915  3455 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 19:48:24.002353  3455 solver.cpp:266] Iteration 8300 (30.6234 iter/s, 3.26547s/100 iter), loss = 0.19109
I0109 19:48:24.002418  3455 solver.cpp:285]     Train net output #0: loss = 0.19109 (* 1 = 0.19109 loss)
I0109 19:48:24.002430  3455 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 19:48:27.269018  3455 solver.cpp:266] Iteration 8400 (30.6126 iter/s, 3.26663s/100 iter), loss = 0.137485
I0109 19:48:27.269088  3455 solver.cpp:285]     Train net output #0: loss = 0.137485 (* 1 = 0.137485 loss)
I0109 19:48:27.269100  3455 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 19:48:30.536725  3455 solver.cpp:266] Iteration 8500 (30.603 iter/s, 3.26766s/100 iter), loss = 0.183872
I0109 19:48:30.536813  3455 solver.cpp:285]     Train net output #0: loss = 0.183872 (* 1 = 0.183872 loss)
I0109 19:48:30.536829  3455 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 19:48:33.811254  3455 solver.cpp:266] Iteration 8600 (30.5396 iter/s, 3.27444s/100 iter), loss = 0.163896
I0109 19:48:33.811324  3455 solver.cpp:285]     Train net output #0: loss = 0.163896 (* 1 = 0.163896 loss)
I0109 19:48:33.811337  3455 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 19:48:37.079089  3455 solver.cpp:266] Iteration 8700 (30.6017 iter/s, 3.26779s/100 iter), loss = 0.137603
I0109 19:48:37.079156  3455 solver.cpp:285]     Train net output #0: loss = 0.137603 (* 1 = 0.137603 loss)
I0109 19:48:37.079169  3455 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 19:48:40.343979  3455 solver.cpp:266] Iteration 8800 (30.6293 iter/s, 3.26484s/100 iter), loss = 0.152323
I0109 19:48:40.344065  3455 solver.cpp:285]     Train net output #0: loss = 0.152323 (* 1 = 0.152323 loss)
I0109 19:48:40.344080  3455 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 19:48:43.609630  3455 solver.cpp:266] Iteration 8900 (30.6223 iter/s, 3.26559s/100 iter), loss = 0.187945
I0109 19:48:43.609699  3455 solver.cpp:285]     Train net output #0: loss = 0.187945 (* 1 = 0.187945 loss)
I0109 19:48:43.609712  3455 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 19:48:46.844326  3455 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 19:48:47.662799  3455 solver.cpp:517]     Test net output #0: loss = 0.46626 (* 1 = 0.46626 loss)
I0109 19:48:47.662971  3455 solver.cpp:517]     Test net output #1: top-1 = 0.854222
I0109 19:48:47.662983  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 19:48:47.693984  3455 solver.cpp:266] Iteration 9000 (24.484 iter/s, 4.0843s/100 iter), loss = 0.121204
I0109 19:48:47.694027  3455 solver.cpp:285]     Train net output #0: loss = 0.121204 (* 1 = 0.121204 loss)
I0109 19:48:47.694042  3455 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 19:48:50.964967  3455 solver.cpp:266] Iteration 9100 (30.572 iter/s, 3.27096s/100 iter), loss = 0.179569
I0109 19:48:50.965039  3455 solver.cpp:285]     Train net output #0: loss = 0.179569 (* 1 = 0.179569 loss)
I0109 19:48:50.965052  3455 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 19:48:54.232995  3455 solver.cpp:266] Iteration 9200 (30.5999 iter/s, 3.26798s/100 iter), loss = 0.215582
I0109 19:48:54.233063  3455 solver.cpp:285]     Train net output #0: loss = 0.215582 (* 1 = 0.215582 loss)
I0109 19:48:54.233075  3455 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 19:48:57.500717  3455 solver.cpp:266] Iteration 9300 (30.6027 iter/s, 3.26768s/100 iter), loss = 0.0957159
I0109 19:48:57.500787  3455 solver.cpp:285]     Train net output #0: loss = 0.0957159 (* 1 = 0.0957159 loss)
I0109 19:48:57.500802  3455 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 19:49:00.766841  3455 solver.cpp:266] Iteration 9400 (30.618 iter/s, 3.26605s/100 iter), loss = 0.1454
I0109 19:49:00.766907  3455 solver.cpp:285]     Train net output #0: loss = 0.1454 (* 1 = 0.1454 loss)
I0109 19:49:00.766921  3455 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 19:49:04.041105  3455 solver.cpp:266] Iteration 9500 (30.5416 iter/s, 3.27422s/100 iter), loss = 0.151877
I0109 19:49:04.041189  3455 solver.cpp:285]     Train net output #0: loss = 0.151877 (* 1 = 0.151877 loss)
I0109 19:49:04.041206  3455 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 19:49:07.313848  3455 solver.cpp:266] Iteration 9600 (30.5559 iter/s, 3.27269s/100 iter), loss = 0.18404
I0109 19:49:07.313922  3455 solver.cpp:285]     Train net output #0: loss = 0.18404 (* 1 = 0.18404 loss)
I0109 19:49:07.313936  3455 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 19:49:10.595944  3455 solver.cpp:266] Iteration 9700 (30.4688 iter/s, 3.28205s/100 iter), loss = 0.14888
I0109 19:49:10.596009  3455 solver.cpp:285]     Train net output #0: loss = 0.14888 (* 1 = 0.14888 loss)
I0109 19:49:10.596022  3455 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 19:49:13.880803  3455 solver.cpp:266] Iteration 9800 (30.4433 iter/s, 3.2848s/100 iter), loss = 0.193903
I0109 19:49:13.880887  3455 solver.cpp:285]     Train net output #0: loss = 0.193903 (* 1 = 0.193903 loss)
I0109 19:49:13.880901  3455 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 19:49:17.166764  3455 solver.cpp:266] Iteration 9900 (30.433 iter/s, 3.28591s/100 iter), loss = 0.109399
I0109 19:49:17.166827  3455 solver.cpp:285]     Train net output #0: loss = 0.109399 (* 1 = 0.109399 loss)
I0109 19:49:17.166839  3455 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 19:49:20.409942  3455 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 19:49:21.230038  3455 solver.cpp:517]     Test net output #0: loss = 0.499362 (* 1 = 0.499362 loss)
I0109 19:49:21.230075  3455 solver.cpp:517]     Test net output #1: top-1 = 0.848111
I0109 19:49:21.230084  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991555
I0109 19:49:21.260944  3455 solver.cpp:266] Iteration 10000 (24.4251 iter/s, 4.09415s/100 iter), loss = 0.148805
I0109 19:49:21.260994  3455 solver.cpp:285]     Train net output #0: loss = 0.148805 (* 1 = 0.148805 loss)
I0109 19:49:21.261009  3455 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 19:49:24.540227  3455 solver.cpp:266] Iteration 10100 (30.4947 iter/s, 3.27926s/100 iter), loss = 0.102447
I0109 19:49:24.540295  3455 solver.cpp:285]     Train net output #0: loss = 0.102447 (* 1 = 0.102447 loss)
I0109 19:49:24.540308  3455 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 19:49:27.818511  3455 solver.cpp:266] Iteration 10200 (30.5044 iter/s, 3.27822s/100 iter), loss = 0.125533
I0109 19:49:27.818574  3455 solver.cpp:285]     Train net output #0: loss = 0.125533 (* 1 = 0.125533 loss)
I0109 19:49:27.818588  3455 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 19:49:31.108728  3455 solver.cpp:266] Iteration 10300 (30.3935 iter/s, 3.29018s/100 iter), loss = 0.123448
I0109 19:49:31.108804  3455 solver.cpp:285]     Train net output #0: loss = 0.123448 (* 1 = 0.123448 loss)
I0109 19:49:31.108819  3455 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 19:49:34.400053  3455 solver.cpp:266] Iteration 10400 (30.3833 iter/s, 3.29128s/100 iter), loss = 0.1994
I0109 19:49:34.400117  3455 solver.cpp:285]     Train net output #0: loss = 0.1994 (* 1 = 0.1994 loss)
I0109 19:49:34.400130  3455 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 19:49:37.693619  3455 solver.cpp:266] Iteration 10500 (30.3628 iter/s, 3.2935s/100 iter), loss = 0.198009
I0109 19:49:37.693686  3455 solver.cpp:285]     Train net output #0: loss = 0.198009 (* 1 = 0.198009 loss)
I0109 19:49:37.693699  3455 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 19:49:40.985250  3455 solver.cpp:266] Iteration 10600 (30.3804 iter/s, 3.29159s/100 iter), loss = 0.213883
I0109 19:49:40.985322  3455 solver.cpp:285]     Train net output #0: loss = 0.213883 (* 1 = 0.213883 loss)
I0109 19:49:40.985335  3455 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 19:49:44.291157  3455 solver.cpp:266] Iteration 10700 (30.2493 iter/s, 3.30587s/100 iter), loss = 0.124424
I0109 19:49:44.291226  3455 solver.cpp:285]     Train net output #0: loss = 0.124424 (* 1 = 0.124424 loss)
I0109 19:49:44.291239  3455 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 19:49:47.587146  3455 solver.cpp:266] Iteration 10800 (30.3403 iter/s, 3.29595s/100 iter), loss = 0.129598
I0109 19:49:47.587215  3455 solver.cpp:285]     Train net output #0: loss = 0.129598 (* 1 = 0.129598 loss)
I0109 19:49:47.587229  3455 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 19:49:50.886126  3455 solver.cpp:266] Iteration 10900 (30.313 iter/s, 3.29891s/100 iter), loss = 0.148914
I0109 19:49:50.886304  3455 solver.cpp:285]     Train net output #0: loss = 0.148914 (* 1 = 0.148914 loss)
I0109 19:49:50.886323  3455 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 19:49:54.146479  3455 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 19:49:54.963178  3455 solver.cpp:517]     Test net output #0: loss = 0.455809 (* 1 = 0.455809 loss)
I0109 19:49:54.963217  3455 solver.cpp:517]     Test net output #1: top-1 = 0.858556
I0109 19:49:54.963227  3455 solver.cpp:517]     Test net output #2: top-5 = 0.993555
I0109 19:49:54.994119  3455 solver.cpp:266] Iteration 11000 (24.3435 iter/s, 4.10787s/100 iter), loss = 0.154011
I0109 19:49:54.994160  3455 solver.cpp:285]     Train net output #0: loss = 0.154011 (* 1 = 0.154011 loss)
I0109 19:49:54.994182  3455 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 19:49:58.299312  3455 solver.cpp:266] Iteration 11100 (30.2555 iter/s, 3.30518s/100 iter), loss = 0.136518
I0109 19:49:58.299401  3455 solver.cpp:285]     Train net output #0: loss = 0.136518 (* 1 = 0.136518 loss)
I0109 19:49:58.299422  3455 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 19:50:01.600083  3455 solver.cpp:266] Iteration 11200 (30.2964 iter/s, 3.30072s/100 iter), loss = 0.185162
I0109 19:50:01.600152  3455 solver.cpp:285]     Train net output #0: loss = 0.185162 (* 1 = 0.185162 loss)
I0109 19:50:01.600165  3455 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 19:50:04.906623  3455 solver.cpp:266] Iteration 11300 (30.2437 iter/s, 3.30648s/100 iter), loss = 0.122151
I0109 19:50:04.906689  3455 solver.cpp:285]     Train net output #0: loss = 0.122151 (* 1 = 0.122151 loss)
I0109 19:50:04.906702  3455 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 19:50:08.212674  3455 solver.cpp:266] Iteration 11400 (30.2479 iter/s, 3.30602s/100 iter), loss = 0.227156
I0109 19:50:08.212743  3455 solver.cpp:285]     Train net output #0: loss = 0.227156 (* 1 = 0.227156 loss)
I0109 19:50:08.212755  3455 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 19:50:11.511737  3455 solver.cpp:266] Iteration 11500 (30.312 iter/s, 3.29902s/100 iter), loss = 0.191543
I0109 19:50:11.511812  3455 solver.cpp:285]     Train net output #0: loss = 0.191543 (* 1 = 0.191543 loss)
I0109 19:50:11.511827  3455 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 19:50:14.825855  3455 solver.cpp:266] Iteration 11600 (30.1746 iter/s, 3.31405s/100 iter), loss = 0.161253
I0109 19:50:14.825942  3455 solver.cpp:285]     Train net output #0: loss = 0.161253 (* 1 = 0.161253 loss)
I0109 19:50:14.825956  3455 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 19:50:18.137322  3455 solver.cpp:266] Iteration 11700 (30.1986 iter/s, 3.31141s/100 iter), loss = 0.14771
I0109 19:50:18.137400  3455 solver.cpp:285]     Train net output #0: loss = 0.14771 (* 1 = 0.14771 loss)
I0109 19:50:18.137416  3455 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 19:50:21.455905  3455 solver.cpp:266] Iteration 11800 (30.1337 iter/s, 3.31854s/100 iter), loss = 0.0955543
I0109 19:50:21.456039  3455 solver.cpp:285]     Train net output #0: loss = 0.0955543 (* 1 = 0.0955543 loss)
I0109 19:50:21.456054  3455 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 19:50:24.770627  3455 solver.cpp:266] Iteration 11900 (30.1696 iter/s, 3.31459s/100 iter), loss = 0.163345
I0109 19:50:24.770692  3455 solver.cpp:285]     Train net output #0: loss = 0.163345 (* 1 = 0.163345 loss)
I0109 19:50:24.770704  3455 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 19:50:28.074321  3455 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 19:50:28.895869  3455 solver.cpp:517]     Test net output #0: loss = 0.494016 (* 1 = 0.494016 loss)
I0109 19:50:28.895905  3455 solver.cpp:517]     Test net output #1: top-1 = 0.851778
I0109 19:50:28.895915  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 19:50:28.927151  3455 solver.cpp:266] Iteration 12000 (24.0587 iter/s, 4.15651s/100 iter), loss = 0.152217
I0109 19:50:28.927187  3455 solver.cpp:285]     Train net output #0: loss = 0.152217 (* 1 = 0.152217 loss)
I0109 19:50:28.927201  3455 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 19:50:32.248469  3455 solver.cpp:266] Iteration 12100 (30.1086 iter/s, 3.32131s/100 iter), loss = 0.159293
I0109 19:50:32.248535  3455 solver.cpp:285]     Train net output #0: loss = 0.159293 (* 1 = 0.159293 loss)
I0109 19:50:32.248550  3455 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 19:50:35.567939  3455 solver.cpp:266] Iteration 12200 (30.1256 iter/s, 3.31944s/100 iter), loss = 0.0700209
I0109 19:50:35.568001  3455 solver.cpp:285]     Train net output #0: loss = 0.0700208 (* 1 = 0.0700208 loss)
I0109 19:50:35.568013  3455 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 19:50:38.885049  3455 solver.cpp:266] Iteration 12300 (30.1473 iter/s, 3.31705s/100 iter), loss = 0.171649
I0109 19:50:38.885135  3455 solver.cpp:285]     Train net output #0: loss = 0.171649 (* 1 = 0.171649 loss)
I0109 19:50:38.885150  3455 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 19:50:42.207743  3455 solver.cpp:266] Iteration 12400 (30.0965 iter/s, 3.32264s/100 iter), loss = 0.0941771
I0109 19:50:42.207810  3455 solver.cpp:285]     Train net output #0: loss = 0.0941771 (* 1 = 0.0941771 loss)
I0109 19:50:42.207823  3455 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 19:50:45.528547  3455 solver.cpp:266] Iteration 12500 (30.1135 iter/s, 3.32077s/100 iter), loss = 0.134653
I0109 19:50:45.528614  3455 solver.cpp:285]     Train net output #0: loss = 0.134653 (* 1 = 0.134653 loss)
I0109 19:50:45.528628  3455 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 19:50:48.866981  3455 solver.cpp:266] Iteration 12600 (29.9547 iter/s, 3.33838s/100 iter), loss = 0.148697
I0109 19:50:48.867049  3455 solver.cpp:285]     Train net output #0: loss = 0.148697 (* 1 = 0.148697 loss)
I0109 19:50:48.867061  3455 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 19:50:52.203853  3455 solver.cpp:266] Iteration 12700 (29.9685 iter/s, 3.33684s/100 iter), loss = 0.117353
I0109 19:50:52.204056  3455 solver.cpp:285]     Train net output #0: loss = 0.117353 (* 1 = 0.117353 loss)
I0109 19:50:52.204071  3455 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 19:50:55.532903  3455 solver.cpp:266] Iteration 12800 (30.0401 iter/s, 3.32889s/100 iter), loss = 0.122091
I0109 19:50:55.532972  3455 solver.cpp:285]     Train net output #0: loss = 0.122091 (* 1 = 0.122091 loss)
I0109 19:50:55.532985  3455 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 19:50:58.849977  3455 solver.cpp:266] Iteration 12900 (30.1476 iter/s, 3.31702s/100 iter), loss = 0.182472
I0109 19:50:58.850047  3455 solver.cpp:285]     Train net output #0: loss = 0.182472 (* 1 = 0.182472 loss)
I0109 19:50:58.850060  3455 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 19:51:02.146111  3455 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 19:51:02.970399  3455 solver.cpp:517]     Test net output #0: loss = 0.513478 (* 1 = 0.513478 loss)
I0109 19:51:02.970441  3455 solver.cpp:517]     Test net output #1: top-1 = 0.849889
I0109 19:51:02.970451  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0109 19:51:03.001530  3455 solver.cpp:266] Iteration 13000 (24.0875 iter/s, 4.15152s/100 iter), loss = 0.153757
I0109 19:51:03.001641  3455 solver.cpp:285]     Train net output #0: loss = 0.153757 (* 1 = 0.153757 loss)
I0109 19:51:03.001667  3455 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 19:51:06.322690  3455 solver.cpp:266] Iteration 13100 (30.1106 iter/s, 3.32109s/100 iter), loss = 0.160078
I0109 19:51:06.322769  3455 solver.cpp:285]     Train net output #0: loss = 0.160078 (* 1 = 0.160078 loss)
I0109 19:51:06.322784  3455 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 19:51:09.655735  3455 solver.cpp:266] Iteration 13200 (30.003 iter/s, 3.333s/100 iter), loss = 0.199153
I0109 19:51:09.655813  3455 solver.cpp:285]     Train net output #0: loss = 0.199153 (* 1 = 0.199153 loss)
I0109 19:51:09.655827  3455 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 19:51:12.981385  3455 solver.cpp:266] Iteration 13300 (30.0699 iter/s, 3.32559s/100 iter), loss = 0.114219
I0109 19:51:12.981454  3455 solver.cpp:285]     Train net output #0: loss = 0.114219 (* 1 = 0.114219 loss)
I0109 19:51:12.981469  3455 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 19:51:16.299818  3455 solver.cpp:266] Iteration 13400 (30.135 iter/s, 3.3184s/100 iter), loss = 0.122017
I0109 19:51:16.299887  3455 solver.cpp:285]     Train net output #0: loss = 0.122017 (* 1 = 0.122017 loss)
I0109 19:51:16.299899  3455 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 19:51:19.611940  3455 solver.cpp:266] Iteration 13500 (30.1924 iter/s, 3.31209s/100 iter), loss = 0.161009
I0109 19:51:19.612031  3455 solver.cpp:285]     Train net output #0: loss = 0.161009 (* 1 = 0.161009 loss)
I0109 19:51:19.612046  3455 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 19:51:22.928828  3455 solver.cpp:266] Iteration 13600 (30.1494 iter/s, 3.31681s/100 iter), loss = 0.127182
I0109 19:51:22.928994  3455 solver.cpp:285]     Train net output #0: loss = 0.127182 (* 1 = 0.127182 loss)
I0109 19:51:22.929014  3455 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 19:51:26.244022  3455 solver.cpp:266] Iteration 13700 (30.1652 iter/s, 3.31507s/100 iter), loss = 0.11766
I0109 19:51:26.244097  3455 solver.cpp:285]     Train net output #0: loss = 0.11766 (* 1 = 0.11766 loss)
I0109 19:51:26.244117  3455 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 19:51:29.550660  3455 solver.cpp:266] Iteration 13800 (30.2425 iter/s, 3.30661s/100 iter), loss = 0.154096
I0109 19:51:29.550729  3455 solver.cpp:285]     Train net output #0: loss = 0.154095 (* 1 = 0.154095 loss)
I0109 19:51:29.550741  3455 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 19:51:32.876466  3455 solver.cpp:266] Iteration 13900 (30.0684 iter/s, 3.32575s/100 iter), loss = 0.180458
I0109 19:51:32.876544  3455 solver.cpp:285]     Train net output #0: loss = 0.180458 (* 1 = 0.180458 loss)
I0109 19:51:32.876560  3455 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 19:51:36.161681  3455 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 19:51:36.985937  3455 solver.cpp:517]     Test net output #0: loss = 0.481812 (* 1 = 0.481812 loss)
I0109 19:51:36.985975  3455 solver.cpp:517]     Test net output #1: top-1 = 0.853667
I0109 19:51:36.985985  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:51:37.017288  3455 solver.cpp:266] Iteration 14000 (24.15 iter/s, 4.1408s/100 iter), loss = 0.135588
I0109 19:51:37.017352  3455 solver.cpp:285]     Train net output #0: loss = 0.135588 (* 1 = 0.135588 loss)
I0109 19:51:37.017370  3455 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 19:51:40.327472  3455 solver.cpp:266] Iteration 14100 (30.21 iter/s, 3.31016s/100 iter), loss = 0.172954
I0109 19:51:40.327543  3455 solver.cpp:285]     Train net output #0: loss = 0.172954 (* 1 = 0.172954 loss)
I0109 19:51:40.327558  3455 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 19:51:43.650599  3455 solver.cpp:266] Iteration 14200 (30.0924 iter/s, 3.3231s/100 iter), loss = 0.183298
I0109 19:51:43.650663  3455 solver.cpp:285]     Train net output #0: loss = 0.183298 (* 1 = 0.183298 loss)
I0109 19:51:43.650676  3455 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 19:51:46.963608  3455 solver.cpp:266] Iteration 14300 (30.1845 iter/s, 3.31296s/100 iter), loss = 0.11926
I0109 19:51:46.963676  3455 solver.cpp:285]     Train net output #0: loss = 0.11926 (* 1 = 0.11926 loss)
I0109 19:51:46.963688  3455 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 19:51:50.279853  3455 solver.cpp:266] Iteration 14400 (30.1548 iter/s, 3.31622s/100 iter), loss = 0.0727108
I0109 19:51:50.279922  3455 solver.cpp:285]     Train net output #0: loss = 0.0727108 (* 1 = 0.0727108 loss)
I0109 19:51:50.279934  3455 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 19:51:53.612043  3455 solver.cpp:266] Iteration 14500 (30.0105 iter/s, 3.33217s/100 iter), loss = 0.0820186
I0109 19:51:53.612231  3455 solver.cpp:285]     Train net output #0: loss = 0.0820185 (* 1 = 0.0820185 loss)
I0109 19:51:53.612247  3455 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 19:51:56.933109  3455 solver.cpp:266] Iteration 14600 (30.1124 iter/s, 3.3209s/100 iter), loss = 0.139643
I0109 19:51:56.933197  3455 solver.cpp:285]     Train net output #0: loss = 0.139643 (* 1 = 0.139643 loss)
I0109 19:51:56.933212  3455 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 19:52:00.253098  3455 solver.cpp:266] Iteration 14700 (30.121 iter/s, 3.31994s/100 iter), loss = 0.106441
I0109 19:52:00.253177  3455 solver.cpp:285]     Train net output #0: loss = 0.106441 (* 1 = 0.106441 loss)
I0109 19:52:00.253192  3455 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 19:52:03.566704  3455 solver.cpp:266] Iteration 14800 (30.1789 iter/s, 3.31357s/100 iter), loss = 0.118852
I0109 19:52:03.566771  3455 solver.cpp:285]     Train net output #0: loss = 0.118851 (* 1 = 0.118851 loss)
I0109 19:52:03.566785  3455 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 19:52:06.886607  3455 solver.cpp:266] Iteration 14900 (30.1218 iter/s, 3.31985s/100 iter), loss = 0.121728
I0109 19:52:06.886674  3455 solver.cpp:285]     Train net output #0: loss = 0.121727 (* 1 = 0.121727 loss)
I0109 19:52:06.886687  3455 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 19:52:10.174623  3455 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 19:52:10.998639  3455 solver.cpp:517]     Test net output #0: loss = 0.509695 (* 1 = 0.509695 loss)
I0109 19:52:10.998682  3455 solver.cpp:517]     Test net output #1: top-1 = 0.851444
I0109 19:52:10.998690  3455 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 19:52:11.030021  3455 solver.cpp:266] Iteration 15000 (24.1348 iter/s, 4.1434s/100 iter), loss = 0.0912263
I0109 19:52:11.030088  3455 solver.cpp:285]     Train net output #0: loss = 0.0912263 (* 1 = 0.0912263 loss)
I0109 19:52:11.030103  3455 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 19:52:14.336603  3455 solver.cpp:266] Iteration 15100 (30.2429 iter/s, 3.30656s/100 iter), loss = 0.183547
I0109 19:52:14.336688  3455 solver.cpp:285]     Train net output #0: loss = 0.183547 (* 1 = 0.183547 loss)
I0109 19:52:14.336709  3455 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 19:52:17.649402  3455 solver.cpp:266] Iteration 15200 (30.1863 iter/s, 3.31276s/100 iter), loss = 0.167775
I0109 19:52:17.649492  3455 solver.cpp:285]     Train net output #0: loss = 0.167774 (* 1 = 0.167774 loss)
I0109 19:52:17.649513  3455 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 19:52:20.958214  3455 solver.cpp:266] Iteration 15300 (30.223 iter/s, 3.30874s/100 iter), loss = 0.221237
I0109 19:52:20.958297  3455 solver.cpp:285]     Train net output #0: loss = 0.221237 (* 1 = 0.221237 loss)
I0109 19:52:20.958313  3455 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 19:52:24.272704  3455 solver.cpp:266] Iteration 15400 (30.1709 iter/s, 3.31445s/100 iter), loss = 0.14657
I0109 19:52:24.272919  3455 solver.cpp:285]     Train net output #0: loss = 0.14657 (* 1 = 0.14657 loss)
I0109 19:52:24.272934  3455 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 19:52:27.589066  3455 solver.cpp:266] Iteration 15500 (30.1551 iter/s, 3.31619s/100 iter), loss = 0.164188
I0109 19:52:27.589139  3455 solver.cpp:285]     Train net output #0: loss = 0.164188 (* 1 = 0.164188 loss)
I0109 19:52:27.589154  3455 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 19:52:30.905894  3455 solver.cpp:266] Iteration 15600 (30.1498 iter/s, 3.31677s/100 iter), loss = 0.0851572
I0109 19:52:30.905968  3455 solver.cpp:285]     Train net output #0: loss = 0.0851571 (* 1 = 0.0851571 loss)
I0109 19:52:30.905982  3455 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 19:52:34.231626  3455 solver.cpp:266] Iteration 15700 (30.0688 iter/s, 3.3257s/100 iter), loss = 0.128102
I0109 19:52:34.231690  3455 solver.cpp:285]     Train net output #0: loss = 0.128102 (* 1 = 0.128102 loss)
I0109 19:52:34.231704  3455 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 19:52:37.538796  3455 solver.cpp:266] Iteration 15800 (30.2375 iter/s, 3.30715s/100 iter), loss = 0.127485
I0109 19:52:37.538862  3455 solver.cpp:285]     Train net output #0: loss = 0.127485 (* 1 = 0.127485 loss)
I0109 19:52:37.538875  3455 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 19:52:40.867580  3455 solver.cpp:266] Iteration 15900 (30.0415 iter/s, 3.32873s/100 iter), loss = 0.162661
I0109 19:52:40.867666  3455 solver.cpp:285]     Train net output #0: loss = 0.162661 (* 1 = 0.162661 loss)
I0109 19:52:40.867681  3455 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 19:52:44.151628  3455 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 19:52:44.982357  3455 solver.cpp:517]     Test net output #0: loss = 0.456361 (* 1 = 0.456361 loss)
I0109 19:52:44.982395  3455 solver.cpp:517]     Test net output #1: top-1 = 0.860111
I0109 19:52:44.982403  3455 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 19:52:45.013660  3455 solver.cpp:266] Iteration 16000 (24.1193 iter/s, 4.14605s/100 iter), loss = 0.154769
I0109 19:52:45.013720  3455 solver.cpp:285]     Train net output #0: loss = 0.154769 (* 1 = 0.154769 loss)
I0109 19:52:45.013736  3455 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 19:52:48.325094  3455 solver.cpp:266] Iteration 16100 (30.1986 iter/s, 3.31142s/100 iter), loss = 0.159048
I0109 19:52:48.325162  3455 solver.cpp:285]     Train net output #0: loss = 0.159048 (* 1 = 0.159048 loss)
I0109 19:52:48.325176  3455 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 19:52:51.652658  3455 solver.cpp:266] Iteration 16200 (30.0522 iter/s, 3.32754s/100 iter), loss = 0.141223
I0109 19:52:51.652724  3455 solver.cpp:285]     Train net output #0: loss = 0.141223 (* 1 = 0.141223 loss)
I0109 19:52:51.652737  3455 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 19:52:54.968328  3455 solver.cpp:266] Iteration 16300 (30.1603 iter/s, 3.31562s/100 iter), loss = 0.108554
I0109 19:52:54.968514  3455 solver.cpp:285]     Train net output #0: loss = 0.108554 (* 1 = 0.108554 loss)
I0109 19:52:54.968528  3455 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 19:52:58.280910  3455 solver.cpp:266] Iteration 16400 (30.1892 iter/s, 3.31244s/100 iter), loss = 0.160147
I0109 19:52:58.280987  3455 solver.cpp:285]     Train net output #0: loss = 0.160147 (* 1 = 0.160147 loss)
I0109 19:52:58.281003  3455 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 19:53:01.602916  3455 solver.cpp:266] Iteration 16500 (30.1026 iter/s, 3.32197s/100 iter), loss = 0.113485
I0109 19:53:01.602982  3455 solver.cpp:285]     Train net output #0: loss = 0.113485 (* 1 = 0.113485 loss)
I0109 19:53:01.602995  3455 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 19:53:04.917536  3455 solver.cpp:266] Iteration 16600 (30.1698 iter/s, 3.31457s/100 iter), loss = 0.131001
I0109 19:53:04.917613  3455 solver.cpp:285]     Train net output #0: loss = 0.131001 (* 1 = 0.131001 loss)
I0109 19:53:04.917626  3455 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 19:53:08.226191  3455 solver.cpp:266] Iteration 16700 (30.2241 iter/s, 3.30862s/100 iter), loss = 0.0941566
I0109 19:53:08.226277  3455 solver.cpp:285]     Train net output #0: loss = 0.0941565 (* 1 = 0.0941565 loss)
I0109 19:53:08.226292  3455 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 19:53:11.531344  3455 solver.cpp:266] Iteration 16800 (30.2562 iter/s, 3.30511s/100 iter), loss = 0.112705
I0109 19:53:11.531432  3455 solver.cpp:285]     Train net output #0: loss = 0.112705 (* 1 = 0.112705 loss)
I0109 19:53:11.531447  3455 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 19:53:14.845854  3455 solver.cpp:266] Iteration 16900 (30.171 iter/s, 3.31444s/100 iter), loss = 0.112585
I0109 19:53:14.845919  3455 solver.cpp:285]     Train net output #0: loss = 0.112585 (* 1 = 0.112585 loss)
I0109 19:53:14.845932  3455 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 19:53:18.119611  3455 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 19:53:18.940582  3455 solver.cpp:517]     Test net output #0: loss = 0.445836 (* 1 = 0.445836 loss)
I0109 19:53:18.940616  3455 solver.cpp:517]     Test net output #1: top-1 = 0.865444
I0109 19:53:18.940625  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992444
I0109 19:53:18.971529  3455 solver.cpp:266] Iteration 17000 (24.2385 iter/s, 4.12567s/100 iter), loss = 0.156811
I0109 19:53:18.971563  3455 solver.cpp:285]     Train net output #0: loss = 0.156811 (* 1 = 0.156811 loss)
I0109 19:53:18.971580  3455 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 19:53:22.272963  3455 solver.cpp:266] Iteration 17100 (30.2898 iter/s, 3.30144s/100 iter), loss = 0.107956
I0109 19:53:22.273027  3455 solver.cpp:285]     Train net output #0: loss = 0.107956 (* 1 = 0.107956 loss)
I0109 19:53:22.273039  3455 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 19:53:25.574488  3455 solver.cpp:266] Iteration 17200 (30.2893 iter/s, 3.3015s/100 iter), loss = 0.13633
I0109 19:53:25.574604  3455 solver.cpp:285]     Train net output #0: loss = 0.13633 (* 1 = 0.13633 loss)
I0109 19:53:25.574617  3455 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 19:53:28.883456  3455 solver.cpp:266] Iteration 17300 (30.2218 iter/s, 3.30887s/100 iter), loss = 0.0962406
I0109 19:53:28.883522  3455 solver.cpp:285]     Train net output #0: loss = 0.0962406 (* 1 = 0.0962406 loss)
I0109 19:53:28.883535  3455 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 19:53:32.200505  3455 solver.cpp:266] Iteration 17400 (30.1475 iter/s, 3.31702s/100 iter), loss = 0.114902
I0109 19:53:32.200574  3455 solver.cpp:285]     Train net output #0: loss = 0.114902 (* 1 = 0.114902 loss)
I0109 19:53:32.200587  3455 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 19:53:35.511198  3455 solver.cpp:266] Iteration 17500 (30.2054 iter/s, 3.31066s/100 iter), loss = 0.0806198
I0109 19:53:35.511263  3455 solver.cpp:285]     Train net output #0: loss = 0.0806198 (* 1 = 0.0806198 loss)
I0109 19:53:35.511276  3455 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 19:53:38.820030  3455 solver.cpp:266] Iteration 17600 (30.2226 iter/s, 3.30878s/100 iter), loss = 0.0683193
I0109 19:53:38.820101  3455 solver.cpp:285]     Train net output #0: loss = 0.0683192 (* 1 = 0.0683192 loss)
I0109 19:53:38.820112  3455 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 19:53:42.122066  3455 solver.cpp:266] Iteration 17700 (30.2846 iter/s, 3.302s/100 iter), loss = 0.0879566
I0109 19:53:42.122131  3455 solver.cpp:285]     Train net output #0: loss = 0.0879565 (* 1 = 0.0879565 loss)
I0109 19:53:42.122144  3455 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 19:53:45.425866  3455 solver.cpp:266] Iteration 17800 (30.2684 iter/s, 3.30377s/100 iter), loss = 0.131862
I0109 19:53:45.425956  3455 solver.cpp:285]     Train net output #0: loss = 0.131862 (* 1 = 0.131862 loss)
I0109 19:53:45.425971  3455 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 19:53:48.745000  3455 solver.cpp:266] Iteration 17900 (30.129 iter/s, 3.31906s/100 iter), loss = 0.14728
I0109 19:53:48.745071  3455 solver.cpp:285]     Train net output #0: loss = 0.14728 (* 1 = 0.14728 loss)
I0109 19:53:48.745084  3455 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 19:53:52.024073  3455 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 19:53:52.850050  3455 solver.cpp:517]     Test net output #0: loss = 0.460046 (* 1 = 0.460046 loss)
I0109 19:53:52.850095  3455 solver.cpp:517]     Test net output #1: top-1 = 0.860555
I0109 19:53:52.850101  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 19:53:52.881026  3455 solver.cpp:266] Iteration 18000 (24.178 iter/s, 4.136s/100 iter), loss = 0.0799467
I0109 19:53:52.881112  3455 solver.cpp:285]     Train net output #0: loss = 0.0799466 (* 1 = 0.0799466 loss)
I0109 19:53:52.881129  3455 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 19:53:56.193992  3455 solver.cpp:266] Iteration 18100 (30.1849 iter/s, 3.31292s/100 iter), loss = 0.176641
I0109 19:53:56.194198  3455 solver.cpp:285]     Train net output #0: loss = 0.176641 (* 1 = 0.176641 loss)
I0109 19:53:56.194216  3455 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 19:53:59.504371  3455 solver.cpp:266] Iteration 18200 (30.2095 iter/s, 3.31022s/100 iter), loss = 0.106543
I0109 19:53:59.504433  3455 solver.cpp:285]     Train net output #0: loss = 0.106543 (* 1 = 0.106543 loss)
I0109 19:53:59.504446  3455 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 19:54:02.815374  3455 solver.cpp:266] Iteration 18300 (30.2028 iter/s, 3.31095s/100 iter), loss = 0.111465
I0109 19:54:02.815438  3455 solver.cpp:285]     Train net output #0: loss = 0.111465 (* 1 = 0.111465 loss)
I0109 19:54:02.815450  3455 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 19:54:06.115566  3455 solver.cpp:266] Iteration 18400 (30.3015 iter/s, 3.30017s/100 iter), loss = 0.121946
I0109 19:54:06.115633  3455 solver.cpp:285]     Train net output #0: loss = 0.121946 (* 1 = 0.121946 loss)
I0109 19:54:06.115646  3455 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 19:54:09.422960  3455 solver.cpp:266] Iteration 18500 (30.2356 iter/s, 3.30736s/100 iter), loss = 0.165345
I0109 19:54:09.423041  3455 solver.cpp:285]     Train net output #0: loss = 0.165345 (* 1 = 0.165345 loss)
I0109 19:54:09.423058  3455 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 19:54:12.731175  3455 solver.cpp:266] Iteration 18600 (30.2284 iter/s, 3.30815s/100 iter), loss = 0.0844621
I0109 19:54:12.731240  3455 solver.cpp:285]     Train net output #0: loss = 0.084462 (* 1 = 0.084462 loss)
I0109 19:54:12.731252  3455 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 19:54:16.042073  3455 solver.cpp:266] Iteration 18700 (30.2036 iter/s, 3.31087s/100 iter), loss = 0.105101
I0109 19:54:16.042155  3455 solver.cpp:285]     Train net output #0: loss = 0.105101 (* 1 = 0.105101 loss)
I0109 19:54:16.042171  3455 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 19:54:19.350337  3455 solver.cpp:266] Iteration 18800 (30.2277 iter/s, 3.30822s/100 iter), loss = 0.106112
I0109 19:54:19.350425  3455 solver.cpp:285]     Train net output #0: loss = 0.106112 (* 1 = 0.106112 loss)
I0109 19:54:19.350442  3455 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 19:54:22.643049  3455 solver.cpp:266] Iteration 18900 (30.3705 iter/s, 3.29266s/100 iter), loss = 0.130297
I0109 19:54:22.643116  3455 solver.cpp:285]     Train net output #0: loss = 0.130297 (* 1 = 0.130297 loss)
I0109 19:54:22.643129  3455 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 19:54:25.916667  3455 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 19:54:26.738044  3455 solver.cpp:517]     Test net output #0: loss = 0.442396 (* 1 = 0.442396 loss)
I0109 19:54:26.738225  3455 solver.cpp:517]     Test net output #1: top-1 = 0.868111
I0109 19:54:26.738237  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:54:26.769501  3455 solver.cpp:266] Iteration 19000 (24.2341 iter/s, 4.12642s/100 iter), loss = 0.124538
I0109 19:54:26.769538  3455 solver.cpp:285]     Train net output #0: loss = 0.124538 (* 1 = 0.124538 loss)
I0109 19:54:26.769552  3455 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 19:54:30.075167  3455 solver.cpp:266] Iteration 19100 (30.2511 iter/s, 3.30567s/100 iter), loss = 0.078043
I0109 19:54:30.075244  3455 solver.cpp:285]     Train net output #0: loss = 0.0780429 (* 1 = 0.0780429 loss)
I0109 19:54:30.075268  3455 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 19:54:33.382793  3455 solver.cpp:266] Iteration 19200 (30.2335 iter/s, 3.30759s/100 iter), loss = 0.188461
I0109 19:54:33.382858  3455 solver.cpp:285]     Train net output #0: loss = 0.188461 (* 1 = 0.188461 loss)
I0109 19:54:33.382871  3455 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 19:54:36.688589  3455 solver.cpp:266] Iteration 19300 (30.2501 iter/s, 3.30577s/100 iter), loss = 0.128737
I0109 19:54:36.688658  3455 solver.cpp:285]     Train net output #0: loss = 0.128737 (* 1 = 0.128737 loss)
I0109 19:54:36.688671  3455 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 19:54:40.001814  3455 solver.cpp:266] Iteration 19400 (30.1826 iter/s, 3.31317s/100 iter), loss = 0.107066
I0109 19:54:40.001906  3455 solver.cpp:285]     Train net output #0: loss = 0.107066 (* 1 = 0.107066 loss)
I0109 19:54:40.001922  3455 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 19:54:43.326249  3455 solver.cpp:266] Iteration 19500 (30.0807 iter/s, 3.32439s/100 iter), loss = 0.0846474
I0109 19:54:43.326324  3455 solver.cpp:285]     Train net output #0: loss = 0.0846473 (* 1 = 0.0846473 loss)
I0109 19:54:43.326339  3455 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 19:54:46.644721  3455 solver.cpp:266] Iteration 19600 (30.1347 iter/s, 3.31844s/100 iter), loss = 0.131431
I0109 19:54:46.644794  3455 solver.cpp:285]     Train net output #0: loss = 0.13143 (* 1 = 0.13143 loss)
I0109 19:54:46.644806  3455 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 19:54:49.957558  3455 solver.cpp:266] Iteration 19700 (30.1861 iter/s, 3.31278s/100 iter), loss = 0.0962161
I0109 19:54:49.957643  3455 solver.cpp:285]     Train net output #0: loss = 0.096216 (* 1 = 0.096216 loss)
I0109 19:54:49.957659  3455 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 19:54:53.274801  3455 solver.cpp:266] Iteration 19800 (30.1459 iter/s, 3.3172s/100 iter), loss = 0.12769
I0109 19:54:53.274888  3455 solver.cpp:285]     Train net output #0: loss = 0.12769 (* 1 = 0.12769 loss)
I0109 19:54:53.274904  3455 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 19:54:56.587746  3455 solver.cpp:266] Iteration 19900 (30.185 iter/s, 3.3129s/100 iter), loss = 0.161837
I0109 19:54:56.587824  3455 solver.cpp:285]     Train net output #0: loss = 0.161837 (* 1 = 0.161837 loss)
I0109 19:54:56.587838  3455 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 19:54:59.866840  3455 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/_iter_20000.caffemodel
I0109 19:54:59.968092  3455 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/_iter_20000.solverstate
I0109 19:54:59.994408  3455 solver.cpp:378] Iteration 20000, loss = 0.011655
I0109 19:54:59.994454  3455 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 19:55:00.807394  3455 solver.cpp:517]     Test net output #0: loss = 0.442212 (* 1 = 0.442212 loss)
I0109 19:55:00.807437  3455 solver.cpp:517]     Test net output #1: top-1 = 0.868444
I0109 19:55:00.807449  3455 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0109 19:55:00.807458  3455 solver.cpp:386] Optimization Done (29.6917 iter/s).
I0109 19:55:00.807467  3455 caffe_interface.cpp:530] Optimization Done.
I0109 19:55:00.970499  3493 pruning_runner.cpp:190] Sens info found, use it.
I0109 19:55:01.015662  3493 pruning_runner.cpp:217] Start compressing, please wait...
I0109 19:55:02.073024  3493 pruning_runner.cpp:264] Compression complete 0.000469607%
I0109 19:55:02.404464  3493 pruning_runner.cpp:264] Compression complete 0.00093921%
I0109 19:55:02.720948  3493 pruning_runner.cpp:264] Compression complete 50.0005%
I0109 19:55:03.039400  3493 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 19:55:03.039710  3493 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 19:55:03.040086  3493 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:55:03.040298  3493 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:55:03.040449  3493 layer_factory.hpp:77] Creating layer data
I0109 19:55:03.040513  3493 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:03.040634  3493 net.cpp:94] Creating Layer data
I0109 19:55:03.040657  3493 net.cpp:409] data -> data
I0109 19:55:03.040673  3493 net.cpp:409] data -> label
I0109 19:55:03.041695  3574 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:55:03.041733  3574 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:55:03.041832  3493 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:55:03.041993  3493 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:55:03.048202  3493 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:03.048256  3493 net.cpp:144] Setting up data
I0109 19:55:03.048276  3493 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:55:03.048286  3493 net.cpp:151] Top shape: 50 (50)
I0109 19:55:03.048292  3493 net.cpp:159] Memory required for data: 614600
I0109 19:55:03.048300  3493 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:55:03.048313  3493 net.cpp:94] Creating Layer label_data_1_split
I0109 19:55:03.048327  3493 net.cpp:435] label_data_1_split <- label
I0109 19:55:03.048341  3493 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:55:03.048354  3493 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:55:03.048386  3493 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:55:03.048461  3493 net.cpp:144] Setting up label_data_1_split
I0109 19:55:03.048476  3493 net.cpp:151] Top shape: 50 (50)
I0109 19:55:03.048485  3493 net.cpp:151] Top shape: 50 (50)
I0109 19:55:03.048492  3493 net.cpp:151] Top shape: 50 (50)
I0109 19:55:03.048498  3493 net.cpp:159] Memory required for data: 615200
I0109 19:55:03.048511  3493 layer_factory.hpp:77] Creating layer conv1
I0109 19:55:03.048529  3493 net.cpp:94] Creating Layer conv1
I0109 19:55:03.048543  3493 net.cpp:435] conv1 <- data
I0109 19:55:03.048557  3493 net.cpp:409] conv1 -> conv1
I0109 19:55:03.049544  3493 net.cpp:144] Setting up conv1
I0109 19:55:03.049566  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.049574  3493 net.cpp:159] Memory required for data: 7168800
I0109 19:55:03.049607  3493 layer_factory.hpp:77] Creating layer bn1
I0109 19:55:03.049633  3493 net.cpp:94] Creating Layer bn1
I0109 19:55:03.049640  3493 net.cpp:435] bn1 <- conv1
I0109 19:55:03.049660  3493 net.cpp:409] bn1 -> scale1
I0109 19:55:03.050664  3493 net.cpp:144] Setting up bn1
I0109 19:55:03.050683  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.050722  3493 net.cpp:159] Memory required for data: 13722400
I0109 19:55:03.050745  3493 layer_factory.hpp:77] Creating layer relu1
I0109 19:55:03.050763  3493 net.cpp:94] Creating Layer relu1
I0109 19:55:03.050772  3493 net.cpp:435] relu1 <- scale1
I0109 19:55:03.050781  3493 net.cpp:409] relu1 -> relu1
I0109 19:55:03.050824  3493 net.cpp:144] Setting up relu1
I0109 19:55:03.050859  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.050915  3493 net.cpp:159] Memory required for data: 20276000
I0109 19:55:03.050938  3493 layer_factory.hpp:77] Creating layer conv2
I0109 19:55:03.051002  3493 net.cpp:94] Creating Layer conv2
I0109 19:55:03.051046  3493 net.cpp:435] conv2 <- relu1
I0109 19:55:03.051208  3493 net.cpp:409] conv2 -> conv2
I0109 19:55:03.052232  3493 net.cpp:144] Setting up conv2
I0109 19:55:03.052278  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.052286  3493 net.cpp:159] Memory required for data: 26829600
I0109 19:55:03.052376  3493 layer_factory.hpp:77] Creating layer bn2
I0109 19:55:03.052533  3493 net.cpp:94] Creating Layer bn2
I0109 19:55:03.052578  3493 net.cpp:435] bn2 <- conv2
I0109 19:55:03.052625  3493 net.cpp:409] bn2 -> scale2
I0109 19:55:03.053346  3493 net.cpp:144] Setting up bn2
I0109 19:55:03.053369  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.053375  3493 net.cpp:159] Memory required for data: 33383200
I0109 19:55:03.053385  3493 layer_factory.hpp:77] Creating layer relu2
I0109 19:55:03.053391  3493 net.cpp:94] Creating Layer relu2
I0109 19:55:03.053406  3493 net.cpp:435] relu2 <- scale2
I0109 19:55:03.053418  3493 net.cpp:409] relu2 -> relu2
I0109 19:55:03.053463  3493 net.cpp:144] Setting up relu2
I0109 19:55:03.053483  3493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:03.053490  3493 net.cpp:159] Memory required for data: 39936800
I0109 19:55:03.053498  3493 layer_factory.hpp:77] Creating layer pool1
I0109 19:55:03.053519  3493 net.cpp:94] Creating Layer pool1
I0109 19:55:03.053527  3493 net.cpp:435] pool1 <- relu2
I0109 19:55:03.053536  3493 net.cpp:409] pool1 -> pool1
I0109 19:55:03.053653  3493 net.cpp:144] Setting up pool1
I0109 19:55:03.053676  3493 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:55:03.053683  3493 net.cpp:159] Memory required for data: 41575200
I0109 19:55:03.053690  3493 layer_factory.hpp:77] Creating layer drop1
I0109 19:55:03.053701  3493 net.cpp:94] Creating Layer drop1
I0109 19:55:03.053730  3493 net.cpp:435] drop1 <- pool1
I0109 19:55:03.053766  3493 net.cpp:409] drop1 -> drop1
I0109 19:55:03.053923  3493 net.cpp:144] Setting up drop1
I0109 19:55:03.053972  3493 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:55:03.054005  3493 net.cpp:159] Memory required for data: 43213600
I0109 19:55:03.054054  3493 layer_factory.hpp:77] Creating layer conv3
I0109 19:55:03.054111  3493 net.cpp:94] Creating Layer conv3
I0109 19:55:03.054153  3493 net.cpp:435] conv3 <- drop1
I0109 19:55:03.054198  3493 net.cpp:409] conv3 -> conv3
I0109 19:55:03.055533  3493 net.cpp:144] Setting up conv3
I0109 19:55:03.055591  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.055634  3493 net.cpp:159] Memory required for data: 46490400
I0109 19:55:03.055678  3493 layer_factory.hpp:77] Creating layer bn3
I0109 19:55:03.055724  3493 net.cpp:94] Creating Layer bn3
I0109 19:55:03.055765  3493 net.cpp:435] bn3 <- conv3
I0109 19:55:03.055819  3493 net.cpp:409] bn3 -> scale3
I0109 19:55:03.057165  3493 net.cpp:144] Setting up bn3
I0109 19:55:03.057191  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.057199  3493 net.cpp:159] Memory required for data: 49767200
I0109 19:55:03.057294  3493 layer_factory.hpp:77] Creating layer relu3
I0109 19:55:03.057340  3493 net.cpp:94] Creating Layer relu3
I0109 19:55:03.057381  3493 net.cpp:435] relu3 <- scale3
I0109 19:55:03.057425  3493 net.cpp:409] relu3 -> relu3
I0109 19:55:03.057500  3493 net.cpp:144] Setting up relu3
I0109 19:55:03.057546  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.057585  3493 net.cpp:159] Memory required for data: 53044000
I0109 19:55:03.057653  3493 layer_factory.hpp:77] Creating layer conv4
I0109 19:55:03.057703  3493 net.cpp:94] Creating Layer conv4
I0109 19:55:03.057742  3493 net.cpp:435] conv4 <- relu3
I0109 19:55:03.057760  3493 net.cpp:409] conv4 -> conv4
I0109 19:55:03.058645  3493 net.cpp:144] Setting up conv4
I0109 19:55:03.058673  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.058681  3493 net.cpp:159] Memory required for data: 56320800
I0109 19:55:03.058765  3493 layer_factory.hpp:77] Creating layer bn4
I0109 19:55:03.058815  3493 net.cpp:94] Creating Layer bn4
I0109 19:55:03.058861  3493 net.cpp:435] bn4 <- conv4
I0109 19:55:03.058909  3493 net.cpp:409] bn4 -> scale4
I0109 19:55:03.060086  3493 net.cpp:144] Setting up bn4
I0109 19:55:03.060111  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.060120  3493 net.cpp:159] Memory required for data: 59597600
I0109 19:55:03.060209  3493 layer_factory.hpp:77] Creating layer relu4
I0109 19:55:03.060256  3493 net.cpp:94] Creating Layer relu4
I0109 19:55:03.060302  3493 net.cpp:435] relu4 <- scale4
I0109 19:55:03.060348  3493 net.cpp:409] relu4 -> relu4
I0109 19:55:03.060423  3493 net.cpp:144] Setting up relu4
I0109 19:55:03.060508  3493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:03.060533  3493 net.cpp:159] Memory required for data: 62874400
I0109 19:55:03.060564  3493 layer_factory.hpp:77] Creating layer pool2
I0109 19:55:03.060614  3493 net.cpp:94] Creating Layer pool2
I0109 19:55:03.060660  3493 net.cpp:435] pool2 <- relu4
I0109 19:55:03.060706  3493 net.cpp:409] pool2 -> pool2
I0109 19:55:03.060801  3493 net.cpp:144] Setting up pool2
I0109 19:55:03.060843  3493 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:55:03.060885  3493 net.cpp:159] Memory required for data: 63693600
I0109 19:55:03.060920  3493 layer_factory.hpp:77] Creating layer drop2
I0109 19:55:03.060961  3493 net.cpp:94] Creating Layer drop2
I0109 19:55:03.060973  3493 net.cpp:435] drop2 <- pool2
I0109 19:55:03.060984  3493 net.cpp:409] drop2 -> drop2
I0109 19:55:03.061076  3493 net.cpp:144] Setting up drop2
I0109 19:55:03.061094  3493 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:55:03.061100  3493 net.cpp:159] Memory required for data: 64512800
I0109 19:55:03.061103  3493 layer_factory.hpp:77] Creating layer fc1
I0109 19:55:03.061115  3493 net.cpp:94] Creating Layer fc1
I0109 19:55:03.061131  3493 net.cpp:435] fc1 <- drop2
I0109 19:55:03.061143  3493 net.cpp:409] fc1 -> fc1
I0109 19:55:03.081910  3493 net.cpp:144] Setting up fc1
I0109 19:55:03.081938  3493 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:03.081944  3493 net.cpp:159] Memory required for data: 64615200
I0109 19:55:03.081954  3493 layer_factory.hpp:77] Creating layer bn5
I0109 19:55:03.081969  3493 net.cpp:94] Creating Layer bn5
I0109 19:55:03.082015  3493 net.cpp:435] bn5 <- fc1
I0109 19:55:03.082036  3493 net.cpp:409] bn5 -> scale5
I0109 19:55:03.082634  3493 net.cpp:144] Setting up bn5
I0109 19:55:03.082655  3493 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:03.082664  3493 net.cpp:159] Memory required for data: 64717600
I0109 19:55:03.082686  3493 layer_factory.hpp:77] Creating layer relu5
I0109 19:55:03.082700  3493 net.cpp:94] Creating Layer relu5
I0109 19:55:03.082707  3493 net.cpp:435] relu5 <- scale5
I0109 19:55:03.082741  3493 net.cpp:409] relu5 -> relu5
I0109 19:55:03.082794  3493 net.cpp:144] Setting up relu5
I0109 19:55:03.082811  3493 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:03.082818  3493 net.cpp:159] Memory required for data: 64820000
I0109 19:55:03.082825  3493 layer_factory.hpp:77] Creating layer drop3
I0109 19:55:03.082836  3493 net.cpp:94] Creating Layer drop3
I0109 19:55:03.082845  3493 net.cpp:435] drop3 <- relu5
I0109 19:55:03.082856  3493 net.cpp:409] drop3 -> drop3
I0109 19:55:03.082911  3493 net.cpp:144] Setting up drop3
I0109 19:55:03.082926  3493 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:03.082931  3493 net.cpp:159] Memory required for data: 64922400
I0109 19:55:03.082934  3493 layer_factory.hpp:77] Creating layer fc2
I0109 19:55:03.082947  3493 net.cpp:94] Creating Layer fc2
I0109 19:55:03.082963  3493 net.cpp:435] fc2 <- drop3
I0109 19:55:03.082975  3493 net.cpp:409] fc2 -> fc2
I0109 19:55:03.083179  3493 net.cpp:144] Setting up fc2
I0109 19:55:03.083197  3493 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:03.083202  3493 net.cpp:159] Memory required for data: 64924400
I0109 19:55:03.083212  3493 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:55:03.083233  3493 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:55:03.083243  3493 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:55:03.083256  3493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:55:03.083276  3493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:55:03.083287  3493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:55:03.083364  3493 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:55:03.083379  3493 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:03.083384  3493 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:03.083389  3493 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:03.083392  3493 net.cpp:159] Memory required for data: 64930400
I0109 19:55:03.083397  3493 layer_factory.hpp:77] Creating layer loss
I0109 19:55:03.083406  3493 net.cpp:94] Creating Layer loss
I0109 19:55:03.083411  3493 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:55:03.083417  3493 net.cpp:435] loss <- label_data_1_split_0
I0109 19:55:03.083429  3493 net.cpp:409] loss -> loss
I0109 19:55:03.083444  3493 layer_factory.hpp:77] Creating layer loss
I0109 19:55:03.083577  3493 net.cpp:144] Setting up loss
I0109 19:55:03.083592  3493 net.cpp:151] Top shape: (1)
I0109 19:55:03.083597  3493 net.cpp:154]     with loss weight 1
I0109 19:55:03.083618  3493 net.cpp:159] Memory required for data: 64930404
I0109 19:55:03.083622  3493 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:55:03.083633  3493 net.cpp:94] Creating Layer accuracy-top1
I0109 19:55:03.083642  3493 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:55:03.083649  3493 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:55:03.083695  3493 net.cpp:409] accuracy-top1 -> top-1
I0109 19:55:03.083741  3493 net.cpp:144] Setting up accuracy-top1
I0109 19:55:03.083762  3493 net.cpp:151] Top shape: (1)
I0109 19:55:03.083770  3493 net.cpp:159] Memory required for data: 64930408
I0109 19:55:03.083775  3493 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:55:03.083791  3493 net.cpp:94] Creating Layer accuracy-top5
I0109 19:55:03.083797  3493 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:55:03.083806  3493 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:55:03.083847  3493 net.cpp:409] accuracy-top5 -> top-5
I0109 19:55:03.083890  3493 net.cpp:144] Setting up accuracy-top5
I0109 19:55:03.083911  3493 net.cpp:151] Top shape: (1)
I0109 19:55:03.083931  3493 net.cpp:159] Memory required for data: 64930412
I0109 19:55:03.083940  3493 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:55:03.083945  3493 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:55:03.083954  3493 net.cpp:220] loss needs backward computation.
I0109 19:55:03.083962  3493 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:55:03.083971  3493 net.cpp:220] fc2 needs backward computation.
I0109 19:55:03.083978  3493 net.cpp:220] drop3 needs backward computation.
I0109 19:55:03.083986  3493 net.cpp:220] relu5 needs backward computation.
I0109 19:55:03.083994  3493 net.cpp:220] bn5 needs backward computation.
I0109 19:55:03.084002  3493 net.cpp:220] fc1 needs backward computation.
I0109 19:55:03.084007  3493 net.cpp:220] drop2 needs backward computation.
I0109 19:55:03.084014  3493 net.cpp:220] pool2 needs backward computation.
I0109 19:55:03.084022  3493 net.cpp:220] relu4 needs backward computation.
I0109 19:55:03.084028  3493 net.cpp:220] bn4 needs backward computation.
I0109 19:55:03.084034  3493 net.cpp:220] conv4 needs backward computation.
I0109 19:55:03.084044  3493 net.cpp:220] relu3 needs backward computation.
I0109 19:55:03.084053  3493 net.cpp:220] bn3 needs backward computation.
I0109 19:55:03.084060  3493 net.cpp:220] conv3 needs backward computation.
I0109 19:55:03.084066  3493 net.cpp:220] drop1 needs backward computation.
I0109 19:55:03.084076  3493 net.cpp:220] pool1 needs backward computation.
I0109 19:55:03.084084  3493 net.cpp:220] relu2 needs backward computation.
I0109 19:55:03.084094  3493 net.cpp:220] bn2 needs backward computation.
I0109 19:55:03.084100  3493 net.cpp:220] conv2 needs backward computation.
I0109 19:55:03.084107  3493 net.cpp:220] relu1 needs backward computation.
I0109 19:55:03.084116  3493 net.cpp:220] bn1 needs backward computation.
I0109 19:55:03.084123  3493 net.cpp:220] conv1 needs backward computation.
I0109 19:55:03.084133  3493 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:55:03.084142  3493 net.cpp:222] data does not need backward computation.
I0109 19:55:03.084148  3493 net.cpp:264] This network produces output loss
I0109 19:55:03.084154  3493 net.cpp:264] This network produces output top-1
I0109 19:55:03.084161  3493 net.cpp:264] This network produces output top-5
I0109 19:55:03.084223  3493 net.cpp:284] Network initialization done.
I0109 19:55:03.088178  3493 caffe_interface.cpp:363] Running for 180 iterations.
I0109 19:55:03.098217  3493 caffe_interface.cpp:125] Batch 0, loss = 0.735301
I0109 19:55:03.098246  3493 caffe_interface.cpp:125] Batch 0, top-1 = 0.8
I0109 19:55:03.098254  3493 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0109 19:55:03.102877  3493 caffe_interface.cpp:125] Batch 1, loss = 0.498965
I0109 19:55:03.102901  3493 caffe_interface.cpp:125] Batch 1, top-1 = 0.88
I0109 19:55:03.102907  3493 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 19:55:03.107514  3493 caffe_interface.cpp:125] Batch 2, loss = 1.13359
I0109 19:55:03.107538  3493 caffe_interface.cpp:125] Batch 2, top-1 = 0.78
I0109 19:55:03.107544  3493 caffe_interface.cpp:125] Batch 2, top-5 = 0.98
I0109 19:55:03.112102  3493 caffe_interface.cpp:125] Batch 3, loss = 0.592542
I0109 19:55:03.112126  3493 caffe_interface.cpp:125] Batch 3, top-1 = 0.84
I0109 19:55:03.112133  3493 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 19:55:03.116760  3493 caffe_interface.cpp:125] Batch 4, loss = 0.421008
I0109 19:55:03.116784  3493 caffe_interface.cpp:125] Batch 4, top-1 = 0.84
I0109 19:55:03.116791  3493 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 19:55:03.121400  3493 caffe_interface.cpp:125] Batch 5, loss = 1.0041
I0109 19:55:03.121428  3493 caffe_interface.cpp:125] Batch 5, top-1 = 0.7
I0109 19:55:03.121433  3493 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 19:55:03.126018  3493 caffe_interface.cpp:125] Batch 6, loss = 0.943277
I0109 19:55:03.126044  3493 caffe_interface.cpp:125] Batch 6, top-1 = 0.8
I0109 19:55:03.126050  3493 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 19:55:03.130662  3493 caffe_interface.cpp:125] Batch 7, loss = 0.393047
I0109 19:55:03.130686  3493 caffe_interface.cpp:125] Batch 7, top-1 = 0.84
I0109 19:55:03.130692  3493 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 19:55:03.135291  3493 caffe_interface.cpp:125] Batch 8, loss = 0.58943
I0109 19:55:03.135315  3493 caffe_interface.cpp:125] Batch 8, top-1 = 0.8
I0109 19:55:03.135321  3493 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0109 19:55:03.139932  3493 caffe_interface.cpp:125] Batch 9, loss = 0.53964
I0109 19:55:03.139957  3493 caffe_interface.cpp:125] Batch 9, top-1 = 0.84
I0109 19:55:03.139963  3493 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 19:55:03.144569  3493 caffe_interface.cpp:125] Batch 10, loss = 0.433381
I0109 19:55:03.144593  3493 caffe_interface.cpp:125] Batch 10, top-1 = 0.8
I0109 19:55:03.144598  3493 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0109 19:55:03.149189  3493 caffe_interface.cpp:125] Batch 11, loss = 0.698594
I0109 19:55:03.149214  3493 caffe_interface.cpp:125] Batch 11, top-1 = 0.82
I0109 19:55:03.149220  3493 caffe_interface.cpp:125] Batch 11, top-5 = 0.96
I0109 19:55:03.153852  3493 caffe_interface.cpp:125] Batch 12, loss = 0.670734
I0109 19:55:03.153877  3493 caffe_interface.cpp:125] Batch 12, top-1 = 0.76
I0109 19:55:03.153883  3493 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0109 19:55:03.158488  3493 caffe_interface.cpp:125] Batch 13, loss = 0.657005
I0109 19:55:03.158512  3493 caffe_interface.cpp:125] Batch 13, top-1 = 0.82
I0109 19:55:03.158519  3493 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0109 19:55:03.163115  3493 caffe_interface.cpp:125] Batch 14, loss = 0.471031
I0109 19:55:03.163138  3493 caffe_interface.cpp:125] Batch 14, top-1 = 0.84
I0109 19:55:03.163144  3493 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0109 19:55:03.167749  3493 caffe_interface.cpp:125] Batch 15, loss = 0.694276
I0109 19:55:03.167781  3493 caffe_interface.cpp:125] Batch 15, top-1 = 0.86
I0109 19:55:03.167788  3493 caffe_interface.cpp:125] Batch 15, top-5 = 0.96
I0109 19:55:03.172420  3493 caffe_interface.cpp:125] Batch 16, loss = 0.504983
I0109 19:55:03.172446  3493 caffe_interface.cpp:125] Batch 16, top-1 = 0.84
I0109 19:55:03.172452  3493 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0109 19:55:03.177048  3493 caffe_interface.cpp:125] Batch 17, loss = 0.67779
I0109 19:55:03.177072  3493 caffe_interface.cpp:125] Batch 17, top-1 = 0.84
I0109 19:55:03.177078  3493 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0109 19:55:03.181689  3493 caffe_interface.cpp:125] Batch 18, loss = 0.721233
I0109 19:55:03.181710  3493 caffe_interface.cpp:125] Batch 18, top-1 = 0.8
I0109 19:55:03.181716  3493 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 19:55:03.186333  3493 caffe_interface.cpp:125] Batch 19, loss = 0.447583
I0109 19:55:03.186357  3493 caffe_interface.cpp:125] Batch 19, top-1 = 0.86
I0109 19:55:03.186362  3493 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0109 19:55:03.190982  3493 caffe_interface.cpp:125] Batch 20, loss = 0.580764
I0109 19:55:03.191005  3493 caffe_interface.cpp:125] Batch 20, top-1 = 0.8
I0109 19:55:03.191011  3493 caffe_interface.cpp:125] Batch 20, top-5 = 0.98
I0109 19:55:03.195616  3493 caffe_interface.cpp:125] Batch 21, loss = 1.14
I0109 19:55:03.195638  3493 caffe_interface.cpp:125] Batch 21, top-1 = 0.76
I0109 19:55:03.195644  3493 caffe_interface.cpp:125] Batch 21, top-5 = 0.96
I0109 19:55:03.200239  3493 caffe_interface.cpp:125] Batch 22, loss = 0.953891
I0109 19:55:03.200263  3493 caffe_interface.cpp:125] Batch 22, top-1 = 0.72
I0109 19:55:03.200268  3493 caffe_interface.cpp:125] Batch 22, top-5 = 0.98
I0109 19:55:03.204859  3493 caffe_interface.cpp:125] Batch 23, loss = 0.584375
I0109 19:55:03.204882  3493 caffe_interface.cpp:125] Batch 23, top-1 = 0.8
I0109 19:55:03.204888  3493 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 19:55:03.209468  3493 caffe_interface.cpp:125] Batch 24, loss = 0.572445
I0109 19:55:03.209492  3493 caffe_interface.cpp:125] Batch 24, top-1 = 0.86
I0109 19:55:03.209498  3493 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0109 19:55:03.214104  3493 caffe_interface.cpp:125] Batch 25, loss = 1.08352
I0109 19:55:03.214128  3493 caffe_interface.cpp:125] Batch 25, top-1 = 0.78
I0109 19:55:03.214133  3493 caffe_interface.cpp:125] Batch 25, top-5 = 0.98
I0109 19:55:03.218750  3493 caffe_interface.cpp:125] Batch 26, loss = 0.788228
I0109 19:55:03.218772  3493 caffe_interface.cpp:125] Batch 26, top-1 = 0.78
I0109 19:55:03.218777  3493 caffe_interface.cpp:125] Batch 26, top-5 = 1
I0109 19:55:03.223378  3493 caffe_interface.cpp:125] Batch 27, loss = 0.377798
I0109 19:55:03.223402  3493 caffe_interface.cpp:125] Batch 27, top-1 = 0.9
I0109 19:55:03.223407  3493 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 19:55:03.228009  3493 caffe_interface.cpp:125] Batch 28, loss = 0.370778
I0109 19:55:03.228032  3493 caffe_interface.cpp:125] Batch 28, top-1 = 0.88
I0109 19:55:03.228039  3493 caffe_interface.cpp:125] Batch 28, top-5 = 1
I0109 19:55:03.232630  3493 caffe_interface.cpp:125] Batch 29, loss = 0.489793
I0109 19:55:03.232653  3493 caffe_interface.cpp:125] Batch 29, top-1 = 0.84
I0109 19:55:03.232658  3493 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 19:55:03.237229  3493 caffe_interface.cpp:125] Batch 30, loss = 0.493014
I0109 19:55:03.237252  3493 caffe_interface.cpp:125] Batch 30, top-1 = 0.9
I0109 19:55:03.237258  3493 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0109 19:55:03.241873  3493 caffe_interface.cpp:125] Batch 31, loss = 0.487057
I0109 19:55:03.241897  3493 caffe_interface.cpp:125] Batch 31, top-1 = 0.88
I0109 19:55:03.241904  3493 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 19:55:03.246480  3493 caffe_interface.cpp:125] Batch 32, loss = 0.918941
I0109 19:55:03.246503  3493 caffe_interface.cpp:125] Batch 32, top-1 = 0.7
I0109 19:55:03.246510  3493 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 19:55:03.251102  3493 caffe_interface.cpp:125] Batch 33, loss = 0.682054
I0109 19:55:03.251125  3493 caffe_interface.cpp:125] Batch 33, top-1 = 0.78
I0109 19:55:03.251132  3493 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 19:55:03.255739  3493 caffe_interface.cpp:125] Batch 34, loss = 0.67233
I0109 19:55:03.255762  3493 caffe_interface.cpp:125] Batch 34, top-1 = 0.76
I0109 19:55:03.255769  3493 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0109 19:55:03.260378  3493 caffe_interface.cpp:125] Batch 35, loss = 0.409122
I0109 19:55:03.260402  3493 caffe_interface.cpp:125] Batch 35, top-1 = 0.86
I0109 19:55:03.260408  3493 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 19:55:03.264982  3493 caffe_interface.cpp:125] Batch 36, loss = 0.990164
I0109 19:55:03.265007  3493 caffe_interface.cpp:125] Batch 36, top-1 = 0.74
I0109 19:55:03.265012  3493 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0109 19:55:03.269614  3493 caffe_interface.cpp:125] Batch 37, loss = 1.02008
I0109 19:55:03.269634  3493 caffe_interface.cpp:125] Batch 37, top-1 = 0.72
I0109 19:55:03.269639  3493 caffe_interface.cpp:125] Batch 37, top-5 = 0.98
I0109 19:55:03.274222  3493 caffe_interface.cpp:125] Batch 38, loss = 0.935825
I0109 19:55:03.274245  3493 caffe_interface.cpp:125] Batch 38, top-1 = 0.8
I0109 19:55:03.274250  3493 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0109 19:55:03.278841  3493 caffe_interface.cpp:125] Batch 39, loss = 0.377731
I0109 19:55:03.278865  3493 caffe_interface.cpp:125] Batch 39, top-1 = 0.84
I0109 19:55:03.278872  3493 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 19:55:03.283432  3493 caffe_interface.cpp:125] Batch 40, loss = 0.775535
I0109 19:55:03.283455  3493 caffe_interface.cpp:125] Batch 40, top-1 = 0.84
I0109 19:55:03.283463  3493 caffe_interface.cpp:125] Batch 40, top-5 = 0.98
I0109 19:55:03.288050  3493 caffe_interface.cpp:125] Batch 41, loss = 0.617783
I0109 19:55:03.288074  3493 caffe_interface.cpp:125] Batch 41, top-1 = 0.82
I0109 19:55:03.288079  3493 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 19:55:03.292618  3493 caffe_interface.cpp:125] Batch 42, loss = 0.453777
I0109 19:55:03.292642  3493 caffe_interface.cpp:125] Batch 42, top-1 = 0.84
I0109 19:55:03.292649  3493 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0109 19:55:03.297107  3493 caffe_interface.cpp:125] Batch 43, loss = 0.788696
I0109 19:55:03.297134  3493 caffe_interface.cpp:125] Batch 43, top-1 = 0.82
I0109 19:55:03.297139  3493 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0109 19:55:03.301565  3493 caffe_interface.cpp:125] Batch 44, loss = 0.291191
I0109 19:55:03.301604  3493 caffe_interface.cpp:125] Batch 44, top-1 = 0.92
I0109 19:55:03.301612  3493 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0109 19:55:03.306028  3493 caffe_interface.cpp:125] Batch 45, loss = 1.24346
I0109 19:55:03.306051  3493 caffe_interface.cpp:125] Batch 45, top-1 = 0.72
I0109 19:55:03.306056  3493 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0109 19:55:03.310434  3493 caffe_interface.cpp:125] Batch 46, loss = 0.961688
I0109 19:55:03.310457  3493 caffe_interface.cpp:125] Batch 46, top-1 = 0.7
I0109 19:55:03.310463  3493 caffe_interface.cpp:125] Batch 46, top-5 = 0.98
I0109 19:55:03.314869  3493 caffe_interface.cpp:125] Batch 47, loss = 1.16413
I0109 19:55:03.314893  3493 caffe_interface.cpp:125] Batch 47, top-1 = 0.74
I0109 19:55:03.314899  3493 caffe_interface.cpp:125] Batch 47, top-5 = 0.98
I0109 19:55:03.319315  3493 caffe_interface.cpp:125] Batch 48, loss = 0.397433
I0109 19:55:03.319339  3493 caffe_interface.cpp:125] Batch 48, top-1 = 0.84
I0109 19:55:03.319344  3493 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0109 19:55:03.323778  3493 caffe_interface.cpp:125] Batch 49, loss = 0.688431
I0109 19:55:03.323802  3493 caffe_interface.cpp:125] Batch 49, top-1 = 0.84
I0109 19:55:03.323808  3493 caffe_interface.cpp:125] Batch 49, top-5 = 0.96
I0109 19:55:03.328238  3493 caffe_interface.cpp:125] Batch 50, loss = 0.709952
I0109 19:55:03.328263  3493 caffe_interface.cpp:125] Batch 50, top-1 = 0.84
I0109 19:55:03.328269  3493 caffe_interface.cpp:125] Batch 50, top-5 = 0.98
I0109 19:55:03.332692  3493 caffe_interface.cpp:125] Batch 51, loss = 1.01327
I0109 19:55:03.332717  3493 caffe_interface.cpp:125] Batch 51, top-1 = 0.78
I0109 19:55:03.332723  3493 caffe_interface.cpp:125] Batch 51, top-5 = 0.96
I0109 19:55:03.337138  3493 caffe_interface.cpp:125] Batch 52, loss = 0.96062
I0109 19:55:03.337162  3493 caffe_interface.cpp:125] Batch 52, top-1 = 0.7
I0109 19:55:03.337168  3493 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0109 19:55:03.341580  3493 caffe_interface.cpp:125] Batch 53, loss = 0.742444
I0109 19:55:03.341616  3493 caffe_interface.cpp:125] Batch 53, top-1 = 0.74
I0109 19:55:03.341622  3493 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0109 19:55:03.346060  3493 caffe_interface.cpp:125] Batch 54, loss = 0.679533
I0109 19:55:03.346083  3493 caffe_interface.cpp:125] Batch 54, top-1 = 0.82
I0109 19:55:03.346089  3493 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0109 19:55:03.350507  3493 caffe_interface.cpp:125] Batch 55, loss = 0.435583
I0109 19:55:03.350531  3493 caffe_interface.cpp:125] Batch 55, top-1 = 0.84
I0109 19:55:03.350538  3493 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0109 19:55:03.354964  3493 caffe_interface.cpp:125] Batch 56, loss = 0.543148
I0109 19:55:03.354987  3493 caffe_interface.cpp:125] Batch 56, top-1 = 0.84
I0109 19:55:03.354993  3493 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0109 19:55:03.359408  3493 caffe_interface.cpp:125] Batch 57, loss = 0.749353
I0109 19:55:03.359431  3493 caffe_interface.cpp:125] Batch 57, top-1 = 0.78
I0109 19:55:03.359436  3493 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0109 19:55:03.363853  3493 caffe_interface.cpp:125] Batch 58, loss = 0.970797
I0109 19:55:03.363876  3493 caffe_interface.cpp:125] Batch 58, top-1 = 0.8
I0109 19:55:03.363883  3493 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0109 19:55:03.368294  3493 caffe_interface.cpp:125] Batch 59, loss = 0.731064
I0109 19:55:03.368319  3493 caffe_interface.cpp:125] Batch 59, top-1 = 0.82
I0109 19:55:03.368324  3493 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0109 19:55:03.372728  3493 caffe_interface.cpp:125] Batch 60, loss = 0.643078
I0109 19:55:03.372752  3493 caffe_interface.cpp:125] Batch 60, top-1 = 0.84
I0109 19:55:03.372779  3493 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0109 19:55:03.377203  3493 caffe_interface.cpp:125] Batch 61, loss = 0.757908
I0109 19:55:03.377226  3493 caffe_interface.cpp:125] Batch 61, top-1 = 0.76
I0109 19:55:03.377233  3493 caffe_interface.cpp:125] Batch 61, top-5 = 0.98
I0109 19:55:03.381669  3493 caffe_interface.cpp:125] Batch 62, loss = 0.572945
I0109 19:55:03.381690  3493 caffe_interface.cpp:125] Batch 62, top-1 = 0.86
I0109 19:55:03.381695  3493 caffe_interface.cpp:125] Batch 62, top-5 = 0.98
I0109 19:55:03.386106  3493 caffe_interface.cpp:125] Batch 63, loss = 0.860968
I0109 19:55:03.386129  3493 caffe_interface.cpp:125] Batch 63, top-1 = 0.84
I0109 19:55:03.386135  3493 caffe_interface.cpp:125] Batch 63, top-5 = 0.94
I0109 19:55:03.390576  3493 caffe_interface.cpp:125] Batch 64, loss = 0.268892
I0109 19:55:03.390599  3493 caffe_interface.cpp:125] Batch 64, top-1 = 0.92
I0109 19:55:03.390605  3493 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 19:55:03.395005  3493 caffe_interface.cpp:125] Batch 65, loss = 0.321325
I0109 19:55:03.395030  3493 caffe_interface.cpp:125] Batch 65, top-1 = 0.8
I0109 19:55:03.395035  3493 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 19:55:03.399456  3493 caffe_interface.cpp:125] Batch 66, loss = 0.808164
I0109 19:55:03.399479  3493 caffe_interface.cpp:125] Batch 66, top-1 = 0.78
I0109 19:55:03.399484  3493 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0109 19:55:03.403901  3493 caffe_interface.cpp:125] Batch 67, loss = 0.614936
I0109 19:55:03.403925  3493 caffe_interface.cpp:125] Batch 67, top-1 = 0.8
I0109 19:55:03.403931  3493 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 19:55:03.408344  3493 caffe_interface.cpp:125] Batch 68, loss = 0.995877
I0109 19:55:03.408368  3493 caffe_interface.cpp:125] Batch 68, top-1 = 0.76
I0109 19:55:03.408375  3493 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0109 19:55:03.412732  3493 caffe_interface.cpp:125] Batch 69, loss = 1.24606
I0109 19:55:03.412755  3493 caffe_interface.cpp:125] Batch 69, top-1 = 0.74
I0109 19:55:03.412761  3493 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 19:55:03.417187  3493 caffe_interface.cpp:125] Batch 70, loss = 0.726425
I0109 19:55:03.417210  3493 caffe_interface.cpp:125] Batch 70, top-1 = 0.8
I0109 19:55:03.417217  3493 caffe_interface.cpp:125] Batch 70, top-5 = 0.98
I0109 19:55:03.421697  3493 caffe_interface.cpp:125] Batch 71, loss = 0.593601
I0109 19:55:03.421721  3493 caffe_interface.cpp:125] Batch 71, top-1 = 0.8
I0109 19:55:03.421726  3493 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0109 19:55:03.426205  3493 caffe_interface.cpp:125] Batch 72, loss = 0.503926
I0109 19:55:03.426229  3493 caffe_interface.cpp:125] Batch 72, top-1 = 0.82
I0109 19:55:03.426234  3493 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 19:55:03.430701  3493 caffe_interface.cpp:125] Batch 73, loss = 1.02146
I0109 19:55:03.430722  3493 caffe_interface.cpp:125] Batch 73, top-1 = 0.82
I0109 19:55:03.430728  3493 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0109 19:55:03.435194  3493 caffe_interface.cpp:125] Batch 74, loss = 0.645414
I0109 19:55:03.435217  3493 caffe_interface.cpp:125] Batch 74, top-1 = 0.82
I0109 19:55:03.435223  3493 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 19:55:03.439707  3493 caffe_interface.cpp:125] Batch 75, loss = 0.838703
I0109 19:55:03.439730  3493 caffe_interface.cpp:125] Batch 75, top-1 = 0.82
I0109 19:55:03.439736  3493 caffe_interface.cpp:125] Batch 75, top-5 = 1
I0109 19:55:03.444223  3493 caffe_interface.cpp:125] Batch 76, loss = 0.736673
I0109 19:55:03.444247  3493 caffe_interface.cpp:125] Batch 76, top-1 = 0.74
I0109 19:55:03.444253  3493 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 19:55:03.448740  3493 caffe_interface.cpp:125] Batch 77, loss = 0.399169
I0109 19:55:03.448765  3493 caffe_interface.cpp:125] Batch 77, top-1 = 0.82
I0109 19:55:03.448770  3493 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0109 19:55:03.453243  3493 caffe_interface.cpp:125] Batch 78, loss = 0.942486
I0109 19:55:03.453267  3493 caffe_interface.cpp:125] Batch 78, top-1 = 0.7
I0109 19:55:03.453299  3493 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0109 19:55:03.457783  3493 caffe_interface.cpp:125] Batch 79, loss = 0.978987
I0109 19:55:03.457809  3493 caffe_interface.cpp:125] Batch 79, top-1 = 0.8
I0109 19:55:03.457814  3493 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0109 19:55:03.462271  3493 caffe_interface.cpp:125] Batch 80, loss = 0.679556
I0109 19:55:03.462294  3493 caffe_interface.cpp:125] Batch 80, top-1 = 0.82
I0109 19:55:03.462301  3493 caffe_interface.cpp:125] Batch 80, top-5 = 0.96
I0109 19:55:03.466763  3493 caffe_interface.cpp:125] Batch 81, loss = 0.876951
I0109 19:55:03.466791  3493 caffe_interface.cpp:125] Batch 81, top-1 = 0.76
I0109 19:55:03.466797  3493 caffe_interface.cpp:125] Batch 81, top-5 = 0.98
I0109 19:55:03.471262  3493 caffe_interface.cpp:125] Batch 82, loss = 0.4542
I0109 19:55:03.471285  3493 caffe_interface.cpp:125] Batch 82, top-1 = 0.84
I0109 19:55:03.471290  3493 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 19:55:03.475762  3493 caffe_interface.cpp:125] Batch 83, loss = 0.72375
I0109 19:55:03.475786  3493 caffe_interface.cpp:125] Batch 83, top-1 = 0.76
I0109 19:55:03.475792  3493 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0109 19:55:03.480301  3493 caffe_interface.cpp:125] Batch 84, loss = 0.989689
I0109 19:55:03.480324  3493 caffe_interface.cpp:125] Batch 84, top-1 = 0.7
I0109 19:55:03.480330  3493 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 19:55:03.484812  3493 caffe_interface.cpp:125] Batch 85, loss = 0.798592
I0109 19:55:03.484836  3493 caffe_interface.cpp:125] Batch 85, top-1 = 0.76
I0109 19:55:03.484843  3493 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 19:55:03.489295  3493 caffe_interface.cpp:125] Batch 86, loss = 0.991015
I0109 19:55:03.489318  3493 caffe_interface.cpp:125] Batch 86, top-1 = 0.72
I0109 19:55:03.489325  3493 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 19:55:03.493839  3493 caffe_interface.cpp:125] Batch 87, loss = 0.851625
I0109 19:55:03.493862  3493 caffe_interface.cpp:125] Batch 87, top-1 = 0.78
I0109 19:55:03.493868  3493 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0109 19:55:03.498330  3493 caffe_interface.cpp:125] Batch 88, loss = 0.682194
I0109 19:55:03.498353  3493 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0109 19:55:03.498359  3493 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 19:55:03.502822  3493 caffe_interface.cpp:125] Batch 89, loss = 0.663186
I0109 19:55:03.502846  3493 caffe_interface.cpp:125] Batch 89, top-1 = 0.78
I0109 19:55:03.502851  3493 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0109 19:55:03.507318  3493 caffe_interface.cpp:125] Batch 90, loss = 0.263077
I0109 19:55:03.507340  3493 caffe_interface.cpp:125] Batch 90, top-1 = 0.88
I0109 19:55:03.507346  3493 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 19:55:03.511762  3493 caffe_interface.cpp:125] Batch 91, loss = 0.407075
I0109 19:55:03.511787  3493 caffe_interface.cpp:125] Batch 91, top-1 = 0.82
I0109 19:55:03.511792  3493 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 19:55:03.516244  3493 caffe_interface.cpp:125] Batch 92, loss = 0.656424
I0109 19:55:03.516268  3493 caffe_interface.cpp:125] Batch 92, top-1 = 0.78
I0109 19:55:03.516274  3493 caffe_interface.cpp:125] Batch 92, top-5 = 1
I0109 19:55:03.520741  3493 caffe_interface.cpp:125] Batch 93, loss = 0.233302
I0109 19:55:03.520766  3493 caffe_interface.cpp:125] Batch 93, top-1 = 0.92
I0109 19:55:03.520771  3493 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 19:55:03.525259  3493 caffe_interface.cpp:125] Batch 94, loss = 0.821127
I0109 19:55:03.525281  3493 caffe_interface.cpp:125] Batch 94, top-1 = 0.74
I0109 19:55:03.525286  3493 caffe_interface.cpp:125] Batch 94, top-5 = 0.98
I0109 19:55:03.529783  3493 caffe_interface.cpp:125] Batch 95, loss = 0.948736
I0109 19:55:03.529805  3493 caffe_interface.cpp:125] Batch 95, top-1 = 0.74
I0109 19:55:03.529811  3493 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 19:55:03.534312  3493 caffe_interface.cpp:125] Batch 96, loss = 0.281201
I0109 19:55:03.534335  3493 caffe_interface.cpp:125] Batch 96, top-1 = 0.84
I0109 19:55:03.534364  3493 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 19:55:03.538842  3493 caffe_interface.cpp:125] Batch 97, loss = 0.807464
I0109 19:55:03.538866  3493 caffe_interface.cpp:125] Batch 97, top-1 = 0.72
I0109 19:55:03.538870  3493 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0109 19:55:03.543360  3493 caffe_interface.cpp:125] Batch 98, loss = 0.532376
I0109 19:55:03.543383  3493 caffe_interface.cpp:125] Batch 98, top-1 = 0.8
I0109 19:55:03.543388  3493 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0109 19:55:03.547879  3493 caffe_interface.cpp:125] Batch 99, loss = 0.800182
I0109 19:55:03.547902  3493 caffe_interface.cpp:125] Batch 99, top-1 = 0.8
I0109 19:55:03.547909  3493 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0109 19:55:03.552402  3493 caffe_interface.cpp:125] Batch 100, loss = 0.440615
I0109 19:55:03.552426  3493 caffe_interface.cpp:125] Batch 100, top-1 = 0.84
I0109 19:55:03.552433  3493 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 19:55:03.556936  3493 caffe_interface.cpp:125] Batch 101, loss = 0.969228
I0109 19:55:03.556959  3493 caffe_interface.cpp:125] Batch 101, top-1 = 0.78
I0109 19:55:03.556965  3493 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 19:55:03.561478  3493 caffe_interface.cpp:125] Batch 102, loss = 0.509302
I0109 19:55:03.561511  3493 caffe_interface.cpp:125] Batch 102, top-1 = 0.78
I0109 19:55:03.561517  3493 caffe_interface.cpp:125] Batch 102, top-5 = 1
I0109 19:55:03.566083  3493 caffe_interface.cpp:125] Batch 103, loss = 0.609564
I0109 19:55:03.566117  3493 caffe_interface.cpp:125] Batch 103, top-1 = 0.84
I0109 19:55:03.566123  3493 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 19:55:03.570619  3493 caffe_interface.cpp:125] Batch 104, loss = 0.480781
I0109 19:55:03.570644  3493 caffe_interface.cpp:125] Batch 104, top-1 = 0.82
I0109 19:55:03.570650  3493 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 19:55:03.575146  3493 caffe_interface.cpp:125] Batch 105, loss = 0.581199
I0109 19:55:03.575170  3493 caffe_interface.cpp:125] Batch 105, top-1 = 0.84
I0109 19:55:03.575176  3493 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 19:55:03.579681  3493 caffe_interface.cpp:125] Batch 106, loss = 0.835258
I0109 19:55:03.579704  3493 caffe_interface.cpp:125] Batch 106, top-1 = 0.8
I0109 19:55:03.579710  3493 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0109 19:55:03.584182  3493 caffe_interface.cpp:125] Batch 107, loss = 0.661931
I0109 19:55:03.584208  3493 caffe_interface.cpp:125] Batch 107, top-1 = 0.84
I0109 19:55:03.584213  3493 caffe_interface.cpp:125] Batch 107, top-5 = 0.98
I0109 19:55:03.588718  3493 caffe_interface.cpp:125] Batch 108, loss = 1.20035
I0109 19:55:03.588742  3493 caffe_interface.cpp:125] Batch 108, top-1 = 0.72
I0109 19:55:03.588748  3493 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0109 19:55:03.593215  3493 caffe_interface.cpp:125] Batch 109, loss = 0.398029
I0109 19:55:03.593240  3493 caffe_interface.cpp:125] Batch 109, top-1 = 0.86
I0109 19:55:03.593245  3493 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0109 19:55:03.597760  3493 caffe_interface.cpp:125] Batch 110, loss = 0.348778
I0109 19:55:03.597789  3493 caffe_interface.cpp:125] Batch 110, top-1 = 0.82
I0109 19:55:03.597795  3493 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 19:55:03.602316  3493 caffe_interface.cpp:125] Batch 111, loss = 0.371469
I0109 19:55:03.602355  3493 caffe_interface.cpp:125] Batch 111, top-1 = 0.84
I0109 19:55:03.602360  3493 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0109 19:55:03.606866  3493 caffe_interface.cpp:125] Batch 112, loss = 0.846739
I0109 19:55:03.606894  3493 caffe_interface.cpp:125] Batch 112, top-1 = 0.76
I0109 19:55:03.606899  3493 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0109 19:55:03.611357  3493 caffe_interface.cpp:125] Batch 113, loss = 0.635307
I0109 19:55:03.611382  3493 caffe_interface.cpp:125] Batch 113, top-1 = 0.82
I0109 19:55:03.611387  3493 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0109 19:55:03.615865  3493 caffe_interface.cpp:125] Batch 114, loss = 1.10798
I0109 19:55:03.615907  3493 caffe_interface.cpp:125] Batch 114, top-1 = 0.72
I0109 19:55:03.615914  3493 caffe_interface.cpp:125] Batch 114, top-5 = 0.94
I0109 19:55:03.620460  3493 caffe_interface.cpp:125] Batch 115, loss = 0.624413
I0109 19:55:03.620486  3493 caffe_interface.cpp:125] Batch 115, top-1 = 0.74
I0109 19:55:03.620491  3493 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 19:55:03.625026  3493 caffe_interface.cpp:125] Batch 116, loss = 0.840012
I0109 19:55:03.625051  3493 caffe_interface.cpp:125] Batch 116, top-1 = 0.8
I0109 19:55:03.625056  3493 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0109 19:55:03.629629  3493 caffe_interface.cpp:125] Batch 117, loss = 0.442062
I0109 19:55:03.629652  3493 caffe_interface.cpp:125] Batch 117, top-1 = 0.84
I0109 19:55:03.629657  3493 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0109 19:55:03.634208  3493 caffe_interface.cpp:125] Batch 118, loss = 0.831521
I0109 19:55:03.634232  3493 caffe_interface.cpp:125] Batch 118, top-1 = 0.68
I0109 19:55:03.634238  3493 caffe_interface.cpp:125] Batch 118, top-5 = 0.98
I0109 19:55:03.638779  3493 caffe_interface.cpp:125] Batch 119, loss = 0.393983
I0109 19:55:03.638803  3493 caffe_interface.cpp:125] Batch 119, top-1 = 0.84
I0109 19:55:03.638809  3493 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 19:55:03.643388  3493 caffe_interface.cpp:125] Batch 120, loss = 1.03144
I0109 19:55:03.643411  3493 caffe_interface.cpp:125] Batch 120, top-1 = 0.74
I0109 19:55:03.643419  3493 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0109 19:55:03.647959  3493 caffe_interface.cpp:125] Batch 121, loss = 0.712325
I0109 19:55:03.647984  3493 caffe_interface.cpp:125] Batch 121, top-1 = 0.8
I0109 19:55:03.647990  3493 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 19:55:03.652549  3493 caffe_interface.cpp:125] Batch 122, loss = 0.69753
I0109 19:55:03.652571  3493 caffe_interface.cpp:125] Batch 122, top-1 = 0.78
I0109 19:55:03.652577  3493 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 19:55:03.657120  3493 caffe_interface.cpp:125] Batch 123, loss = 0.748538
I0109 19:55:03.657147  3493 caffe_interface.cpp:125] Batch 123, top-1 = 0.76
I0109 19:55:03.657153  3493 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0109 19:55:03.661746  3493 caffe_interface.cpp:125] Batch 124, loss = 0.397475
I0109 19:55:03.661769  3493 caffe_interface.cpp:125] Batch 124, top-1 = 0.92
I0109 19:55:03.661777  3493 caffe_interface.cpp:125] Batch 124, top-5 = 0.98
I0109 19:55:03.666355  3493 caffe_interface.cpp:125] Batch 125, loss = 0.575476
I0109 19:55:03.666378  3493 caffe_interface.cpp:125] Batch 125, top-1 = 0.82
I0109 19:55:03.666383  3493 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0109 19:55:03.670948  3493 caffe_interface.cpp:125] Batch 126, loss = 0.461129
I0109 19:55:03.670971  3493 caffe_interface.cpp:125] Batch 126, top-1 = 0.88
I0109 19:55:03.670977  3493 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 19:55:03.675540  3493 caffe_interface.cpp:125] Batch 127, loss = 0.608104
I0109 19:55:03.675565  3493 caffe_interface.cpp:125] Batch 127, top-1 = 0.76
I0109 19:55:03.675571  3493 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 19:55:03.680124  3493 caffe_interface.cpp:125] Batch 128, loss = 1.18159
I0109 19:55:03.680146  3493 caffe_interface.cpp:125] Batch 128, top-1 = 0.68
I0109 19:55:03.680153  3493 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0109 19:55:03.684700  3493 caffe_interface.cpp:125] Batch 129, loss = 0.510071
I0109 19:55:03.684725  3493 caffe_interface.cpp:125] Batch 129, top-1 = 0.86
I0109 19:55:03.684731  3493 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 19:55:03.689282  3493 caffe_interface.cpp:125] Batch 130, loss = 0.556115
I0109 19:55:03.689306  3493 caffe_interface.cpp:125] Batch 130, top-1 = 0.76
I0109 19:55:03.689312  3493 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 19:55:03.693862  3493 caffe_interface.cpp:125] Batch 131, loss = 0.840036
I0109 19:55:03.693887  3493 caffe_interface.cpp:125] Batch 131, top-1 = 0.78
I0109 19:55:03.693892  3493 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 19:55:03.698511  3493 caffe_interface.cpp:125] Batch 132, loss = 0.373184
I0109 19:55:03.698534  3493 caffe_interface.cpp:125] Batch 132, top-1 = 0.86
I0109 19:55:03.698541  3493 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 19:55:03.703078  3493 caffe_interface.cpp:125] Batch 133, loss = 0.732686
I0109 19:55:03.703100  3493 caffe_interface.cpp:125] Batch 133, top-1 = 0.76
I0109 19:55:03.703106  3493 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0109 19:55:03.707653  3493 caffe_interface.cpp:125] Batch 134, loss = 0.922188
I0109 19:55:03.707677  3493 caffe_interface.cpp:125] Batch 134, top-1 = 0.76
I0109 19:55:03.707682  3493 caffe_interface.cpp:125] Batch 134, top-5 = 0.98
I0109 19:55:03.712227  3493 caffe_interface.cpp:125] Batch 135, loss = 0.499547
I0109 19:55:03.712251  3493 caffe_interface.cpp:125] Batch 135, top-1 = 0.8
I0109 19:55:03.712257  3493 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0109 19:55:03.716797  3493 caffe_interface.cpp:125] Batch 136, loss = 0.363947
I0109 19:55:03.716821  3493 caffe_interface.cpp:125] Batch 136, top-1 = 0.84
I0109 19:55:03.716827  3493 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 19:55:03.721380  3493 caffe_interface.cpp:125] Batch 137, loss = 0.586439
I0109 19:55:03.721403  3493 caffe_interface.cpp:125] Batch 137, top-1 = 0.82
I0109 19:55:03.721408  3493 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 19:55:03.725970  3493 caffe_interface.cpp:125] Batch 138, loss = 0.664447
I0109 19:55:03.725992  3493 caffe_interface.cpp:125] Batch 138, top-1 = 0.84
I0109 19:55:03.725998  3493 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0109 19:55:03.730562  3493 caffe_interface.cpp:125] Batch 139, loss = 0.633702
I0109 19:55:03.730587  3493 caffe_interface.cpp:125] Batch 139, top-1 = 0.82
I0109 19:55:03.730592  3493 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 19:55:03.735158  3493 caffe_interface.cpp:125] Batch 140, loss = 0.787108
I0109 19:55:03.735182  3493 caffe_interface.cpp:125] Batch 140, top-1 = 0.7
I0109 19:55:03.735188  3493 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0109 19:55:03.739744  3493 caffe_interface.cpp:125] Batch 141, loss = 0.512857
I0109 19:55:03.739768  3493 caffe_interface.cpp:125] Batch 141, top-1 = 0.84
I0109 19:55:03.739773  3493 caffe_interface.cpp:125] Batch 141, top-5 = 1
I0109 19:55:03.744338  3493 caffe_interface.cpp:125] Batch 142, loss = 0.45179
I0109 19:55:03.744361  3493 caffe_interface.cpp:125] Batch 142, top-1 = 0.88
I0109 19:55:03.744367  3493 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 19:55:03.748905  3493 caffe_interface.cpp:125] Batch 143, loss = 0.427818
I0109 19:55:03.748929  3493 caffe_interface.cpp:125] Batch 143, top-1 = 0.84
I0109 19:55:03.748934  3493 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 19:55:03.753477  3493 caffe_interface.cpp:125] Batch 144, loss = 0.818411
I0109 19:55:03.753501  3493 caffe_interface.cpp:125] Batch 144, top-1 = 0.78
I0109 19:55:03.753507  3493 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0109 19:55:03.758075  3493 caffe_interface.cpp:125] Batch 145, loss = 0.659697
I0109 19:55:03.758098  3493 caffe_interface.cpp:125] Batch 145, top-1 = 0.78
I0109 19:55:03.758105  3493 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0109 19:55:03.762657  3493 caffe_interface.cpp:125] Batch 146, loss = 0.572216
I0109 19:55:03.762681  3493 caffe_interface.cpp:125] Batch 146, top-1 = 0.84
I0109 19:55:03.762686  3493 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0109 19:55:03.767258  3493 caffe_interface.cpp:125] Batch 147, loss = 0.775085
I0109 19:55:03.767282  3493 caffe_interface.cpp:125] Batch 147, top-1 = 0.78
I0109 19:55:03.767288  3493 caffe_interface.cpp:125] Batch 147, top-5 = 0.98
I0109 19:55:03.771853  3493 caffe_interface.cpp:125] Batch 148, loss = 1.11648
I0109 19:55:03.771876  3493 caffe_interface.cpp:125] Batch 148, top-1 = 0.72
I0109 19:55:03.771883  3493 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0109 19:55:03.776441  3493 caffe_interface.cpp:125] Batch 149, loss = 0.885791
I0109 19:55:03.776464  3493 caffe_interface.cpp:125] Batch 149, top-1 = 0.74
I0109 19:55:03.776494  3493 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0109 19:55:03.781067  3493 caffe_interface.cpp:125] Batch 150, loss = 0.444262
I0109 19:55:03.781091  3493 caffe_interface.cpp:125] Batch 150, top-1 = 0.86
I0109 19:55:03.781096  3493 caffe_interface.cpp:125] Batch 150, top-5 = 0.98
I0109 19:55:03.785655  3493 caffe_interface.cpp:125] Batch 151, loss = 0.421911
I0109 19:55:03.785678  3493 caffe_interface.cpp:125] Batch 151, top-1 = 0.8
I0109 19:55:03.785684  3493 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 19:55:03.790218  3493 caffe_interface.cpp:125] Batch 152, loss = 0.712239
I0109 19:55:03.790241  3493 caffe_interface.cpp:125] Batch 152, top-1 = 0.8
I0109 19:55:03.790248  3493 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0109 19:55:03.794818  3493 caffe_interface.cpp:125] Batch 153, loss = 0.399531
I0109 19:55:03.794842  3493 caffe_interface.cpp:125] Batch 153, top-1 = 0.9
I0109 19:55:03.794847  3493 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 19:55:03.799407  3493 caffe_interface.cpp:125] Batch 154, loss = 0.251152
I0109 19:55:03.799429  3493 caffe_interface.cpp:125] Batch 154, top-1 = 0.9
I0109 19:55:03.799435  3493 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 19:55:03.803993  3493 caffe_interface.cpp:125] Batch 155, loss = 0.434517
I0109 19:55:03.804018  3493 caffe_interface.cpp:125] Batch 155, top-1 = 0.88
I0109 19:55:03.804023  3493 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 19:55:03.808579  3493 caffe_interface.cpp:125] Batch 156, loss = 0.622233
I0109 19:55:03.808604  3493 caffe_interface.cpp:125] Batch 156, top-1 = 0.84
I0109 19:55:03.808609  3493 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 19:55:03.813117  3493 caffe_interface.cpp:125] Batch 157, loss = 0.806794
I0109 19:55:03.813140  3493 caffe_interface.cpp:125] Batch 157, top-1 = 0.74
I0109 19:55:03.813145  3493 caffe_interface.cpp:125] Batch 157, top-5 = 0.98
I0109 19:55:03.817682  3493 caffe_interface.cpp:125] Batch 158, loss = 0.864456
I0109 19:55:03.817703  3493 caffe_interface.cpp:125] Batch 158, top-1 = 0.78
I0109 19:55:03.817709  3493 caffe_interface.cpp:125] Batch 158, top-5 = 0.92
I0109 19:55:03.822263  3493 caffe_interface.cpp:125] Batch 159, loss = 0.283344
I0109 19:55:03.822286  3493 caffe_interface.cpp:125] Batch 159, top-1 = 0.86
I0109 19:55:03.822293  3493 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0109 19:55:03.826879  3493 caffe_interface.cpp:125] Batch 160, loss = 1.00554
I0109 19:55:03.826920  3493 caffe_interface.cpp:125] Batch 160, top-1 = 0.74
I0109 19:55:03.826927  3493 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0109 19:55:03.831490  3493 caffe_interface.cpp:125] Batch 161, loss = 0.487209
I0109 19:55:03.831516  3493 caffe_interface.cpp:125] Batch 161, top-1 = 0.86
I0109 19:55:03.831521  3493 caffe_interface.cpp:125] Batch 161, top-5 = 0.98
I0109 19:55:03.836074  3493 caffe_interface.cpp:125] Batch 162, loss = 0.860463
I0109 19:55:03.836097  3493 caffe_interface.cpp:125] Batch 162, top-1 = 0.8
I0109 19:55:03.836102  3493 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0109 19:55:03.840647  3493 caffe_interface.cpp:125] Batch 163, loss = 0.626732
I0109 19:55:03.840672  3493 caffe_interface.cpp:125] Batch 163, top-1 = 0.8
I0109 19:55:03.840677  3493 caffe_interface.cpp:125] Batch 163, top-5 = 0.98
I0109 19:55:03.845237  3493 caffe_interface.cpp:125] Batch 164, loss = 0.679971
I0109 19:55:03.845261  3493 caffe_interface.cpp:125] Batch 164, top-1 = 0.8
I0109 19:55:03.845268  3493 caffe_interface.cpp:125] Batch 164, top-5 = 1
I0109 19:55:03.849864  3493 caffe_interface.cpp:125] Batch 165, loss = 0.424692
I0109 19:55:03.849886  3493 caffe_interface.cpp:125] Batch 165, top-1 = 0.86
I0109 19:55:03.849892  3493 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 19:55:03.854454  3493 caffe_interface.cpp:125] Batch 166, loss = 0.275942
I0109 19:55:03.854477  3493 caffe_interface.cpp:125] Batch 166, top-1 = 0.9
I0109 19:55:03.854483  3493 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0109 19:55:03.859014  3493 caffe_interface.cpp:125] Batch 167, loss = 0.524955
I0109 19:55:03.859056  3493 caffe_interface.cpp:125] Batch 167, top-1 = 0.84
I0109 19:55:03.859063  3493 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0109 19:55:03.863584  3493 caffe_interface.cpp:125] Batch 168, loss = 1.01292
I0109 19:55:03.863606  3493 caffe_interface.cpp:125] Batch 168, top-1 = 0.72
I0109 19:55:03.863612  3493 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0109 19:55:03.868175  3493 caffe_interface.cpp:125] Batch 169, loss = 0.720676
I0109 19:55:03.868199  3493 caffe_interface.cpp:125] Batch 169, top-1 = 0.82
I0109 19:55:03.868204  3493 caffe_interface.cpp:125] Batch 169, top-5 = 1
I0109 19:55:03.872766  3493 caffe_interface.cpp:125] Batch 170, loss = 0.626545
I0109 19:55:03.872788  3493 caffe_interface.cpp:125] Batch 170, top-1 = 0.82
I0109 19:55:03.872793  3493 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0109 19:55:03.877341  3493 caffe_interface.cpp:125] Batch 171, loss = 0.438545
I0109 19:55:03.877367  3493 caffe_interface.cpp:125] Batch 171, top-1 = 0.88
I0109 19:55:03.877372  3493 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 19:55:03.881925  3493 caffe_interface.cpp:125] Batch 172, loss = 0.297791
I0109 19:55:03.881948  3493 caffe_interface.cpp:125] Batch 172, top-1 = 0.88
I0109 19:55:03.881954  3493 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 19:55:03.886507  3493 caffe_interface.cpp:125] Batch 173, loss = 1.10195
I0109 19:55:03.886530  3493 caffe_interface.cpp:125] Batch 173, top-1 = 0.74
I0109 19:55:03.886535  3493 caffe_interface.cpp:125] Batch 173, top-5 = 0.98
I0109 19:55:03.891086  3493 caffe_interface.cpp:125] Batch 174, loss = 0.951519
I0109 19:55:03.891109  3493 caffe_interface.cpp:125] Batch 174, top-1 = 0.8
I0109 19:55:03.891115  3493 caffe_interface.cpp:125] Batch 174, top-5 = 0.98
I0109 19:55:03.895653  3493 caffe_interface.cpp:125] Batch 175, loss = 0.708862
I0109 19:55:03.895676  3493 caffe_interface.cpp:125] Batch 175, top-1 = 0.8
I0109 19:55:03.895682  3493 caffe_interface.cpp:125] Batch 175, top-5 = 0.98
I0109 19:55:03.900249  3493 caffe_interface.cpp:125] Batch 176, loss = 0.704438
I0109 19:55:03.900274  3493 caffe_interface.cpp:125] Batch 176, top-1 = 0.8
I0109 19:55:03.900280  3493 caffe_interface.cpp:125] Batch 176, top-5 = 1
I0109 19:55:03.904829  3493 caffe_interface.cpp:125] Batch 177, loss = 0.445508
I0109 19:55:03.904851  3493 caffe_interface.cpp:125] Batch 177, top-1 = 0.8
I0109 19:55:03.904857  3493 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 19:55:03.909406  3493 caffe_interface.cpp:125] Batch 178, loss = 0.367319
I0109 19:55:03.909430  3493 caffe_interface.cpp:125] Batch 178, top-1 = 0.92
I0109 19:55:03.909435  3493 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 19:55:03.913944  3493 caffe_interface.cpp:125] Batch 179, loss = 0.726833
I0109 19:55:03.913966  3493 caffe_interface.cpp:125] Batch 179, top-1 = 0.78
I0109 19:55:03.913972  3493 caffe_interface.cpp:125] Batch 179, top-5 = 0.98
I0109 19:55:03.913976  3493 caffe_interface.cpp:130] Loss: 0.672626
I0109 19:55:03.913985  3493 caffe_interface.cpp:142] loss = 0.672626 (* 1 = 0.672626 loss)
I0109 19:55:03.913992  3493 caffe_interface.cpp:142] top-1 = 0.805111
I0109 19:55:03.914006  3493 caffe_interface.cpp:142] top-5 = 0.991666
I0109 19:55:03.928396  3493 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/sparse.caffemodel
I0109 19:55:03.928432  3493 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.4:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.805110991    | -0.0598888397  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 44527          | -34.8915749%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 29454336       | -39.9549141%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config4.prototxt
I0109 19:55:04.045063  3582 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0109 19:55:04.157146  3582 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 19:55:04.157698  3582 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 19:55:04.157723  3582 caffe_interface.cpp:493] Using GPUs 0
I0109 19:55:04.157984  3582 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 19:55:04.821853  3582 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt"
type: "SGD"
I0109 19:55:04.822022  3582 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0109 19:55:04.822343  3582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 19:55:04.822371  3582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 19:55:04.822376  3582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 19:55:04.822562  3582 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 19:55:04.822654  3582 layer_factory.hpp:77] Creating layer data
I0109 19:55:04.822785  3582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:04.822974  3582 net.cpp:94] Creating Layer data
I0109 19:55:04.823010  3582 net.cpp:409] data -> data
I0109 19:55:04.823032  3582 net.cpp:409] data -> label
I0109 19:55:04.824643  3593 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 19:55:04.824694  3593 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 19:55:04.824882  3582 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 19:55:04.825124  3582 data_layer.cpp:83] output data size: 128,3,32,32
I0109 19:55:04.836872  3582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:04.836944  3582 net.cpp:144] Setting up data
I0109 19:55:04.836967  3582 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 19:55:04.836977  3582 net.cpp:151] Top shape: 128 (128)
I0109 19:55:04.837003  3582 net.cpp:159] Memory required for data: 1573376
I0109 19:55:04.837015  3582 layer_factory.hpp:77] Creating layer conv1
I0109 19:55:04.837036  3582 net.cpp:94] Creating Layer conv1
I0109 19:55:04.837059  3582 net.cpp:435] conv1 <- data
I0109 19:55:04.837106  3582 net.cpp:409] conv1 -> conv1
I0109 19:55:04.838506  3582 net.cpp:144] Setting up conv1
I0109 19:55:04.838531  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.838538  3582 net.cpp:159] Memory required for data: 18350592
I0109 19:55:04.838564  3582 layer_factory.hpp:77] Creating layer bn1
I0109 19:55:04.838603  3582 net.cpp:94] Creating Layer bn1
I0109 19:55:04.838625  3582 net.cpp:435] bn1 <- conv1
I0109 19:55:04.838657  3582 net.cpp:409] bn1 -> scale1
I0109 19:55:04.839720  3582 net.cpp:144] Setting up bn1
I0109 19:55:04.839742  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.839751  3582 net.cpp:159] Memory required for data: 35127808
I0109 19:55:04.839771  3582 layer_factory.hpp:77] Creating layer relu1
I0109 19:55:04.839812  3582 net.cpp:94] Creating Layer relu1
I0109 19:55:04.839835  3582 net.cpp:435] relu1 <- scale1
I0109 19:55:04.839861  3582 net.cpp:409] relu1 -> relu1
I0109 19:55:04.839926  3582 net.cpp:144] Setting up relu1
I0109 19:55:04.839943  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.839951  3582 net.cpp:159] Memory required for data: 51905024
I0109 19:55:04.839958  3582 layer_factory.hpp:77] Creating layer conv2
I0109 19:55:04.839979  3582 net.cpp:94] Creating Layer conv2
I0109 19:55:04.840004  3582 net.cpp:435] conv2 <- relu1
I0109 19:55:04.840050  3582 net.cpp:409] conv2 -> conv2
I0109 19:55:04.841369  3582 net.cpp:144] Setting up conv2
I0109 19:55:04.841428  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.841475  3582 net.cpp:159] Memory required for data: 68682240
I0109 19:55:04.841527  3582 layer_factory.hpp:77] Creating layer bn2
I0109 19:55:04.841610  3582 net.cpp:94] Creating Layer bn2
I0109 19:55:04.841667  3582 net.cpp:435] bn2 <- conv2
I0109 19:55:04.841719  3582 net.cpp:409] bn2 -> scale2
I0109 19:55:04.842789  3582 net.cpp:144] Setting up bn2
I0109 19:55:04.842811  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.842818  3582 net.cpp:159] Memory required for data: 85459456
I0109 19:55:04.842867  3582 layer_factory.hpp:77] Creating layer relu2
I0109 19:55:04.842893  3582 net.cpp:94] Creating Layer relu2
I0109 19:55:04.842926  3582 net.cpp:435] relu2 <- scale2
I0109 19:55:04.842947  3582 net.cpp:409] relu2 -> relu2
I0109 19:55:04.843189  3582 net.cpp:144] Setting up relu2
I0109 19:55:04.843242  3582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 19:55:04.843287  3582 net.cpp:159] Memory required for data: 102236672
I0109 19:55:04.843333  3582 layer_factory.hpp:77] Creating layer pool1
I0109 19:55:04.843379  3582 net.cpp:94] Creating Layer pool1
I0109 19:55:04.843441  3582 net.cpp:435] pool1 <- relu2
I0109 19:55:04.843488  3582 net.cpp:409] pool1 -> pool1
I0109 19:55:04.843592  3582 net.cpp:144] Setting up pool1
I0109 19:55:04.843848  3582 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:55:04.843891  3582 net.cpp:159] Memory required for data: 106430976
I0109 19:55:04.843941  3582 layer_factory.hpp:77] Creating layer drop1
I0109 19:55:04.843992  3582 net.cpp:94] Creating Layer drop1
I0109 19:55:04.844051  3582 net.cpp:435] drop1 <- pool1
I0109 19:55:04.844096  3582 net.cpp:409] drop1 -> drop1
I0109 19:55:04.844197  3582 net.cpp:144] Setting up drop1
I0109 19:55:04.844244  3582 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 19:55:04.844285  3582 net.cpp:159] Memory required for data: 110625280
I0109 19:55:04.844326  3582 layer_factory.hpp:77] Creating layer conv3
I0109 19:55:04.844379  3582 net.cpp:94] Creating Layer conv3
I0109 19:55:04.844421  3582 net.cpp:435] conv3 <- drop1
I0109 19:55:04.844480  3582 net.cpp:409] conv3 -> conv3
I0109 19:55:04.845866  3582 net.cpp:144] Setting up conv3
I0109 19:55:04.845932  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.845978  3582 net.cpp:159] Memory required for data: 119013888
I0109 19:55:04.846024  3582 layer_factory.hpp:77] Creating layer bn3
I0109 19:55:04.846074  3582 net.cpp:94] Creating Layer bn3
I0109 19:55:04.846117  3582 net.cpp:435] bn3 <- conv3
I0109 19:55:04.846161  3582 net.cpp:409] bn3 -> scale3
I0109 19:55:04.847288  3582 net.cpp:144] Setting up bn3
I0109 19:55:04.847316  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.847323  3582 net.cpp:159] Memory required for data: 127402496
I0109 19:55:04.847429  3582 layer_factory.hpp:77] Creating layer relu3
I0109 19:55:04.847453  3582 net.cpp:94] Creating Layer relu3
I0109 19:55:04.847498  3582 net.cpp:435] relu3 <- scale3
I0109 19:55:04.847576  3582 net.cpp:409] relu3 -> relu3
I0109 19:55:04.847651  3582 net.cpp:144] Setting up relu3
I0109 19:55:04.847699  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.847738  3582 net.cpp:159] Memory required for data: 135791104
I0109 19:55:04.847777  3582 layer_factory.hpp:77] Creating layer conv4
I0109 19:55:04.847832  3582 net.cpp:94] Creating Layer conv4
I0109 19:55:04.847874  3582 net.cpp:435] conv4 <- relu3
I0109 19:55:04.847923  3582 net.cpp:409] conv4 -> conv4
I0109 19:55:04.848747  3582 net.cpp:144] Setting up conv4
I0109 19:55:04.848791  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.848836  3582 net.cpp:159] Memory required for data: 144179712
I0109 19:55:04.848878  3582 layer_factory.hpp:77] Creating layer bn4
I0109 19:55:04.848927  3582 net.cpp:94] Creating Layer bn4
I0109 19:55:04.848961  3582 net.cpp:435] bn4 <- conv4
I0109 19:55:04.849014  3582 net.cpp:409] bn4 -> scale4
I0109 19:55:04.850098  3582 net.cpp:144] Setting up bn4
I0109 19:55:04.850158  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.850199  3582 net.cpp:159] Memory required for data: 152568320
I0109 19:55:04.850224  3582 layer_factory.hpp:77] Creating layer relu4
I0109 19:55:04.850296  3582 net.cpp:94] Creating Layer relu4
I0109 19:55:04.850339  3582 net.cpp:435] relu4 <- scale4
I0109 19:55:04.850384  3582 net.cpp:409] relu4 -> relu4
I0109 19:55:04.850488  3582 net.cpp:144] Setting up relu4
I0109 19:55:04.850510  3582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 19:55:04.850549  3582 net.cpp:159] Memory required for data: 160956928
I0109 19:55:04.850589  3582 layer_factory.hpp:77] Creating layer pool2
I0109 19:55:04.850636  3582 net.cpp:94] Creating Layer pool2
I0109 19:55:04.850677  3582 net.cpp:435] pool2 <- relu4
I0109 19:55:04.850724  3582 net.cpp:409] pool2 -> pool2
I0109 19:55:04.850824  3582 net.cpp:144] Setting up pool2
I0109 19:55:04.850875  3582 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:55:04.850914  3582 net.cpp:159] Memory required for data: 163054080
I0109 19:55:04.850951  3582 layer_factory.hpp:77] Creating layer drop2
I0109 19:55:04.850999  3582 net.cpp:94] Creating Layer drop2
I0109 19:55:04.851039  3582 net.cpp:435] drop2 <- pool2
I0109 19:55:04.851079  3582 net.cpp:409] drop2 -> drop2
I0109 19:55:04.851171  3582 net.cpp:144] Setting up drop2
I0109 19:55:04.851218  3582 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 19:55:04.851258  3582 net.cpp:159] Memory required for data: 165151232
I0109 19:55:04.851294  3582 layer_factory.hpp:77] Creating layer fc1
I0109 19:55:04.851352  3582 net.cpp:94] Creating Layer fc1
I0109 19:55:04.851393  3582 net.cpp:435] fc1 <- drop2
I0109 19:55:04.851450  3582 net.cpp:409] fc1 -> fc1
I0109 19:55:04.874086  3582 net.cpp:144] Setting up fc1
I0109 19:55:04.874110  3582 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:55:04.874115  3582 net.cpp:159] Memory required for data: 165413376
I0109 19:55:04.874126  3582 layer_factory.hpp:77] Creating layer bn5
I0109 19:55:04.874145  3582 net.cpp:94] Creating Layer bn5
I0109 19:55:04.874162  3582 net.cpp:435] bn5 <- fc1
I0109 19:55:04.874176  3582 net.cpp:409] bn5 -> scale5
I0109 19:55:04.874796  3582 net.cpp:144] Setting up bn5
I0109 19:55:04.874817  3582 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:55:04.874822  3582 net.cpp:159] Memory required for data: 165675520
I0109 19:55:04.874846  3582 layer_factory.hpp:77] Creating layer relu5
I0109 19:55:04.874883  3582 net.cpp:94] Creating Layer relu5
I0109 19:55:04.874900  3582 net.cpp:435] relu5 <- scale5
I0109 19:55:04.874912  3582 net.cpp:409] relu5 -> relu5
I0109 19:55:04.874950  3582 net.cpp:144] Setting up relu5
I0109 19:55:04.874967  3582 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:55:04.874974  3582 net.cpp:159] Memory required for data: 165937664
I0109 19:55:04.874980  3582 layer_factory.hpp:77] Creating layer drop3
I0109 19:55:04.874994  3582 net.cpp:94] Creating Layer drop3
I0109 19:55:04.875017  3582 net.cpp:435] drop3 <- relu5
I0109 19:55:04.875046  3582 net.cpp:409] drop3 -> drop3
I0109 19:55:04.875109  3582 net.cpp:144] Setting up drop3
I0109 19:55:04.875126  3582 net.cpp:151] Top shape: 128 512 (65536)
I0109 19:55:04.875134  3582 net.cpp:159] Memory required for data: 166199808
I0109 19:55:04.875140  3582 layer_factory.hpp:77] Creating layer fc2
I0109 19:55:04.875155  3582 net.cpp:94] Creating Layer fc2
I0109 19:55:04.875169  3582 net.cpp:435] fc2 <- drop3
I0109 19:55:04.875185  3582 net.cpp:409] fc2 -> fc2
I0109 19:55:04.875365  3582 net.cpp:144] Setting up fc2
I0109 19:55:04.875382  3582 net.cpp:151] Top shape: 128 10 (1280)
I0109 19:55:04.875388  3582 net.cpp:159] Memory required for data: 166204928
I0109 19:55:04.875401  3582 layer_factory.hpp:77] Creating layer loss
I0109 19:55:04.875412  3582 net.cpp:94] Creating Layer loss
I0109 19:55:04.875423  3582 net.cpp:435] loss <- fc2
I0109 19:55:04.875432  3582 net.cpp:435] loss <- label
I0109 19:55:04.875447  3582 net.cpp:409] loss -> loss
I0109 19:55:04.875463  3582 layer_factory.hpp:77] Creating layer loss
I0109 19:55:04.876274  3582 net.cpp:144] Setting up loss
I0109 19:55:04.876297  3582 net.cpp:151] Top shape: (1)
I0109 19:55:04.876307  3582 net.cpp:154]     with loss weight 1
I0109 19:55:04.876334  3582 net.cpp:159] Memory required for data: 166204932
I0109 19:55:04.876343  3582 net.cpp:220] loss needs backward computation.
I0109 19:55:04.876363  3582 net.cpp:220] fc2 needs backward computation.
I0109 19:55:04.876370  3582 net.cpp:220] drop3 needs backward computation.
I0109 19:55:04.876376  3582 net.cpp:220] relu5 needs backward computation.
I0109 19:55:04.876384  3582 net.cpp:220] bn5 needs backward computation.
I0109 19:55:04.876391  3582 net.cpp:220] fc1 needs backward computation.
I0109 19:55:04.876399  3582 net.cpp:220] drop2 needs backward computation.
I0109 19:55:04.876405  3582 net.cpp:220] pool2 needs backward computation.
I0109 19:55:04.876410  3582 net.cpp:220] relu4 needs backward computation.
I0109 19:55:04.876416  3582 net.cpp:220] bn4 needs backward computation.
I0109 19:55:04.876425  3582 net.cpp:220] conv4 needs backward computation.
I0109 19:55:04.876431  3582 net.cpp:220] relu3 needs backward computation.
I0109 19:55:04.876437  3582 net.cpp:220] bn3 needs backward computation.
I0109 19:55:04.876443  3582 net.cpp:220] conv3 needs backward computation.
I0109 19:55:04.876451  3582 net.cpp:220] drop1 needs backward computation.
I0109 19:55:04.876457  3582 net.cpp:220] pool1 needs backward computation.
I0109 19:55:04.876464  3582 net.cpp:220] relu2 needs backward computation.
I0109 19:55:04.876471  3582 net.cpp:220] bn2 needs backward computation.
I0109 19:55:04.876477  3582 net.cpp:220] conv2 needs backward computation.
I0109 19:55:04.876483  3582 net.cpp:220] relu1 needs backward computation.
I0109 19:55:04.876503  3582 net.cpp:220] bn1 needs backward computation.
I0109 19:55:04.876510  3582 net.cpp:220] conv1 needs backward computation.
I0109 19:55:04.876518  3582 net.cpp:222] data does not need backward computation.
I0109 19:55:04.876524  3582 net.cpp:264] This network produces output loss
I0109 19:55:04.876560  3582 net.cpp:284] Network initialization done.
I0109 19:55:04.876912  3582 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0109 19:55:04.876977  3582 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 19:55:04.877220  3582 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 19:55:04.877367  3582 layer_factory.hpp:77] Creating layer data
I0109 19:55:04.877441  3582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:04.877694  3582 net.cpp:94] Creating Layer data
I0109 19:55:04.877725  3582 net.cpp:409] data -> data
I0109 19:55:04.877751  3582 net.cpp:409] data -> label
I0109 19:55:04.878763  3599 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 19:55:04.878806  3599 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 19:55:04.878891  3582 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 19:55:04.879007  3582 data_layer.cpp:83] output data size: 50,3,32,32
I0109 19:55:04.887248  3582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 19:55:04.887318  3582 net.cpp:144] Setting up data
I0109 19:55:04.887338  3582 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 19:55:04.887347  3582 net.cpp:151] Top shape: 50 (50)
I0109 19:55:04.887352  3582 net.cpp:159] Memory required for data: 614600
I0109 19:55:04.887360  3582 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 19:55:04.887385  3582 net.cpp:94] Creating Layer label_data_1_split
I0109 19:55:04.887394  3582 net.cpp:435] label_data_1_split <- label
I0109 19:55:04.887403  3582 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 19:55:04.887429  3582 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 19:55:04.887442  3582 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 19:55:04.887681  3582 net.cpp:144] Setting up label_data_1_split
I0109 19:55:04.887698  3582 net.cpp:151] Top shape: 50 (50)
I0109 19:55:04.887707  3582 net.cpp:151] Top shape: 50 (50)
I0109 19:55:04.887714  3582 net.cpp:151] Top shape: 50 (50)
I0109 19:55:04.887719  3582 net.cpp:159] Memory required for data: 615200
I0109 19:55:04.887733  3582 layer_factory.hpp:77] Creating layer conv1
I0109 19:55:04.887755  3582 net.cpp:94] Creating Layer conv1
I0109 19:55:04.887769  3582 net.cpp:435] conv1 <- data
I0109 19:55:04.887786  3582 net.cpp:409] conv1 -> conv1
I0109 19:55:04.888108  3582 net.cpp:144] Setting up conv1
I0109 19:55:04.888130  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.888137  3582 net.cpp:159] Memory required for data: 7168800
I0109 19:55:04.888152  3582 layer_factory.hpp:77] Creating layer bn1
I0109 19:55:04.888177  3582 net.cpp:94] Creating Layer bn1
I0109 19:55:04.888187  3582 net.cpp:435] bn1 <- conv1
I0109 19:55:04.888206  3582 net.cpp:409] bn1 -> scale1
I0109 19:55:04.889209  3582 net.cpp:144] Setting up bn1
I0109 19:55:04.889226  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.889235  3582 net.cpp:159] Memory required for data: 13722400
I0109 19:55:04.889259  3582 layer_factory.hpp:77] Creating layer relu1
I0109 19:55:04.889281  3582 net.cpp:94] Creating Layer relu1
I0109 19:55:04.889288  3582 net.cpp:435] relu1 <- scale1
I0109 19:55:04.889305  3582 net.cpp:409] relu1 -> relu1
I0109 19:55:04.889508  3582 net.cpp:144] Setting up relu1
I0109 19:55:04.889523  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.889530  3582 net.cpp:159] Memory required for data: 20276000
I0109 19:55:04.889537  3582 layer_factory.hpp:77] Creating layer conv2
I0109 19:55:04.889554  3582 net.cpp:94] Creating Layer conv2
I0109 19:55:04.889567  3582 net.cpp:435] conv2 <- relu1
I0109 19:55:04.889583  3582 net.cpp:409] conv2 -> conv2
I0109 19:55:04.890142  3582 net.cpp:144] Setting up conv2
I0109 19:55:04.890166  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.890173  3582 net.cpp:159] Memory required for data: 26829600
I0109 19:55:04.890192  3582 layer_factory.hpp:77] Creating layer bn2
I0109 19:55:04.890209  3582 net.cpp:94] Creating Layer bn2
I0109 19:55:04.890235  3582 net.cpp:435] bn2 <- conv2
I0109 19:55:04.890257  3582 net.cpp:409] bn2 -> scale2
I0109 19:55:04.891379  3582 net.cpp:144] Setting up bn2
I0109 19:55:04.891400  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.891407  3582 net.cpp:159] Memory required for data: 33383200
I0109 19:55:04.891423  3582 layer_factory.hpp:77] Creating layer relu2
I0109 19:55:04.891449  3582 net.cpp:94] Creating Layer relu2
I0109 19:55:04.891475  3582 net.cpp:435] relu2 <- scale2
I0109 19:55:04.891500  3582 net.cpp:409] relu2 -> relu2
I0109 19:55:04.891621  3582 net.cpp:144] Setting up relu2
I0109 19:55:04.891674  3582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 19:55:04.891722  3582 net.cpp:159] Memory required for data: 39936800
I0109 19:55:04.891855  3582 layer_factory.hpp:77] Creating layer pool1
I0109 19:55:04.891916  3582 net.cpp:94] Creating Layer pool1
I0109 19:55:04.891958  3582 net.cpp:435] pool1 <- relu2
I0109 19:55:04.892004  3582 net.cpp:409] pool1 -> pool1
I0109 19:55:04.892352  3582 net.cpp:144] Setting up pool1
I0109 19:55:04.892370  3582 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:55:04.892375  3582 net.cpp:159] Memory required for data: 41575200
I0109 19:55:04.892380  3582 layer_factory.hpp:77] Creating layer drop1
I0109 19:55:04.892390  3582 net.cpp:94] Creating Layer drop1
I0109 19:55:04.892405  3582 net.cpp:435] drop1 <- pool1
I0109 19:55:04.892418  3582 net.cpp:409] drop1 -> drop1
I0109 19:55:04.892493  3582 net.cpp:144] Setting up drop1
I0109 19:55:04.892511  3582 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 19:55:04.892518  3582 net.cpp:159] Memory required for data: 43213600
I0109 19:55:04.892524  3582 layer_factory.hpp:77] Creating layer conv3
I0109 19:55:04.892544  3582 net.cpp:94] Creating Layer conv3
I0109 19:55:04.892556  3582 net.cpp:435] conv3 <- drop1
I0109 19:55:04.892573  3582 net.cpp:409] conv3 -> conv3
I0109 19:55:04.893097  3582 net.cpp:144] Setting up conv3
I0109 19:55:04.893117  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.893121  3582 net.cpp:159] Memory required for data: 46490400
I0109 19:55:04.893128  3582 layer_factory.hpp:77] Creating layer bn3
I0109 19:55:04.893144  3582 net.cpp:94] Creating Layer bn3
I0109 19:55:04.893162  3582 net.cpp:435] bn3 <- conv3
I0109 19:55:04.893175  3582 net.cpp:409] bn3 -> scale3
I0109 19:55:04.894031  3582 net.cpp:144] Setting up bn3
I0109 19:55:04.894052  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.894057  3582 net.cpp:159] Memory required for data: 49767200
I0109 19:55:04.894078  3582 layer_factory.hpp:77] Creating layer relu3
I0109 19:55:04.894098  3582 net.cpp:94] Creating Layer relu3
I0109 19:55:04.894114  3582 net.cpp:435] relu3 <- scale3
I0109 19:55:04.894129  3582 net.cpp:409] relu3 -> relu3
I0109 19:55:04.894183  3582 net.cpp:144] Setting up relu3
I0109 19:55:04.894199  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.894204  3582 net.cpp:159] Memory required for data: 53044000
I0109 19:55:04.894209  3582 layer_factory.hpp:77] Creating layer conv4
I0109 19:55:04.894225  3582 net.cpp:94] Creating Layer conv4
I0109 19:55:04.894242  3582 net.cpp:435] conv4 <- relu3
I0109 19:55:04.894255  3582 net.cpp:409] conv4 -> conv4
I0109 19:55:04.894819  3582 net.cpp:144] Setting up conv4
I0109 19:55:04.894840  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.894845  3582 net.cpp:159] Memory required for data: 56320800
I0109 19:55:04.894855  3582 layer_factory.hpp:77] Creating layer bn4
I0109 19:55:04.894881  3582 net.cpp:94] Creating Layer bn4
I0109 19:55:04.894896  3582 net.cpp:435] bn4 <- conv4
I0109 19:55:04.894907  3582 net.cpp:409] bn4 -> scale4
I0109 19:55:04.895642  3582 net.cpp:144] Setting up bn4
I0109 19:55:04.895663  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.895666  3582 net.cpp:159] Memory required for data: 59597600
I0109 19:55:04.895681  3582 layer_factory.hpp:77] Creating layer relu4
I0109 19:55:04.895704  3582 net.cpp:94] Creating Layer relu4
I0109 19:55:04.895720  3582 net.cpp:435] relu4 <- scale4
I0109 19:55:04.895730  3582 net.cpp:409] relu4 -> relu4
I0109 19:55:04.895779  3582 net.cpp:144] Setting up relu4
I0109 19:55:04.895795  3582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 19:55:04.895802  3582 net.cpp:159] Memory required for data: 62874400
I0109 19:55:04.895805  3582 layer_factory.hpp:77] Creating layer pool2
I0109 19:55:04.895823  3582 net.cpp:94] Creating Layer pool2
I0109 19:55:04.895967  3582 net.cpp:435] pool2 <- relu4
I0109 19:55:04.896040  3582 net.cpp:409] pool2 -> pool2
I0109 19:55:04.896138  3582 net.cpp:144] Setting up pool2
I0109 19:55:04.896567  3582 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:55:04.896612  3582 net.cpp:159] Memory required for data: 63693600
I0109 19:55:04.896653  3582 layer_factory.hpp:77] Creating layer drop2
I0109 19:55:04.896695  3582 net.cpp:94] Creating Layer drop2
I0109 19:55:04.896746  3582 net.cpp:435] drop2 <- pool2
I0109 19:55:04.896795  3582 net.cpp:409] drop2 -> drop2
I0109 19:55:04.896895  3582 net.cpp:144] Setting up drop2
I0109 19:55:04.896927  3582 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 19:55:04.896934  3582 net.cpp:159] Memory required for data: 64512800
I0109 19:55:04.897286  3582 layer_factory.hpp:77] Creating layer fc1
I0109 19:55:04.897343  3582 net.cpp:94] Creating Layer fc1
I0109 19:55:04.897373  3582 net.cpp:435] fc1 <- drop2
I0109 19:55:04.897393  3582 net.cpp:409] fc1 -> fc1
I0109 19:55:04.919427  3582 net.cpp:144] Setting up fc1
I0109 19:55:04.919453  3582 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:04.919458  3582 net.cpp:159] Memory required for data: 64615200
I0109 19:55:04.919467  3582 layer_factory.hpp:77] Creating layer bn5
I0109 19:55:04.919486  3582 net.cpp:94] Creating Layer bn5
I0109 19:55:04.919503  3582 net.cpp:435] bn5 <- fc1
I0109 19:55:04.919517  3582 net.cpp:409] bn5 -> scale5
I0109 19:55:04.920205  3582 net.cpp:144] Setting up bn5
I0109 19:55:04.920225  3582 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:04.920230  3582 net.cpp:159] Memory required for data: 64717600
I0109 19:55:04.920255  3582 layer_factory.hpp:77] Creating layer relu5
I0109 19:55:04.920287  3582 net.cpp:94] Creating Layer relu5
I0109 19:55:04.920305  3582 net.cpp:435] relu5 <- scale5
I0109 19:55:04.920322  3582 net.cpp:409] relu5 -> relu5
I0109 19:55:04.920363  3582 net.cpp:144] Setting up relu5
I0109 19:55:04.920380  3582 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:04.920387  3582 net.cpp:159] Memory required for data: 64820000
I0109 19:55:04.920393  3582 layer_factory.hpp:77] Creating layer drop3
I0109 19:55:04.920408  3582 net.cpp:94] Creating Layer drop3
I0109 19:55:04.920415  3582 net.cpp:435] drop3 <- relu5
I0109 19:55:04.920424  3582 net.cpp:409] drop3 -> drop3
I0109 19:55:04.920480  3582 net.cpp:144] Setting up drop3
I0109 19:55:04.920495  3582 net.cpp:151] Top shape: 50 512 (25600)
I0109 19:55:04.920501  3582 net.cpp:159] Memory required for data: 64922400
I0109 19:55:04.920507  3582 layer_factory.hpp:77] Creating layer fc2
I0109 19:55:04.920523  3582 net.cpp:94] Creating Layer fc2
I0109 19:55:04.920531  3582 net.cpp:435] fc2 <- drop3
I0109 19:55:04.920541  3582 net.cpp:409] fc2 -> fc2
I0109 19:55:04.920725  3582 net.cpp:144] Setting up fc2
I0109 19:55:04.920742  3582 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:04.920749  3582 net.cpp:159] Memory required for data: 64924400
I0109 19:55:04.920760  3582 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 19:55:04.920770  3582 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 19:55:04.920785  3582 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 19:55:04.920799  3582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 19:55:04.920812  3582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 19:55:04.920840  3582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 19:55:04.920922  3582 net.cpp:144] Setting up fc2_fc2_0_split
I0109 19:55:04.920939  3582 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:04.920948  3582 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:04.920953  3582 net.cpp:151] Top shape: 50 10 (500)
I0109 19:55:04.920959  3582 net.cpp:159] Memory required for data: 64930400
I0109 19:55:04.920965  3582 layer_factory.hpp:77] Creating layer loss
I0109 19:55:04.920976  3582 net.cpp:94] Creating Layer loss
I0109 19:55:04.920984  3582 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 19:55:04.920994  3582 net.cpp:435] loss <- label_data_1_split_0
I0109 19:55:04.921008  3582 net.cpp:409] loss -> loss
I0109 19:55:04.921037  3582 layer_factory.hpp:77] Creating layer loss
I0109 19:55:04.921166  3582 net.cpp:144] Setting up loss
I0109 19:55:04.921185  3582 net.cpp:151] Top shape: (1)
I0109 19:55:04.921191  3582 net.cpp:154]     with loss weight 1
I0109 19:55:04.921205  3582 net.cpp:159] Memory required for data: 64930404
I0109 19:55:04.921211  3582 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 19:55:04.921224  3582 net.cpp:94] Creating Layer accuracy-top1
I0109 19:55:04.921231  3582 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 19:55:04.921241  3582 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 19:55:04.921284  3582 net.cpp:409] accuracy-top1 -> top-1
I0109 19:55:04.921311  3582 net.cpp:144] Setting up accuracy-top1
I0109 19:55:04.921320  3582 net.cpp:151] Top shape: (1)
I0109 19:55:04.921325  3582 net.cpp:159] Memory required for data: 64930408
I0109 19:55:04.921331  3582 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 19:55:04.921347  3582 net.cpp:94] Creating Layer accuracy-top5
I0109 19:55:04.921355  3582 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 19:55:04.921362  3582 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 19:55:04.921371  3582 net.cpp:409] accuracy-top5 -> top-5
I0109 19:55:04.921392  3582 net.cpp:144] Setting up accuracy-top5
I0109 19:55:04.921401  3582 net.cpp:151] Top shape: (1)
I0109 19:55:04.921406  3582 net.cpp:159] Memory required for data: 64930412
I0109 19:55:04.921412  3582 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 19:55:04.921439  3582 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 19:55:04.921458  3582 net.cpp:220] loss needs backward computation.
I0109 19:55:04.921466  3582 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 19:55:04.921473  3582 net.cpp:220] fc2 needs backward computation.
I0109 19:55:04.921479  3582 net.cpp:220] drop3 needs backward computation.
I0109 19:55:04.921494  3582 net.cpp:220] relu5 needs backward computation.
I0109 19:55:04.921501  3582 net.cpp:220] bn5 needs backward computation.
I0109 19:55:04.921509  3582 net.cpp:220] fc1 needs backward computation.
I0109 19:55:04.921535  3582 net.cpp:220] drop2 needs backward computation.
I0109 19:55:04.921558  3582 net.cpp:220] pool2 needs backward computation.
I0109 19:55:04.921568  3582 net.cpp:220] relu4 needs backward computation.
I0109 19:55:04.921576  3582 net.cpp:220] bn4 needs backward computation.
I0109 19:55:04.921582  3582 net.cpp:220] conv4 needs backward computation.
I0109 19:55:04.921612  3582 net.cpp:220] relu3 needs backward computation.
I0109 19:55:04.921620  3582 net.cpp:220] bn3 needs backward computation.
I0109 19:55:04.921625  3582 net.cpp:220] conv3 needs backward computation.
I0109 19:55:04.921646  3582 net.cpp:220] drop1 needs backward computation.
I0109 19:55:04.921656  3582 net.cpp:220] pool1 needs backward computation.
I0109 19:55:04.921663  3582 net.cpp:220] relu2 needs backward computation.
I0109 19:55:04.921669  3582 net.cpp:220] bn2 needs backward computation.
I0109 19:55:04.921675  3582 net.cpp:220] conv2 needs backward computation.
I0109 19:55:04.921689  3582 net.cpp:220] relu1 needs backward computation.
I0109 19:55:04.921695  3582 net.cpp:220] bn1 needs backward computation.
I0109 19:55:04.921701  3582 net.cpp:220] conv1 needs backward computation.
I0109 19:55:04.921710  3582 net.cpp:222] label_data_1_split does not need backward computation.
I0109 19:55:04.921717  3582 net.cpp:222] data does not need backward computation.
I0109 19:55:04.921741  3582 net.cpp:264] This network produces output loss
I0109 19:55:04.921757  3582 net.cpp:264] This network produces output top-1
I0109 19:55:04.921767  3582 net.cpp:264] This network produces output top-5
I0109 19:55:04.921804  3582 net.cpp:284] Network initialization done.
I0109 19:55:04.921910  3582 solver.cpp:63] Solver scaffolding done.
I0109 19:55:04.923238  3582 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/sparse.caffemodel
I0109 19:55:04.988425  3582 caffe_interface.cpp:527] Starting Optimization
I0109 19:55:04.988477  3582 solver.cpp:335] Solving 
I0109 19:55:04.988485  3582 solver.cpp:336] Learning Rate Policy: poly
I0109 19:55:04.989827  3582 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 19:55:05.813009  3582 solver.cpp:517]     Test net output #0: loss = 0.672626 (* 1 = 0.672626 loss)
I0109 19:55:05.813066  3582 solver.cpp:517]     Test net output #1: top-1 = 0.805111
I0109 19:55:05.813077  3582 solver.cpp:517]     Test net output #2: top-5 = 0.991666
I0109 19:55:05.860543  3582 solver.cpp:266] Iteration 0 (0 iter/s, 0.872005s/100 iter), loss = 0.175905
I0109 19:55:05.860587  3582 solver.cpp:285]     Train net output #0: loss = 0.175905 (* 1 = 0.175905 loss)
I0109 19:55:05.860644  3582 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 19:55:09.135910  3582 solver.cpp:266] Iteration 100 (30.531 iter/s, 3.27536s/100 iter), loss = 0.143051
I0109 19:55:09.135978  3582 solver.cpp:285]     Train net output #0: loss = 0.143051 (* 1 = 0.143051 loss)
I0109 19:55:09.135993  3582 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 19:55:12.422341  3582 solver.cpp:266] Iteration 200 (30.4285 iter/s, 3.28639s/100 iter), loss = 0.176931
I0109 19:55:12.422403  3582 solver.cpp:285]     Train net output #0: loss = 0.176931 (* 1 = 0.176931 loss)
I0109 19:55:12.422415  3582 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 19:55:15.706296  3582 solver.cpp:266] Iteration 300 (30.4514 iter/s, 3.28393s/100 iter), loss = 0.17627
I0109 19:55:15.706360  3582 solver.cpp:285]     Train net output #0: loss = 0.17627 (* 1 = 0.17627 loss)
I0109 19:55:15.706373  3582 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 19:55:18.993600  3582 solver.cpp:266] Iteration 400 (30.4207 iter/s, 3.28723s/100 iter), loss = 0.25895
I0109 19:55:18.993664  3582 solver.cpp:285]     Train net output #0: loss = 0.25895 (* 1 = 0.25895 loss)
I0109 19:55:18.993677  3582 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 19:55:22.270485  3582 solver.cpp:266] Iteration 500 (30.5171 iter/s, 3.27685s/100 iter), loss = 0.256647
I0109 19:55:22.270547  3582 solver.cpp:285]     Train net output #0: loss = 0.256647 (* 1 = 0.256647 loss)
I0109 19:55:22.270561  3582 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 19:55:25.548074  3582 solver.cpp:266] Iteration 600 (30.5105 iter/s, 3.27756s/100 iter), loss = 0.309173
I0109 19:55:25.548148  3582 solver.cpp:285]     Train net output #0: loss = 0.309173 (* 1 = 0.309173 loss)
I0109 19:55:25.548163  3582 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 19:55:28.819699  3582 solver.cpp:266] Iteration 700 (30.5665 iter/s, 3.27156s/100 iter), loss = 0.246623
I0109 19:55:28.819766  3582 solver.cpp:285]     Train net output #0: loss = 0.246623 (* 1 = 0.246623 loss)
I0109 19:55:28.819777  3582 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 19:55:32.106925  3582 solver.cpp:266] Iteration 800 (30.4211 iter/s, 3.28719s/100 iter), loss = 0.139364
I0109 19:55:32.107007  3582 solver.cpp:285]     Train net output #0: loss = 0.139364 (* 1 = 0.139364 loss)
I0109 19:55:32.107023  3582 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 19:55:35.382478  3582 solver.cpp:266] Iteration 900 (30.5296 iter/s, 3.27551s/100 iter), loss = 0.252587
I0109 19:55:35.382711  3582 solver.cpp:285]     Train net output #0: loss = 0.252587 (* 1 = 0.252587 loss)
I0109 19:55:35.382730  3582 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 19:55:38.631729  3582 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 19:55:39.450542  3582 solver.cpp:517]     Test net output #0: loss = 0.83696 (* 1 = 0.83696 loss)
I0109 19:55:39.450579  3582 solver.cpp:517]     Test net output #1: top-1 = 0.783333
I0109 19:55:39.450587  3582 solver.cpp:517]     Test net output #2: top-5 = 0.980222
I0109 19:55:39.481675  3582 solver.cpp:266] Iteration 1000 (24.3961 iter/s, 4.09902s/100 iter), loss = 0.149433
I0109 19:55:39.481719  3582 solver.cpp:285]     Train net output #0: loss = 0.149433 (* 1 = 0.149433 loss)
I0109 19:55:39.481734  3582 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 19:55:42.769065  3582 solver.cpp:266] Iteration 1100 (30.4196 iter/s, 3.28735s/100 iter), loss = 0.148587
I0109 19:55:42.769129  3582 solver.cpp:285]     Train net output #0: loss = 0.148587 (* 1 = 0.148587 loss)
I0109 19:55:42.769141  3582 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 19:55:46.049420  3582 solver.cpp:266] Iteration 1200 (30.4848 iter/s, 3.28033s/100 iter), loss = 0.24374
I0109 19:55:46.049485  3582 solver.cpp:285]     Train net output #0: loss = 0.24374 (* 1 = 0.24374 loss)
I0109 19:55:46.049497  3582 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 19:55:49.333686  3582 solver.cpp:266] Iteration 1300 (30.4485 iter/s, 3.28424s/100 iter), loss = 0.186659
I0109 19:55:49.333750  3582 solver.cpp:285]     Train net output #0: loss = 0.186659 (* 1 = 0.186659 loss)
I0109 19:55:49.333762  3582 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 19:55:52.617323  3582 solver.cpp:266] Iteration 1400 (30.4543 iter/s, 3.28361s/100 iter), loss = 0.238608
I0109 19:55:52.617388  3582 solver.cpp:285]     Train net output #0: loss = 0.238608 (* 1 = 0.238608 loss)
I0109 19:55:52.617399  3582 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 19:55:55.895764  3582 solver.cpp:266] Iteration 1500 (30.5028 iter/s, 3.27839s/100 iter), loss = 0.166709
I0109 19:55:55.895828  3582 solver.cpp:285]     Train net output #0: loss = 0.166709 (* 1 = 0.166709 loss)
I0109 19:55:55.895840  3582 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 19:55:59.181674  3582 solver.cpp:266] Iteration 1600 (30.4332 iter/s, 3.28588s/100 iter), loss = 0.264119
I0109 19:55:59.181747  3582 solver.cpp:285]     Train net output #0: loss = 0.264119 (* 1 = 0.264119 loss)
I0109 19:55:59.181762  3582 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 19:56:02.477584  3582 solver.cpp:266] Iteration 1700 (30.341 iter/s, 3.29587s/100 iter), loss = 0.193837
I0109 19:56:02.477684  3582 solver.cpp:285]     Train net output #0: loss = 0.193837 (* 1 = 0.193837 loss)
I0109 19:56:02.477700  3582 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 19:56:05.756338  3582 solver.cpp:266] Iteration 1800 (30.5002 iter/s, 3.27867s/100 iter), loss = 0.257397
I0109 19:56:05.756536  3582 solver.cpp:285]     Train net output #0: loss = 0.257397 (* 1 = 0.257397 loss)
I0109 19:56:05.756552  3582 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 19:56:09.037477  3582 solver.cpp:266] Iteration 1900 (30.4787 iter/s, 3.28098s/100 iter), loss = 0.227807
I0109 19:56:09.037549  3582 solver.cpp:285]     Train net output #0: loss = 0.227807 (* 1 = 0.227807 loss)
I0109 19:56:09.037561  3582 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 19:56:12.282209  3582 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 19:56:13.102216  3582 solver.cpp:517]     Test net output #0: loss = 0.773869 (* 1 = 0.773869 loss)
I0109 19:56:13.102254  3582 solver.cpp:517]     Test net output #1: top-1 = 0.801778
I0109 19:56:13.102262  3582 solver.cpp:517]     Test net output #2: top-5 = 0.980111
I0109 19:56:13.133358  3582 solver.cpp:266] Iteration 2000 (24.4149 iter/s, 4.09586s/100 iter), loss = 0.159642
I0109 19:56:13.133414  3582 solver.cpp:285]     Train net output #0: loss = 0.159642 (* 1 = 0.159642 loss)
I0109 19:56:13.133430  3582 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 19:56:16.415709  3582 solver.cpp:266] Iteration 2100 (30.4662 iter/s, 3.28233s/100 iter), loss = 0.223986
I0109 19:56:16.415791  3582 solver.cpp:285]     Train net output #0: loss = 0.223986 (* 1 = 0.223986 loss)
I0109 19:56:16.415808  3582 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 19:56:19.703212  3582 solver.cpp:266] Iteration 2200 (30.4186 iter/s, 3.28746s/100 iter), loss = 0.217425
I0109 19:56:19.703275  3582 solver.cpp:285]     Train net output #0: loss = 0.217425 (* 1 = 0.217425 loss)
I0109 19:56:19.703289  3582 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 19:56:22.979845  3582 solver.cpp:266] Iteration 2300 (30.5196 iter/s, 3.27658s/100 iter), loss = 0.241151
I0109 19:56:22.979907  3582 solver.cpp:285]     Train net output #0: loss = 0.241151 (* 1 = 0.241151 loss)
I0109 19:56:22.979919  3582 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 19:56:26.263257  3582 solver.cpp:266] Iteration 2400 (30.4564 iter/s, 3.28339s/100 iter), loss = 0.129612
I0109 19:56:26.263319  3582 solver.cpp:285]     Train net output #0: loss = 0.129612 (* 1 = 0.129612 loss)
I0109 19:56:26.263332  3582 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 19:56:29.541318  3582 solver.cpp:266] Iteration 2500 (30.5061 iter/s, 3.27803s/100 iter), loss = 0.231091
I0109 19:56:29.541406  3582 solver.cpp:285]     Train net output #0: loss = 0.231091 (* 1 = 0.231091 loss)
I0109 19:56:29.541424  3582 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 19:56:32.817281  3582 solver.cpp:266] Iteration 2600 (30.5261 iter/s, 3.27589s/100 iter), loss = 0.192486
I0109 19:56:32.817351  3582 solver.cpp:285]     Train net output #0: loss = 0.192486 (* 1 = 0.192486 loss)
I0109 19:56:32.817364  3582 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 19:56:36.094461  3582 solver.cpp:266] Iteration 2700 (30.5144 iter/s, 3.27714s/100 iter), loss = 0.223304
I0109 19:56:36.094722  3582 solver.cpp:285]     Train net output #0: loss = 0.223304 (* 1 = 0.223304 loss)
I0109 19:56:36.094738  3582 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 19:56:39.371126  3582 solver.cpp:266] Iteration 2800 (30.5209 iter/s, 3.27644s/100 iter), loss = 0.343451
I0109 19:56:39.371189  3582 solver.cpp:285]     Train net output #0: loss = 0.343451 (* 1 = 0.343451 loss)
I0109 19:56:39.371202  3582 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 19:56:42.652320  3582 solver.cpp:266] Iteration 2900 (30.477 iter/s, 3.28116s/100 iter), loss = 0.215462
I0109 19:56:42.652384  3582 solver.cpp:285]     Train net output #0: loss = 0.215462 (* 1 = 0.215462 loss)
I0109 19:56:42.652398  3582 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 19:56:45.896342  3582 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 19:56:46.716007  3582 solver.cpp:517]     Test net output #0: loss = 0.595135 (* 1 = 0.595135 loss)
I0109 19:56:46.716047  3582 solver.cpp:517]     Test net output #1: top-1 = 0.824555
I0109 19:56:46.716054  3582 solver.cpp:517]     Test net output #2: top-5 = 0.987778
I0109 19:56:46.747198  3582 solver.cpp:266] Iteration 3000 (24.421 iter/s, 4.09483s/100 iter), loss = 0.213276
I0109 19:56:46.747244  3582 solver.cpp:285]     Train net output #0: loss = 0.213276 (* 1 = 0.213276 loss)
I0109 19:56:46.747265  3582 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 19:56:50.021682  3582 solver.cpp:266] Iteration 3100 (30.5393 iter/s, 3.27447s/100 iter), loss = 0.146914
I0109 19:56:50.021745  3582 solver.cpp:285]     Train net output #0: loss = 0.146914 (* 1 = 0.146914 loss)
I0109 19:56:50.021759  3582 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 19:56:53.297199  3582 solver.cpp:266] Iteration 3200 (30.5298 iter/s, 3.27548s/100 iter), loss = 0.210537
I0109 19:56:53.297262  3582 solver.cpp:285]     Train net output #0: loss = 0.210537 (* 1 = 0.210537 loss)
I0109 19:56:53.297276  3582 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 19:56:56.565768  3582 solver.cpp:266] Iteration 3300 (30.5947 iter/s, 3.26853s/100 iter), loss = 0.221997
I0109 19:56:56.565829  3582 solver.cpp:285]     Train net output #0: loss = 0.221997 (* 1 = 0.221997 loss)
I0109 19:56:56.565842  3582 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 19:56:59.833523  3582 solver.cpp:266] Iteration 3400 (30.6026 iter/s, 3.2677s/100 iter), loss = 0.157321
I0109 19:56:59.833598  3582 solver.cpp:285]     Train net output #0: loss = 0.157321 (* 1 = 0.157321 loss)
I0109 19:56:59.833612  3582 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 19:57:03.101630  3582 solver.cpp:266] Iteration 3500 (30.5991 iter/s, 3.26807s/100 iter), loss = 0.277208
I0109 19:57:03.101696  3582 solver.cpp:285]     Train net output #0: loss = 0.277208 (* 1 = 0.277208 loss)
I0109 19:57:03.101709  3582 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 19:57:06.369271  3582 solver.cpp:266] Iteration 3600 (30.6034 iter/s, 3.26761s/100 iter), loss = 0.152095
I0109 19:57:06.369439  3582 solver.cpp:285]     Train net output #0: loss = 0.152095 (* 1 = 0.152095 loss)
I0109 19:57:06.369454  3582 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 19:57:09.644774  3582 solver.cpp:266] Iteration 3700 (30.5309 iter/s, 3.27537s/100 iter), loss = 0.136816
I0109 19:57:09.644848  3582 solver.cpp:285]     Train net output #0: loss = 0.136816 (* 1 = 0.136816 loss)
I0109 19:57:09.644865  3582 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 19:57:12.917713  3582 solver.cpp:266] Iteration 3800 (30.5542 iter/s, 3.27287s/100 iter), loss = 0.1955
I0109 19:57:12.917778  3582 solver.cpp:285]     Train net output #0: loss = 0.1955 (* 1 = 0.1955 loss)
I0109 19:57:12.917791  3582 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 19:57:16.185286  3582 solver.cpp:266] Iteration 3900 (30.6041 iter/s, 3.26754s/100 iter), loss = 0.200276
I0109 19:57:16.185349  3582 solver.cpp:285]     Train net output #0: loss = 0.200276 (* 1 = 0.200276 loss)
I0109 19:57:16.185361  3582 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 19:57:19.424510  3582 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 19:57:20.242981  3582 solver.cpp:517]     Test net output #0: loss = 0.520715 (* 1 = 0.520715 loss)
I0109 19:57:20.243021  3582 solver.cpp:517]     Test net output #1: top-1 = 0.842333
I0109 19:57:20.243028  3582 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0109 19:57:20.273912  3582 solver.cpp:266] Iteration 4000 (24.4582 iter/s, 4.0886s/100 iter), loss = 0.150795
I0109 19:57:20.273955  3582 solver.cpp:285]     Train net output #0: loss = 0.150795 (* 1 = 0.150795 loss)
I0109 19:57:20.273969  3582 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 19:57:23.543630  3582 solver.cpp:266] Iteration 4100 (30.5838 iter/s, 3.2697s/100 iter), loss = 0.145982
I0109 19:57:23.543704  3582 solver.cpp:285]     Train net output #0: loss = 0.145982 (* 1 = 0.145982 loss)
I0109 19:57:23.543720  3582 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 19:57:26.813714  3582 solver.cpp:266] Iteration 4200 (30.5809 iter/s, 3.27002s/100 iter), loss = 0.221049
I0109 19:57:26.813781  3582 solver.cpp:285]     Train net output #0: loss = 0.221049 (* 1 = 0.221049 loss)
I0109 19:57:26.813799  3582 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 19:57:30.090205  3582 solver.cpp:266] Iteration 4300 (30.5208 iter/s, 3.27646s/100 iter), loss = 0.189159
I0109 19:57:30.090279  3582 solver.cpp:285]     Train net output #0: loss = 0.189159 (* 1 = 0.189159 loss)
I0109 19:57:30.090301  3582 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 19:57:33.364384  3582 solver.cpp:266] Iteration 4400 (30.5424 iter/s, 3.27414s/100 iter), loss = 0.147623
I0109 19:57:33.364449  3582 solver.cpp:285]     Train net output #0: loss = 0.147623 (* 1 = 0.147623 loss)
I0109 19:57:33.364461  3582 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 19:57:36.635210  3582 solver.cpp:266] Iteration 4500 (30.5736 iter/s, 3.27079s/100 iter), loss = 0.234882
I0109 19:57:36.635360  3582 solver.cpp:285]     Train net output #0: loss = 0.234882 (* 1 = 0.234882 loss)
I0109 19:57:36.635376  3582 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 19:57:39.903395  3582 solver.cpp:266] Iteration 4600 (30.5994 iter/s, 3.26804s/100 iter), loss = 0.27561
I0109 19:57:39.903477  3582 solver.cpp:285]     Train net output #0: loss = 0.27561 (* 1 = 0.27561 loss)
I0109 19:57:39.903492  3582 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 19:57:43.181049  3582 solver.cpp:266] Iteration 4700 (30.5101 iter/s, 3.2776s/100 iter), loss = 0.183535
I0109 19:57:43.181123  3582 solver.cpp:285]     Train net output #0: loss = 0.183535 (* 1 = 0.183535 loss)
I0109 19:57:43.181138  3582 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 19:57:46.451731  3582 solver.cpp:266] Iteration 4800 (30.5751 iter/s, 3.27064s/100 iter), loss = 0.254284
I0109 19:57:46.451794  3582 solver.cpp:285]     Train net output #0: loss = 0.254284 (* 1 = 0.254284 loss)
I0109 19:57:46.451807  3582 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 19:57:49.721349  3582 solver.cpp:266] Iteration 4900 (30.5849 iter/s, 3.26958s/100 iter), loss = 0.183837
I0109 19:57:49.721417  3582 solver.cpp:285]     Train net output #0: loss = 0.183837 (* 1 = 0.183837 loss)
I0109 19:57:49.721431  3582 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 19:57:52.955971  3582 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 19:57:53.774016  3582 solver.cpp:517]     Test net output #0: loss = 0.591408 (* 1 = 0.591408 loss)
I0109 19:57:53.774057  3582 solver.cpp:517]     Test net output #1: top-1 = 0.823667
I0109 19:57:53.774066  3582 solver.cpp:517]     Test net output #2: top-5 = 0.989111
I0109 19:57:53.804893  3582 solver.cpp:266] Iteration 5000 (24.4888 iter/s, 4.08349s/100 iter), loss = 0.140803
I0109 19:57:53.804934  3582 solver.cpp:285]     Train net output #0: loss = 0.140803 (* 1 = 0.140803 loss)
I0109 19:57:53.804949  3582 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 19:57:57.083990  3582 solver.cpp:266] Iteration 5100 (30.4963 iter/s, 3.27908s/100 iter), loss = 0.182282
I0109 19:57:57.084074  3582 solver.cpp:285]     Train net output #0: loss = 0.182282 (* 1 = 0.182282 loss)
I0109 19:57:57.084095  3582 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 19:58:00.356940  3582 solver.cpp:266] Iteration 5200 (30.554 iter/s, 3.2729s/100 iter), loss = 0.176436
I0109 19:58:00.357010  3582 solver.cpp:285]     Train net output #0: loss = 0.176436 (* 1 = 0.176436 loss)
I0109 19:58:00.357023  3582 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 19:58:03.631211  3582 solver.cpp:266] Iteration 5300 (30.5415 iter/s, 3.27423s/100 iter), loss = 0.172648
I0109 19:58:03.631273  3582 solver.cpp:285]     Train net output #0: loss = 0.172648 (* 1 = 0.172648 loss)
I0109 19:58:03.631285  3582 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 19:58:06.904490  3582 solver.cpp:266] Iteration 5400 (30.5509 iter/s, 3.27322s/100 iter), loss = 0.187499
I0109 19:58:06.904709  3582 solver.cpp:285]     Train net output #0: loss = 0.187499 (* 1 = 0.187499 loss)
I0109 19:58:06.904724  3582 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 19:58:10.179208  3582 solver.cpp:266] Iteration 5500 (30.5387 iter/s, 3.27453s/100 iter), loss = 0.187534
I0109 19:58:10.179291  3582 solver.cpp:285]     Train net output #0: loss = 0.187534 (* 1 = 0.187534 loss)
I0109 19:58:10.179306  3582 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 19:58:13.456796  3582 solver.cpp:266] Iteration 5600 (30.5107 iter/s, 3.27754s/100 iter), loss = 0.20011
I0109 19:58:13.456861  3582 solver.cpp:285]     Train net output #0: loss = 0.20011 (* 1 = 0.20011 loss)
I0109 19:58:13.456873  3582 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 19:58:16.745781  3582 solver.cpp:266] Iteration 5700 (30.4051 iter/s, 3.28892s/100 iter), loss = 0.189262
I0109 19:58:16.745863  3582 solver.cpp:285]     Train net output #0: loss = 0.189262 (* 1 = 0.189262 loss)
I0109 19:58:16.745879  3582 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 19:58:20.040483  3582 solver.cpp:266] Iteration 5800 (30.3522 iter/s, 3.29465s/100 iter), loss = 0.183887
I0109 19:58:20.040556  3582 solver.cpp:285]     Train net output #0: loss = 0.183887 (* 1 = 0.183887 loss)
I0109 19:58:20.040571  3582 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 19:58:23.331225  3582 solver.cpp:266] Iteration 5900 (30.3887 iter/s, 3.2907s/100 iter), loss = 0.224357
I0109 19:58:23.331287  3582 solver.cpp:285]     Train net output #0: loss = 0.224357 (* 1 = 0.224357 loss)
I0109 19:58:23.331300  3582 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 19:58:26.588299  3582 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 19:58:27.413516  3582 solver.cpp:517]     Test net output #0: loss = 0.499473 (* 1 = 0.499473 loss)
I0109 19:58:27.413555  3582 solver.cpp:517]     Test net output #1: top-1 = 0.838667
I0109 19:58:27.413563  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 19:58:27.444898  3582 solver.cpp:266] Iteration 6000 (24.3093 iter/s, 4.11366s/100 iter), loss = 0.226117
I0109 19:58:27.444939  3582 solver.cpp:285]     Train net output #0: loss = 0.226117 (* 1 = 0.226117 loss)
I0109 19:58:27.444954  3582 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 19:58:30.740384  3582 solver.cpp:266] Iteration 6100 (30.3449 iter/s, 3.29545s/100 iter), loss = 0.272668
I0109 19:58:30.740448  3582 solver.cpp:285]     Train net output #0: loss = 0.272668 (* 1 = 0.272668 loss)
I0109 19:58:30.740463  3582 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 19:58:34.038435  3582 solver.cpp:266] Iteration 6200 (30.3212 iter/s, 3.29802s/100 iter), loss = 0.211122
I0109 19:58:34.038502  3582 solver.cpp:285]     Train net output #0: loss = 0.211122 (* 1 = 0.211122 loss)
I0109 19:58:34.038516  3582 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 19:58:37.331017  3582 solver.cpp:266] Iteration 6300 (30.3716 iter/s, 3.29255s/100 iter), loss = 0.121658
I0109 19:58:37.332111  3582 solver.cpp:285]     Train net output #0: loss = 0.121658 (* 1 = 0.121658 loss)
I0109 19:58:37.332134  3582 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 19:58:40.620262  3582 solver.cpp:266] Iteration 6400 (30.4119 iter/s, 3.28819s/100 iter), loss = 0.222068
I0109 19:58:40.620326  3582 solver.cpp:285]     Train net output #0: loss = 0.222068 (* 1 = 0.222068 loss)
I0109 19:58:40.620339  3582 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 19:58:43.908895  3582 solver.cpp:266] Iteration 6500 (30.4083 iter/s, 3.28857s/100 iter), loss = 0.262069
I0109 19:58:43.908967  3582 solver.cpp:285]     Train net output #0: loss = 0.262069 (* 1 = 0.262069 loss)
I0109 19:58:43.908980  3582 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 19:58:47.194725  3582 solver.cpp:266] Iteration 6600 (30.4341 iter/s, 3.28579s/100 iter), loss = 0.199829
I0109 19:58:47.194803  3582 solver.cpp:285]     Train net output #0: loss = 0.199829 (* 1 = 0.199829 loss)
I0109 19:58:47.194819  3582 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 19:58:50.480959  3582 solver.cpp:266] Iteration 6700 (30.4303 iter/s, 3.2862s/100 iter), loss = 0.193938
I0109 19:58:50.481029  3582 solver.cpp:285]     Train net output #0: loss = 0.193938 (* 1 = 0.193938 loss)
I0109 19:58:50.481041  3582 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 19:58:53.772044  3582 solver.cpp:266] Iteration 6800 (30.3857 iter/s, 3.29102s/100 iter), loss = 0.2027
I0109 19:58:53.772131  3582 solver.cpp:285]     Train net output #0: loss = 0.2027 (* 1 = 0.2027 loss)
I0109 19:58:53.772147  3582 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 19:58:57.068620  3582 solver.cpp:266] Iteration 6900 (30.335 iter/s, 3.29652s/100 iter), loss = 0.185134
I0109 19:58:57.068706  3582 solver.cpp:285]     Train net output #0: loss = 0.185134 (* 1 = 0.185134 loss)
I0109 19:58:57.068722  3582 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 19:59:00.330915  3582 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 19:59:01.156442  3582 solver.cpp:517]     Test net output #0: loss = 0.568282 (* 1 = 0.568282 loss)
I0109 19:59:01.156483  3582 solver.cpp:517]     Test net output #1: top-1 = 0.826889
I0109 19:59:01.156493  3582 solver.cpp:517]     Test net output #2: top-5 = 0.990222
I0109 19:59:01.187392  3582 solver.cpp:266] Iteration 7000 (24.2792 iter/s, 4.11874s/100 iter), loss = 0.116117
I0109 19:59:01.187430  3582 solver.cpp:285]     Train net output #0: loss = 0.116117 (* 1 = 0.116117 loss)
I0109 19:59:01.187445  3582 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 19:59:04.486954  3582 solver.cpp:266] Iteration 7100 (30.3071 iter/s, 3.29956s/100 iter), loss = 0.206261
I0109 19:59:04.487037  3582 solver.cpp:285]     Train net output #0: loss = 0.206261 (* 1 = 0.206261 loss)
I0109 19:59:04.487053  3582 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 19:59:07.780012  3582 solver.cpp:266] Iteration 7200 (30.3675 iter/s, 3.29299s/100 iter), loss = 0.20494
I0109 19:59:07.780158  3582 solver.cpp:285]     Train net output #0: loss = 0.20494 (* 1 = 0.20494 loss)
I0109 19:59:07.780174  3582 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 19:59:11.072240  3582 solver.cpp:266] Iteration 7300 (30.3755 iter/s, 3.29213s/100 iter), loss = 0.199876
I0109 19:59:11.072304  3582 solver.cpp:285]     Train net output #0: loss = 0.199876 (* 1 = 0.199876 loss)
I0109 19:59:11.072319  3582 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 19:59:14.364059  3582 solver.cpp:266] Iteration 7400 (30.3786 iter/s, 3.29179s/100 iter), loss = 0.206901
I0109 19:59:14.364145  3582 solver.cpp:285]     Train net output #0: loss = 0.206901 (* 1 = 0.206901 loss)
I0109 19:59:14.364159  3582 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 19:59:17.656317  3582 solver.cpp:266] Iteration 7500 (30.3747 iter/s, 3.29222s/100 iter), loss = 0.129784
I0109 19:59:17.656381  3582 solver.cpp:285]     Train net output #0: loss = 0.129784 (* 1 = 0.129784 loss)
I0109 19:59:17.656394  3582 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 19:59:20.929435  3582 solver.cpp:266] Iteration 7600 (30.5524 iter/s, 3.27307s/100 iter), loss = 0.175466
I0109 19:59:20.929500  3582 solver.cpp:285]     Train net output #0: loss = 0.175466 (* 1 = 0.175466 loss)
I0109 19:59:20.929513  3582 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 19:59:24.200835  3582 solver.cpp:266] Iteration 7700 (30.5682 iter/s, 3.27137s/100 iter), loss = 0.198185
I0109 19:59:24.200898  3582 solver.cpp:285]     Train net output #0: loss = 0.198185 (* 1 = 0.198185 loss)
I0109 19:59:24.200911  3582 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 19:59:27.471081  3582 solver.cpp:266] Iteration 7800 (30.5791 iter/s, 3.27021s/100 iter), loss = 0.145132
I0109 19:59:27.471158  3582 solver.cpp:285]     Train net output #0: loss = 0.145132 (* 1 = 0.145132 loss)
I0109 19:59:27.471174  3582 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 19:59:30.743703  3582 solver.cpp:266] Iteration 7900 (30.5572 iter/s, 3.27255s/100 iter), loss = 0.195224
I0109 19:59:30.743775  3582 solver.cpp:285]     Train net output #0: loss = 0.195224 (* 1 = 0.195224 loss)
I0109 19:59:30.743788  3582 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 19:59:33.980481  3582 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 19:59:34.800664  3582 solver.cpp:517]     Test net output #0: loss = 0.517245 (* 1 = 0.517245 loss)
I0109 19:59:34.800704  3582 solver.cpp:517]     Test net output #1: top-1 = 0.841667
I0109 19:59:34.800710  3582 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0109 19:59:34.831559  3582 solver.cpp:266] Iteration 8000 (24.4628 iter/s, 4.08783s/100 iter), loss = 0.186071
I0109 19:59:34.831598  3582 solver.cpp:285]     Train net output #0: loss = 0.186071 (* 1 = 0.186071 loss)
I0109 19:59:34.831612  3582 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 19:59:38.096213  3582 solver.cpp:266] Iteration 8100 (30.6312 iter/s, 3.26464s/100 iter), loss = 0.223267
I0109 19:59:38.096406  3582 solver.cpp:285]     Train net output #0: loss = 0.223267 (* 1 = 0.223267 loss)
I0109 19:59:38.096423  3582 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 19:59:41.369124  3582 solver.cpp:266] Iteration 8200 (30.5554 iter/s, 3.27275s/100 iter), loss = 0.148997
I0109 19:59:41.369200  3582 solver.cpp:285]     Train net output #0: loss = 0.148997 (* 1 = 0.148997 loss)
I0109 19:59:41.369215  3582 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 19:59:44.637086  3582 solver.cpp:266] Iteration 8300 (30.6005 iter/s, 3.26792s/100 iter), loss = 0.205641
I0109 19:59:44.637151  3582 solver.cpp:285]     Train net output #0: loss = 0.205641 (* 1 = 0.205641 loss)
I0109 19:59:44.637162  3582 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 19:59:47.907188  3582 solver.cpp:266] Iteration 8400 (30.5807 iter/s, 3.27004s/100 iter), loss = 0.167359
I0109 19:59:47.907250  3582 solver.cpp:285]     Train net output #0: loss = 0.167359 (* 1 = 0.167359 loss)
I0109 19:59:47.907263  3582 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 19:59:51.172888  3582 solver.cpp:266] Iteration 8500 (30.6216 iter/s, 3.26567s/100 iter), loss = 0.157278
I0109 19:59:51.172953  3582 solver.cpp:285]     Train net output #0: loss = 0.157278 (* 1 = 0.157278 loss)
I0109 19:59:51.172966  3582 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 19:59:54.440191  3582 solver.cpp:266] Iteration 8600 (30.6066 iter/s, 3.26727s/100 iter), loss = 0.183648
I0109 19:59:54.440254  3582 solver.cpp:285]     Train net output #0: loss = 0.183648 (* 1 = 0.183648 loss)
I0109 19:59:54.440266  3582 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 19:59:57.705627  3582 solver.cpp:266] Iteration 8700 (30.6241 iter/s, 3.2654s/100 iter), loss = 0.18225
I0109 19:59:57.705687  3582 solver.cpp:285]     Train net output #0: loss = 0.18225 (* 1 = 0.18225 loss)
I0109 19:59:57.705699  3582 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 20:00:00.972028  3582 solver.cpp:266] Iteration 8800 (30.6153 iter/s, 3.26634s/100 iter), loss = 0.182708
I0109 20:00:00.972090  3582 solver.cpp:285]     Train net output #0: loss = 0.182708 (* 1 = 0.182708 loss)
I0109 20:00:00.972101  3582 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 20:00:04.239362  3582 solver.cpp:266] Iteration 8900 (30.6063 iter/s, 3.2673s/100 iter), loss = 0.196442
I0109 20:00:04.239425  3582 solver.cpp:285]     Train net output #0: loss = 0.196442 (* 1 = 0.196442 loss)
I0109 20:00:04.239439  3582 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 20:00:07.472065  3582 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 20:00:08.290100  3582 solver.cpp:517]     Test net output #0: loss = 0.489671 (* 1 = 0.489671 loss)
I0109 20:00:08.290266  3582 solver.cpp:517]     Test net output #1: top-1 = 0.844667
I0109 20:00:08.290277  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 20:00:08.321034  3582 solver.cpp:266] Iteration 9000 (24.4999 iter/s, 4.08165s/100 iter), loss = 0.168959
I0109 20:00:08.321081  3582 solver.cpp:285]     Train net output #0: loss = 0.168958 (* 1 = 0.168958 loss)
I0109 20:00:08.321096  3582 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 20:00:11.587888  3582 solver.cpp:266] Iteration 9100 (30.6107 iter/s, 3.26683s/100 iter), loss = 0.166536
I0109 20:00:11.587972  3582 solver.cpp:285]     Train net output #0: loss = 0.166536 (* 1 = 0.166536 loss)
I0109 20:00:11.587988  3582 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 20:00:14.859830  3582 solver.cpp:266] Iteration 9200 (30.5636 iter/s, 3.27186s/100 iter), loss = 0.238639
I0109 20:00:14.859896  3582 solver.cpp:285]     Train net output #0: loss = 0.238639 (* 1 = 0.238639 loss)
I0109 20:00:14.859915  3582 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 20:00:18.126835  3582 solver.cpp:266] Iteration 9300 (30.6094 iter/s, 3.26697s/100 iter), loss = 0.116451
I0109 20:00:18.126904  3582 solver.cpp:285]     Train net output #0: loss = 0.116451 (* 1 = 0.116451 loss)
I0109 20:00:18.126919  3582 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 20:00:21.397128  3582 solver.cpp:266] Iteration 9400 (30.5787 iter/s, 3.27025s/100 iter), loss = 0.187212
I0109 20:00:21.397192  3582 solver.cpp:285]     Train net output #0: loss = 0.187212 (* 1 = 0.187212 loss)
I0109 20:00:21.397203  3582 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 20:00:24.664676  3582 solver.cpp:266] Iteration 9500 (30.6043 iter/s, 3.26751s/100 iter), loss = 0.161977
I0109 20:00:24.664741  3582 solver.cpp:285]     Train net output #0: loss = 0.161977 (* 1 = 0.161977 loss)
I0109 20:00:24.664753  3582 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 20:00:27.930763  3582 solver.cpp:266] Iteration 9600 (30.6183 iter/s, 3.26602s/100 iter), loss = 0.172274
I0109 20:00:27.930824  3582 solver.cpp:285]     Train net output #0: loss = 0.172274 (* 1 = 0.172274 loss)
I0109 20:00:27.930836  3582 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 20:00:31.197633  3582 solver.cpp:266] Iteration 9700 (30.6107 iter/s, 3.26684s/100 iter), loss = 0.170407
I0109 20:00:31.197696  3582 solver.cpp:285]     Train net output #0: loss = 0.170407 (* 1 = 0.170407 loss)
I0109 20:00:31.197710  3582 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 20:00:34.465303  3582 solver.cpp:266] Iteration 9800 (30.6031 iter/s, 3.26764s/100 iter), loss = 0.128649
I0109 20:00:34.465368  3582 solver.cpp:285]     Train net output #0: loss = 0.128649 (* 1 = 0.128649 loss)
I0109 20:00:34.465380  3582 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 20:00:37.731052  3582 solver.cpp:266] Iteration 9900 (30.6212 iter/s, 3.26571s/100 iter), loss = 0.12928
I0109 20:00:37.731115  3582 solver.cpp:285]     Train net output #0: loss = 0.12928 (* 1 = 0.12928 loss)
I0109 20:00:37.731127  3582 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 20:00:40.971266  3582 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 20:00:41.792299  3582 solver.cpp:517]     Test net output #0: loss = 0.513931 (* 1 = 0.513931 loss)
I0109 20:00:41.792341  3582 solver.cpp:517]     Test net output #1: top-1 = 0.844222
I0109 20:00:41.792348  3582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 20:00:41.823185  3582 solver.cpp:266] Iteration 10000 (24.4374 iter/s, 4.09208s/100 iter), loss = 0.162851
I0109 20:00:41.823246  3582 solver.cpp:285]     Train net output #0: loss = 0.162851 (* 1 = 0.162851 loss)
I0109 20:00:41.823261  3582 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 20:00:45.087728  3582 solver.cpp:266] Iteration 10100 (30.6325 iter/s, 3.26451s/100 iter), loss = 0.136006
I0109 20:00:45.087790  3582 solver.cpp:285]     Train net output #0: loss = 0.136006 (* 1 = 0.136006 loss)
I0109 20:00:45.087803  3582 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 20:00:48.356848  3582 solver.cpp:266] Iteration 10200 (30.5896 iter/s, 3.26909s/100 iter), loss = 0.14254
I0109 20:00:48.356920  3582 solver.cpp:285]     Train net output #0: loss = 0.14254 (* 1 = 0.14254 loss)
I0109 20:00:48.356935  3582 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 20:00:51.626873  3582 solver.cpp:266] Iteration 10300 (30.5812 iter/s, 3.26998s/100 iter), loss = 0.139348
I0109 20:00:51.626950  3582 solver.cpp:285]     Train net output #0: loss = 0.139348 (* 1 = 0.139348 loss)
I0109 20:00:51.626963  3582 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 20:00:54.898017  3582 solver.cpp:266] Iteration 10400 (30.571 iter/s, 3.27107s/100 iter), loss = 0.195026
I0109 20:00:54.898083  3582 solver.cpp:285]     Train net output #0: loss = 0.195026 (* 1 = 0.195026 loss)
I0109 20:00:54.898095  3582 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 20:00:58.166002  3582 solver.cpp:266] Iteration 10500 (30.6002 iter/s, 3.26795s/100 iter), loss = 0.196394
I0109 20:00:58.166066  3582 solver.cpp:285]     Train net output #0: loss = 0.196394 (* 1 = 0.196394 loss)
I0109 20:00:58.166079  3582 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 20:01:01.438021  3582 solver.cpp:266] Iteration 10600 (30.5625 iter/s, 3.27198s/100 iter), loss = 0.185445
I0109 20:01:01.438097  3582 solver.cpp:285]     Train net output #0: loss = 0.185445 (* 1 = 0.185445 loss)
I0109 20:01:01.438112  3582 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 20:01:04.713377  3582 solver.cpp:266] Iteration 10700 (30.5314 iter/s, 3.27531s/100 iter), loss = 0.113609
I0109 20:01:04.713441  3582 solver.cpp:285]     Train net output #0: loss = 0.113609 (* 1 = 0.113609 loss)
I0109 20:01:04.713454  3582 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 20:01:07.983302  3582 solver.cpp:266] Iteration 10800 (30.5823 iter/s, 3.26986s/100 iter), loss = 0.139485
I0109 20:01:07.983362  3582 solver.cpp:285]     Train net output #0: loss = 0.139485 (* 1 = 0.139485 loss)
I0109 20:01:07.983374  3582 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 20:01:11.255028  3582 solver.cpp:266] Iteration 10900 (30.5652 iter/s, 3.27169s/100 iter), loss = 0.141384
I0109 20:01:11.255172  3582 solver.cpp:285]     Train net output #0: loss = 0.141384 (* 1 = 0.141384 loss)
I0109 20:01:11.255187  3582 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 20:01:14.497092  3582 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 20:01:15.312886  3582 solver.cpp:517]     Test net output #0: loss = 0.476409 (* 1 = 0.476409 loss)
I0109 20:01:15.312928  3582 solver.cpp:517]     Test net output #1: top-1 = 0.845777
I0109 20:01:15.312937  3582 solver.cpp:517]     Test net output #2: top-5 = 0.993111
I0109 20:01:15.343770  3582 solver.cpp:266] Iteration 11000 (24.458 iter/s, 4.08863s/100 iter), loss = 0.156835
I0109 20:01:15.343844  3582 solver.cpp:285]     Train net output #0: loss = 0.156835 (* 1 = 0.156835 loss)
I0109 20:01:15.343859  3582 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 20:01:18.626207  3582 solver.cpp:266] Iteration 11100 (30.4656 iter/s, 3.28239s/100 iter), loss = 0.114064
I0109 20:01:18.626291  3582 solver.cpp:285]     Train net output #0: loss = 0.114064 (* 1 = 0.114064 loss)
I0109 20:01:18.626305  3582 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 20:01:21.915647  3582 solver.cpp:266] Iteration 11200 (30.401 iter/s, 3.28936s/100 iter), loss = 0.184688
I0109 20:01:21.915712  3582 solver.cpp:285]     Train net output #0: loss = 0.184688 (* 1 = 0.184688 loss)
I0109 20:01:21.915725  3582 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 20:01:25.191283  3582 solver.cpp:266] Iteration 11300 (30.5287 iter/s, 3.2756s/100 iter), loss = 0.146471
I0109 20:01:25.191347  3582 solver.cpp:285]     Train net output #0: loss = 0.146471 (* 1 = 0.146471 loss)
I0109 20:01:25.191359  3582 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 20:01:28.476761  3582 solver.cpp:266] Iteration 11400 (30.4373 iter/s, 3.28544s/100 iter), loss = 0.203266
I0109 20:01:28.476846  3582 solver.cpp:285]     Train net output #0: loss = 0.203266 (* 1 = 0.203266 loss)
I0109 20:01:28.476861  3582 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 20:01:31.761524  3582 solver.cpp:266] Iteration 11500 (30.4443 iter/s, 3.28468s/100 iter), loss = 0.177967
I0109 20:01:31.761605  3582 solver.cpp:285]     Train net output #0: loss = 0.177967 (* 1 = 0.177967 loss)
I0109 20:01:31.761618  3582 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 20:01:35.038365  3582 solver.cpp:266] Iteration 11600 (30.5175 iter/s, 3.2768s/100 iter), loss = 0.238947
I0109 20:01:35.038444  3582 solver.cpp:285]     Train net output #0: loss = 0.238947 (* 1 = 0.238947 loss)
I0109 20:01:35.038460  3582 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 20:01:38.334697  3582 solver.cpp:266] Iteration 11700 (30.3372 iter/s, 3.29629s/100 iter), loss = 0.107433
I0109 20:01:38.334761  3582 solver.cpp:285]     Train net output #0: loss = 0.107433 (* 1 = 0.107433 loss)
I0109 20:01:38.334774  3582 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 20:01:41.629945  3582 solver.cpp:266] Iteration 11800 (30.3471 iter/s, 3.29521s/100 iter), loss = 0.146921
I0109 20:01:41.630165  3582 solver.cpp:285]     Train net output #0: loss = 0.146921 (* 1 = 0.146921 loss)
I0109 20:01:41.630182  3582 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 20:01:44.921212  3582 solver.cpp:266] Iteration 11900 (30.3854 iter/s, 3.29106s/100 iter), loss = 0.13371
I0109 20:01:44.921277  3582 solver.cpp:285]     Train net output #0: loss = 0.13371 (* 1 = 0.13371 loss)
I0109 20:01:44.921290  3582 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 20:01:48.179894  3582 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 20:01:49.003290  3582 solver.cpp:517]     Test net output #0: loss = 0.513168 (* 1 = 0.513168 loss)
I0109 20:01:49.003327  3582 solver.cpp:517]     Test net output #1: top-1 = 0.847778
I0109 20:01:49.003334  3582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 20:01:49.034627  3582 solver.cpp:266] Iteration 12000 (24.3108 iter/s, 4.1134s/100 iter), loss = 0.177843
I0109 20:01:49.034662  3582 solver.cpp:285]     Train net output #0: loss = 0.177843 (* 1 = 0.177843 loss)
I0109 20:01:49.034677  3582 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 20:01:52.335283  3582 solver.cpp:266] Iteration 12100 (30.2971 iter/s, 3.30065s/100 iter), loss = 0.165492
I0109 20:01:52.335353  3582 solver.cpp:285]     Train net output #0: loss = 0.165492 (* 1 = 0.165492 loss)
I0109 20:01:52.335367  3582 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 20:01:55.634933  3582 solver.cpp:266] Iteration 12200 (30.3066 iter/s, 3.29962s/100 iter), loss = 0.167469
I0109 20:01:55.634995  3582 solver.cpp:285]     Train net output #0: loss = 0.167469 (* 1 = 0.167469 loss)
I0109 20:01:55.635007  3582 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 20:01:58.934532  3582 solver.cpp:266] Iteration 12300 (30.3072 iter/s, 3.29955s/100 iter), loss = 0.178891
I0109 20:01:58.934594  3582 solver.cpp:285]     Train net output #0: loss = 0.178891 (* 1 = 0.178891 loss)
I0109 20:01:58.934607  3582 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 20:02:02.236634  3582 solver.cpp:266] Iteration 12400 (30.284 iter/s, 3.30207s/100 iter), loss = 0.105524
I0109 20:02:02.236698  3582 solver.cpp:285]     Train net output #0: loss = 0.105524 (* 1 = 0.105524 loss)
I0109 20:02:02.236711  3582 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 20:02:05.545392  3582 solver.cpp:266] Iteration 12500 (30.2231 iter/s, 3.30873s/100 iter), loss = 0.129575
I0109 20:02:05.545454  3582 solver.cpp:285]     Train net output #0: loss = 0.129575 (* 1 = 0.129575 loss)
I0109 20:02:05.545466  3582 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 20:02:08.840179  3582 solver.cpp:266] Iteration 12600 (30.3514 iter/s, 3.29474s/100 iter), loss = 0.0884743
I0109 20:02:08.840243  3582 solver.cpp:285]     Train net output #0: loss = 0.0884743 (* 1 = 0.0884743 loss)
I0109 20:02:08.840255  3582 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 20:02:12.136663  3582 solver.cpp:266] Iteration 12700 (30.3356 iter/s, 3.29646s/100 iter), loss = 0.148662
I0109 20:02:12.136924  3582 solver.cpp:285]     Train net output #0: loss = 0.148662 (* 1 = 0.148662 loss)
I0109 20:02:12.136943  3582 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 20:02:15.434078  3582 solver.cpp:266] Iteration 12800 (30.3288 iter/s, 3.2972s/100 iter), loss = 0.122153
I0109 20:02:15.434142  3582 solver.cpp:285]     Train net output #0: loss = 0.122153 (* 1 = 0.122153 loss)
I0109 20:02:15.434155  3582 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 20:02:18.729513  3582 solver.cpp:266] Iteration 12900 (30.3453 iter/s, 3.29541s/100 iter), loss = 0.192837
I0109 20:02:18.729604  3582 solver.cpp:285]     Train net output #0: loss = 0.192837 (* 1 = 0.192837 loss)
I0109 20:02:18.729622  3582 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 20:02:21.992955  3582 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 20:02:22.816273  3582 solver.cpp:517]     Test net output #0: loss = 0.507757 (* 1 = 0.507757 loss)
I0109 20:02:22.816313  3582 solver.cpp:517]     Test net output #1: top-1 = 0.850222
I0109 20:02:22.816321  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 20:02:22.847640  3582 solver.cpp:266] Iteration 13000 (24.2832 iter/s, 4.11808s/100 iter), loss = 0.153846
I0109 20:02:22.847676  3582 solver.cpp:285]     Train net output #0: loss = 0.153846 (* 1 = 0.153846 loss)
I0109 20:02:22.847690  3582 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 20:02:26.135090  3582 solver.cpp:266] Iteration 13100 (30.4187 iter/s, 3.28745s/100 iter), loss = 0.122646
I0109 20:02:26.135156  3582 solver.cpp:285]     Train net output #0: loss = 0.122646 (* 1 = 0.122646 loss)
I0109 20:02:26.135169  3582 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 20:02:29.422230  3582 solver.cpp:266] Iteration 13200 (30.4218 iter/s, 3.28711s/100 iter), loss = 0.228977
I0109 20:02:29.422292  3582 solver.cpp:285]     Train net output #0: loss = 0.228977 (* 1 = 0.228977 loss)
I0109 20:02:29.422304  3582 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 20:02:32.700350  3582 solver.cpp:266] Iteration 13300 (30.5055 iter/s, 3.27809s/100 iter), loss = 0.0907418
I0109 20:02:32.700412  3582 solver.cpp:285]     Train net output #0: loss = 0.0907418 (* 1 = 0.0907418 loss)
I0109 20:02:32.700425  3582 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 20:02:35.991405  3582 solver.cpp:266] Iteration 13400 (30.3859 iter/s, 3.291s/100 iter), loss = 0.201043
I0109 20:02:35.991473  3582 solver.cpp:285]     Train net output #0: loss = 0.201043 (* 1 = 0.201043 loss)
I0109 20:02:35.991487  3582 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 20:02:39.274276  3582 solver.cpp:266] Iteration 13500 (30.4614 iter/s, 3.28284s/100 iter), loss = 0.191152
I0109 20:02:39.274338  3582 solver.cpp:285]     Train net output #0: loss = 0.191152 (* 1 = 0.191152 loss)
I0109 20:02:39.274351  3582 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 20:02:42.560475  3582 solver.cpp:266] Iteration 13600 (30.4305 iter/s, 3.28618s/100 iter), loss = 0.173363
I0109 20:02:42.560606  3582 solver.cpp:285]     Train net output #0: loss = 0.173363 (* 1 = 0.173363 loss)
I0109 20:02:42.560622  3582 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 20:02:45.857812  3582 solver.cpp:266] Iteration 13700 (30.3285 iter/s, 3.29722s/100 iter), loss = 0.142766
I0109 20:02:45.857873  3582 solver.cpp:285]     Train net output #0: loss = 0.142766 (* 1 = 0.142766 loss)
I0109 20:02:45.857887  3582 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 20:02:49.152674  3582 solver.cpp:266] Iteration 13800 (30.3505 iter/s, 3.29484s/100 iter), loss = 0.135292
I0109 20:02:49.152742  3582 solver.cpp:285]     Train net output #0: loss = 0.135292 (* 1 = 0.135292 loss)
I0109 20:02:49.152755  3582 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 20:02:52.460590  3582 solver.cpp:266] Iteration 13900 (30.2307 iter/s, 3.30789s/100 iter), loss = 0.18591
I0109 20:02:52.460654  3582 solver.cpp:285]     Train net output #0: loss = 0.18591 (* 1 = 0.18591 loss)
I0109 20:02:52.460666  3582 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 20:02:55.722515  3582 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 20:02:56.542161  3582 solver.cpp:517]     Test net output #0: loss = 0.479111 (* 1 = 0.479111 loss)
I0109 20:02:56.542202  3582 solver.cpp:517]     Test net output #1: top-1 = 0.856444
I0109 20:02:56.542209  3582 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 20:02:56.573120  3582 solver.cpp:266] Iteration 14000 (24.316 iter/s, 4.11252s/100 iter), loss = 0.157101
I0109 20:02:56.573158  3582 solver.cpp:285]     Train net output #0: loss = 0.157101 (* 1 = 0.157101 loss)
I0109 20:02:56.573173  3582 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 20:02:59.868593  3582 solver.cpp:266] Iteration 14100 (30.3449 iter/s, 3.29544s/100 iter), loss = 0.197455
I0109 20:02:59.868679  3582 solver.cpp:285]     Train net output #0: loss = 0.197455 (* 1 = 0.197455 loss)
I0109 20:02:59.868695  3582 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 20:03:03.168246  3582 solver.cpp:266] Iteration 14200 (30.3066 iter/s, 3.29961s/100 iter), loss = 0.197184
I0109 20:03:03.168341  3582 solver.cpp:285]     Train net output #0: loss = 0.197184 (* 1 = 0.197184 loss)
I0109 20:03:03.168365  3582 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 20:03:06.468672  3582 solver.cpp:266] Iteration 14300 (30.2995 iter/s, 3.30038s/100 iter), loss = 0.177252
I0109 20:03:06.468744  3582 solver.cpp:285]     Train net output #0: loss = 0.177252 (* 1 = 0.177252 loss)
I0109 20:03:06.468763  3582 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 20:03:09.769438  3582 solver.cpp:266] Iteration 14400 (30.2965 iter/s, 3.30071s/100 iter), loss = 0.0787674
I0109 20:03:09.769511  3582 solver.cpp:285]     Train net output #0: loss = 0.0787674 (* 1 = 0.0787674 loss)
I0109 20:03:09.769529  3582 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 20:03:13.063531  3582 solver.cpp:266] Iteration 14500 (30.3576 iter/s, 3.29407s/100 iter), loss = 0.0842147
I0109 20:03:13.063752  3582 solver.cpp:285]     Train net output #0: loss = 0.0842147 (* 1 = 0.0842147 loss)
I0109 20:03:13.063776  3582 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 20:03:16.358918  3582 solver.cpp:266] Iteration 14600 (30.347 iter/s, 3.29521s/100 iter), loss = 0.142873
I0109 20:03:16.358995  3582 solver.cpp:285]     Train net output #0: loss = 0.142873 (* 1 = 0.142873 loss)
I0109 20:03:16.359009  3582 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 20:03:19.651381  3582 solver.cpp:266] Iteration 14700 (30.3727 iter/s, 3.29243s/100 iter), loss = 0.0994193
I0109 20:03:19.651443  3582 solver.cpp:285]     Train net output #0: loss = 0.0994193 (* 1 = 0.0994193 loss)
I0109 20:03:19.651455  3582 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 20:03:22.944772  3582 solver.cpp:266] Iteration 14800 (30.3643 iter/s, 3.29334s/100 iter), loss = 0.115657
I0109 20:03:22.944859  3582 solver.cpp:285]     Train net output #0: loss = 0.115657 (* 1 = 0.115657 loss)
I0109 20:03:22.944877  3582 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 20:03:26.241389  3582 solver.cpp:266] Iteration 14900 (30.3345 iter/s, 3.29658s/100 iter), loss = 0.0881328
I0109 20:03:26.241456  3582 solver.cpp:285]     Train net output #0: loss = 0.0881328 (* 1 = 0.0881328 loss)
I0109 20:03:26.241469  3582 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 20:03:29.498327  3582 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 20:03:30.321547  3582 solver.cpp:517]     Test net output #0: loss = 0.486135 (* 1 = 0.486135 loss)
I0109 20:03:30.321600  3582 solver.cpp:517]     Test net output #1: top-1 = 0.852222
I0109 20:03:30.321609  3582 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 20:03:30.352398  3582 solver.cpp:266] Iteration 15000 (24.325 iter/s, 4.111s/100 iter), loss = 0.124608
I0109 20:03:30.352433  3582 solver.cpp:285]     Train net output #0: loss = 0.124608 (* 1 = 0.124608 loss)
I0109 20:03:30.352448  3582 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 20:03:33.643321  3582 solver.cpp:266] Iteration 15100 (30.3866 iter/s, 3.29093s/100 iter), loss = 0.161858
I0109 20:03:33.643401  3582 solver.cpp:285]     Train net output #0: loss = 0.161858 (* 1 = 0.161858 loss)
I0109 20:03:33.643416  3582 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 20:03:36.937510  3582 solver.cpp:266] Iteration 15200 (30.3571 iter/s, 3.29412s/100 iter), loss = 0.164545
I0109 20:03:36.937606  3582 solver.cpp:285]     Train net output #0: loss = 0.164545 (* 1 = 0.164545 loss)
I0109 20:03:36.937623  3582 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 20:03:40.230444  3582 solver.cpp:266] Iteration 15300 (30.3684 iter/s, 3.2929s/100 iter), loss = 0.210174
I0109 20:03:40.230510  3582 solver.cpp:285]     Train net output #0: loss = 0.210174 (* 1 = 0.210174 loss)
I0109 20:03:40.230525  3582 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 20:03:43.520567  3582 solver.cpp:266] Iteration 15400 (30.3942 iter/s, 3.2901s/100 iter), loss = 0.137688
I0109 20:03:43.520759  3582 solver.cpp:285]     Train net output #0: loss = 0.137688 (* 1 = 0.137688 loss)
I0109 20:03:43.520777  3582 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 20:03:46.811528  3582 solver.cpp:266] Iteration 15500 (30.3879 iter/s, 3.29079s/100 iter), loss = 0.157239
I0109 20:03:46.811602  3582 solver.cpp:285]     Train net output #0: loss = 0.157239 (* 1 = 0.157239 loss)
I0109 20:03:46.811615  3582 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 20:03:50.109326  3582 solver.cpp:266] Iteration 15600 (30.3236 iter/s, 3.29777s/100 iter), loss = 0.101125
I0109 20:03:50.109407  3582 solver.cpp:285]     Train net output #0: loss = 0.101125 (* 1 = 0.101125 loss)
I0109 20:03:50.109423  3582 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 20:03:53.390317  3582 solver.cpp:266] Iteration 15700 (30.4789 iter/s, 3.28095s/100 iter), loss = 0.151732
I0109 20:03:53.390385  3582 solver.cpp:285]     Train net output #0: loss = 0.151732 (* 1 = 0.151732 loss)
I0109 20:03:53.390398  3582 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 20:03:56.664368  3582 solver.cpp:266] Iteration 15800 (30.5435 iter/s, 3.27402s/100 iter), loss = 0.0924708
I0109 20:03:56.664441  3582 solver.cpp:285]     Train net output #0: loss = 0.0924708 (* 1 = 0.0924708 loss)
I0109 20:03:56.664456  3582 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 20:03:59.939520  3582 solver.cpp:266] Iteration 15900 (30.5335 iter/s, 3.2751s/100 iter), loss = 0.170953
I0109 20:03:59.939589  3582 solver.cpp:285]     Train net output #0: loss = 0.170953 (* 1 = 0.170953 loss)
I0109 20:03:59.939602  3582 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 20:04:03.197065  3582 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 20:04:04.016918  3582 solver.cpp:517]     Test net output #0: loss = 0.453814 (* 1 = 0.453814 loss)
I0109 20:04:04.016961  3582 solver.cpp:517]     Test net output #1: top-1 = 0.860334
I0109 20:04:04.016970  3582 solver.cpp:517]     Test net output #2: top-5 = 0.993555
I0109 20:04:04.047749  3582 solver.cpp:266] Iteration 16000 (24.3415 iter/s, 4.10821s/100 iter), loss = 0.188752
I0109 20:04:04.047823  3582 solver.cpp:285]     Train net output #0: loss = 0.188752 (* 1 = 0.188752 loss)
I0109 20:04:04.047839  3582 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 20:04:07.330209  3582 solver.cpp:266] Iteration 16100 (30.4653 iter/s, 3.28243s/100 iter), loss = 0.213746
I0109 20:04:07.330271  3582 solver.cpp:285]     Train net output #0: loss = 0.213746 (* 1 = 0.213746 loss)
I0109 20:04:07.330284  3582 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 20:04:10.604813  3582 solver.cpp:266] Iteration 16200 (30.5383 iter/s, 3.27458s/100 iter), loss = 0.156661
I0109 20:04:10.604881  3582 solver.cpp:285]     Train net output #0: loss = 0.156661 (* 1 = 0.156661 loss)
I0109 20:04:10.604893  3582 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 20:04:13.888554  3582 solver.cpp:266] Iteration 16300 (30.4536 iter/s, 3.28369s/100 iter), loss = 0.113018
I0109 20:04:13.888732  3582 solver.cpp:285]     Train net output #0: loss = 0.113018 (* 1 = 0.113018 loss)
I0109 20:04:13.888756  3582 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 20:04:17.171540  3582 solver.cpp:266] Iteration 16400 (30.4613 iter/s, 3.28285s/100 iter), loss = 0.145647
I0109 20:04:17.171613  3582 solver.cpp:285]     Train net output #0: loss = 0.145647 (* 1 = 0.145647 loss)
I0109 20:04:17.171633  3582 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 20:04:20.454072  3582 solver.cpp:266] Iteration 16500 (30.4646 iter/s, 3.2825s/100 iter), loss = 0.130101
I0109 20:04:20.454152  3582 solver.cpp:285]     Train net output #0: loss = 0.130101 (* 1 = 0.130101 loss)
I0109 20:04:20.454174  3582 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 20:04:23.745973  3582 solver.cpp:266] Iteration 16600 (30.3779 iter/s, 3.29187s/100 iter), loss = 0.118827
I0109 20:04:23.746047  3582 solver.cpp:285]     Train net output #0: loss = 0.118827 (* 1 = 0.118827 loss)
I0109 20:04:23.746065  3582 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 20:04:27.035558  3582 solver.cpp:266] Iteration 16700 (30.3995 iter/s, 3.28953s/100 iter), loss = 0.132118
I0109 20:04:27.035630  3582 solver.cpp:285]     Train net output #0: loss = 0.132118 (* 1 = 0.132118 loss)
I0109 20:04:27.035648  3582 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 20:04:30.338259  3582 solver.cpp:266] Iteration 16800 (30.2785 iter/s, 3.30267s/100 iter), loss = 0.117838
I0109 20:04:30.338341  3582 solver.cpp:285]     Train net output #0: loss = 0.117838 (* 1 = 0.117838 loss)
I0109 20:04:30.338363  3582 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 20:04:33.627845  3582 solver.cpp:266] Iteration 16900 (30.3994 iter/s, 3.28954s/100 iter), loss = 0.138996
I0109 20:04:33.627938  3582 solver.cpp:285]     Train net output #0: loss = 0.138996 (* 1 = 0.138996 loss)
I0109 20:04:33.627962  3582 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 20:04:36.882306  3582 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 20:04:37.701833  3582 solver.cpp:517]     Test net output #0: loss = 0.454906 (* 1 = 0.454906 loss)
I0109 20:04:37.701874  3582 solver.cpp:517]     Test net output #1: top-1 = 0.860778
I0109 20:04:37.701889  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992111
I0109 20:04:37.732731  3582 solver.cpp:266] Iteration 17000 (24.3614 iter/s, 4.10485s/100 iter), loss = 0.136874
I0109 20:04:37.732791  3582 solver.cpp:285]     Train net output #0: loss = 0.136874 (* 1 = 0.136874 loss)
I0109 20:04:37.732813  3582 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 20:04:41.015017  3582 solver.cpp:266] Iteration 17100 (30.467 iter/s, 3.28224s/100 iter), loss = 0.087781
I0109 20:04:41.015092  3582 solver.cpp:285]     Train net output #0: loss = 0.0877811 (* 1 = 0.0877811 loss)
I0109 20:04:41.015112  3582 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 20:04:44.291755  3582 solver.cpp:266] Iteration 17200 (30.5185 iter/s, 3.27671s/100 iter), loss = 0.125597
I0109 20:04:44.291923  3582 solver.cpp:285]     Train net output #0: loss = 0.125597 (* 1 = 0.125597 loss)
I0109 20:04:44.291944  3582 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 20:04:47.570861  3582 solver.cpp:266] Iteration 17300 (30.4973 iter/s, 3.27898s/100 iter), loss = 0.109234
I0109 20:04:47.570945  3582 solver.cpp:285]     Train net output #0: loss = 0.109234 (* 1 = 0.109234 loss)
I0109 20:04:47.570967  3582 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 20:04:50.844493  3582 solver.cpp:266] Iteration 17400 (30.5478 iter/s, 3.27356s/100 iter), loss = 0.145007
I0109 20:04:50.844564  3582 solver.cpp:285]     Train net output #0: loss = 0.145008 (* 1 = 0.145008 loss)
I0109 20:04:50.844585  3582 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 20:04:54.119652  3582 solver.cpp:266] Iteration 17500 (30.5332 iter/s, 3.27513s/100 iter), loss = 0.0829761
I0109 20:04:54.119729  3582 solver.cpp:285]     Train net output #0: loss = 0.0829761 (* 1 = 0.0829761 loss)
I0109 20:04:54.119750  3582 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 20:04:57.391716  3582 solver.cpp:266] Iteration 17600 (30.5621 iter/s, 3.27202s/100 iter), loss = 0.0840353
I0109 20:04:57.391789  3582 solver.cpp:285]     Train net output #0: loss = 0.0840354 (* 1 = 0.0840354 loss)
I0109 20:04:57.391809  3582 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 20:05:00.668118  3582 solver.cpp:266] Iteration 17700 (30.5216 iter/s, 3.27637s/100 iter), loss = 0.0964885
I0109 20:05:00.668198  3582 solver.cpp:285]     Train net output #0: loss = 0.0964885 (* 1 = 0.0964885 loss)
I0109 20:05:00.668220  3582 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 20:05:03.942728  3582 solver.cpp:266] Iteration 17800 (30.5386 iter/s, 3.27454s/100 iter), loss = 0.133141
I0109 20:05:03.942806  3582 solver.cpp:285]     Train net output #0: loss = 0.133141 (* 1 = 0.133141 loss)
I0109 20:05:03.942826  3582 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 20:05:07.214229  3582 solver.cpp:266] Iteration 17900 (30.5674 iter/s, 3.27146s/100 iter), loss = 0.12377
I0109 20:05:07.214308  3582 solver.cpp:285]     Train net output #0: loss = 0.12377 (* 1 = 0.12377 loss)
I0109 20:05:07.214329  3582 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 20:05:10.453913  3582 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 20:05:11.271247  3582 solver.cpp:517]     Test net output #0: loss = 0.457147 (* 1 = 0.457147 loss)
I0109 20:05:11.271289  3582 solver.cpp:517]     Test net output #1: top-1 = 0.859667
I0109 20:05:11.271296  3582 solver.cpp:517]     Test net output #2: top-5 = 0.993889
I0109 20:05:11.302147  3582 solver.cpp:266] Iteration 18000 (24.4625 iter/s, 4.08788s/100 iter), loss = 0.140292
I0109 20:05:11.302208  3582 solver.cpp:285]     Train net output #0: loss = 0.140292 (* 1 = 0.140292 loss)
I0109 20:05:11.302223  3582 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 20:05:14.575239  3582 solver.cpp:266] Iteration 18100 (30.5524 iter/s, 3.27306s/100 iter), loss = 0.156974
I0109 20:05:14.575428  3582 solver.cpp:285]     Train net output #0: loss = 0.156974 (* 1 = 0.156974 loss)
I0109 20:05:14.575445  3582 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 20:05:17.848253  3582 solver.cpp:266] Iteration 18200 (30.5546 iter/s, 3.27283s/100 iter), loss = 0.0869366
I0109 20:05:17.848318  3582 solver.cpp:285]     Train net output #0: loss = 0.0869366 (* 1 = 0.0869366 loss)
I0109 20:05:17.848332  3582 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 20:05:21.121629  3582 solver.cpp:266] Iteration 18300 (30.5498 iter/s, 3.27334s/100 iter), loss = 0.161806
I0109 20:05:21.121697  3582 solver.cpp:285]     Train net output #0: loss = 0.161807 (* 1 = 0.161807 loss)
I0109 20:05:21.121711  3582 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 20:05:24.392930  3582 solver.cpp:266] Iteration 18400 (30.5693 iter/s, 3.27126s/100 iter), loss = 0.124229
I0109 20:05:24.393023  3582 solver.cpp:285]     Train net output #0: loss = 0.124229 (* 1 = 0.124229 loss)
I0109 20:05:24.393044  3582 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 20:05:27.664921  3582 solver.cpp:266] Iteration 18500 (30.563 iter/s, 3.27193s/100 iter), loss = 0.175115
I0109 20:05:27.664997  3582 solver.cpp:285]     Train net output #0: loss = 0.175115 (* 1 = 0.175115 loss)
I0109 20:05:27.665017  3582 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 20:05:30.939186  3582 solver.cpp:266] Iteration 18600 (30.5419 iter/s, 3.27419s/100 iter), loss = 0.0833052
I0109 20:05:30.939275  3582 solver.cpp:285]     Train net output #0: loss = 0.0833053 (* 1 = 0.0833053 loss)
I0109 20:05:30.939296  3582 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 20:05:34.211243  3582 solver.cpp:266] Iteration 18700 (30.5623 iter/s, 3.272s/100 iter), loss = 0.124555
I0109 20:05:34.211308  3582 solver.cpp:285]     Train net output #0: loss = 0.124555 (* 1 = 0.124555 loss)
I0109 20:05:34.211323  3582 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 20:05:37.492282  3582 solver.cpp:266] Iteration 18800 (30.4785 iter/s, 3.281s/100 iter), loss = 0.100942
I0109 20:05:37.492368  3582 solver.cpp:285]     Train net output #0: loss = 0.100942 (* 1 = 0.100942 loss)
I0109 20:05:37.492384  3582 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 20:05:40.771031  3582 solver.cpp:266] Iteration 18900 (30.5002 iter/s, 3.27867s/100 iter), loss = 0.11031
I0109 20:05:40.771109  3582 solver.cpp:285]     Train net output #0: loss = 0.11031 (* 1 = 0.11031 loss)
I0109 20:05:40.771131  3582 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 20:05:44.016160  3582 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 20:05:44.834805  3582 solver.cpp:517]     Test net output #0: loss = 0.446498 (* 1 = 0.446498 loss)
I0109 20:05:44.834923  3582 solver.cpp:517]     Test net output #1: top-1 = 0.862778
I0109 20:05:44.834939  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 20:05:44.865836  3582 solver.cpp:266] Iteration 19000 (24.4214 iter/s, 4.09477s/100 iter), loss = 0.192493
I0109 20:05:44.865885  3582 solver.cpp:285]     Train net output #0: loss = 0.192493 (* 1 = 0.192493 loss)
I0109 20:05:44.865906  3582 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 20:05:48.137154  3582 solver.cpp:266] Iteration 19100 (30.5689 iter/s, 3.2713s/100 iter), loss = 0.0995522
I0109 20:05:48.137234  3582 solver.cpp:285]     Train net output #0: loss = 0.0995523 (* 1 = 0.0995523 loss)
I0109 20:05:48.137249  3582 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 20:05:51.409261  3582 solver.cpp:266] Iteration 19200 (30.5618 iter/s, 3.27206s/100 iter), loss = 0.154062
I0109 20:05:51.409327  3582 solver.cpp:285]     Train net output #0: loss = 0.154062 (* 1 = 0.154062 loss)
I0109 20:05:51.409341  3582 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 20:05:54.682407  3582 solver.cpp:266] Iteration 19300 (30.552 iter/s, 3.27311s/100 iter), loss = 0.12964
I0109 20:05:54.682479  3582 solver.cpp:285]     Train net output #0: loss = 0.12964 (* 1 = 0.12964 loss)
I0109 20:05:54.682494  3582 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 20:05:57.965000  3582 solver.cpp:266] Iteration 19400 (30.4644 iter/s, 3.28252s/100 iter), loss = 0.101558
I0109 20:05:57.965082  3582 solver.cpp:285]     Train net output #0: loss = 0.101558 (* 1 = 0.101558 loss)
I0109 20:05:57.965097  3582 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 20:06:01.244364  3582 solver.cpp:266] Iteration 19500 (30.4942 iter/s, 3.27931s/100 iter), loss = 0.103913
I0109 20:06:01.244437  3582 solver.cpp:285]     Train net output #0: loss = 0.103913 (* 1 = 0.103913 loss)
I0109 20:06:01.244451  3582 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 20:06:04.524847  3582 solver.cpp:266] Iteration 19600 (30.4837 iter/s, 3.28044s/100 iter), loss = 0.138488
I0109 20:06:04.524912  3582 solver.cpp:285]     Train net output #0: loss = 0.138488 (* 1 = 0.138488 loss)
I0109 20:06:04.524925  3582 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 20:06:07.797673  3582 solver.cpp:266] Iteration 19700 (30.5552 iter/s, 3.27276s/100 iter), loss = 0.0981853
I0109 20:06:07.797740  3582 solver.cpp:285]     Train net output #0: loss = 0.0981853 (* 1 = 0.0981853 loss)
I0109 20:06:07.797755  3582 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 20:06:11.069160  3582 solver.cpp:266] Iteration 19800 (30.5675 iter/s, 3.27145s/100 iter), loss = 0.12459
I0109 20:06:11.069228  3582 solver.cpp:285]     Train net output #0: loss = 0.12459 (* 1 = 0.12459 loss)
I0109 20:06:11.069242  3582 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 20:06:14.340121  3582 solver.cpp:266] Iteration 19900 (30.5724 iter/s, 3.27092s/100 iter), loss = 0.198851
I0109 20:06:14.340186  3582 solver.cpp:285]     Train net output #0: loss = 0.198851 (* 1 = 0.198851 loss)
I0109 20:06:14.340198  3582 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 20:06:17.577270  3582 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/_iter_20000.caffemodel
I0109 20:06:17.679255  3582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/_iter_20000.solverstate
I0109 20:06:17.705557  3582 solver.cpp:378] Iteration 20000, loss = 0.0112047
I0109 20:06:17.705626  3582 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 20:06:18.520026  3582 solver.cpp:517]     Test net output #0: loss = 0.449462 (* 1 = 0.449462 loss)
I0109 20:06:18.520066  3582 solver.cpp:517]     Test net output #1: top-1 = 0.863556
I0109 20:06:18.520076  3582 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 20:06:18.520082  3582 solver.cpp:386] Optimization Done (29.8801 iter/s).
I0109 20:06:18.520089  3582 caffe_interface.cpp:530] Optimization Done.
I0109 20:06:18.683753  3609 pruning_runner.cpp:190] Sens info found, use it.
I0109 20:06:18.730767  3609 pruning_runner.cpp:217] Start compressing, please wait...
I0109 20:06:19.788060  3609 pruning_runner.cpp:264] Compression complete 0.000327392%
I0109 20:06:20.110280  3609 pruning_runner.cpp:264] Compression complete 50.0002%
I0109 20:06:20.429834  3609 pruning_runner.cpp:264] Compression complete 66.6668%
I0109 20:06:20.756381  3609 pruning_runner.cpp:264] Compression complete 80.0001%
I0109 20:06:21.076423  3609 pruning_runner.cpp:264] Compression complete 94.1177%
I0109 20:06:21.394317  3609 pruning_runner.cpp:264] Compression complete 97.0588%
I0109 20:06:21.720283  3609 pruning_runner.cpp:264] Compression complete 99.6226%
I0109 20:06:22.038062  3609 pruning_runner.cpp:264] Compression complete 99.8113%
I0109 20:06:22.359907  3609 pruning_runner.cpp:264] Compression complete 99.9056%
I0109 20:06:22.689997  3609 pruning_runner.cpp:264] Compression complete 99.9528%
I0109 20:06:23.016842  3609 pruning_runner.cpp:264] Compression complete 99.9764%
I0109 20:06:23.349072  3609 pruning_runner.cpp:264] Compression complete 99.9882%
I0109 20:06:23.676123  3609 pruning_runner.cpp:264] Compression complete 99.9985%
I0109 20:06:24.001520  3609 pruning_runner.cpp:264] Compression complete 99.9996%
I0109 20:06:24.327167  3609 pruning_runner.cpp:264] Compression complete 99.9998%
I0109 20:06:24.648743  3609 pruning_runner.cpp:264] Compression complete 99.9999%
I0109 20:06:24.974876  3609 pruning_runner.cpp:264] Compression complete 100%
I0109 20:06:25.300835  3609 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 20:06:25.301172  3609 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 20:06:25.301618  3609 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:06:25.301852  3609 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:06:25.302017  3609 layer_factory.hpp:77] Creating layer data
I0109 20:06:25.302084  3609 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:25.302206  3609 net.cpp:94] Creating Layer data
I0109 20:06:25.302258  3609 net.cpp:409] data -> data
I0109 20:06:25.302292  3609 net.cpp:409] data -> label
I0109 20:06:25.303570  3942 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:06:25.303607  3942 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:06:25.303696  3609 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:06:25.303824  3609 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:06:25.310575  3609 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:25.310644  3609 net.cpp:144] Setting up data
I0109 20:06:25.310685  3609 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:06:25.310715  3609 net.cpp:151] Top shape: 50 (50)
I0109 20:06:25.310741  3609 net.cpp:159] Memory required for data: 614600
I0109 20:06:25.310770  3609 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:06:25.310801  3609 net.cpp:94] Creating Layer label_data_1_split
I0109 20:06:25.310827  3609 net.cpp:435] label_data_1_split <- label
I0109 20:06:25.310868  3609 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:06:25.310911  3609 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:06:25.310950  3609 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:06:25.311053  3609 net.cpp:144] Setting up label_data_1_split
I0109 20:06:25.311074  3609 net.cpp:151] Top shape: 50 (50)
I0109 20:06:25.311082  3609 net.cpp:151] Top shape: 50 (50)
I0109 20:06:25.311089  3609 net.cpp:151] Top shape: 50 (50)
I0109 20:06:25.311094  3609 net.cpp:159] Memory required for data: 615200
I0109 20:06:25.311100  3609 layer_factory.hpp:77] Creating layer conv1
I0109 20:06:25.311117  3609 net.cpp:94] Creating Layer conv1
I0109 20:06:25.311146  3609 net.cpp:435] conv1 <- data
I0109 20:06:25.311178  3609 net.cpp:409] conv1 -> conv1
I0109 20:06:25.312376  3609 net.cpp:144] Setting up conv1
I0109 20:06:25.312399  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.312407  3609 net.cpp:159] Memory required for data: 7168800
I0109 20:06:25.312424  3609 layer_factory.hpp:77] Creating layer bn1
I0109 20:06:25.312440  3609 net.cpp:94] Creating Layer bn1
I0109 20:06:25.312455  3609 net.cpp:435] bn1 <- conv1
I0109 20:06:25.312495  3609 net.cpp:409] bn1 -> scale1
I0109 20:06:25.313261  3609 net.cpp:144] Setting up bn1
I0109 20:06:25.313285  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.313292  3609 net.cpp:159] Memory required for data: 13722400
I0109 20:06:25.313313  3609 layer_factory.hpp:77] Creating layer relu1
I0109 20:06:25.313341  3609 net.cpp:94] Creating Layer relu1
I0109 20:06:25.313367  3609 net.cpp:435] relu1 <- scale1
I0109 20:06:25.313392  3609 net.cpp:409] relu1 -> relu1
I0109 20:06:25.313449  3609 net.cpp:144] Setting up relu1
I0109 20:06:25.313477  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.313498  3609 net.cpp:159] Memory required for data: 20276000
I0109 20:06:25.313519  3609 layer_factory.hpp:77] Creating layer conv2
I0109 20:06:25.313549  3609 net.cpp:94] Creating Layer conv2
I0109 20:06:25.313571  3609 net.cpp:435] conv2 <- relu1
I0109 20:06:25.313624  3609 net.cpp:409] conv2 -> conv2
I0109 20:06:25.314759  3609 net.cpp:144] Setting up conv2
I0109 20:06:25.314787  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.314795  3609 net.cpp:159] Memory required for data: 26829600
I0109 20:06:25.314811  3609 layer_factory.hpp:77] Creating layer bn2
I0109 20:06:25.314826  3609 net.cpp:94] Creating Layer bn2
I0109 20:06:25.314849  3609 net.cpp:435] bn2 <- conv2
I0109 20:06:25.314880  3609 net.cpp:409] bn2 -> scale2
I0109 20:06:25.315663  3609 net.cpp:144] Setting up bn2
I0109 20:06:25.315687  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.315696  3609 net.cpp:159] Memory required for data: 33383200
I0109 20:06:25.315712  3609 layer_factory.hpp:77] Creating layer relu2
I0109 20:06:25.315740  3609 net.cpp:94] Creating Layer relu2
I0109 20:06:25.315776  3609 net.cpp:435] relu2 <- scale2
I0109 20:06:25.315811  3609 net.cpp:409] relu2 -> relu2
I0109 20:06:25.315870  3609 net.cpp:144] Setting up relu2
I0109 20:06:25.315910  3609 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:25.315932  3609 net.cpp:159] Memory required for data: 39936800
I0109 20:06:25.315953  3609 layer_factory.hpp:77] Creating layer pool1
I0109 20:06:25.315980  3609 net.cpp:94] Creating Layer pool1
I0109 20:06:25.316016  3609 net.cpp:435] pool1 <- relu2
I0109 20:06:25.316045  3609 net.cpp:409] pool1 -> pool1
I0109 20:06:25.316126  3609 net.cpp:144] Setting up pool1
I0109 20:06:25.316265  3609 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:06:25.316288  3609 net.cpp:159] Memory required for data: 41575200
I0109 20:06:25.316309  3609 layer_factory.hpp:77] Creating layer drop1
I0109 20:06:25.316334  3609 net.cpp:94] Creating Layer drop1
I0109 20:06:25.316356  3609 net.cpp:435] drop1 <- pool1
I0109 20:06:25.316382  3609 net.cpp:409] drop1 -> drop1
I0109 20:06:25.316457  3609 net.cpp:144] Setting up drop1
I0109 20:06:25.316484  3609 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:06:25.316505  3609 net.cpp:159] Memory required for data: 43213600
I0109 20:06:25.316526  3609 layer_factory.hpp:77] Creating layer conv3
I0109 20:06:25.316556  3609 net.cpp:94] Creating Layer conv3
I0109 20:06:25.316579  3609 net.cpp:435] conv3 <- drop1
I0109 20:06:25.316606  3609 net.cpp:409] conv3 -> conv3
I0109 20:06:25.317914  3609 net.cpp:144] Setting up conv3
I0109 20:06:25.317939  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.317946  3609 net.cpp:159] Memory required for data: 46490400
I0109 20:06:25.317958  3609 layer_factory.hpp:77] Creating layer bn3
I0109 20:06:25.317975  3609 net.cpp:94] Creating Layer bn3
I0109 20:06:25.317999  3609 net.cpp:435] bn3 <- conv3
I0109 20:06:25.318028  3609 net.cpp:409] bn3 -> scale3
I0109 20:06:25.318828  3609 net.cpp:144] Setting up bn3
I0109 20:06:25.318853  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.318861  3609 net.cpp:159] Memory required for data: 49767200
I0109 20:06:25.318881  3609 layer_factory.hpp:77] Creating layer relu3
I0109 20:06:25.318908  3609 net.cpp:94] Creating Layer relu3
I0109 20:06:25.318934  3609 net.cpp:435] relu3 <- scale3
I0109 20:06:25.318959  3609 net.cpp:409] relu3 -> relu3
I0109 20:06:25.319080  3609 net.cpp:144] Setting up relu3
I0109 20:06:25.319103  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.319111  3609 net.cpp:159] Memory required for data: 53044000
I0109 20:06:25.319118  3609 layer_factory.hpp:77] Creating layer conv4
I0109 20:06:25.319136  3609 net.cpp:94] Creating Layer conv4
I0109 20:06:25.319159  3609 net.cpp:435] conv4 <- relu3
I0109 20:06:25.319175  3609 net.cpp:409] conv4 -> conv4
I0109 20:06:25.319674  3609 net.cpp:144] Setting up conv4
I0109 20:06:25.319699  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.319705  3609 net.cpp:159] Memory required for data: 56320800
I0109 20:06:25.319717  3609 layer_factory.hpp:77] Creating layer bn4
I0109 20:06:25.319743  3609 net.cpp:94] Creating Layer bn4
I0109 20:06:25.319754  3609 net.cpp:435] bn4 <- conv4
I0109 20:06:25.319766  3609 net.cpp:409] bn4 -> scale4
I0109 20:06:25.320693  3609 net.cpp:144] Setting up bn4
I0109 20:06:25.320710  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.320717  3609 net.cpp:159] Memory required for data: 59597600
I0109 20:06:25.320734  3609 layer_factory.hpp:77] Creating layer relu4
I0109 20:06:25.320758  3609 net.cpp:94] Creating Layer relu4
I0109 20:06:25.320767  3609 net.cpp:435] relu4 <- scale4
I0109 20:06:25.320778  3609 net.cpp:409] relu4 -> relu4
I0109 20:06:25.320837  3609 net.cpp:144] Setting up relu4
I0109 20:06:25.320853  3609 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:25.320861  3609 net.cpp:159] Memory required for data: 62874400
I0109 20:06:25.320868  3609 layer_factory.hpp:77] Creating layer pool2
I0109 20:06:25.320880  3609 net.cpp:94] Creating Layer pool2
I0109 20:06:25.320909  3609 net.cpp:435] pool2 <- relu4
I0109 20:06:25.320924  3609 net.cpp:409] pool2 -> pool2
I0109 20:06:25.321014  3609 net.cpp:144] Setting up pool2
I0109 20:06:25.321036  3609 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:06:25.321110  3609 net.cpp:159] Memory required for data: 63693600
I0109 20:06:25.321120  3609 layer_factory.hpp:77] Creating layer drop2
I0109 20:06:25.321132  3609 net.cpp:94] Creating Layer drop2
I0109 20:06:25.321205  3609 net.cpp:435] drop2 <- pool2
I0109 20:06:25.321219  3609 net.cpp:409] drop2 -> drop2
I0109 20:06:25.321331  3609 net.cpp:144] Setting up drop2
I0109 20:06:25.321350  3609 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:06:25.321357  3609 net.cpp:159] Memory required for data: 64512800
I0109 20:06:25.321364  3609 layer_factory.hpp:77] Creating layer fc1
I0109 20:06:25.321379  3609 net.cpp:94] Creating Layer fc1
I0109 20:06:25.321434  3609 net.cpp:435] fc1 <- drop2
I0109 20:06:25.321449  3609 net.cpp:409] fc1 -> fc1
I0109 20:06:25.343456  3609 net.cpp:144] Setting up fc1
I0109 20:06:25.343498  3609 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:25.343508  3609 net.cpp:159] Memory required for data: 64615200
I0109 20:06:25.343524  3609 layer_factory.hpp:77] Creating layer bn5
I0109 20:06:25.343547  3609 net.cpp:94] Creating Layer bn5
I0109 20:06:25.343559  3609 net.cpp:435] bn5 <- fc1
I0109 20:06:25.343576  3609 net.cpp:409] bn5 -> scale5
I0109 20:06:25.344210  3609 net.cpp:144] Setting up bn5
I0109 20:06:25.344231  3609 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:25.344238  3609 net.cpp:159] Memory required for data: 64717600
I0109 20:06:25.344264  3609 layer_factory.hpp:77] Creating layer relu5
I0109 20:06:25.344282  3609 net.cpp:94] Creating Layer relu5
I0109 20:06:25.344292  3609 net.cpp:435] relu5 <- scale5
I0109 20:06:25.344302  3609 net.cpp:409] relu5 -> relu5
I0109 20:06:25.344362  3609 net.cpp:144] Setting up relu5
I0109 20:06:25.344377  3609 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:25.344383  3609 net.cpp:159] Memory required for data: 64820000
I0109 20:06:25.344386  3609 layer_factory.hpp:77] Creating layer drop3
I0109 20:06:25.344395  3609 net.cpp:94] Creating Layer drop3
I0109 20:06:25.344403  3609 net.cpp:435] drop3 <- relu5
I0109 20:06:25.344419  3609 net.cpp:409] drop3 -> drop3
I0109 20:06:25.344478  3609 net.cpp:144] Setting up drop3
I0109 20:06:25.344492  3609 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:25.344496  3609 net.cpp:159] Memory required for data: 64922400
I0109 20:06:25.344501  3609 layer_factory.hpp:77] Creating layer fc2
I0109 20:06:25.344514  3609 net.cpp:94] Creating Layer fc2
I0109 20:06:25.344528  3609 net.cpp:435] fc2 <- drop3
I0109 20:06:25.344542  3609 net.cpp:409] fc2 -> fc2
I0109 20:06:25.344740  3609 net.cpp:144] Setting up fc2
I0109 20:06:25.344756  3609 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:25.344759  3609 net.cpp:159] Memory required for data: 64924400
I0109 20:06:25.344768  3609 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:06:25.344780  3609 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:06:25.344794  3609 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:06:25.344805  3609 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:06:25.344820  3609 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:06:25.344836  3609 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:06:25.344904  3609 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:06:25.344918  3609 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:25.344923  3609 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:25.344928  3609 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:25.344931  3609 net.cpp:159] Memory required for data: 64930400
I0109 20:06:25.344936  3609 layer_factory.hpp:77] Creating layer loss
I0109 20:06:25.344951  3609 net.cpp:94] Creating Layer loss
I0109 20:06:25.344961  3609 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:06:25.344970  3609 net.cpp:435] loss <- label_data_1_split_0
I0109 20:06:25.344980  3609 net.cpp:409] loss -> loss
I0109 20:06:25.344997  3609 layer_factory.hpp:77] Creating layer loss
I0109 20:06:25.345113  3609 net.cpp:144] Setting up loss
I0109 20:06:25.345127  3609 net.cpp:151] Top shape: (1)
I0109 20:06:25.345131  3609 net.cpp:154]     with loss weight 1
I0109 20:06:25.345188  3609 net.cpp:159] Memory required for data: 64930404
I0109 20:06:25.345202  3609 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:06:25.345216  3609 net.cpp:94] Creating Layer accuracy-top1
I0109 20:06:25.345230  3609 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:06:25.345239  3609 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:06:25.345253  3609 net.cpp:409] accuracy-top1 -> top-1
I0109 20:06:25.345271  3609 net.cpp:144] Setting up accuracy-top1
I0109 20:06:25.345286  3609 net.cpp:151] Top shape: (1)
I0109 20:06:25.345293  3609 net.cpp:159] Memory required for data: 64930408
I0109 20:06:25.345300  3609 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:06:25.345311  3609 net.cpp:94] Creating Layer accuracy-top5
I0109 20:06:25.345321  3609 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:06:25.345329  3609 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:06:25.345343  3609 net.cpp:409] accuracy-top5 -> top-5
I0109 20:06:25.345356  3609 net.cpp:144] Setting up accuracy-top5
I0109 20:06:25.345366  3609 net.cpp:151] Top shape: (1)
I0109 20:06:25.345372  3609 net.cpp:159] Memory required for data: 64930412
I0109 20:06:25.345382  3609 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:06:25.345391  3609 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:06:25.345399  3609 net.cpp:220] loss needs backward computation.
I0109 20:06:25.345405  3609 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:06:25.345413  3609 net.cpp:220] fc2 needs backward computation.
I0109 20:06:25.345422  3609 net.cpp:220] drop3 needs backward computation.
I0109 20:06:25.345428  3609 net.cpp:220] relu5 needs backward computation.
I0109 20:06:25.345434  3609 net.cpp:220] bn5 needs backward computation.
I0109 20:06:25.345444  3609 net.cpp:220] fc1 needs backward computation.
I0109 20:06:25.345450  3609 net.cpp:220] drop2 needs backward computation.
I0109 20:06:25.345458  3609 net.cpp:220] pool2 needs backward computation.
I0109 20:06:25.345463  3609 net.cpp:220] relu4 needs backward computation.
I0109 20:06:25.345471  3609 net.cpp:220] bn4 needs backward computation.
I0109 20:06:25.345479  3609 net.cpp:220] conv4 needs backward computation.
I0109 20:06:25.345486  3609 net.cpp:220] relu3 needs backward computation.
I0109 20:06:25.345494  3609 net.cpp:220] bn3 needs backward computation.
I0109 20:06:25.345501  3609 net.cpp:220] conv3 needs backward computation.
I0109 20:06:25.345507  3609 net.cpp:220] drop1 needs backward computation.
I0109 20:06:25.345516  3609 net.cpp:220] pool1 needs backward computation.
I0109 20:06:25.345522  3609 net.cpp:220] relu2 needs backward computation.
I0109 20:06:25.345530  3609 net.cpp:220] bn2 needs backward computation.
I0109 20:06:25.345537  3609 net.cpp:220] conv2 needs backward computation.
I0109 20:06:25.345543  3609 net.cpp:220] relu1 needs backward computation.
I0109 20:06:25.345553  3609 net.cpp:220] bn1 needs backward computation.
I0109 20:06:25.345561  3609 net.cpp:220] conv1 needs backward computation.
I0109 20:06:25.345569  3609 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:06:25.345577  3609 net.cpp:222] data does not need backward computation.
I0109 20:06:25.345628  3609 net.cpp:264] This network produces output loss
I0109 20:06:25.345647  3609 net.cpp:264] This network produces output top-1
I0109 20:06:25.345654  3609 net.cpp:264] This network produces output top-5
I0109 20:06:25.345695  3609 net.cpp:284] Network initialization done.
I0109 20:06:25.352931  3609 caffe_interface.cpp:363] Running for 180 iterations.
I0109 20:06:25.365097  3609 caffe_interface.cpp:125] Batch 0, loss = 0.918952
I0109 20:06:25.365135  3609 caffe_interface.cpp:125] Batch 0, top-1 = 0.8
I0109 20:06:25.365142  3609 caffe_interface.cpp:125] Batch 0, top-5 = 0.98
I0109 20:06:25.371448  3609 caffe_interface.cpp:125] Batch 1, loss = 0.603029
I0109 20:06:25.371474  3609 caffe_interface.cpp:125] Batch 1, top-1 = 0.8
I0109 20:06:25.371480  3609 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 20:06:25.377823  3609 caffe_interface.cpp:125] Batch 2, loss = 1.38348
I0109 20:06:25.377847  3609 caffe_interface.cpp:125] Batch 2, top-1 = 0.66
I0109 20:06:25.377854  3609 caffe_interface.cpp:125] Batch 2, top-5 = 0.94
I0109 20:06:25.384176  3609 caffe_interface.cpp:125] Batch 3, loss = 0.525744
I0109 20:06:25.384202  3609 caffe_interface.cpp:125] Batch 3, top-1 = 0.88
I0109 20:06:25.384207  3609 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 20:06:25.390466  3609 caffe_interface.cpp:125] Batch 4, loss = 0.701125
I0109 20:06:25.390491  3609 caffe_interface.cpp:125] Batch 4, top-1 = 0.76
I0109 20:06:25.390496  3609 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 20:06:25.396760  3609 caffe_interface.cpp:125] Batch 5, loss = 0.86936
I0109 20:06:25.396785  3609 caffe_interface.cpp:125] Batch 5, top-1 = 0.66
I0109 20:06:25.396790  3609 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 20:06:25.403092  3609 caffe_interface.cpp:125] Batch 6, loss = 1.24853
I0109 20:06:25.403117  3609 caffe_interface.cpp:125] Batch 6, top-1 = 0.72
I0109 20:06:25.403125  3609 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 20:06:25.409387  3609 caffe_interface.cpp:125] Batch 7, loss = 0.558308
I0109 20:06:25.409411  3609 caffe_interface.cpp:125] Batch 7, top-1 = 0.74
I0109 20:06:25.409417  3609 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 20:06:25.415714  3609 caffe_interface.cpp:125] Batch 8, loss = 0.872818
I0109 20:06:25.415738  3609 caffe_interface.cpp:125] Batch 8, top-1 = 0.72
I0109 20:06:25.415745  3609 caffe_interface.cpp:125] Batch 8, top-5 = 0.96
I0109 20:06:25.422044  3609 caffe_interface.cpp:125] Batch 9, loss = 0.863085
I0109 20:06:25.422070  3609 caffe_interface.cpp:125] Batch 9, top-1 = 0.76
I0109 20:06:25.422076  3609 caffe_interface.cpp:125] Batch 9, top-5 = 1
I0109 20:06:25.428369  3609 caffe_interface.cpp:125] Batch 10, loss = 0.608874
I0109 20:06:25.428393  3609 caffe_interface.cpp:125] Batch 10, top-1 = 0.8
I0109 20:06:25.428400  3609 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 20:06:25.434689  3609 caffe_interface.cpp:125] Batch 11, loss = 0.401149
I0109 20:06:25.434713  3609 caffe_interface.cpp:125] Batch 11, top-1 = 0.9
I0109 20:06:25.434720  3609 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 20:06:25.441026  3609 caffe_interface.cpp:125] Batch 12, loss = 0.721611
I0109 20:06:25.441051  3609 caffe_interface.cpp:125] Batch 12, top-1 = 0.78
I0109 20:06:25.441056  3609 caffe_interface.cpp:125] Batch 12, top-5 = 0.98
I0109 20:06:25.447350  3609 caffe_interface.cpp:125] Batch 13, loss = 0.427342
I0109 20:06:25.447373  3609 caffe_interface.cpp:125] Batch 13, top-1 = 0.8
I0109 20:06:25.447379  3609 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0109 20:06:25.453698  3609 caffe_interface.cpp:125] Batch 14, loss = 0.500527
I0109 20:06:25.453722  3609 caffe_interface.cpp:125] Batch 14, top-1 = 0.86
I0109 20:06:25.453728  3609 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0109 20:06:25.460013  3609 caffe_interface.cpp:125] Batch 15, loss = 0.837786
I0109 20:06:25.460036  3609 caffe_interface.cpp:125] Batch 15, top-1 = 0.8
I0109 20:06:25.460042  3609 caffe_interface.cpp:125] Batch 15, top-5 = 0.96
I0109 20:06:25.466323  3609 caffe_interface.cpp:125] Batch 16, loss = 0.929366
I0109 20:06:25.466348  3609 caffe_interface.cpp:125] Batch 16, top-1 = 0.78
I0109 20:06:25.466353  3609 caffe_interface.cpp:125] Batch 16, top-5 = 0.94
I0109 20:06:25.472543  3609 caffe_interface.cpp:125] Batch 17, loss = 0.690274
I0109 20:06:25.472568  3609 caffe_interface.cpp:125] Batch 17, top-1 = 0.82
I0109 20:06:25.472574  3609 caffe_interface.cpp:125] Batch 17, top-5 = 0.96
I0109 20:06:25.478796  3609 caffe_interface.cpp:125] Batch 18, loss = 0.852251
I0109 20:06:25.478819  3609 caffe_interface.cpp:125] Batch 18, top-1 = 0.78
I0109 20:06:25.478826  3609 caffe_interface.cpp:125] Batch 18, top-5 = 0.98
I0109 20:06:25.485023  3609 caffe_interface.cpp:125] Batch 19, loss = 0.620968
I0109 20:06:25.485046  3609 caffe_interface.cpp:125] Batch 19, top-1 = 0.8
I0109 20:06:25.485052  3609 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0109 20:06:25.491248  3609 caffe_interface.cpp:125] Batch 20, loss = 0.578493
I0109 20:06:25.491272  3609 caffe_interface.cpp:125] Batch 20, top-1 = 0.8
I0109 20:06:25.491278  3609 caffe_interface.cpp:125] Batch 20, top-5 = 0.96
I0109 20:06:25.497468  3609 caffe_interface.cpp:125] Batch 21, loss = 1.53872
I0109 20:06:25.497493  3609 caffe_interface.cpp:125] Batch 21, top-1 = 0.7
I0109 20:06:25.497498  3609 caffe_interface.cpp:125] Batch 21, top-5 = 0.96
I0109 20:06:25.503705  3609 caffe_interface.cpp:125] Batch 22, loss = 0.990181
I0109 20:06:25.503728  3609 caffe_interface.cpp:125] Batch 22, top-1 = 0.72
I0109 20:06:25.503736  3609 caffe_interface.cpp:125] Batch 22, top-5 = 0.96
I0109 20:06:25.509908  3609 caffe_interface.cpp:125] Batch 23, loss = 0.623555
I0109 20:06:25.509932  3609 caffe_interface.cpp:125] Batch 23, top-1 = 0.8
I0109 20:06:25.509938  3609 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 20:06:25.516140  3609 caffe_interface.cpp:125] Batch 24, loss = 0.768103
I0109 20:06:25.516165  3609 caffe_interface.cpp:125] Batch 24, top-1 = 0.78
I0109 20:06:25.516170  3609 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0109 20:06:25.522377  3609 caffe_interface.cpp:125] Batch 25, loss = 1.19401
I0109 20:06:25.522403  3609 caffe_interface.cpp:125] Batch 25, top-1 = 0.76
I0109 20:06:25.522408  3609 caffe_interface.cpp:125] Batch 25, top-5 = 0.94
I0109 20:06:25.528586  3609 caffe_interface.cpp:125] Batch 26, loss = 0.957996
I0109 20:06:25.528610  3609 caffe_interface.cpp:125] Batch 26, top-1 = 0.76
I0109 20:06:25.528617  3609 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0109 20:06:25.534813  3609 caffe_interface.cpp:125] Batch 27, loss = 0.570588
I0109 20:06:25.534837  3609 caffe_interface.cpp:125] Batch 27, top-1 = 0.86
I0109 20:06:25.534843  3609 caffe_interface.cpp:125] Batch 27, top-5 = 0.98
I0109 20:06:25.541051  3609 caffe_interface.cpp:125] Batch 28, loss = 0.605195
I0109 20:06:25.541075  3609 caffe_interface.cpp:125] Batch 28, top-1 = 0.84
I0109 20:06:25.541082  3609 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0109 20:06:25.547260  3609 caffe_interface.cpp:125] Batch 29, loss = 0.418731
I0109 20:06:25.547284  3609 caffe_interface.cpp:125] Batch 29, top-1 = 0.88
I0109 20:06:25.547291  3609 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 20:06:25.553477  3609 caffe_interface.cpp:125] Batch 30, loss = 0.53008
I0109 20:06:25.553499  3609 caffe_interface.cpp:125] Batch 30, top-1 = 0.8
I0109 20:06:25.553506  3609 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0109 20:06:25.559742  3609 caffe_interface.cpp:125] Batch 31, loss = 0.702986
I0109 20:06:25.559765  3609 caffe_interface.cpp:125] Batch 31, top-1 = 0.74
I0109 20:06:25.559772  3609 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0109 20:06:25.565959  3609 caffe_interface.cpp:125] Batch 32, loss = 1.00318
I0109 20:06:25.565984  3609 caffe_interface.cpp:125] Batch 32, top-1 = 0.78
I0109 20:06:25.565989  3609 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 20:06:25.572198  3609 caffe_interface.cpp:125] Batch 33, loss = 0.733243
I0109 20:06:25.572227  3609 caffe_interface.cpp:125] Batch 33, top-1 = 0.76
I0109 20:06:25.572232  3609 caffe_interface.cpp:125] Batch 33, top-5 = 0.98
I0109 20:06:25.578415  3609 caffe_interface.cpp:125] Batch 34, loss = 0.815679
I0109 20:06:25.578439  3609 caffe_interface.cpp:125] Batch 34, top-1 = 0.78
I0109 20:06:25.578446  3609 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0109 20:06:25.584647  3609 caffe_interface.cpp:125] Batch 35, loss = 0.648137
I0109 20:06:25.584671  3609 caffe_interface.cpp:125] Batch 35, top-1 = 0.78
I0109 20:06:25.584677  3609 caffe_interface.cpp:125] Batch 35, top-5 = 0.98
I0109 20:06:25.590842  3609 caffe_interface.cpp:125] Batch 36, loss = 0.817208
I0109 20:06:25.590865  3609 caffe_interface.cpp:125] Batch 36, top-1 = 0.82
I0109 20:06:25.590871  3609 caffe_interface.cpp:125] Batch 36, top-5 = 0.96
I0109 20:06:25.597018  3609 caffe_interface.cpp:125] Batch 37, loss = 0.645998
I0109 20:06:25.597043  3609 caffe_interface.cpp:125] Batch 37, top-1 = 0.72
I0109 20:06:25.597074  3609 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0109 20:06:25.603277  3609 caffe_interface.cpp:125] Batch 38, loss = 0.914101
I0109 20:06:25.603302  3609 caffe_interface.cpp:125] Batch 38, top-1 = 0.8
I0109 20:06:25.603308  3609 caffe_interface.cpp:125] Batch 38, top-5 = 0.94
I0109 20:06:25.609491  3609 caffe_interface.cpp:125] Batch 39, loss = 0.443745
I0109 20:06:25.609514  3609 caffe_interface.cpp:125] Batch 39, top-1 = 0.88
I0109 20:06:25.609520  3609 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 20:06:25.615718  3609 caffe_interface.cpp:125] Batch 40, loss = 0.802344
I0109 20:06:25.615741  3609 caffe_interface.cpp:125] Batch 40, top-1 = 0.8
I0109 20:06:25.615747  3609 caffe_interface.cpp:125] Batch 40, top-5 = 0.96
I0109 20:06:25.621918  3609 caffe_interface.cpp:125] Batch 41, loss = 0.890586
I0109 20:06:25.621942  3609 caffe_interface.cpp:125] Batch 41, top-1 = 0.7
I0109 20:06:25.621948  3609 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 20:06:25.628129  3609 caffe_interface.cpp:125] Batch 42, loss = 0.737468
I0109 20:06:25.628152  3609 caffe_interface.cpp:125] Batch 42, top-1 = 0.7
I0109 20:06:25.628159  3609 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0109 20:06:25.634299  3609 caffe_interface.cpp:125] Batch 43, loss = 0.53174
I0109 20:06:25.634325  3609 caffe_interface.cpp:125] Batch 43, top-1 = 0.82
I0109 20:06:25.634330  3609 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0109 20:06:25.639869  3609 caffe_interface.cpp:125] Batch 44, loss = 0.567701
I0109 20:06:25.639894  3609 caffe_interface.cpp:125] Batch 44, top-1 = 0.86
I0109 20:06:25.639900  3609 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0109 20:06:25.645442  3609 caffe_interface.cpp:125] Batch 45, loss = 1.02614
I0109 20:06:25.645467  3609 caffe_interface.cpp:125] Batch 45, top-1 = 0.76
I0109 20:06:25.645473  3609 caffe_interface.cpp:125] Batch 45, top-5 = 0.96
I0109 20:06:25.651031  3609 caffe_interface.cpp:125] Batch 46, loss = 0.703344
I0109 20:06:25.651054  3609 caffe_interface.cpp:125] Batch 46, top-1 = 0.78
I0109 20:06:25.651062  3609 caffe_interface.cpp:125] Batch 46, top-5 = 0.94
I0109 20:06:25.656616  3609 caffe_interface.cpp:125] Batch 47, loss = 0.935956
I0109 20:06:25.656641  3609 caffe_interface.cpp:125] Batch 47, top-1 = 0.78
I0109 20:06:25.656646  3609 caffe_interface.cpp:125] Batch 47, top-5 = 0.96
I0109 20:06:25.662191  3609 caffe_interface.cpp:125] Batch 48, loss = 0.695768
I0109 20:06:25.662216  3609 caffe_interface.cpp:125] Batch 48, top-1 = 0.86
I0109 20:06:25.662222  3609 caffe_interface.cpp:125] Batch 48, top-5 = 0.96
I0109 20:06:25.667770  3609 caffe_interface.cpp:125] Batch 49, loss = 0.807153
I0109 20:06:25.667795  3609 caffe_interface.cpp:125] Batch 49, top-1 = 0.7
I0109 20:06:25.667801  3609 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0109 20:06:25.673336  3609 caffe_interface.cpp:125] Batch 50, loss = 0.765512
I0109 20:06:25.673359  3609 caffe_interface.cpp:125] Batch 50, top-1 = 0.78
I0109 20:06:25.673367  3609 caffe_interface.cpp:125] Batch 50, top-5 = 0.98
I0109 20:06:25.678921  3609 caffe_interface.cpp:125] Batch 51, loss = 0.997166
I0109 20:06:25.678946  3609 caffe_interface.cpp:125] Batch 51, top-1 = 0.78
I0109 20:06:25.678951  3609 caffe_interface.cpp:125] Batch 51, top-5 = 0.94
I0109 20:06:25.684515  3609 caffe_interface.cpp:125] Batch 52, loss = 0.774126
I0109 20:06:25.684540  3609 caffe_interface.cpp:125] Batch 52, top-1 = 0.78
I0109 20:06:25.684545  3609 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0109 20:06:25.690099  3609 caffe_interface.cpp:125] Batch 53, loss = 0.772303
I0109 20:06:25.690124  3609 caffe_interface.cpp:125] Batch 53, top-1 = 0.72
I0109 20:06:25.690130  3609 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0109 20:06:25.695680  3609 caffe_interface.cpp:125] Batch 54, loss = 0.829968
I0109 20:06:25.695704  3609 caffe_interface.cpp:125] Batch 54, top-1 = 0.8
I0109 20:06:25.695711  3609 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0109 20:06:25.701244  3609 caffe_interface.cpp:125] Batch 55, loss = 0.54321
I0109 20:06:25.701268  3609 caffe_interface.cpp:125] Batch 55, top-1 = 0.82
I0109 20:06:25.701301  3609 caffe_interface.cpp:125] Batch 55, top-5 = 0.98
I0109 20:06:25.706856  3609 caffe_interface.cpp:125] Batch 56, loss = 0.623682
I0109 20:06:25.706881  3609 caffe_interface.cpp:125] Batch 56, top-1 = 0.8
I0109 20:06:25.706887  3609 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0109 20:06:25.712431  3609 caffe_interface.cpp:125] Batch 57, loss = 0.684115
I0109 20:06:25.712456  3609 caffe_interface.cpp:125] Batch 57, top-1 = 0.8
I0109 20:06:25.712462  3609 caffe_interface.cpp:125] Batch 57, top-5 = 0.96
I0109 20:06:25.718037  3609 caffe_interface.cpp:125] Batch 58, loss = 0.67463
I0109 20:06:25.718063  3609 caffe_interface.cpp:125] Batch 58, top-1 = 0.78
I0109 20:06:25.718070  3609 caffe_interface.cpp:125] Batch 58, top-5 = 0.96
I0109 20:06:25.723624  3609 caffe_interface.cpp:125] Batch 59, loss = 0.697201
I0109 20:06:25.723649  3609 caffe_interface.cpp:125] Batch 59, top-1 = 0.8
I0109 20:06:25.723655  3609 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0109 20:06:25.729205  3609 caffe_interface.cpp:125] Batch 60, loss = 0.777521
I0109 20:06:25.729230  3609 caffe_interface.cpp:125] Batch 60, top-1 = 0.8
I0109 20:06:25.729236  3609 caffe_interface.cpp:125] Batch 60, top-5 = 0.98
I0109 20:06:25.734776  3609 caffe_interface.cpp:125] Batch 61, loss = 0.455544
I0109 20:06:25.734800  3609 caffe_interface.cpp:125] Batch 61, top-1 = 0.84
I0109 20:06:25.734807  3609 caffe_interface.cpp:125] Batch 61, top-5 = 1
I0109 20:06:25.740357  3609 caffe_interface.cpp:125] Batch 62, loss = 0.309768
I0109 20:06:25.740382  3609 caffe_interface.cpp:125] Batch 62, top-1 = 0.88
I0109 20:06:25.740391  3609 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0109 20:06:25.745954  3609 caffe_interface.cpp:125] Batch 63, loss = 0.743326
I0109 20:06:25.745988  3609 caffe_interface.cpp:125] Batch 63, top-1 = 0.78
I0109 20:06:25.745995  3609 caffe_interface.cpp:125] Batch 63, top-5 = 1
I0109 20:06:25.751557  3609 caffe_interface.cpp:125] Batch 64, loss = 0.429885
I0109 20:06:25.751586  3609 caffe_interface.cpp:125] Batch 64, top-1 = 0.82
I0109 20:06:25.751592  3609 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0109 20:06:25.757128  3609 caffe_interface.cpp:125] Batch 65, loss = 0.568835
I0109 20:06:25.757153  3609 caffe_interface.cpp:125] Batch 65, top-1 = 0.84
I0109 20:06:25.757160  3609 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 20:06:25.762687  3609 caffe_interface.cpp:125] Batch 66, loss = 0.625525
I0109 20:06:25.762712  3609 caffe_interface.cpp:125] Batch 66, top-1 = 0.8
I0109 20:06:25.762718  3609 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0109 20:06:25.768263  3609 caffe_interface.cpp:125] Batch 67, loss = 0.489186
I0109 20:06:25.768288  3609 caffe_interface.cpp:125] Batch 67, top-1 = 0.84
I0109 20:06:25.768294  3609 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 20:06:25.773919  3609 caffe_interface.cpp:125] Batch 68, loss = 1.25254
I0109 20:06:25.773943  3609 caffe_interface.cpp:125] Batch 68, top-1 = 0.68
I0109 20:06:25.773949  3609 caffe_interface.cpp:125] Batch 68, top-5 = 0.92
I0109 20:06:25.779510  3609 caffe_interface.cpp:125] Batch 69, loss = 1.13317
I0109 20:06:25.779536  3609 caffe_interface.cpp:125] Batch 69, top-1 = 0.66
I0109 20:06:25.779541  3609 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0109 20:06:25.785069  3609 caffe_interface.cpp:125] Batch 70, loss = 0.930407
I0109 20:06:25.785094  3609 caffe_interface.cpp:125] Batch 70, top-1 = 0.8
I0109 20:06:25.785100  3609 caffe_interface.cpp:125] Batch 70, top-5 = 0.96
I0109 20:06:25.790645  3609 caffe_interface.cpp:125] Batch 71, loss = 0.572801
I0109 20:06:25.790669  3609 caffe_interface.cpp:125] Batch 71, top-1 = 0.82
I0109 20:06:25.790676  3609 caffe_interface.cpp:125] Batch 71, top-5 = 0.98
I0109 20:06:25.796196  3609 caffe_interface.cpp:125] Batch 72, loss = 0.777983
I0109 20:06:25.796221  3609 caffe_interface.cpp:125] Batch 72, top-1 = 0.78
I0109 20:06:25.796227  3609 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 20:06:25.801731  3609 caffe_interface.cpp:125] Batch 73, loss = 0.999849
I0109 20:06:25.801777  3609 caffe_interface.cpp:125] Batch 73, top-1 = 0.7
I0109 20:06:25.801784  3609 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0109 20:06:25.806917  3609 caffe_interface.cpp:125] Batch 74, loss = 0.557217
I0109 20:06:25.806941  3609 caffe_interface.cpp:125] Batch 74, top-1 = 0.86
I0109 20:06:25.806947  3609 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 20:06:25.812085  3609 caffe_interface.cpp:125] Batch 75, loss = 0.63598
I0109 20:06:25.812110  3609 caffe_interface.cpp:125] Batch 75, top-1 = 0.84
I0109 20:06:25.812116  3609 caffe_interface.cpp:125] Batch 75, top-5 = 0.94
I0109 20:06:25.817255  3609 caffe_interface.cpp:125] Batch 76, loss = 0.752333
I0109 20:06:25.817281  3609 caffe_interface.cpp:125] Batch 76, top-1 = 0.74
I0109 20:06:25.817286  3609 caffe_interface.cpp:125] Batch 76, top-5 = 1
I0109 20:06:25.822453  3609 caffe_interface.cpp:125] Batch 77, loss = 0.565469
I0109 20:06:25.822482  3609 caffe_interface.cpp:125] Batch 77, top-1 = 0.84
I0109 20:06:25.822489  3609 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0109 20:06:25.827659  3609 caffe_interface.cpp:125] Batch 78, loss = 1.0012
I0109 20:06:25.827702  3609 caffe_interface.cpp:125] Batch 78, top-1 = 0.72
I0109 20:06:25.827709  3609 caffe_interface.cpp:125] Batch 78, top-5 = 0.98
I0109 20:06:25.832902  3609 caffe_interface.cpp:125] Batch 79, loss = 1.00978
I0109 20:06:25.832936  3609 caffe_interface.cpp:125] Batch 79, top-1 = 0.8
I0109 20:06:25.832942  3609 caffe_interface.cpp:125] Batch 79, top-5 = 0.92
I0109 20:06:25.838140  3609 caffe_interface.cpp:125] Batch 80, loss = 0.718995
I0109 20:06:25.838176  3609 caffe_interface.cpp:125] Batch 80, top-1 = 0.82
I0109 20:06:25.838182  3609 caffe_interface.cpp:125] Batch 80, top-5 = 0.96
I0109 20:06:25.843389  3609 caffe_interface.cpp:125] Batch 81, loss = 1.08436
I0109 20:06:25.843426  3609 caffe_interface.cpp:125] Batch 81, top-1 = 0.72
I0109 20:06:25.843433  3609 caffe_interface.cpp:125] Batch 81, top-5 = 0.94
I0109 20:06:25.848630  3609 caffe_interface.cpp:125] Batch 82, loss = 0.534409
I0109 20:06:25.848661  3609 caffe_interface.cpp:125] Batch 82, top-1 = 0.82
I0109 20:06:25.848668  3609 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0109 20:06:25.853859  3609 caffe_interface.cpp:125] Batch 83, loss = 0.621829
I0109 20:06:25.853888  3609 caffe_interface.cpp:125] Batch 83, top-1 = 0.82
I0109 20:06:25.853893  3609 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0109 20:06:25.859077  3609 caffe_interface.cpp:125] Batch 84, loss = 0.783178
I0109 20:06:25.859114  3609 caffe_interface.cpp:125] Batch 84, top-1 = 0.76
I0109 20:06:25.859120  3609 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 20:06:25.864353  3609 caffe_interface.cpp:125] Batch 85, loss = 0.736917
I0109 20:06:25.864392  3609 caffe_interface.cpp:125] Batch 85, top-1 = 0.76
I0109 20:06:25.864398  3609 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0109 20:06:25.869621  3609 caffe_interface.cpp:125] Batch 86, loss = 0.919438
I0109 20:06:25.869658  3609 caffe_interface.cpp:125] Batch 86, top-1 = 0.7
I0109 20:06:25.869665  3609 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0109 20:06:25.874846  3609 caffe_interface.cpp:125] Batch 87, loss = 0.803816
I0109 20:06:25.874871  3609 caffe_interface.cpp:125] Batch 87, top-1 = 0.74
I0109 20:06:25.874877  3609 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0109 20:06:25.880070  3609 caffe_interface.cpp:125] Batch 88, loss = 0.963243
I0109 20:06:25.880095  3609 caffe_interface.cpp:125] Batch 88, top-1 = 0.68
I0109 20:06:25.880101  3609 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0109 20:06:25.885246  3609 caffe_interface.cpp:125] Batch 89, loss = 0.776962
I0109 20:06:25.885279  3609 caffe_interface.cpp:125] Batch 89, top-1 = 0.78
I0109 20:06:25.885287  3609 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0109 20:06:25.890487  3609 caffe_interface.cpp:125] Batch 90, loss = 0.367319
I0109 20:06:25.890523  3609 caffe_interface.cpp:125] Batch 90, top-1 = 0.86
I0109 20:06:25.890530  3609 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0109 20:06:25.895661  3609 caffe_interface.cpp:125] Batch 91, loss = 0.487939
I0109 20:06:25.895706  3609 caffe_interface.cpp:125] Batch 91, top-1 = 0.78
I0109 20:06:25.895714  3609 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 20:06:25.900857  3609 caffe_interface.cpp:125] Batch 92, loss = 0.756591
I0109 20:06:25.900880  3609 caffe_interface.cpp:125] Batch 92, top-1 = 0.68
I0109 20:06:25.900887  3609 caffe_interface.cpp:125] Batch 92, top-5 = 0.98
I0109 20:06:25.906016  3609 caffe_interface.cpp:125] Batch 93, loss = 0.489576
I0109 20:06:25.906044  3609 caffe_interface.cpp:125] Batch 93, top-1 = 0.8
I0109 20:06:25.906054  3609 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0109 20:06:25.911190  3609 caffe_interface.cpp:125] Batch 94, loss = 1.19999
I0109 20:06:25.911217  3609 caffe_interface.cpp:125] Batch 94, top-1 = 0.62
I0109 20:06:25.911228  3609 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0109 20:06:25.916352  3609 caffe_interface.cpp:125] Batch 95, loss = 0.82692
I0109 20:06:25.916375  3609 caffe_interface.cpp:125] Batch 95, top-1 = 0.7
I0109 20:06:25.916386  3609 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 20:06:25.921525  3609 caffe_interface.cpp:125] Batch 96, loss = 0.552266
I0109 20:06:25.921550  3609 caffe_interface.cpp:125] Batch 96, top-1 = 0.78
I0109 20:06:25.921561  3609 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 20:06:25.926697  3609 caffe_interface.cpp:125] Batch 97, loss = 0.911423
I0109 20:06:25.926721  3609 caffe_interface.cpp:125] Batch 97, top-1 = 0.74
I0109 20:06:25.926731  3609 caffe_interface.cpp:125] Batch 97, top-5 = 0.98
I0109 20:06:25.931864  3609 caffe_interface.cpp:125] Batch 98, loss = 0.922805
I0109 20:06:25.931888  3609 caffe_interface.cpp:125] Batch 98, top-1 = 0.7
I0109 20:06:25.931900  3609 caffe_interface.cpp:125] Batch 98, top-5 = 0.98
I0109 20:06:25.937002  3609 caffe_interface.cpp:125] Batch 99, loss = 0.736328
I0109 20:06:25.937026  3609 caffe_interface.cpp:125] Batch 99, top-1 = 0.76
I0109 20:06:25.937036  3609 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0109 20:06:25.942167  3609 caffe_interface.cpp:125] Batch 100, loss = 0.44874
I0109 20:06:25.942191  3609 caffe_interface.cpp:125] Batch 100, top-1 = 0.84
I0109 20:06:25.942201  3609 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0109 20:06:25.947316  3609 caffe_interface.cpp:125] Batch 101, loss = 0.962617
I0109 20:06:25.947342  3609 caffe_interface.cpp:125] Batch 101, top-1 = 0.72
I0109 20:06:25.947352  3609 caffe_interface.cpp:125] Batch 101, top-5 = 0.96
I0109 20:06:25.952497  3609 caffe_interface.cpp:125] Batch 102, loss = 0.482909
I0109 20:06:25.952522  3609 caffe_interface.cpp:125] Batch 102, top-1 = 0.82
I0109 20:06:25.952533  3609 caffe_interface.cpp:125] Batch 102, top-5 = 1
I0109 20:06:25.957656  3609 caffe_interface.cpp:125] Batch 103, loss = 0.645126
I0109 20:06:25.957679  3609 caffe_interface.cpp:125] Batch 103, top-1 = 0.78
I0109 20:06:25.957687  3609 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 20:06:25.962843  3609 caffe_interface.cpp:125] Batch 104, loss = 0.527896
I0109 20:06:25.962869  3609 caffe_interface.cpp:125] Batch 104, top-1 = 0.82
I0109 20:06:25.962880  3609 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 20:06:25.967996  3609 caffe_interface.cpp:125] Batch 105, loss = 0.448662
I0109 20:06:25.968022  3609 caffe_interface.cpp:125] Batch 105, top-1 = 0.86
I0109 20:06:25.968031  3609 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0109 20:06:25.972848  3609 caffe_interface.cpp:125] Batch 106, loss = 0.932607
I0109 20:06:25.972875  3609 caffe_interface.cpp:125] Batch 106, top-1 = 0.74
I0109 20:06:25.972887  3609 caffe_interface.cpp:125] Batch 106, top-5 = 0.96
I0109 20:06:25.977658  3609 caffe_interface.cpp:125] Batch 107, loss = 0.78661
I0109 20:06:25.977679  3609 caffe_interface.cpp:125] Batch 107, top-1 = 0.78
I0109 20:06:25.977689  3609 caffe_interface.cpp:125] Batch 107, top-5 = 0.98
I0109 20:06:25.982431  3609 caffe_interface.cpp:125] Batch 108, loss = 0.81078
I0109 20:06:25.982455  3609 caffe_interface.cpp:125] Batch 108, top-1 = 0.76
I0109 20:06:25.982465  3609 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0109 20:06:25.987217  3609 caffe_interface.cpp:125] Batch 109, loss = 0.617469
I0109 20:06:25.987258  3609 caffe_interface.cpp:125] Batch 109, top-1 = 0.8
I0109 20:06:25.987270  3609 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0109 20:06:25.992038  3609 caffe_interface.cpp:125] Batch 110, loss = 0.612238
I0109 20:06:25.992063  3609 caffe_interface.cpp:125] Batch 110, top-1 = 0.86
I0109 20:06:25.992074  3609 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 20:06:25.996789  3609 caffe_interface.cpp:125] Batch 111, loss = 0.510883
I0109 20:06:25.996814  3609 caffe_interface.cpp:125] Batch 111, top-1 = 0.8
I0109 20:06:25.996824  3609 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0109 20:06:26.001576  3609 caffe_interface.cpp:125] Batch 112, loss = 0.834181
I0109 20:06:26.001611  3609 caffe_interface.cpp:125] Batch 112, top-1 = 0.72
I0109 20:06:26.001623  3609 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0109 20:06:26.006384  3609 caffe_interface.cpp:125] Batch 113, loss = 0.922036
I0109 20:06:26.006408  3609 caffe_interface.cpp:125] Batch 113, top-1 = 0.68
I0109 20:06:26.006418  3609 caffe_interface.cpp:125] Batch 113, top-5 = 0.96
I0109 20:06:26.011170  3609 caffe_interface.cpp:125] Batch 114, loss = 0.984058
I0109 20:06:26.011194  3609 caffe_interface.cpp:125] Batch 114, top-1 = 0.7
I0109 20:06:26.011205  3609 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0109 20:06:26.015962  3609 caffe_interface.cpp:125] Batch 115, loss = 0.755988
I0109 20:06:26.015985  3609 caffe_interface.cpp:125] Batch 115, top-1 = 0.78
I0109 20:06:26.015995  3609 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0109 20:06:26.020750  3609 caffe_interface.cpp:125] Batch 116, loss = 1.00758
I0109 20:06:26.020772  3609 caffe_interface.cpp:125] Batch 116, top-1 = 0.68
I0109 20:06:26.020782  3609 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0109 20:06:26.025559  3609 caffe_interface.cpp:125] Batch 117, loss = 0.630765
I0109 20:06:26.025619  3609 caffe_interface.cpp:125] Batch 117, top-1 = 0.78
I0109 20:06:26.025629  3609 caffe_interface.cpp:125] Batch 117, top-5 = 0.96
I0109 20:06:26.030395  3609 caffe_interface.cpp:125] Batch 118, loss = 1.15904
I0109 20:06:26.030418  3609 caffe_interface.cpp:125] Batch 118, top-1 = 0.64
I0109 20:06:26.030429  3609 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0109 20:06:26.035195  3609 caffe_interface.cpp:125] Batch 119, loss = 0.38948
I0109 20:06:26.035218  3609 caffe_interface.cpp:125] Batch 119, top-1 = 0.84
I0109 20:06:26.035229  3609 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 20:06:26.039978  3609 caffe_interface.cpp:125] Batch 120, loss = 0.926571
I0109 20:06:26.040001  3609 caffe_interface.cpp:125] Batch 120, top-1 = 0.7
I0109 20:06:26.040012  3609 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0109 20:06:26.044773  3609 caffe_interface.cpp:125] Batch 121, loss = 0.856202
I0109 20:06:26.044797  3609 caffe_interface.cpp:125] Batch 121, top-1 = 0.78
I0109 20:06:26.044808  3609 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 20:06:26.049551  3609 caffe_interface.cpp:125] Batch 122, loss = 0.821378
I0109 20:06:26.049573  3609 caffe_interface.cpp:125] Batch 122, top-1 = 0.74
I0109 20:06:26.049585  3609 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0109 20:06:26.054375  3609 caffe_interface.cpp:125] Batch 123, loss = 0.804896
I0109 20:06:26.054399  3609 caffe_interface.cpp:125] Batch 123, top-1 = 0.76
I0109 20:06:26.054409  3609 caffe_interface.cpp:125] Batch 123, top-5 = 0.92
I0109 20:06:26.059170  3609 caffe_interface.cpp:125] Batch 124, loss = 0.635268
I0109 20:06:26.059193  3609 caffe_interface.cpp:125] Batch 124, top-1 = 0.78
I0109 20:06:26.059203  3609 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 20:06:26.063963  3609 caffe_interface.cpp:125] Batch 125, loss = 0.733598
I0109 20:06:26.063987  3609 caffe_interface.cpp:125] Batch 125, top-1 = 0.78
I0109 20:06:26.063997  3609 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0109 20:06:26.068768  3609 caffe_interface.cpp:125] Batch 126, loss = 0.778699
I0109 20:06:26.068792  3609 caffe_interface.cpp:125] Batch 126, top-1 = 0.78
I0109 20:06:26.068832  3609 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 20:06:26.073606  3609 caffe_interface.cpp:125] Batch 127, loss = 0.742359
I0109 20:06:26.073628  3609 caffe_interface.cpp:125] Batch 127, top-1 = 0.74
I0109 20:06:26.073638  3609 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 20:06:26.078428  3609 caffe_interface.cpp:125] Batch 128, loss = 1.21017
I0109 20:06:26.078454  3609 caffe_interface.cpp:125] Batch 128, top-1 = 0.74
I0109 20:06:26.078462  3609 caffe_interface.cpp:125] Batch 128, top-5 = 0.94
I0109 20:06:26.083257  3609 caffe_interface.cpp:125] Batch 129, loss = 0.512143
I0109 20:06:26.083283  3609 caffe_interface.cpp:125] Batch 129, top-1 = 0.86
I0109 20:06:26.083294  3609 caffe_interface.cpp:125] Batch 129, top-5 = 0.98
I0109 20:06:26.088052  3609 caffe_interface.cpp:125] Batch 130, loss = 0.790504
I0109 20:06:26.088075  3609 caffe_interface.cpp:125] Batch 130, top-1 = 0.76
I0109 20:06:26.088086  3609 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 20:06:26.092813  3609 caffe_interface.cpp:125] Batch 131, loss = 0.655787
I0109 20:06:26.092836  3609 caffe_interface.cpp:125] Batch 131, top-1 = 0.84
I0109 20:06:26.092846  3609 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0109 20:06:26.097563  3609 caffe_interface.cpp:125] Batch 132, loss = 0.593121
I0109 20:06:26.097599  3609 caffe_interface.cpp:125] Batch 132, top-1 = 0.86
I0109 20:06:26.097611  3609 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0109 20:06:26.102372  3609 caffe_interface.cpp:125] Batch 133, loss = 0.588079
I0109 20:06:26.102397  3609 caffe_interface.cpp:125] Batch 133, top-1 = 0.8
I0109 20:06:26.102407  3609 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0109 20:06:26.107162  3609 caffe_interface.cpp:125] Batch 134, loss = 0.712123
I0109 20:06:26.107185  3609 caffe_interface.cpp:125] Batch 134, top-1 = 0.78
I0109 20:06:26.107197  3609 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0109 20:06:26.111944  3609 caffe_interface.cpp:125] Batch 135, loss = 0.631333
I0109 20:06:26.111968  3609 caffe_interface.cpp:125] Batch 135, top-1 = 0.82
I0109 20:06:26.111979  3609 caffe_interface.cpp:125] Batch 135, top-5 = 0.96
I0109 20:06:26.116732  3609 caffe_interface.cpp:125] Batch 136, loss = 0.514736
I0109 20:06:26.116755  3609 caffe_interface.cpp:125] Batch 136, top-1 = 0.82
I0109 20:06:26.116765  3609 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 20:06:26.121507  3609 caffe_interface.cpp:125] Batch 137, loss = 0.631427
I0109 20:06:26.121531  3609 caffe_interface.cpp:125] Batch 137, top-1 = 0.86
I0109 20:06:26.121541  3609 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 20:06:26.126291  3609 caffe_interface.cpp:125] Batch 138, loss = 0.89151
I0109 20:06:26.126312  3609 caffe_interface.cpp:125] Batch 138, top-1 = 0.78
I0109 20:06:26.126324  3609 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0109 20:06:26.131085  3609 caffe_interface.cpp:125] Batch 139, loss = 0.770585
I0109 20:06:26.131108  3609 caffe_interface.cpp:125] Batch 139, top-1 = 0.78
I0109 20:06:26.131119  3609 caffe_interface.cpp:125] Batch 139, top-5 = 1
I0109 20:06:26.135843  3609 caffe_interface.cpp:125] Batch 140, loss = 0.840533
I0109 20:06:26.135865  3609 caffe_interface.cpp:125] Batch 140, top-1 = 0.74
I0109 20:06:26.135875  3609 caffe_interface.cpp:125] Batch 140, top-5 = 0.98
I0109 20:06:26.140522  3609 caffe_interface.cpp:125] Batch 141, loss = 0.515064
I0109 20:06:26.140547  3609 caffe_interface.cpp:125] Batch 141, top-1 = 0.84
I0109 20:06:26.140556  3609 caffe_interface.cpp:125] Batch 141, top-5 = 0.96
I0109 20:06:26.145051  3609 caffe_interface.cpp:125] Batch 142, loss = 0.606023
I0109 20:06:26.145074  3609 caffe_interface.cpp:125] Batch 142, top-1 = 0.82
I0109 20:06:26.145087  3609 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0109 20:06:26.149569  3609 caffe_interface.cpp:125] Batch 143, loss = 0.686072
I0109 20:06:26.149607  3609 caffe_interface.cpp:125] Batch 143, top-1 = 0.78
I0109 20:06:26.149619  3609 caffe_interface.cpp:125] Batch 143, top-5 = 0.98
I0109 20:06:26.154124  3609 caffe_interface.cpp:125] Batch 144, loss = 0.777067
I0109 20:06:26.154165  3609 caffe_interface.cpp:125] Batch 144, top-1 = 0.78
I0109 20:06:26.154176  3609 caffe_interface.cpp:125] Batch 144, top-5 = 0.96
I0109 20:06:26.158658  3609 caffe_interface.cpp:125] Batch 145, loss = 0.866524
I0109 20:06:26.158682  3609 caffe_interface.cpp:125] Batch 145, top-1 = 0.78
I0109 20:06:26.158692  3609 caffe_interface.cpp:125] Batch 145, top-5 = 0.98
I0109 20:06:26.163156  3609 caffe_interface.cpp:125] Batch 146, loss = 0.883225
I0109 20:06:26.163178  3609 caffe_interface.cpp:125] Batch 146, top-1 = 0.78
I0109 20:06:26.163189  3609 caffe_interface.cpp:125] Batch 146, top-5 = 0.96
I0109 20:06:26.167687  3609 caffe_interface.cpp:125] Batch 147, loss = 0.754482
I0109 20:06:26.167711  3609 caffe_interface.cpp:125] Batch 147, top-1 = 0.72
I0109 20:06:26.167721  3609 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0109 20:06:26.172204  3609 caffe_interface.cpp:125] Batch 148, loss = 0.905296
I0109 20:06:26.172228  3609 caffe_interface.cpp:125] Batch 148, top-1 = 0.8
I0109 20:06:26.172238  3609 caffe_interface.cpp:125] Batch 148, top-5 = 0.98
I0109 20:06:26.176707  3609 caffe_interface.cpp:125] Batch 149, loss = 0.653239
I0109 20:06:26.176731  3609 caffe_interface.cpp:125] Batch 149, top-1 = 0.76
I0109 20:06:26.176743  3609 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0109 20:06:26.181217  3609 caffe_interface.cpp:125] Batch 150, loss = 0.709805
I0109 20:06:26.181241  3609 caffe_interface.cpp:125] Batch 150, top-1 = 0.78
I0109 20:06:26.181252  3609 caffe_interface.cpp:125] Batch 150, top-5 = 0.98
I0109 20:06:26.185734  3609 caffe_interface.cpp:125] Batch 151, loss = 0.589751
I0109 20:06:26.185756  3609 caffe_interface.cpp:125] Batch 151, top-1 = 0.84
I0109 20:06:26.185766  3609 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0109 20:06:26.190259  3609 caffe_interface.cpp:125] Batch 152, loss = 0.709571
I0109 20:06:26.190284  3609 caffe_interface.cpp:125] Batch 152, top-1 = 0.8
I0109 20:06:26.190292  3609 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0109 20:06:26.194727  3609 caffe_interface.cpp:125] Batch 153, loss = 0.530638
I0109 20:06:26.194751  3609 caffe_interface.cpp:125] Batch 153, top-1 = 0.88
I0109 20:06:26.194761  3609 caffe_interface.cpp:125] Batch 153, top-5 = 0.98
I0109 20:06:26.199218  3609 caffe_interface.cpp:125] Batch 154, loss = 0.366649
I0109 20:06:26.199241  3609 caffe_interface.cpp:125] Batch 154, top-1 = 0.84
I0109 20:06:26.199252  3609 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 20:06:26.203728  3609 caffe_interface.cpp:125] Batch 155, loss = 0.275193
I0109 20:06:26.203752  3609 caffe_interface.cpp:125] Batch 155, top-1 = 0.84
I0109 20:06:26.203763  3609 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0109 20:06:26.208235  3609 caffe_interface.cpp:125] Batch 156, loss = 0.682773
I0109 20:06:26.208257  3609 caffe_interface.cpp:125] Batch 156, top-1 = 0.7
I0109 20:06:26.208268  3609 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 20:06:26.212741  3609 caffe_interface.cpp:125] Batch 157, loss = 1.09015
I0109 20:06:26.212765  3609 caffe_interface.cpp:125] Batch 157, top-1 = 0.6
I0109 20:06:26.212776  3609 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0109 20:06:26.217273  3609 caffe_interface.cpp:125] Batch 158, loss = 1.02442
I0109 20:06:26.217295  3609 caffe_interface.cpp:125] Batch 158, top-1 = 0.7
I0109 20:06:26.217308  3609 caffe_interface.cpp:125] Batch 158, top-5 = 0.94
I0109 20:06:26.221815  3609 caffe_interface.cpp:125] Batch 159, loss = 0.514292
I0109 20:06:26.221838  3609 caffe_interface.cpp:125] Batch 159, top-1 = 0.82
I0109 20:06:26.221848  3609 caffe_interface.cpp:125] Batch 159, top-5 = 0.98
I0109 20:06:26.226342  3609 caffe_interface.cpp:125] Batch 160, loss = 1.0889
I0109 20:06:26.226366  3609 caffe_interface.cpp:125] Batch 160, top-1 = 0.7
I0109 20:06:26.226377  3609 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0109 20:06:26.230875  3609 caffe_interface.cpp:125] Batch 161, loss = 0.414907
I0109 20:06:26.230899  3609 caffe_interface.cpp:125] Batch 161, top-1 = 0.84
I0109 20:06:26.230909  3609 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0109 20:06:26.235419  3609 caffe_interface.cpp:125] Batch 162, loss = 0.952922
I0109 20:06:26.235441  3609 caffe_interface.cpp:125] Batch 162, top-1 = 0.8
I0109 20:06:26.235453  3609 caffe_interface.cpp:125] Batch 162, top-5 = 0.96
I0109 20:06:26.239915  3609 caffe_interface.cpp:125] Batch 163, loss = 0.458605
I0109 20:06:26.239939  3609 caffe_interface.cpp:125] Batch 163, top-1 = 0.8
I0109 20:06:26.239949  3609 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0109 20:06:26.244421  3609 caffe_interface.cpp:125] Batch 164, loss = 0.732211
I0109 20:06:26.244446  3609 caffe_interface.cpp:125] Batch 164, top-1 = 0.74
I0109 20:06:26.244455  3609 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0109 20:06:26.248965  3609 caffe_interface.cpp:125] Batch 165, loss = 0.788721
I0109 20:06:26.248991  3609 caffe_interface.cpp:125] Batch 165, top-1 = 0.76
I0109 20:06:26.249001  3609 caffe_interface.cpp:125] Batch 165, top-5 = 0.96
I0109 20:06:26.253465  3609 caffe_interface.cpp:125] Batch 166, loss = 0.631978
I0109 20:06:26.253489  3609 caffe_interface.cpp:125] Batch 166, top-1 = 0.88
I0109 20:06:26.253500  3609 caffe_interface.cpp:125] Batch 166, top-5 = 0.96
I0109 20:06:26.257995  3609 caffe_interface.cpp:125] Batch 167, loss = 0.831861
I0109 20:06:26.258018  3609 caffe_interface.cpp:125] Batch 167, top-1 = 0.68
I0109 20:06:26.258029  3609 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0109 20:06:26.262538  3609 caffe_interface.cpp:125] Batch 168, loss = 0.949777
I0109 20:06:26.262562  3609 caffe_interface.cpp:125] Batch 168, top-1 = 0.7
I0109 20:06:26.262571  3609 caffe_interface.cpp:125] Batch 168, top-5 = 0.96
I0109 20:06:26.267037  3609 caffe_interface.cpp:125] Batch 169, loss = 1.35478
I0109 20:06:26.267060  3609 caffe_interface.cpp:125] Batch 169, top-1 = 0.74
I0109 20:06:26.267071  3609 caffe_interface.cpp:125] Batch 169, top-5 = 0.94
I0109 20:06:26.271545  3609 caffe_interface.cpp:125] Batch 170, loss = 0.468092
I0109 20:06:26.271569  3609 caffe_interface.cpp:125] Batch 170, top-1 = 0.86
I0109 20:06:26.271579  3609 caffe_interface.cpp:125] Batch 170, top-5 = 0.98
I0109 20:06:26.276059  3609 caffe_interface.cpp:125] Batch 171, loss = 0.646518
I0109 20:06:26.276082  3609 caffe_interface.cpp:125] Batch 171, top-1 = 0.8
I0109 20:06:26.276093  3609 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 20:06:26.280573  3609 caffe_interface.cpp:125] Batch 172, loss = 0.556176
I0109 20:06:26.280596  3609 caffe_interface.cpp:125] Batch 172, top-1 = 0.82
I0109 20:06:26.280606  3609 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 20:06:26.285084  3609 caffe_interface.cpp:125] Batch 173, loss = 1.08259
I0109 20:06:26.285107  3609 caffe_interface.cpp:125] Batch 173, top-1 = 0.7
I0109 20:06:26.285118  3609 caffe_interface.cpp:125] Batch 173, top-5 = 0.98
I0109 20:06:26.289603  3609 caffe_interface.cpp:125] Batch 174, loss = 0.952172
I0109 20:06:26.289623  3609 caffe_interface.cpp:125] Batch 174, top-1 = 0.72
I0109 20:06:26.289634  3609 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0109 20:06:26.294088  3609 caffe_interface.cpp:125] Batch 175, loss = 0.538371
I0109 20:06:26.294111  3609 caffe_interface.cpp:125] Batch 175, top-1 = 0.78
I0109 20:06:26.294121  3609 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0109 20:06:26.298573  3609 caffe_interface.cpp:125] Batch 176, loss = 0.923137
I0109 20:06:26.298595  3609 caffe_interface.cpp:125] Batch 176, top-1 = 0.8
I0109 20:06:26.298605  3609 caffe_interface.cpp:125] Batch 176, top-5 = 1
I0109 20:06:26.303099  3609 caffe_interface.cpp:125] Batch 177, loss = 0.617792
I0109 20:06:26.303123  3609 caffe_interface.cpp:125] Batch 177, top-1 = 0.82
I0109 20:06:26.303133  3609 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 20:06:26.307593  3609 caffe_interface.cpp:125] Batch 178, loss = 0.3365
I0109 20:06:26.307616  3609 caffe_interface.cpp:125] Batch 178, top-1 = 0.9
I0109 20:06:26.307628  3609 caffe_interface.cpp:125] Batch 178, top-5 = 1
I0109 20:06:26.312077  3609 caffe_interface.cpp:125] Batch 179, loss = 0.681964
I0109 20:06:26.312099  3609 caffe_interface.cpp:125] Batch 179, top-1 = 0.74
I0109 20:06:26.312139  3609 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0109 20:06:26.312151  3609 caffe_interface.cpp:130] Loss: 0.742979
I0109 20:06:26.312165  3609 caffe_interface.cpp:142] loss = 0.742979 (* 1 = 0.742979 loss)
I0109 20:06:26.312183  3609 caffe_interface.cpp:142] top-1 = 0.778889
I0109 20:06:26.312196  3609 caffe_interface.cpp:142] top-5 = 0.983556
I0109 20:06:26.326716  3609 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/sparse.caffemodel
I0109 20:06:26.326757  3609 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.5:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.778888643    | -0.0861111879  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 39391          | -42.4015579%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 24801792       | -49.4395027%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config5.prototxt
I0109 20:06:26.447293  3950 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0109 20:06:26.561102  3950 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 20:06:26.561707  3950 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 20:06:26.561734  3950 caffe_interface.cpp:493] Using GPUs 0
I0109 20:06:26.562016  3950 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 20:06:27.240643  3950 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt"
type: "SGD"
I0109 20:06:27.240816  3950 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0109 20:06:27.241194  3950 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 20:06:27.241225  3950 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 20:06:27.241230  3950 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 20:06:27.241456  3950 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 20:06:27.241577  3950 layer_factory.hpp:77] Creating layer data
I0109 20:06:27.241760  3950 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:27.242691  3950 net.cpp:94] Creating Layer data
I0109 20:06:27.242723  3950 net.cpp:409] data -> data
I0109 20:06:27.242739  3950 net.cpp:409] data -> label
I0109 20:06:27.243839  3961 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 20:06:27.243898  3961 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 20:06:27.244033  3950 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 20:06:27.244194  3950 data_layer.cpp:83] output data size: 128,3,32,32
I0109 20:06:27.255581  3950 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:27.255697  3950 net.cpp:144] Setting up data
I0109 20:06:27.255722  3950 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 20:06:27.255730  3950 net.cpp:151] Top shape: 128 (128)
I0109 20:06:27.255758  3950 net.cpp:159] Memory required for data: 1573376
I0109 20:06:27.255795  3950 layer_factory.hpp:77] Creating layer conv1
I0109 20:06:27.255857  3950 net.cpp:94] Creating Layer conv1
I0109 20:06:27.255879  3950 net.cpp:435] conv1 <- data
I0109 20:06:27.255924  3950 net.cpp:409] conv1 -> conv1
I0109 20:06:27.257516  3950 net.cpp:144] Setting up conv1
I0109 20:06:27.257541  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.257550  3950 net.cpp:159] Memory required for data: 18350592
I0109 20:06:27.257578  3950 layer_factory.hpp:77] Creating layer bn1
I0109 20:06:27.257632  3950 net.cpp:94] Creating Layer bn1
I0109 20:06:27.257642  3950 net.cpp:435] bn1 <- conv1
I0109 20:06:27.257680  3950 net.cpp:409] bn1 -> scale1
I0109 20:06:27.258797  3950 net.cpp:144] Setting up bn1
I0109 20:06:27.258860  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.258901  3950 net.cpp:159] Memory required for data: 35127808
I0109 20:06:27.258960  3950 layer_factory.hpp:77] Creating layer relu1
I0109 20:06:27.259023  3950 net.cpp:94] Creating Layer relu1
I0109 20:06:27.259073  3950 net.cpp:435] relu1 <- scale1
I0109 20:06:27.259127  3950 net.cpp:409] relu1 -> relu1
I0109 20:06:27.259217  3950 net.cpp:144] Setting up relu1
I0109 20:06:27.259276  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.259289  3950 net.cpp:159] Memory required for data: 51905024
I0109 20:06:27.259363  3950 layer_factory.hpp:77] Creating layer conv2
I0109 20:06:27.259429  3950 net.cpp:94] Creating Layer conv2
I0109 20:06:27.259474  3950 net.cpp:435] conv2 <- relu1
I0109 20:06:27.259518  3950 net.cpp:409] conv2 -> conv2
I0109 20:06:27.260695  3950 net.cpp:144] Setting up conv2
I0109 20:06:27.260720  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.260730  3950 net.cpp:159] Memory required for data: 68682240
I0109 20:06:27.260787  3950 layer_factory.hpp:77] Creating layer bn2
I0109 20:06:27.260818  3950 net.cpp:94] Creating Layer bn2
I0109 20:06:27.260861  3950 net.cpp:435] bn2 <- conv2
I0109 20:06:27.260939  3950 net.cpp:409] bn2 -> scale2
I0109 20:06:27.261978  3950 net.cpp:144] Setting up bn2
I0109 20:06:27.262053  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.262105  3950 net.cpp:159] Memory required for data: 85459456
I0109 20:06:27.262135  3950 layer_factory.hpp:77] Creating layer relu2
I0109 20:06:27.262152  3950 net.cpp:94] Creating Layer relu2
I0109 20:06:27.262193  3950 net.cpp:435] relu2 <- scale2
I0109 20:06:27.262207  3950 net.cpp:409] relu2 -> relu2
I0109 20:06:27.262369  3950 net.cpp:144] Setting up relu2
I0109 20:06:27.262425  3950 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:06:27.262454  3950 net.cpp:159] Memory required for data: 102236672
I0109 20:06:27.262478  3950 layer_factory.hpp:77] Creating layer pool1
I0109 20:06:27.262526  3950 net.cpp:94] Creating Layer pool1
I0109 20:06:27.262570  3950 net.cpp:435] pool1 <- relu2
I0109 20:06:27.262620  3950 net.cpp:409] pool1 -> pool1
I0109 20:06:27.262735  3950 net.cpp:144] Setting up pool1
I0109 20:06:27.262789  3950 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:06:27.262832  3950 net.cpp:159] Memory required for data: 106430976
I0109 20:06:27.262876  3950 layer_factory.hpp:77] Creating layer drop1
I0109 20:06:27.262933  3950 net.cpp:94] Creating Layer drop1
I0109 20:06:27.262998  3950 net.cpp:435] drop1 <- pool1
I0109 20:06:27.263051  3950 net.cpp:409] drop1 -> drop1
I0109 20:06:27.263146  3950 net.cpp:144] Setting up drop1
I0109 20:06:27.263201  3950 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:06:27.263247  3950 net.cpp:159] Memory required for data: 110625280
I0109 20:06:27.263291  3950 layer_factory.hpp:77] Creating layer conv3
I0109 20:06:27.263345  3950 net.cpp:94] Creating Layer conv3
I0109 20:06:27.263391  3950 net.cpp:435] conv3 <- drop1
I0109 20:06:27.263444  3950 net.cpp:409] conv3 -> conv3
I0109 20:06:27.264700  3950 net.cpp:144] Setting up conv3
I0109 20:06:27.264724  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.264731  3950 net.cpp:159] Memory required for data: 119013888
I0109 20:06:27.264744  3950 layer_factory.hpp:77] Creating layer bn3
I0109 20:06:27.264820  3950 net.cpp:94] Creating Layer bn3
I0109 20:06:27.264840  3950 net.cpp:435] bn3 <- conv3
I0109 20:06:27.264856  3950 net.cpp:409] bn3 -> scale3
I0109 20:06:27.265853  3950 net.cpp:144] Setting up bn3
I0109 20:06:27.265916  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.265967  3950 net.cpp:159] Memory required for data: 127402496
I0109 20:06:27.266024  3950 layer_factory.hpp:77] Creating layer relu3
I0109 20:06:27.266068  3950 net.cpp:94] Creating Layer relu3
I0109 20:06:27.266106  3950 net.cpp:435] relu3 <- scale3
I0109 20:06:27.266142  3950 net.cpp:409] relu3 -> relu3
I0109 20:06:27.266211  3950 net.cpp:144] Setting up relu3
I0109 20:06:27.266234  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.266242  3950 net.cpp:159] Memory required for data: 135791104
I0109 20:06:27.266248  3950 layer_factory.hpp:77] Creating layer conv4
I0109 20:06:27.266269  3950 net.cpp:94] Creating Layer conv4
I0109 20:06:27.266294  3950 net.cpp:435] conv4 <- relu3
I0109 20:06:27.266319  3950 net.cpp:409] conv4 -> conv4
I0109 20:06:27.266928  3950 net.cpp:144] Setting up conv4
I0109 20:06:27.266985  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.267033  3950 net.cpp:159] Memory required for data: 144179712
I0109 20:06:27.267082  3950 layer_factory.hpp:77] Creating layer bn4
I0109 20:06:27.267138  3950 net.cpp:94] Creating Layer bn4
I0109 20:06:27.267179  3950 net.cpp:435] bn4 <- conv4
I0109 20:06:27.267228  3950 net.cpp:409] bn4 -> scale4
I0109 20:06:27.268276  3950 net.cpp:144] Setting up bn4
I0109 20:06:27.268301  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.268307  3950 net.cpp:159] Memory required for data: 152568320
I0109 20:06:27.268364  3950 layer_factory.hpp:77] Creating layer relu4
I0109 20:06:27.268385  3950 net.cpp:94] Creating Layer relu4
I0109 20:06:27.268394  3950 net.cpp:435] relu4 <- scale4
I0109 20:06:27.268430  3950 net.cpp:409] relu4 -> relu4
I0109 20:06:27.268667  3950 net.cpp:144] Setting up relu4
I0109 20:06:27.268725  3950 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:06:27.268779  3950 net.cpp:159] Memory required for data: 160956928
I0109 20:06:27.268834  3950 layer_factory.hpp:77] Creating layer pool2
I0109 20:06:27.268898  3950 net.cpp:94] Creating Layer pool2
I0109 20:06:27.268950  3950 net.cpp:435] pool2 <- relu4
I0109 20:06:27.269006  3950 net.cpp:409] pool2 -> pool2
I0109 20:06:27.269121  3950 net.cpp:144] Setting up pool2
I0109 20:06:27.269174  3950 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:06:27.269217  3950 net.cpp:159] Memory required for data: 163054080
I0109 20:06:27.269260  3950 layer_factory.hpp:77] Creating layer drop2
I0109 20:06:27.269315  3950 net.cpp:94] Creating Layer drop2
I0109 20:06:27.269361  3950 net.cpp:435] drop2 <- pool2
I0109 20:06:27.269407  3950 net.cpp:409] drop2 -> drop2
I0109 20:06:27.269507  3950 net.cpp:144] Setting up drop2
I0109 20:06:27.269556  3950 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:06:27.269636  3950 net.cpp:159] Memory required for data: 165151232
I0109 20:06:27.269682  3950 layer_factory.hpp:77] Creating layer fc1
I0109 20:06:27.269742  3950 net.cpp:94] Creating Layer fc1
I0109 20:06:27.269784  3950 net.cpp:435] fc1 <- drop2
I0109 20:06:27.269852  3950 net.cpp:409] fc1 -> fc1
I0109 20:06:27.293370  3950 net.cpp:144] Setting up fc1
I0109 20:06:27.293424  3950 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:06:27.293429  3950 net.cpp:159] Memory required for data: 165413376
I0109 20:06:27.293444  3950 layer_factory.hpp:77] Creating layer bn5
I0109 20:06:27.293471  3950 net.cpp:94] Creating Layer bn5
I0109 20:06:27.293488  3950 net.cpp:435] bn5 <- fc1
I0109 20:06:27.293505  3950 net.cpp:409] bn5 -> scale5
I0109 20:06:27.294209  3950 net.cpp:144] Setting up bn5
I0109 20:06:27.294229  3950 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:06:27.294234  3950 net.cpp:159] Memory required for data: 165675520
I0109 20:06:27.294261  3950 layer_factory.hpp:77] Creating layer relu5
I0109 20:06:27.294284  3950 net.cpp:94] Creating Layer relu5
I0109 20:06:27.294291  3950 net.cpp:435] relu5 <- scale5
I0109 20:06:27.294301  3950 net.cpp:409] relu5 -> relu5
I0109 20:06:27.294358  3950 net.cpp:144] Setting up relu5
I0109 20:06:27.294375  3950 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:06:27.294379  3950 net.cpp:159] Memory required for data: 165937664
I0109 20:06:27.294384  3950 layer_factory.hpp:77] Creating layer drop3
I0109 20:06:27.294397  3950 net.cpp:94] Creating Layer drop3
I0109 20:06:27.294414  3950 net.cpp:435] drop3 <- relu5
I0109 20:06:27.294423  3950 net.cpp:409] drop3 -> drop3
I0109 20:06:27.294490  3950 net.cpp:144] Setting up drop3
I0109 20:06:27.294510  3950 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:06:27.294518  3950 net.cpp:159] Memory required for data: 166199808
I0109 20:06:27.294524  3950 layer_factory.hpp:77] Creating layer fc2
I0109 20:06:27.294538  3950 net.cpp:94] Creating Layer fc2
I0109 20:06:27.294548  3950 net.cpp:435] fc2 <- drop3
I0109 20:06:27.294562  3950 net.cpp:409] fc2 -> fc2
I0109 20:06:27.294751  3950 net.cpp:144] Setting up fc2
I0109 20:06:27.294769  3950 net.cpp:151] Top shape: 128 10 (1280)
I0109 20:06:27.294775  3950 net.cpp:159] Memory required for data: 166204928
I0109 20:06:27.294786  3950 layer_factory.hpp:77] Creating layer loss
I0109 20:06:27.294802  3950 net.cpp:94] Creating Layer loss
I0109 20:06:27.294811  3950 net.cpp:435] loss <- fc2
I0109 20:06:27.294819  3950 net.cpp:435] loss <- label
I0109 20:06:27.294829  3950 net.cpp:409] loss -> loss
I0109 20:06:27.294844  3950 layer_factory.hpp:77] Creating layer loss
I0109 20:06:27.295796  3950 net.cpp:144] Setting up loss
I0109 20:06:27.295820  3950 net.cpp:151] Top shape: (1)
I0109 20:06:27.295830  3950 net.cpp:154]     with loss weight 1
I0109 20:06:27.295856  3950 net.cpp:159] Memory required for data: 166204932
I0109 20:06:27.295864  3950 net.cpp:220] loss needs backward computation.
I0109 20:06:27.295902  3950 net.cpp:220] fc2 needs backward computation.
I0109 20:06:27.295912  3950 net.cpp:220] drop3 needs backward computation.
I0109 20:06:27.295918  3950 net.cpp:220] relu5 needs backward computation.
I0109 20:06:27.295924  3950 net.cpp:220] bn5 needs backward computation.
I0109 20:06:27.295930  3950 net.cpp:220] fc1 needs backward computation.
I0109 20:06:27.295946  3950 net.cpp:220] drop2 needs backward computation.
I0109 20:06:27.295953  3950 net.cpp:220] pool2 needs backward computation.
I0109 20:06:27.295961  3950 net.cpp:220] relu4 needs backward computation.
I0109 20:06:27.295967  3950 net.cpp:220] bn4 needs backward computation.
I0109 20:06:27.295994  3950 net.cpp:220] conv4 needs backward computation.
I0109 20:06:27.296012  3950 net.cpp:220] relu3 needs backward computation.
I0109 20:06:27.296020  3950 net.cpp:220] bn3 needs backward computation.
I0109 20:06:27.296028  3950 net.cpp:220] conv3 needs backward computation.
I0109 20:06:27.296036  3950 net.cpp:220] drop1 needs backward computation.
I0109 20:06:27.296042  3950 net.cpp:220] pool1 needs backward computation.
I0109 20:06:27.296052  3950 net.cpp:220] relu2 needs backward computation.
I0109 20:06:27.296059  3950 net.cpp:220] bn2 needs backward computation.
I0109 20:06:27.296066  3950 net.cpp:220] conv2 needs backward computation.
I0109 20:06:27.296073  3950 net.cpp:220] relu1 needs backward computation.
I0109 20:06:27.296111  3950 net.cpp:220] bn1 needs backward computation.
I0109 20:06:27.296128  3950 net.cpp:220] conv1 needs backward computation.
I0109 20:06:27.296139  3950 net.cpp:222] data does not need backward computation.
I0109 20:06:27.296146  3950 net.cpp:264] This network produces output loss
I0109 20:06:27.296183  3950 net.cpp:284] Network initialization done.
I0109 20:06:27.296581  3950 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0109 20:06:27.296653  3950 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:06:27.296890  3950 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:06:27.297039  3950 layer_factory.hpp:77] Creating layer data
I0109 20:06:27.297116  3950 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:27.297276  3950 net.cpp:94] Creating Layer data
I0109 20:06:27.297299  3950 net.cpp:409] data -> data
I0109 20:06:27.297315  3950 net.cpp:409] data -> label
I0109 20:06:27.298913  3967 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:06:27.298954  3967 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:06:27.299072  3950 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:06:27.299239  3950 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:06:27.307396  3950 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:06:27.307476  3950 net.cpp:144] Setting up data
I0109 20:06:27.307518  3950 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:06:27.307557  3950 net.cpp:151] Top shape: 50 (50)
I0109 20:06:27.307582  3950 net.cpp:159] Memory required for data: 614600
I0109 20:06:27.307616  3950 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:06:27.307649  3950 net.cpp:94] Creating Layer label_data_1_split
I0109 20:06:27.307684  3950 net.cpp:435] label_data_1_split <- label
I0109 20:06:27.307718  3950 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:06:27.307761  3950 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:06:27.307803  3950 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:06:27.307920  3950 net.cpp:144] Setting up label_data_1_split
I0109 20:06:27.307941  3950 net.cpp:151] Top shape: 50 (50)
I0109 20:06:27.307952  3950 net.cpp:151] Top shape: 50 (50)
I0109 20:06:27.307960  3950 net.cpp:151] Top shape: 50 (50)
I0109 20:06:27.307965  3950 net.cpp:159] Memory required for data: 615200
I0109 20:06:27.307971  3950 layer_factory.hpp:77] Creating layer conv1
I0109 20:06:27.307996  3950 net.cpp:94] Creating Layer conv1
I0109 20:06:27.308019  3950 net.cpp:435] conv1 <- data
I0109 20:06:27.308058  3950 net.cpp:409] conv1 -> conv1
I0109 20:06:27.308439  3950 net.cpp:144] Setting up conv1
I0109 20:06:27.308459  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.308466  3950 net.cpp:159] Memory required for data: 7168800
I0109 20:06:27.308482  3950 layer_factory.hpp:77] Creating layer bn1
I0109 20:06:27.308501  3950 net.cpp:94] Creating Layer bn1
I0109 20:06:27.308531  3950 net.cpp:435] bn1 <- conv1
I0109 20:06:27.308563  3950 net.cpp:409] bn1 -> scale1
I0109 20:06:27.309747  3950 net.cpp:144] Setting up bn1
I0109 20:06:27.309769  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.309778  3950 net.cpp:159] Memory required for data: 13722400
I0109 20:06:27.309803  3950 layer_factory.hpp:77] Creating layer relu1
I0109 20:06:27.309818  3950 net.cpp:94] Creating Layer relu1
I0109 20:06:27.309825  3950 net.cpp:435] relu1 <- scale1
I0109 20:06:27.309854  3950 net.cpp:409] relu1 -> relu1
I0109 20:06:27.309933  3950 net.cpp:144] Setting up relu1
I0109 20:06:27.309953  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.309959  3950 net.cpp:159] Memory required for data: 20276000
I0109 20:06:27.309967  3950 layer_factory.hpp:77] Creating layer conv2
I0109 20:06:27.309984  3950 net.cpp:94] Creating Layer conv2
I0109 20:06:27.310009  3950 net.cpp:435] conv2 <- relu1
I0109 20:06:27.310040  3950 net.cpp:409] conv2 -> conv2
I0109 20:06:27.310700  3950 net.cpp:144] Setting up conv2
I0109 20:06:27.310729  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.310735  3950 net.cpp:159] Memory required for data: 26829600
I0109 20:06:27.310755  3950 layer_factory.hpp:77] Creating layer bn2
I0109 20:06:27.310775  3950 net.cpp:94] Creating Layer bn2
I0109 20:06:27.310782  3950 net.cpp:435] bn2 <- conv2
I0109 20:06:27.310798  3950 net.cpp:409] bn2 -> scale2
I0109 20:06:27.312163  3950 net.cpp:144] Setting up bn2
I0109 20:06:27.312191  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.312199  3950 net.cpp:159] Memory required for data: 33383200
I0109 20:06:27.312216  3950 layer_factory.hpp:77] Creating layer relu2
I0109 20:06:27.312260  3950 net.cpp:94] Creating Layer relu2
I0109 20:06:27.312269  3950 net.cpp:435] relu2 <- scale2
I0109 20:06:27.312280  3950 net.cpp:409] relu2 -> relu2
I0109 20:06:27.312438  3950 net.cpp:144] Setting up relu2
I0109 20:06:27.312474  3950 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:06:27.312556  3950 net.cpp:159] Memory required for data: 39936800
I0109 20:06:27.312603  3950 layer_factory.hpp:77] Creating layer pool1
I0109 20:06:27.312670  3950 net.cpp:94] Creating Layer pool1
I0109 20:06:27.312714  3950 net.cpp:435] pool1 <- relu2
I0109 20:06:27.312760  3950 net.cpp:409] pool1 -> pool1
I0109 20:06:27.312889  3950 net.cpp:144] Setting up pool1
I0109 20:06:27.312966  3950 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:06:27.312984  3950 net.cpp:159] Memory required for data: 41575200
I0109 20:06:27.312994  3950 layer_factory.hpp:77] Creating layer drop1
I0109 20:06:27.313009  3950 net.cpp:94] Creating Layer drop1
I0109 20:06:27.313019  3950 net.cpp:435] drop1 <- pool1
I0109 20:06:27.313030  3950 net.cpp:409] drop1 -> drop1
I0109 20:06:27.313156  3950 net.cpp:144] Setting up drop1
I0109 20:06:27.313192  3950 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:06:27.313212  3950 net.cpp:159] Memory required for data: 43213600
I0109 20:06:27.313231  3950 layer_factory.hpp:77] Creating layer conv3
I0109 20:06:27.313263  3950 net.cpp:94] Creating Layer conv3
I0109 20:06:27.313285  3950 net.cpp:435] conv3 <- drop1
I0109 20:06:27.313315  3950 net.cpp:409] conv3 -> conv3
I0109 20:06:27.314123  3950 net.cpp:144] Setting up conv3
I0109 20:06:27.314144  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.314152  3950 net.cpp:159] Memory required for data: 46490400
I0109 20:06:27.314163  3950 layer_factory.hpp:77] Creating layer bn3
I0109 20:06:27.314184  3950 net.cpp:94] Creating Layer bn3
I0109 20:06:27.314194  3950 net.cpp:435] bn3 <- conv3
I0109 20:06:27.314210  3950 net.cpp:409] bn3 -> scale3
I0109 20:06:27.315459  3950 net.cpp:144] Setting up bn3
I0109 20:06:27.315507  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.315516  3950 net.cpp:159] Memory required for data: 49767200
I0109 20:06:27.315536  3950 layer_factory.hpp:77] Creating layer relu3
I0109 20:06:27.315558  3950 net.cpp:94] Creating Layer relu3
I0109 20:06:27.315568  3950 net.cpp:435] relu3 <- scale3
I0109 20:06:27.315582  3950 net.cpp:409] relu3 -> relu3
I0109 20:06:27.315726  3950 net.cpp:144] Setting up relu3
I0109 20:06:27.315766  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.315846  3950 net.cpp:159] Memory required for data: 53044000
I0109 20:06:27.315927  3950 layer_factory.hpp:77] Creating layer conv4
I0109 20:06:27.316015  3950 net.cpp:94] Creating Layer conv4
I0109 20:06:27.316033  3950 net.cpp:435] conv4 <- relu3
I0109 20:06:27.316047  3950 net.cpp:409] conv4 -> conv4
I0109 20:06:27.316856  3950 net.cpp:144] Setting up conv4
I0109 20:06:27.316882  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.316890  3950 net.cpp:159] Memory required for data: 56320800
I0109 20:06:27.316901  3950 layer_factory.hpp:77] Creating layer bn4
I0109 20:06:27.316970  3950 net.cpp:94] Creating Layer bn4
I0109 20:06:27.316988  3950 net.cpp:435] bn4 <- conv4
I0109 20:06:27.317005  3950 net.cpp:409] bn4 -> scale4
I0109 20:06:27.318460  3950 net.cpp:144] Setting up bn4
I0109 20:06:27.318488  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.318496  3950 net.cpp:159] Memory required for data: 59597600
I0109 20:06:27.318513  3950 layer_factory.hpp:77] Creating layer relu4
I0109 20:06:27.318531  3950 net.cpp:94] Creating Layer relu4
I0109 20:06:27.318610  3950 net.cpp:435] relu4 <- scale4
I0109 20:06:27.318624  3950 net.cpp:409] relu4 -> relu4
I0109 20:06:27.318733  3950 net.cpp:144] Setting up relu4
I0109 20:06:27.318755  3950 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:06:27.318763  3950 net.cpp:159] Memory required for data: 62874400
I0109 20:06:27.318769  3950 layer_factory.hpp:77] Creating layer pool2
I0109 20:06:27.318787  3950 net.cpp:94] Creating Layer pool2
I0109 20:06:27.318871  3950 net.cpp:435] pool2 <- relu4
I0109 20:06:27.318922  3950 net.cpp:409] pool2 -> pool2
I0109 20:06:27.319036  3950 net.cpp:144] Setting up pool2
I0109 20:06:27.319072  3950 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:06:27.319097  3950 net.cpp:159] Memory required for data: 63693600
I0109 20:06:27.319136  3950 layer_factory.hpp:77] Creating layer drop2
I0109 20:06:27.319157  3950 net.cpp:94] Creating Layer drop2
I0109 20:06:27.319165  3950 net.cpp:435] drop2 <- pool2
I0109 20:06:27.319180  3950 net.cpp:409] drop2 -> drop2
I0109 20:06:27.319247  3950 net.cpp:144] Setting up drop2
I0109 20:06:27.319275  3950 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:06:27.319283  3950 net.cpp:159] Memory required for data: 64512800
I0109 20:06:27.319289  3950 layer_factory.hpp:77] Creating layer fc1
I0109 20:06:27.319315  3950 net.cpp:94] Creating Layer fc1
I0109 20:06:27.319330  3950 net.cpp:435] fc1 <- drop2
I0109 20:06:27.319342  3950 net.cpp:409] fc1 -> fc1
I0109 20:06:27.342695  3950 net.cpp:144] Setting up fc1
I0109 20:06:27.342751  3950 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:27.342759  3950 net.cpp:159] Memory required for data: 64615200
I0109 20:06:27.342778  3950 layer_factory.hpp:77] Creating layer bn5
I0109 20:06:27.342808  3950 net.cpp:94] Creating Layer bn5
I0109 20:06:27.342819  3950 net.cpp:435] bn5 <- fc1
I0109 20:06:27.342851  3950 net.cpp:409] bn5 -> scale5
I0109 20:06:27.343631  3950 net.cpp:144] Setting up bn5
I0109 20:06:27.343652  3950 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:27.343657  3950 net.cpp:159] Memory required for data: 64717600
I0109 20:06:27.343679  3950 layer_factory.hpp:77] Creating layer relu5
I0109 20:06:27.343695  3950 net.cpp:94] Creating Layer relu5
I0109 20:06:27.343703  3950 net.cpp:435] relu5 <- scale5
I0109 20:06:27.343722  3950 net.cpp:409] relu5 -> relu5
I0109 20:06:27.343762  3950 net.cpp:144] Setting up relu5
I0109 20:06:27.343780  3950 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:27.343786  3950 net.cpp:159] Memory required for data: 64820000
I0109 20:06:27.343792  3950 layer_factory.hpp:77] Creating layer drop3
I0109 20:06:27.343803  3950 net.cpp:94] Creating Layer drop3
I0109 20:06:27.343811  3950 net.cpp:435] drop3 <- relu5
I0109 20:06:27.343823  3950 net.cpp:409] drop3 -> drop3
I0109 20:06:27.343888  3950 net.cpp:144] Setting up drop3
I0109 20:06:27.343909  3950 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:06:27.343915  3950 net.cpp:159] Memory required for data: 64922400
I0109 20:06:27.343921  3950 layer_factory.hpp:77] Creating layer fc2
I0109 20:06:27.343940  3950 net.cpp:94] Creating Layer fc2
I0109 20:06:27.343948  3950 net.cpp:435] fc2 <- drop3
I0109 20:06:27.343969  3950 net.cpp:409] fc2 -> fc2
I0109 20:06:27.344171  3950 net.cpp:144] Setting up fc2
I0109 20:06:27.344187  3950 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:27.344192  3950 net.cpp:159] Memory required for data: 64924400
I0109 20:06:27.344198  3950 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:06:27.344210  3950 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:06:27.344221  3950 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:06:27.344233  3950 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:06:27.344252  3950 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:06:27.344269  3950 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:06:27.344336  3950 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:06:27.344350  3950 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:27.344354  3950 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:27.344359  3950 net.cpp:151] Top shape: 50 10 (500)
I0109 20:06:27.344363  3950 net.cpp:159] Memory required for data: 64930400
I0109 20:06:27.344367  3950 layer_factory.hpp:77] Creating layer loss
I0109 20:06:27.344380  3950 net.cpp:94] Creating Layer loss
I0109 20:06:27.344388  3950 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:06:27.344394  3950 net.cpp:435] loss <- label_data_1_split_0
I0109 20:06:27.344400  3950 net.cpp:409] loss -> loss
I0109 20:06:27.344411  3950 layer_factory.hpp:77] Creating layer loss
I0109 20:06:27.344506  3950 net.cpp:144] Setting up loss
I0109 20:06:27.344521  3950 net.cpp:151] Top shape: (1)
I0109 20:06:27.344525  3950 net.cpp:154]     with loss weight 1
I0109 20:06:27.344547  3950 net.cpp:159] Memory required for data: 64930404
I0109 20:06:27.344552  3950 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:06:27.344561  3950 net.cpp:94] Creating Layer accuracy-top1
I0109 20:06:27.344568  3950 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:06:27.344574  3950 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:06:27.344601  3950 net.cpp:409] accuracy-top1 -> top-1
I0109 20:06:27.344615  3950 net.cpp:144] Setting up accuracy-top1
I0109 20:06:27.344624  3950 net.cpp:151] Top shape: (1)
I0109 20:06:27.344627  3950 net.cpp:159] Memory required for data: 64930408
I0109 20:06:27.344631  3950 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:06:27.344640  3950 net.cpp:94] Creating Layer accuracy-top5
I0109 20:06:27.344645  3950 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:06:27.344650  3950 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:06:27.344656  3950 net.cpp:409] accuracy-top5 -> top-5
I0109 20:06:27.344666  3950 net.cpp:144] Setting up accuracy-top5
I0109 20:06:27.344671  3950 net.cpp:151] Top shape: (1)
I0109 20:06:27.344674  3950 net.cpp:159] Memory required for data: 64930412
I0109 20:06:27.344679  3950 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:06:27.344686  3950 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:06:27.344693  3950 net.cpp:220] loss needs backward computation.
I0109 20:06:27.344699  3950 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:06:27.344703  3950 net.cpp:220] fc2 needs backward computation.
I0109 20:06:27.344712  3950 net.cpp:220] drop3 needs backward computation.
I0109 20:06:27.344715  3950 net.cpp:220] relu5 needs backward computation.
I0109 20:06:27.344719  3950 net.cpp:220] bn5 needs backward computation.
I0109 20:06:27.344724  3950 net.cpp:220] fc1 needs backward computation.
I0109 20:06:27.344729  3950 net.cpp:220] drop2 needs backward computation.
I0109 20:06:27.344738  3950 net.cpp:220] pool2 needs backward computation.
I0109 20:06:27.344743  3950 net.cpp:220] relu4 needs backward computation.
I0109 20:06:27.344748  3950 net.cpp:220] bn4 needs backward computation.
I0109 20:06:27.344753  3950 net.cpp:220] conv4 needs backward computation.
I0109 20:06:27.344760  3950 net.cpp:220] relu3 needs backward computation.
I0109 20:06:27.344768  3950 net.cpp:220] bn3 needs backward computation.
I0109 20:06:27.344774  3950 net.cpp:220] conv3 needs backward computation.
I0109 20:06:27.344810  3950 net.cpp:220] drop1 needs backward computation.
I0109 20:06:27.344843  3950 net.cpp:220] pool1 needs backward computation.
I0109 20:06:27.344872  3950 net.cpp:220] relu2 needs backward computation.
I0109 20:06:27.344894  3950 net.cpp:220] bn2 needs backward computation.
I0109 20:06:27.344925  3950 net.cpp:220] conv2 needs backward computation.
I0109 20:06:27.344946  3950 net.cpp:220] relu1 needs backward computation.
I0109 20:06:27.344975  3950 net.cpp:220] bn1 needs backward computation.
I0109 20:06:27.344997  3950 net.cpp:220] conv1 needs backward computation.
I0109 20:06:27.345016  3950 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:06:27.345024  3950 net.cpp:222] data does not need backward computation.
I0109 20:06:27.345031  3950 net.cpp:264] This network produces output loss
I0109 20:06:27.345037  3950 net.cpp:264] This network produces output top-1
I0109 20:06:27.345060  3950 net.cpp:264] This network produces output top-5
I0109 20:06:27.345103  3950 net.cpp:284] Network initialization done.
I0109 20:06:27.345247  3950 solver.cpp:63] Solver scaffolding done.
I0109 20:06:27.346602  3950 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/sparse.caffemodel
I0109 20:06:27.413728  3950 caffe_interface.cpp:527] Starting Optimization
I0109 20:06:27.413779  3950 solver.cpp:335] Solving 
I0109 20:06:27.413784  3950 solver.cpp:336] Learning Rate Policy: poly
I0109 20:06:27.415140  3950 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 20:06:28.230765  3950 solver.cpp:517]     Test net output #0: loss = 0.742979 (* 1 = 0.742979 loss)
I0109 20:06:28.230813  3950 solver.cpp:517]     Test net output #1: top-1 = 0.778889
I0109 20:06:28.230820  3950 solver.cpp:517]     Test net output #2: top-5 = 0.983556
I0109 20:06:28.277923  3950 solver.cpp:266] Iteration 0 (0 iter/s, 0.864083s/100 iter), loss = 0.250099
I0109 20:06:28.277998  3950 solver.cpp:285]     Train net output #0: loss = 0.250099 (* 1 = 0.250099 loss)
I0109 20:06:28.278060  3950 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 20:06:31.544281  3950 solver.cpp:266] Iteration 100 (30.6156 iter/s, 3.26631s/100 iter), loss = 0.26514
I0109 20:06:31.544364  3950 solver.cpp:285]     Train net output #0: loss = 0.26514 (* 1 = 0.26514 loss)
I0109 20:06:31.544381  3950 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 20:06:34.816104  3950 solver.cpp:266] Iteration 200 (30.5648 iter/s, 3.27174s/100 iter), loss = 0.218345
I0109 20:06:34.816176  3950 solver.cpp:285]     Train net output #0: loss = 0.218345 (* 1 = 0.218345 loss)
I0109 20:06:34.816191  3950 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 20:06:38.088071  3950 solver.cpp:266] Iteration 300 (30.5631 iter/s, 3.27192s/100 iter), loss = 0.238915
I0109 20:06:38.088140  3950 solver.cpp:285]     Train net output #0: loss = 0.238915 (* 1 = 0.238915 loss)
I0109 20:06:38.088155  3950 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 20:06:41.358616  3950 solver.cpp:266] Iteration 400 (30.5763 iter/s, 3.2705s/100 iter), loss = 0.385923
I0109 20:06:41.358687  3950 solver.cpp:285]     Train net output #0: loss = 0.385923 (* 1 = 0.385923 loss)
I0109 20:06:41.358701  3950 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 20:06:44.629407  3950 solver.cpp:266] Iteration 500 (30.574 iter/s, 3.27075s/100 iter), loss = 0.255078
I0109 20:06:44.629485  3950 solver.cpp:285]     Train net output #0: loss = 0.255078 (* 1 = 0.255078 loss)
I0109 20:06:44.629504  3950 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 20:06:47.896469  3950 solver.cpp:266] Iteration 600 (30.6093 iter/s, 3.26699s/100 iter), loss = 0.406475
I0109 20:06:47.896548  3950 solver.cpp:285]     Train net output #0: loss = 0.406475 (* 1 = 0.406475 loss)
I0109 20:06:47.896562  3950 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 20:06:51.177239  3950 solver.cpp:266] Iteration 700 (30.4812 iter/s, 3.28072s/100 iter), loss = 0.350494
I0109 20:06:51.177326  3950 solver.cpp:285]     Train net output #0: loss = 0.350494 (* 1 = 0.350494 loss)
I0109 20:06:51.177342  3950 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 20:06:54.447471  3950 solver.cpp:266] Iteration 800 (30.5794 iter/s, 3.27017s/100 iter), loss = 0.312931
I0109 20:06:54.447536  3950 solver.cpp:285]     Train net output #0: loss = 0.312931 (* 1 = 0.312931 loss)
I0109 20:06:54.447548  3950 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 20:06:57.714828  3950 solver.cpp:266] Iteration 900 (30.6061 iter/s, 3.26732s/100 iter), loss = 0.410086
I0109 20:06:57.715057  3950 solver.cpp:285]     Train net output #0: loss = 0.410086 (* 1 = 0.410086 loss)
I0109 20:06:57.715075  3950 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 20:07:00.952916  3950 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 20:07:01.767055  3950 solver.cpp:517]     Test net output #0: loss = 1.08155 (* 1 = 1.08155 loss)
I0109 20:07:01.767093  3950 solver.cpp:517]     Test net output #1: top-1 = 0.760556
I0109 20:07:01.767105  3950 solver.cpp:517]     Test net output #2: top-5 = 0.977556
I0109 20:07:01.798049  3950 solver.cpp:266] Iteration 1000 (24.4917 iter/s, 4.08301s/100 iter), loss = 0.198126
I0109 20:07:01.798094  3950 solver.cpp:285]     Train net output #0: loss = 0.198126 (* 1 = 0.198126 loss)
I0109 20:07:01.798110  3950 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 20:07:05.065457  3950 solver.cpp:266] Iteration 1100 (30.6055 iter/s, 3.26739s/100 iter), loss = 0.24949
I0109 20:07:05.065524  3950 solver.cpp:285]     Train net output #0: loss = 0.24949 (* 1 = 0.24949 loss)
I0109 20:07:05.065537  3950 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 20:07:08.337872  3950 solver.cpp:266] Iteration 1200 (30.5589 iter/s, 3.27237s/100 iter), loss = 0.220439
I0109 20:07:08.337957  3950 solver.cpp:285]     Train net output #0: loss = 0.220439 (* 1 = 0.220439 loss)
I0109 20:07:08.337973  3950 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 20:07:11.609467  3950 solver.cpp:266] Iteration 1300 (30.5667 iter/s, 3.27154s/100 iter), loss = 0.206673
I0109 20:07:11.609540  3950 solver.cpp:285]     Train net output #0: loss = 0.206673 (* 1 = 0.206673 loss)
I0109 20:07:11.609555  3950 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 20:07:14.880760  3950 solver.cpp:266] Iteration 1400 (30.5696 iter/s, 3.27122s/100 iter), loss = 0.343933
I0109 20:07:14.880828  3950 solver.cpp:285]     Train net output #0: loss = 0.343933 (* 1 = 0.343933 loss)
I0109 20:07:14.880842  3950 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 20:07:18.154484  3950 solver.cpp:266] Iteration 1500 (30.5466 iter/s, 3.27368s/100 iter), loss = 0.196517
I0109 20:07:18.154558  3950 solver.cpp:285]     Train net output #0: loss = 0.196517 (* 1 = 0.196517 loss)
I0109 20:07:18.154572  3950 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 20:07:21.427016  3950 solver.cpp:266] Iteration 1600 (30.5578 iter/s, 3.27249s/100 iter), loss = 0.38125
I0109 20:07:21.427085  3950 solver.cpp:285]     Train net output #0: loss = 0.38125 (* 1 = 0.38125 loss)
I0109 20:07:21.427099  3950 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 20:07:24.701486  3950 solver.cpp:266] Iteration 1700 (30.5397 iter/s, 3.27443s/100 iter), loss = 0.169147
I0109 20:07:24.701553  3950 solver.cpp:285]     Train net output #0: loss = 0.169147 (* 1 = 0.169147 loss)
I0109 20:07:24.701566  3950 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 20:07:27.973146  3950 solver.cpp:266] Iteration 1800 (30.5661 iter/s, 3.2716s/100 iter), loss = 0.321852
I0109 20:07:27.973358  3950 solver.cpp:285]     Train net output #0: loss = 0.321852 (* 1 = 0.321852 loss)
I0109 20:07:27.973374  3950 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 20:07:31.247607  3950 solver.cpp:266] Iteration 1900 (30.5411 iter/s, 3.27428s/100 iter), loss = 0.356092
I0109 20:07:31.247689  3950 solver.cpp:285]     Train net output #0: loss = 0.356092 (* 1 = 0.356092 loss)
I0109 20:07:31.247704  3950 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 20:07:34.487247  3950 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 20:07:35.306452  3950 solver.cpp:517]     Test net output #0: loss = 1.0123 (* 1 = 1.0123 loss)
I0109 20:07:35.306488  3950 solver.cpp:517]     Test net output #1: top-1 = 0.748778
I0109 20:07:35.306495  3950 solver.cpp:517]     Test net output #2: top-5 = 0.980334
I0109 20:07:35.337363  3950 solver.cpp:266] Iteration 2000 (24.4516 iter/s, 4.08972s/100 iter), loss = 0.182053
I0109 20:07:35.337405  3950 solver.cpp:285]     Train net output #0: loss = 0.182053 (* 1 = 0.182053 loss)
I0109 20:07:35.337420  3950 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 20:07:38.614902  3950 solver.cpp:266] Iteration 2100 (30.5109 iter/s, 3.27752s/100 iter), loss = 0.160606
I0109 20:07:38.614981  3950 solver.cpp:285]     Train net output #0: loss = 0.160606 (* 1 = 0.160606 loss)
I0109 20:07:38.614998  3950 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 20:07:41.888960  3950 solver.cpp:266] Iteration 2200 (30.5439 iter/s, 3.27398s/100 iter), loss = 0.173887
I0109 20:07:41.889040  3950 solver.cpp:285]     Train net output #0: loss = 0.173887 (* 1 = 0.173887 loss)
I0109 20:07:41.889055  3950 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 20:07:45.157321  3950 solver.cpp:266] Iteration 2300 (30.5968 iter/s, 3.26831s/100 iter), loss = 0.315221
I0109 20:07:45.157387  3950 solver.cpp:285]     Train net output #0: loss = 0.315221 (* 1 = 0.315221 loss)
I0109 20:07:45.157398  3950 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 20:07:48.429095  3950 solver.cpp:266] Iteration 2400 (30.5648 iter/s, 3.27174s/100 iter), loss = 0.13951
I0109 20:07:48.429165  3950 solver.cpp:285]     Train net output #0: loss = 0.13951 (* 1 = 0.13951 loss)
I0109 20:07:48.429178  3950 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 20:07:51.702250  3950 solver.cpp:266] Iteration 2500 (30.5519 iter/s, 3.27311s/100 iter), loss = 0.230387
I0109 20:07:51.702334  3950 solver.cpp:285]     Train net output #0: loss = 0.230387 (* 1 = 0.230387 loss)
I0109 20:07:51.702350  3950 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 20:07:54.972445  3950 solver.cpp:266] Iteration 2600 (30.58 iter/s, 3.27011s/100 iter), loss = 0.203601
I0109 20:07:54.972510  3950 solver.cpp:285]     Train net output #0: loss = 0.203601 (* 1 = 0.203601 loss)
I0109 20:07:54.972523  3950 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 20:07:58.241763  3950 solver.cpp:266] Iteration 2700 (30.5877 iter/s, 3.26928s/100 iter), loss = 0.264907
I0109 20:07:58.241967  3950 solver.cpp:285]     Train net output #0: loss = 0.264907 (* 1 = 0.264907 loss)
I0109 20:07:58.241983  3950 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 20:08:01.518900  3950 solver.cpp:266] Iteration 2800 (30.5161 iter/s, 3.27696s/100 iter), loss = 0.3545
I0109 20:08:01.518983  3950 solver.cpp:285]     Train net output #0: loss = 0.3545 (* 1 = 0.3545 loss)
I0109 20:08:01.518999  3950 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 20:08:04.796851  3950 solver.cpp:266] Iteration 2900 (30.5076 iter/s, 3.27787s/100 iter), loss = 0.21514
I0109 20:08:04.796931  3950 solver.cpp:285]     Train net output #0: loss = 0.21514 (* 1 = 0.21514 loss)
I0109 20:08:04.796947  3950 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 20:08:08.041453  3950 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 20:08:08.860203  3950 solver.cpp:517]     Test net output #0: loss = 0.576357 (* 1 = 0.576357 loss)
I0109 20:08:08.860240  3950 solver.cpp:517]     Test net output #1: top-1 = 0.822666
I0109 20:08:08.860250  3950 solver.cpp:517]     Test net output #2: top-5 = 0.986334
I0109 20:08:08.891103  3950 solver.cpp:266] Iteration 3000 (24.4247 iter/s, 4.09422s/100 iter), loss = 0.287279
I0109 20:08:08.891158  3950 solver.cpp:285]     Train net output #0: loss = 0.287279 (* 1 = 0.287279 loss)
I0109 20:08:08.891173  3950 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 20:08:12.166908  3950 solver.cpp:266] Iteration 3100 (30.5271 iter/s, 3.27578s/100 iter), loss = 0.219747
I0109 20:08:12.166981  3950 solver.cpp:285]     Train net output #0: loss = 0.219747 (* 1 = 0.219747 loss)
I0109 20:08:12.166996  3950 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 20:08:15.445089  3950 solver.cpp:266] Iteration 3200 (30.5051 iter/s, 3.27814s/100 iter), loss = 0.235709
I0109 20:08:15.445159  3950 solver.cpp:285]     Train net output #0: loss = 0.235709 (* 1 = 0.235709 loss)
I0109 20:08:15.445174  3950 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 20:08:18.728230  3950 solver.cpp:266] Iteration 3300 (30.459 iter/s, 3.2831s/100 iter), loss = 0.244951
I0109 20:08:18.728307  3950 solver.cpp:285]     Train net output #0: loss = 0.244951 (* 1 = 0.244951 loss)
I0109 20:08:18.728323  3950 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 20:08:22.006609  3950 solver.cpp:266] Iteration 3400 (30.5035 iter/s, 3.27831s/100 iter), loss = 0.226927
I0109 20:08:22.006671  3950 solver.cpp:285]     Train net output #0: loss = 0.226927 (* 1 = 0.226927 loss)
I0109 20:08:22.006685  3950 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 20:08:25.279326  3950 solver.cpp:266] Iteration 3500 (30.5559 iter/s, 3.27269s/100 iter), loss = 0.249466
I0109 20:08:25.279394  3950 solver.cpp:285]     Train net output #0: loss = 0.249466 (* 1 = 0.249466 loss)
I0109 20:08:25.279407  3950 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 20:08:28.552768  3950 solver.cpp:266] Iteration 3600 (30.5492 iter/s, 3.27341s/100 iter), loss = 0.189023
I0109 20:08:28.552925  3950 solver.cpp:285]     Train net output #0: loss = 0.189023 (* 1 = 0.189023 loss)
I0109 20:08:28.552942  3950 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 20:08:31.827869  3950 solver.cpp:266] Iteration 3700 (30.5348 iter/s, 3.27495s/100 iter), loss = 0.20699
I0109 20:08:31.827946  3950 solver.cpp:285]     Train net output #0: loss = 0.20699 (* 1 = 0.20699 loss)
I0109 20:08:31.827962  3950 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 20:08:35.104040  3950 solver.cpp:266] Iteration 3800 (30.5238 iter/s, 3.27613s/100 iter), loss = 0.220234
I0109 20:08:35.104117  3950 solver.cpp:285]     Train net output #0: loss = 0.220234 (* 1 = 0.220234 loss)
I0109 20:08:35.104131  3950 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 20:08:38.379165  3950 solver.cpp:266] Iteration 3900 (30.5335 iter/s, 3.27509s/100 iter), loss = 0.247107
I0109 20:08:38.379235  3950 solver.cpp:285]     Train net output #0: loss = 0.247107 (* 1 = 0.247107 loss)
I0109 20:08:38.379247  3950 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 20:08:41.621538  3950 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 20:08:42.439488  3950 solver.cpp:517]     Test net output #0: loss = 0.521225 (* 1 = 0.521225 loss)
I0109 20:08:42.439528  3950 solver.cpp:517]     Test net output #1: top-1 = 0.841111
I0109 20:08:42.439535  3950 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0109 20:08:42.470612  3950 solver.cpp:266] Iteration 4000 (24.4414 iter/s, 4.09142s/100 iter), loss = 0.162557
I0109 20:08:42.470677  3950 solver.cpp:285]     Train net output #0: loss = 0.162557 (* 1 = 0.162557 loss)
I0109 20:08:42.470693  3950 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 20:08:45.748766  3950 solver.cpp:266] Iteration 4100 (30.5052 iter/s, 3.27813s/100 iter), loss = 0.211457
I0109 20:08:45.748834  3950 solver.cpp:285]     Train net output #0: loss = 0.211457 (* 1 = 0.211457 loss)
I0109 20:08:45.748850  3950 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 20:08:49.027714  3950 solver.cpp:266] Iteration 4200 (30.4981 iter/s, 3.27889s/100 iter), loss = 0.256352
I0109 20:08:49.027796  3950 solver.cpp:285]     Train net output #0: loss = 0.256352 (* 1 = 0.256352 loss)
I0109 20:08:49.027812  3950 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 20:08:52.308666  3950 solver.cpp:266] Iteration 4300 (30.4793 iter/s, 3.28091s/100 iter), loss = 0.204587
I0109 20:08:52.308737  3950 solver.cpp:285]     Train net output #0: loss = 0.204587 (* 1 = 0.204587 loss)
I0109 20:08:52.308751  3950 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 20:08:55.583801  3950 solver.cpp:266] Iteration 4400 (30.5334 iter/s, 3.2751s/100 iter), loss = 0.157437
I0109 20:08:55.583878  3950 solver.cpp:285]     Train net output #0: loss = 0.157437 (* 1 = 0.157437 loss)
I0109 20:08:55.583892  3950 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 20:08:58.862138  3950 solver.cpp:266] Iteration 4500 (30.5038 iter/s, 3.27828s/100 iter), loss = 0.300215
I0109 20:08:58.862329  3950 solver.cpp:285]     Train net output #0: loss = 0.300215 (* 1 = 0.300215 loss)
I0109 20:08:58.862349  3950 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 20:09:02.139786  3950 solver.cpp:266] Iteration 4600 (30.5111 iter/s, 3.2775s/100 iter), loss = 0.324875
I0109 20:09:02.139870  3950 solver.cpp:285]     Train net output #0: loss = 0.324875 (* 1 = 0.324875 loss)
I0109 20:09:02.139884  3950 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 20:09:05.416033  3950 solver.cpp:266] Iteration 4700 (30.5231 iter/s, 3.27621s/100 iter), loss = 0.162281
I0109 20:09:05.416100  3950 solver.cpp:285]     Train net output #0: loss = 0.162281 (* 1 = 0.162281 loss)
I0109 20:09:05.416112  3950 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 20:09:08.689882  3950 solver.cpp:266] Iteration 4800 (30.5453 iter/s, 3.27382s/100 iter), loss = 0.271291
I0109 20:09:08.689961  3950 solver.cpp:285]     Train net output #0: loss = 0.271291 (* 1 = 0.271291 loss)
I0109 20:09:08.689976  3950 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 20:09:11.967308  3950 solver.cpp:266] Iteration 4900 (30.5123 iter/s, 3.27736s/100 iter), loss = 0.197477
I0109 20:09:11.967386  3950 solver.cpp:285]     Train net output #0: loss = 0.197477 (* 1 = 0.197477 loss)
I0109 20:09:11.967403  3950 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 20:09:15.207604  3950 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 20:09:16.026193  3950 solver.cpp:517]     Test net output #0: loss = 0.565851 (* 1 = 0.565851 loss)
I0109 20:09:16.026230  3950 solver.cpp:517]     Test net output #1: top-1 = 0.822555
I0109 20:09:16.026240  3950 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0109 20:09:16.057132  3950 solver.cpp:266] Iteration 5000 (24.4511 iter/s, 4.0898s/100 iter), loss = 0.176854
I0109 20:09:16.057183  3950 solver.cpp:285]     Train net output #0: loss = 0.176854 (* 1 = 0.176854 loss)
I0109 20:09:16.057199  3950 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 20:09:19.326601  3950 solver.cpp:266] Iteration 5100 (30.5861 iter/s, 3.26946s/100 iter), loss = 0.201136
I0109 20:09:19.326668  3950 solver.cpp:285]     Train net output #0: loss = 0.201136 (* 1 = 0.201136 loss)
I0109 20:09:19.326680  3950 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 20:09:22.599066  3950 solver.cpp:266] Iteration 5200 (30.5582 iter/s, 3.27244s/100 iter), loss = 0.217404
I0109 20:09:22.599134  3950 solver.cpp:285]     Train net output #0: loss = 0.217404 (* 1 = 0.217404 loss)
I0109 20:09:22.599148  3950 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 20:09:25.868408  3950 solver.cpp:266] Iteration 5300 (30.5877 iter/s, 3.26929s/100 iter), loss = 0.23281
I0109 20:09:25.868474  3950 solver.cpp:285]     Train net output #0: loss = 0.23281 (* 1 = 0.23281 loss)
I0109 20:09:25.868487  3950 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 20:09:29.141633  3950 solver.cpp:266] Iteration 5400 (30.5512 iter/s, 3.2732s/100 iter), loss = 0.181121
I0109 20:09:29.141824  3950 solver.cpp:285]     Train net output #0: loss = 0.181121 (* 1 = 0.181121 loss)
I0109 20:09:29.141840  3950 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 20:09:32.412731  3950 solver.cpp:266] Iteration 5500 (30.5722 iter/s, 3.27095s/100 iter), loss = 0.213851
I0109 20:09:32.412798  3950 solver.cpp:285]     Train net output #0: loss = 0.213851 (* 1 = 0.213851 loss)
I0109 20:09:32.412811  3950 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 20:09:35.683977  3950 solver.cpp:266] Iteration 5600 (30.5697 iter/s, 3.27122s/100 iter), loss = 0.215162
I0109 20:09:35.684056  3950 solver.cpp:285]     Train net output #0: loss = 0.215162 (* 1 = 0.215162 loss)
I0109 20:09:35.684069  3950 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 20:09:38.956531  3950 solver.cpp:266] Iteration 5700 (30.5578 iter/s, 3.27249s/100 iter), loss = 0.175326
I0109 20:09:38.956607  3950 solver.cpp:285]     Train net output #0: loss = 0.175326 (* 1 = 0.175326 loss)
I0109 20:09:38.956624  3950 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 20:09:42.229112  3950 solver.cpp:266] Iteration 5800 (30.5573 iter/s, 3.27255s/100 iter), loss = 0.151522
I0109 20:09:42.229190  3950 solver.cpp:285]     Train net output #0: loss = 0.151522 (* 1 = 0.151522 loss)
I0109 20:09:42.229207  3950 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 20:09:45.501683  3950 solver.cpp:266] Iteration 5900 (30.5573 iter/s, 3.27254s/100 iter), loss = 0.165015
I0109 20:09:45.501747  3950 solver.cpp:285]     Train net output #0: loss = 0.165015 (* 1 = 0.165015 loss)
I0109 20:09:45.501760  3950 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 20:09:48.740465  3950 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 20:09:49.556329  3950 solver.cpp:517]     Test net output #0: loss = 0.497526 (* 1 = 0.497526 loss)
I0109 20:09:49.556367  3950 solver.cpp:517]     Test net output #1: top-1 = 0.838778
I0109 20:09:49.556375  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 20:09:49.587360  3950 solver.cpp:266] Iteration 6000 (24.4758 iter/s, 4.08566s/100 iter), loss = 0.353732
I0109 20:09:49.587429  3950 solver.cpp:285]     Train net output #0: loss = 0.353732 (* 1 = 0.353732 loss)
I0109 20:09:49.587445  3950 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 20:09:52.854902  3950 solver.cpp:266] Iteration 6100 (30.6046 iter/s, 3.26749s/100 iter), loss = 0.288782
I0109 20:09:52.854975  3950 solver.cpp:285]     Train net output #0: loss = 0.288782 (* 1 = 0.288782 loss)
I0109 20:09:52.854990  3950 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 20:09:56.127094  3950 solver.cpp:266] Iteration 6200 (30.5609 iter/s, 3.27216s/100 iter), loss = 0.148271
I0109 20:09:56.127161  3950 solver.cpp:285]     Train net output #0: loss = 0.148271 (* 1 = 0.148271 loss)
I0109 20:09:56.127176  3950 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 20:09:59.401823  3950 solver.cpp:266] Iteration 6300 (30.5372 iter/s, 3.2747s/100 iter), loss = 0.189239
I0109 20:09:59.402031  3950 solver.cpp:285]     Train net output #0: loss = 0.189239 (* 1 = 0.189239 loss)
I0109 20:09:59.402050  3950 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 20:10:02.679834  3950 solver.cpp:266] Iteration 6400 (30.5079 iter/s, 3.27784s/100 iter), loss = 0.2553
I0109 20:10:02.679921  3950 solver.cpp:285]     Train net output #0: loss = 0.2553 (* 1 = 0.2553 loss)
I0109 20:10:02.679936  3950 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 20:10:05.955806  3950 solver.cpp:266] Iteration 6500 (30.526 iter/s, 3.2759s/100 iter), loss = 0.235362
I0109 20:10:05.955871  3950 solver.cpp:285]     Train net output #0: loss = 0.235362 (* 1 = 0.235362 loss)
I0109 20:10:05.955884  3950 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 20:10:09.228371  3950 solver.cpp:266] Iteration 6600 (30.5574 iter/s, 3.27253s/100 iter), loss = 0.14796
I0109 20:10:09.228440  3950 solver.cpp:285]     Train net output #0: loss = 0.14796 (* 1 = 0.14796 loss)
I0109 20:10:09.228454  3950 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 20:10:12.499919  3950 solver.cpp:266] Iteration 6700 (30.5669 iter/s, 3.27152s/100 iter), loss = 0.228218
I0109 20:10:12.499986  3950 solver.cpp:285]     Train net output #0: loss = 0.228218 (* 1 = 0.228218 loss)
I0109 20:10:12.500000  3950 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 20:10:15.771486  3950 solver.cpp:266] Iteration 6800 (30.5667 iter/s, 3.27153s/100 iter), loss = 0.241584
I0109 20:10:15.771549  3950 solver.cpp:285]     Train net output #0: loss = 0.241584 (* 1 = 0.241584 loss)
I0109 20:10:15.771562  3950 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 20:10:19.053086  3950 solver.cpp:266] Iteration 6900 (30.4735 iter/s, 3.28154s/100 iter), loss = 0.164209
I0109 20:10:19.053154  3950 solver.cpp:285]     Train net output #0: loss = 0.164209 (* 1 = 0.164209 loss)
I0109 20:10:19.053170  3950 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 20:10:22.298640  3950 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 20:10:23.117079  3950 solver.cpp:517]     Test net output #0: loss = 0.511974 (* 1 = 0.511974 loss)
I0109 20:10:23.117117  3950 solver.cpp:517]     Test net output #1: top-1 = 0.841111
I0109 20:10:23.117127  3950 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0109 20:10:23.147986  3950 solver.cpp:266] Iteration 7000 (24.4207 iter/s, 4.09488s/100 iter), loss = 0.232642
I0109 20:10:23.148027  3950 solver.cpp:285]     Train net output #0: loss = 0.232642 (* 1 = 0.232642 loss)
I0109 20:10:23.148042  3950 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 20:10:26.417629  3950 solver.cpp:266] Iteration 7100 (30.5845 iter/s, 3.26963s/100 iter), loss = 0.301518
I0109 20:10:26.417699  3950 solver.cpp:285]     Train net output #0: loss = 0.301518 (* 1 = 0.301518 loss)
I0109 20:10:26.417713  3950 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 20:10:29.689785  3950 solver.cpp:266] Iteration 7200 (30.5613 iter/s, 3.27212s/100 iter), loss = 0.185664
I0109 20:10:29.689977  3950 solver.cpp:285]     Train net output #0: loss = 0.185664 (* 1 = 0.185664 loss)
I0109 20:10:29.689994  3950 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 20:10:32.965023  3950 solver.cpp:266] Iteration 7300 (30.5338 iter/s, 3.27506s/100 iter), loss = 0.201895
I0109 20:10:32.965095  3950 solver.cpp:285]     Train net output #0: loss = 0.201895 (* 1 = 0.201895 loss)
I0109 20:10:32.965111  3950 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 20:10:36.233232  3950 solver.cpp:266] Iteration 7400 (30.5982 iter/s, 3.26817s/100 iter), loss = 0.249434
I0109 20:10:36.233299  3950 solver.cpp:285]     Train net output #0: loss = 0.249434 (* 1 = 0.249434 loss)
I0109 20:10:36.233314  3950 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 20:10:39.501996  3950 solver.cpp:266] Iteration 7500 (30.5929 iter/s, 3.26873s/100 iter), loss = 0.153431
I0109 20:10:39.502058  3950 solver.cpp:285]     Train net output #0: loss = 0.153431 (* 1 = 0.153431 loss)
I0109 20:10:39.502071  3950 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 20:10:42.768968  3950 solver.cpp:266] Iteration 7600 (30.6097 iter/s, 3.26694s/100 iter), loss = 0.156055
I0109 20:10:42.769032  3950 solver.cpp:285]     Train net output #0: loss = 0.156055 (* 1 = 0.156055 loss)
I0109 20:10:42.769044  3950 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 20:10:46.035949  3950 solver.cpp:266] Iteration 7700 (30.6098 iter/s, 3.26692s/100 iter), loss = 0.233863
I0109 20:10:46.036010  3950 solver.cpp:285]     Train net output #0: loss = 0.233863 (* 1 = 0.233863 loss)
I0109 20:10:46.036022  3950 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 20:10:49.307659  3950 solver.cpp:266] Iteration 7800 (30.5653 iter/s, 3.27168s/100 iter), loss = 0.16556
I0109 20:10:49.307719  3950 solver.cpp:285]     Train net output #0: loss = 0.16556 (* 1 = 0.16556 loss)
I0109 20:10:49.307732  3950 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 20:10:52.579603  3950 solver.cpp:266] Iteration 7900 (30.5631 iter/s, 3.27192s/100 iter), loss = 0.188774
I0109 20:10:52.579665  3950 solver.cpp:285]     Train net output #0: loss = 0.188774 (* 1 = 0.188774 loss)
I0109 20:10:52.579677  3950 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 20:10:55.814887  3950 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 20:10:56.632840  3950 solver.cpp:517]     Test net output #0: loss = 0.499425 (* 1 = 0.499425 loss)
I0109 20:10:56.632877  3950 solver.cpp:517]     Test net output #1: top-1 = 0.838555
I0109 20:10:56.632885  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 20:10:56.663722  3950 solver.cpp:266] Iteration 8000 (24.4852 iter/s, 4.0841s/100 iter), loss = 0.320673
I0109 20:10:56.663764  3950 solver.cpp:285]     Train net output #0: loss = 0.320673 (* 1 = 0.320673 loss)
I0109 20:10:56.663779  3950 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 20:10:59.929015  3950 solver.cpp:266] Iteration 8100 (30.6255 iter/s, 3.26525s/100 iter), loss = 0.282551
I0109 20:10:59.929193  3950 solver.cpp:285]     Train net output #0: loss = 0.282551 (* 1 = 0.282551 loss)
I0109 20:10:59.929209  3950 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 20:11:03.198446  3950 solver.cpp:266] Iteration 8200 (30.5877 iter/s, 3.26929s/100 iter), loss = 0.134757
I0109 20:11:03.198508  3950 solver.cpp:285]     Train net output #0: loss = 0.134757 (* 1 = 0.134757 loss)
I0109 20:11:03.198519  3950 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 20:11:06.469959  3950 solver.cpp:266] Iteration 8300 (30.5672 iter/s, 3.27148s/100 iter), loss = 0.276138
I0109 20:11:06.470021  3950 solver.cpp:285]     Train net output #0: loss = 0.276138 (* 1 = 0.276138 loss)
I0109 20:11:06.470033  3950 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 20:11:09.738822  3950 solver.cpp:266] Iteration 8400 (30.592 iter/s, 3.26883s/100 iter), loss = 0.163547
I0109 20:11:09.738888  3950 solver.cpp:285]     Train net output #0: loss = 0.163547 (* 1 = 0.163547 loss)
I0109 20:11:09.738900  3950 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 20:11:13.006145  3950 solver.cpp:266] Iteration 8500 (30.6067 iter/s, 3.26726s/100 iter), loss = 0.210548
I0109 20:11:13.006211  3950 solver.cpp:285]     Train net output #0: loss = 0.210548 (* 1 = 0.210548 loss)
I0109 20:11:13.006224  3950 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 20:11:16.275630  3950 solver.cpp:266] Iteration 8600 (30.5862 iter/s, 3.26945s/100 iter), loss = 0.309586
I0109 20:11:16.275691  3950 solver.cpp:285]     Train net output #0: loss = 0.309586 (* 1 = 0.309586 loss)
I0109 20:11:16.275703  3950 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 20:11:19.545017  3950 solver.cpp:266] Iteration 8700 (30.587 iter/s, 3.26936s/100 iter), loss = 0.139644
I0109 20:11:19.545079  3950 solver.cpp:285]     Train net output #0: loss = 0.139644 (* 1 = 0.139644 loss)
I0109 20:11:19.545091  3950 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 20:11:22.815218  3950 solver.cpp:266] Iteration 8800 (30.5794 iter/s, 3.27017s/100 iter), loss = 0.196038
I0109 20:11:22.815284  3950 solver.cpp:285]     Train net output #0: loss = 0.196038 (* 1 = 0.196038 loss)
I0109 20:11:22.815295  3950 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 20:11:26.083972  3950 solver.cpp:266] Iteration 8900 (30.5933 iter/s, 3.26869s/100 iter), loss = 0.242503
I0109 20:11:26.084045  3950 solver.cpp:285]     Train net output #0: loss = 0.242503 (* 1 = 0.242503 loss)
I0109 20:11:26.084061  3950 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 20:11:29.322204  3950 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 20:11:30.138418  3950 solver.cpp:517]     Test net output #0: loss = 0.524241 (* 1 = 0.524241 loss)
I0109 20:11:30.138571  3950 solver.cpp:517]     Test net output #1: top-1 = 0.837556
I0109 20:11:30.138582  3950 solver.cpp:517]     Test net output #2: top-5 = 0.991222
I0109 20:11:30.169466  3950 solver.cpp:266] Iteration 9000 (24.477 iter/s, 4.08547s/100 iter), loss = 0.210659
I0109 20:11:30.169514  3950 solver.cpp:285]     Train net output #0: loss = 0.210659 (* 1 = 0.210659 loss)
I0109 20:11:30.169529  3950 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 20:11:33.438326  3950 solver.cpp:266] Iteration 9100 (30.5919 iter/s, 3.26884s/100 iter), loss = 0.130355
I0109 20:11:33.438387  3950 solver.cpp:285]     Train net output #0: loss = 0.130355 (* 1 = 0.130355 loss)
I0109 20:11:33.438400  3950 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 20:11:36.708932  3950 solver.cpp:266] Iteration 9200 (30.5756 iter/s, 3.27058s/100 iter), loss = 0.262865
I0109 20:11:36.708997  3950 solver.cpp:285]     Train net output #0: loss = 0.262865 (* 1 = 0.262865 loss)
I0109 20:11:36.709008  3950 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 20:11:39.978449  3950 solver.cpp:266] Iteration 9300 (30.5861 iter/s, 3.26946s/100 iter), loss = 0.186552
I0109 20:11:39.978510  3950 solver.cpp:285]     Train net output #0: loss = 0.186552 (* 1 = 0.186552 loss)
I0109 20:11:39.978523  3950 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 20:11:43.248610  3950 solver.cpp:266] Iteration 9400 (30.5798 iter/s, 3.27013s/100 iter), loss = 0.175206
I0109 20:11:43.248673  3950 solver.cpp:285]     Train net output #0: loss = 0.175206 (* 1 = 0.175206 loss)
I0109 20:11:43.248687  3950 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 20:11:46.518427  3950 solver.cpp:266] Iteration 9500 (30.5831 iter/s, 3.26978s/100 iter), loss = 0.229446
I0109 20:11:46.518491  3950 solver.cpp:285]     Train net output #0: loss = 0.229446 (* 1 = 0.229446 loss)
I0109 20:11:46.518502  3950 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 20:11:49.785814  3950 solver.cpp:266] Iteration 9600 (30.6058 iter/s, 3.26735s/100 iter), loss = 0.240149
I0109 20:11:49.785881  3950 solver.cpp:285]     Train net output #0: loss = 0.240149 (* 1 = 0.240149 loss)
I0109 20:11:49.785895  3950 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 20:11:53.055034  3950 solver.cpp:266] Iteration 9700 (30.5889 iter/s, 3.26916s/100 iter), loss = 0.237538
I0109 20:11:53.055094  3950 solver.cpp:285]     Train net output #0: loss = 0.237538 (* 1 = 0.237538 loss)
I0109 20:11:53.055107  3950 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 20:11:56.323320  3950 solver.cpp:266] Iteration 9800 (30.5973 iter/s, 3.26826s/100 iter), loss = 0.133486
I0109 20:11:56.323385  3950 solver.cpp:285]     Train net output #0: loss = 0.133486 (* 1 = 0.133486 loss)
I0109 20:11:56.323397  3950 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 20:11:59.593089  3950 solver.cpp:266] Iteration 9900 (30.5835 iter/s, 3.26974s/100 iter), loss = 0.110936
I0109 20:11:59.593152  3950 solver.cpp:285]     Train net output #0: loss = 0.110936 (* 1 = 0.110936 loss)
I0109 20:11:59.593164  3950 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 20:12:02.829763  3950 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 20:12:03.648407  3950 solver.cpp:517]     Test net output #0: loss = 0.527348 (* 1 = 0.527348 loss)
I0109 20:12:03.648445  3950 solver.cpp:517]     Test net output #1: top-1 = 0.836
I0109 20:12:03.648452  3950 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0109 20:12:03.679312  3950 solver.cpp:266] Iteration 10000 (24.4726 iter/s, 4.0862s/100 iter), loss = 0.194411
I0109 20:12:03.679363  3950 solver.cpp:285]     Train net output #0: loss = 0.194411 (* 1 = 0.194411 loss)
I0109 20:12:03.679378  3950 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 20:12:06.948285  3950 solver.cpp:266] Iteration 10100 (30.5911 iter/s, 3.26892s/100 iter), loss = 0.159163
I0109 20:12:06.948348  3950 solver.cpp:285]     Train net output #0: loss = 0.159163 (* 1 = 0.159163 loss)
I0109 20:12:06.948361  3950 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 20:12:10.217627  3950 solver.cpp:266] Iteration 10200 (30.5875 iter/s, 3.26931s/100 iter), loss = 0.135285
I0109 20:12:10.217691  3950 solver.cpp:285]     Train net output #0: loss = 0.135285 (* 1 = 0.135285 loss)
I0109 20:12:10.217705  3950 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 20:12:13.487900  3950 solver.cpp:266] Iteration 10300 (30.5788 iter/s, 3.27024s/100 iter), loss = 0.181537
I0109 20:12:13.487962  3950 solver.cpp:285]     Train net output #0: loss = 0.181537 (* 1 = 0.181537 loss)
I0109 20:12:13.487973  3950 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 20:12:16.756021  3950 solver.cpp:266] Iteration 10400 (30.5989 iter/s, 3.26809s/100 iter), loss = 0.295013
I0109 20:12:16.756084  3950 solver.cpp:285]     Train net output #0: loss = 0.295013 (* 1 = 0.295013 loss)
I0109 20:12:16.756096  3950 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 20:12:20.024874  3950 solver.cpp:266] Iteration 10500 (30.5923 iter/s, 3.2688s/100 iter), loss = 0.239023
I0109 20:12:20.024935  3950 solver.cpp:285]     Train net output #0: loss = 0.239023 (* 1 = 0.239023 loss)
I0109 20:12:20.024947  3950 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 20:12:23.293493  3950 solver.cpp:266] Iteration 10600 (30.5942 iter/s, 3.26859s/100 iter), loss = 0.183834
I0109 20:12:23.293556  3950 solver.cpp:285]     Train net output #0: loss = 0.183834 (* 1 = 0.183834 loss)
I0109 20:12:23.293570  3950 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 20:12:26.563527  3950 solver.cpp:266] Iteration 10700 (30.581 iter/s, 3.27s/100 iter), loss = 0.168115
I0109 20:12:26.563588  3950 solver.cpp:285]     Train net output #0: loss = 0.168115 (* 1 = 0.168115 loss)
I0109 20:12:26.563601  3950 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 20:12:29.833828  3950 solver.cpp:266] Iteration 10800 (30.5785 iter/s, 3.27027s/100 iter), loss = 0.165283
I0109 20:12:29.833894  3950 solver.cpp:285]     Train net output #0: loss = 0.165283 (* 1 = 0.165283 loss)
I0109 20:12:29.833906  3950 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 20:12:33.105542  3950 solver.cpp:266] Iteration 10900 (30.5656 iter/s, 3.27165s/100 iter), loss = 0.184226
I0109 20:12:33.105729  3950 solver.cpp:285]     Train net output #0: loss = 0.184226 (* 1 = 0.184226 loss)
I0109 20:12:33.105748  3950 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 20:12:36.346508  3950 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 20:12:37.164674  3950 solver.cpp:517]     Test net output #0: loss = 0.472024 (* 1 = 0.472024 loss)
I0109 20:12:37.164710  3950 solver.cpp:517]     Test net output #1: top-1 = 0.848
I0109 20:12:37.164717  3950 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 20:12:37.195492  3950 solver.cpp:266] Iteration 11000 (24.451 iter/s, 4.08981s/100 iter), loss = 0.195851
I0109 20:12:37.195528  3950 solver.cpp:285]     Train net output #0: loss = 0.195851 (* 1 = 0.195851 loss)
I0109 20:12:37.195542  3950 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 20:12:40.466356  3950 solver.cpp:266] Iteration 11100 (30.573 iter/s, 3.27086s/100 iter), loss = 0.130146
I0109 20:12:40.466425  3950 solver.cpp:285]     Train net output #0: loss = 0.130146 (* 1 = 0.130146 loss)
I0109 20:12:40.466439  3950 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 20:12:43.737324  3950 solver.cpp:266] Iteration 11200 (30.5723 iter/s, 3.27093s/100 iter), loss = 0.198167
I0109 20:12:43.737388  3950 solver.cpp:285]     Train net output #0: loss = 0.198167 (* 1 = 0.198167 loss)
I0109 20:12:43.737401  3950 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 20:12:47.012657  3950 solver.cpp:266] Iteration 11300 (30.5318 iter/s, 3.27528s/100 iter), loss = 0.156581
I0109 20:12:47.012722  3950 solver.cpp:285]     Train net output #0: loss = 0.156581 (* 1 = 0.156581 loss)
I0109 20:12:47.012737  3950 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 20:12:50.283109  3950 solver.cpp:266] Iteration 11400 (30.5771 iter/s, 3.27042s/100 iter), loss = 0.202616
I0109 20:12:50.283171  3950 solver.cpp:285]     Train net output #0: loss = 0.202616 (* 1 = 0.202616 loss)
I0109 20:12:50.283183  3950 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 20:12:53.555867  3950 solver.cpp:266] Iteration 11500 (30.5556 iter/s, 3.27273s/100 iter), loss = 0.191256
I0109 20:12:53.555932  3950 solver.cpp:285]     Train net output #0: loss = 0.191256 (* 1 = 0.191256 loss)
I0109 20:12:53.555943  3950 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 20:12:56.827083  3950 solver.cpp:266] Iteration 11600 (30.57 iter/s, 3.27118s/100 iter), loss = 0.233041
I0109 20:12:56.827152  3950 solver.cpp:285]     Train net output #0: loss = 0.233041 (* 1 = 0.233041 loss)
I0109 20:12:56.827164  3950 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 20:13:00.096518  3950 solver.cpp:266] Iteration 11700 (30.5869 iter/s, 3.26937s/100 iter), loss = 0.15764
I0109 20:13:00.096580  3950 solver.cpp:285]     Train net output #0: loss = 0.15764 (* 1 = 0.15764 loss)
I0109 20:13:00.096592  3950 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 20:13:03.366983  3950 solver.cpp:266] Iteration 11800 (30.577 iter/s, 3.27044s/100 iter), loss = 0.137438
I0109 20:13:03.367169  3950 solver.cpp:285]     Train net output #0: loss = 0.137438 (* 1 = 0.137438 loss)
I0109 20:13:03.367184  3950 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 20:13:06.637657  3950 solver.cpp:266] Iteration 11900 (30.5762 iter/s, 3.27052s/100 iter), loss = 0.203472
I0109 20:13:06.637722  3950 solver.cpp:285]     Train net output #0: loss = 0.203472 (* 1 = 0.203472 loss)
I0109 20:13:06.637735  3950 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 20:13:09.875422  3950 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 20:13:10.694031  3950 solver.cpp:517]     Test net output #0: loss = 0.495997 (* 1 = 0.495997 loss)
I0109 20:13:10.694072  3950 solver.cpp:517]     Test net output #1: top-1 = 0.844111
I0109 20:13:10.694078  3950 solver.cpp:517]     Test net output #2: top-5 = 0.990445
I0109 20:13:10.724941  3950 solver.cpp:266] Iteration 12000 (24.4662 iter/s, 4.08726s/100 iter), loss = 0.215313
I0109 20:13:10.724985  3950 solver.cpp:285]     Train net output #0: loss = 0.215312 (* 1 = 0.215312 loss)
I0109 20:13:10.725000  3950 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 20:13:13.994802  3950 solver.cpp:266] Iteration 12100 (30.5827 iter/s, 3.26982s/100 iter), loss = 0.190758
I0109 20:13:13.994865  3950 solver.cpp:285]     Train net output #0: loss = 0.190758 (* 1 = 0.190758 loss)
I0109 20:13:13.994879  3950 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 20:13:17.264094  3950 solver.cpp:266] Iteration 12200 (30.588 iter/s, 3.26926s/100 iter), loss = 0.195472
I0109 20:13:17.264158  3950 solver.cpp:285]     Train net output #0: loss = 0.195472 (* 1 = 0.195472 loss)
I0109 20:13:17.264169  3950 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 20:13:20.533217  3950 solver.cpp:266] Iteration 12300 (30.5895 iter/s, 3.26909s/100 iter), loss = 0.227064
I0109 20:13:20.533289  3950 solver.cpp:285]     Train net output #0: loss = 0.227064 (* 1 = 0.227064 loss)
I0109 20:13:20.533308  3950 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 20:13:23.803557  3950 solver.cpp:266] Iteration 12400 (30.5782 iter/s, 3.2703s/100 iter), loss = 0.171203
I0109 20:13:23.803627  3950 solver.cpp:285]     Train net output #0: loss = 0.171203 (* 1 = 0.171203 loss)
I0109 20:13:23.803647  3950 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 20:13:27.071708  3950 solver.cpp:266] Iteration 12500 (30.5989 iter/s, 3.26809s/100 iter), loss = 0.138298
I0109 20:13:27.071771  3950 solver.cpp:285]     Train net output #0: loss = 0.138298 (* 1 = 0.138298 loss)
I0109 20:13:27.071785  3950 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 20:13:30.340615  3950 solver.cpp:266] Iteration 12600 (30.5916 iter/s, 3.26887s/100 iter), loss = 0.107854
I0109 20:13:30.340682  3950 solver.cpp:285]     Train net output #0: loss = 0.107854 (* 1 = 0.107854 loss)
I0109 20:13:30.340693  3950 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 20:13:33.611415  3950 solver.cpp:266] Iteration 12700 (30.5739 iter/s, 3.27076s/100 iter), loss = 0.105748
I0109 20:13:33.611614  3950 solver.cpp:285]     Train net output #0: loss = 0.105748 (* 1 = 0.105748 loss)
I0109 20:13:33.611629  3950 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 20:13:36.879312  3950 solver.cpp:266] Iteration 12800 (30.6025 iter/s, 3.26771s/100 iter), loss = 0.172627
I0109 20:13:36.879374  3950 solver.cpp:285]     Train net output #0: loss = 0.172627 (* 1 = 0.172627 loss)
I0109 20:13:36.879387  3950 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 20:13:40.148483  3950 solver.cpp:266] Iteration 12900 (30.5891 iter/s, 3.26914s/100 iter), loss = 0.183042
I0109 20:13:40.148546  3950 solver.cpp:285]     Train net output #0: loss = 0.183042 (* 1 = 0.183042 loss)
I0109 20:13:40.148560  3950 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 20:13:43.387380  3950 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 20:13:44.205075  3950 solver.cpp:517]     Test net output #0: loss = 0.52353 (* 1 = 0.52353 loss)
I0109 20:13:44.205112  3950 solver.cpp:517]     Test net output #1: top-1 = 0.838445
I0109 20:13:44.205122  3950 solver.cpp:517]     Test net output #2: top-5 = 0.993778
I0109 20:13:44.235976  3950 solver.cpp:266] Iteration 13000 (24.465 iter/s, 4.08748s/100 iter), loss = 0.210506
I0109 20:13:44.236012  3950 solver.cpp:285]     Train net output #0: loss = 0.210506 (* 1 = 0.210506 loss)
I0109 20:13:44.236027  3950 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 20:13:47.497140  3950 solver.cpp:266] Iteration 13100 (30.664 iter/s, 3.26115s/100 iter), loss = 0.184861
I0109 20:13:47.497201  3950 solver.cpp:285]     Train net output #0: loss = 0.184861 (* 1 = 0.184861 loss)
I0109 20:13:47.497213  3950 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 20:13:50.766218  3950 solver.cpp:266] Iteration 13200 (30.59 iter/s, 3.26905s/100 iter), loss = 0.255893
I0109 20:13:50.766281  3950 solver.cpp:285]     Train net output #0: loss = 0.255893 (* 1 = 0.255893 loss)
I0109 20:13:50.766294  3950 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 20:13:54.035763  3950 solver.cpp:266] Iteration 13300 (30.5858 iter/s, 3.26949s/100 iter), loss = 0.117576
I0109 20:13:54.035825  3950 solver.cpp:285]     Train net output #0: loss = 0.117576 (* 1 = 0.117576 loss)
I0109 20:13:54.035838  3950 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 20:13:57.305116  3950 solver.cpp:266] Iteration 13400 (30.5874 iter/s, 3.26932s/100 iter), loss = 0.275576
I0109 20:13:57.305178  3950 solver.cpp:285]     Train net output #0: loss = 0.275576 (* 1 = 0.275576 loss)
I0109 20:13:57.305191  3950 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 20:14:00.571859  3950 solver.cpp:266] Iteration 13500 (30.6118 iter/s, 3.26671s/100 iter), loss = 0.208169
I0109 20:14:00.571923  3950 solver.cpp:285]     Train net output #0: loss = 0.208169 (* 1 = 0.208169 loss)
I0109 20:14:00.571934  3950 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 20:14:03.839519  3950 solver.cpp:266] Iteration 13600 (30.6033 iter/s, 3.26763s/100 iter), loss = 0.155261
I0109 20:14:03.839624  3950 solver.cpp:285]     Train net output #0: loss = 0.155261 (* 1 = 0.155261 loss)
I0109 20:14:03.839638  3950 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 20:14:07.106757  3950 solver.cpp:266] Iteration 13700 (30.6078 iter/s, 3.26714s/100 iter), loss = 0.187595
I0109 20:14:07.106817  3950 solver.cpp:285]     Train net output #0: loss = 0.187595 (* 1 = 0.187595 loss)
I0109 20:14:07.106832  3950 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 20:14:10.374194  3950 solver.cpp:266] Iteration 13800 (30.6053 iter/s, 3.26741s/100 iter), loss = 0.137759
I0109 20:14:10.374259  3950 solver.cpp:285]     Train net output #0: loss = 0.137759 (* 1 = 0.137759 loss)
I0109 20:14:10.374274  3950 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 20:14:13.642771  3950 solver.cpp:266] Iteration 13900 (30.5947 iter/s, 3.26854s/100 iter), loss = 0.164617
I0109 20:14:13.642832  3950 solver.cpp:285]     Train net output #0: loss = 0.164617 (* 1 = 0.164617 loss)
I0109 20:14:13.642845  3950 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 20:14:16.877444  3950 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 20:14:17.690582  3950 solver.cpp:517]     Test net output #0: loss = 0.473517 (* 1 = 0.473517 loss)
I0109 20:14:17.690618  3950 solver.cpp:517]     Test net output #1: top-1 = 0.851778
I0109 20:14:17.690625  3950 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 20:14:17.721608  3950 solver.cpp:266] Iteration 14000 (24.517 iter/s, 4.0788s/100 iter), loss = 0.164301
I0109 20:14:17.721660  3950 solver.cpp:285]     Train net output #0: loss = 0.164301 (* 1 = 0.164301 loss)
I0109 20:14:17.721675  3950 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 20:14:20.986369  3950 solver.cpp:266] Iteration 14100 (30.6306 iter/s, 3.26471s/100 iter), loss = 0.194188
I0109 20:14:20.986431  3950 solver.cpp:285]     Train net output #0: loss = 0.194188 (* 1 = 0.194188 loss)
I0109 20:14:20.986444  3950 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 20:14:24.251238  3950 solver.cpp:266] Iteration 14200 (30.6294 iter/s, 3.26484s/100 iter), loss = 0.211554
I0109 20:14:24.251300  3950 solver.cpp:285]     Train net output #0: loss = 0.211554 (* 1 = 0.211554 loss)
I0109 20:14:24.251312  3950 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 20:14:27.520447  3950 solver.cpp:266] Iteration 14300 (30.5887 iter/s, 3.26918s/100 iter), loss = 0.261041
I0109 20:14:27.520510  3950 solver.cpp:285]     Train net output #0: loss = 0.261041 (* 1 = 0.261041 loss)
I0109 20:14:27.520522  3950 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 20:14:30.790166  3950 solver.cpp:266] Iteration 14400 (30.584 iter/s, 3.26969s/100 iter), loss = 0.0647778
I0109 20:14:30.790230  3950 solver.cpp:285]     Train net output #0: loss = 0.0647777 (* 1 = 0.0647777 loss)
I0109 20:14:30.790241  3950 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 20:14:34.059751  3950 solver.cpp:266] Iteration 14500 (30.5855 iter/s, 3.26953s/100 iter), loss = 0.117428
I0109 20:14:34.059943  3950 solver.cpp:285]     Train net output #0: loss = 0.117427 (* 1 = 0.117427 loss)
I0109 20:14:34.059964  3950 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 20:14:37.328963  3950 solver.cpp:266] Iteration 14600 (30.5899 iter/s, 3.26905s/100 iter), loss = 0.21203
I0109 20:14:37.329025  3950 solver.cpp:285]     Train net output #0: loss = 0.21203 (* 1 = 0.21203 loss)
I0109 20:14:37.329036  3950 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 20:14:40.595862  3950 solver.cpp:266] Iteration 14700 (30.6104 iter/s, 3.26687s/100 iter), loss = 0.13477
I0109 20:14:40.595926  3950 solver.cpp:285]     Train net output #0: loss = 0.13477 (* 1 = 0.13477 loss)
I0109 20:14:40.595937  3950 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 20:14:43.865201  3950 solver.cpp:266] Iteration 14800 (30.5877 iter/s, 3.26928s/100 iter), loss = 0.216289
I0109 20:14:43.865265  3950 solver.cpp:285]     Train net output #0: loss = 0.216289 (* 1 = 0.216289 loss)
I0109 20:14:43.865278  3950 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 20:14:47.132238  3950 solver.cpp:266] Iteration 14900 (30.6091 iter/s, 3.267s/100 iter), loss = 0.156194
I0109 20:14:47.132302  3950 solver.cpp:285]     Train net output #0: loss = 0.156194 (* 1 = 0.156194 loss)
I0109 20:14:47.132315  3950 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 20:14:50.368146  3950 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 20:14:51.182409  3950 solver.cpp:517]     Test net output #0: loss = 0.467123 (* 1 = 0.467123 loss)
I0109 20:14:51.182447  3950 solver.cpp:517]     Test net output #1: top-1 = 0.852333
I0109 20:14:51.182456  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0109 20:14:51.213264  3950 solver.cpp:266] Iteration 15000 (24.5038 iter/s, 4.081s/100 iter), loss = 0.165248
I0109 20:14:51.213315  3950 solver.cpp:285]     Train net output #0: loss = 0.165248 (* 1 = 0.165248 loss)
I0109 20:14:51.213332  3950 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 20:14:54.479518  3950 solver.cpp:266] Iteration 15100 (30.6163 iter/s, 3.26623s/100 iter), loss = 0.211204
I0109 20:14:54.479580  3950 solver.cpp:285]     Train net output #0: loss = 0.211204 (* 1 = 0.211204 loss)
I0109 20:14:54.479593  3950 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 20:14:57.750730  3950 solver.cpp:266] Iteration 15200 (30.57 iter/s, 3.27118s/100 iter), loss = 0.212913
I0109 20:14:57.750793  3950 solver.cpp:285]     Train net output #0: loss = 0.212913 (* 1 = 0.212913 loss)
I0109 20:14:57.750805  3950 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 20:15:01.018631  3950 solver.cpp:266] Iteration 15300 (30.6012 iter/s, 3.26784s/100 iter), loss = 0.273482
I0109 20:15:01.018697  3950 solver.cpp:285]     Train net output #0: loss = 0.273482 (* 1 = 0.273482 loss)
I0109 20:15:01.018710  3950 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 20:15:04.287804  3950 solver.cpp:266] Iteration 15400 (30.5891 iter/s, 3.26914s/100 iter), loss = 0.187728
I0109 20:15:04.287993  3950 solver.cpp:285]     Train net output #0: loss = 0.187728 (* 1 = 0.187728 loss)
I0109 20:15:04.288009  3950 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 20:15:07.556615  3950 solver.cpp:266] Iteration 15500 (30.5936 iter/s, 3.26865s/100 iter), loss = 0.17175
I0109 20:15:07.556679  3950 solver.cpp:285]     Train net output #0: loss = 0.17175 (* 1 = 0.17175 loss)
I0109 20:15:07.556691  3950 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 20:15:10.826371  3950 solver.cpp:266] Iteration 15600 (30.5836 iter/s, 3.26972s/100 iter), loss = 0.104337
I0109 20:15:10.826436  3950 solver.cpp:285]     Train net output #0: loss = 0.104337 (* 1 = 0.104337 loss)
I0109 20:15:10.826447  3950 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 20:15:14.093850  3950 solver.cpp:266] Iteration 15700 (30.6052 iter/s, 3.26742s/100 iter), loss = 0.213166
I0109 20:15:14.093914  3950 solver.cpp:285]     Train net output #0: loss = 0.213166 (* 1 = 0.213166 loss)
I0109 20:15:14.093928  3950 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 20:15:17.363363  3950 solver.cpp:266] Iteration 15800 (30.5859 iter/s, 3.26948s/100 iter), loss = 0.143294
I0109 20:15:17.363426  3950 solver.cpp:285]     Train net output #0: loss = 0.143294 (* 1 = 0.143294 loss)
I0109 20:15:17.363440  3950 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 20:15:20.632733  3950 solver.cpp:266] Iteration 15900 (30.5872 iter/s, 3.26934s/100 iter), loss = 0.181559
I0109 20:15:20.632797  3950 solver.cpp:285]     Train net output #0: loss = 0.181559 (* 1 = 0.181559 loss)
I0109 20:15:20.632810  3950 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 20:15:23.870693  3950 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 20:15:24.685497  3950 solver.cpp:517]     Test net output #0: loss = 0.451344 (* 1 = 0.451344 loss)
I0109 20:15:24.685536  3950 solver.cpp:517]     Test net output #1: top-1 = 0.855778
I0109 20:15:24.685544  3950 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 20:15:24.716398  3950 solver.cpp:266] Iteration 16000 (24.4879 iter/s, 4.08364s/100 iter), loss = 0.224274
I0109 20:15:24.716434  3950 solver.cpp:285]     Train net output #0: loss = 0.224274 (* 1 = 0.224274 loss)
I0109 20:15:24.716449  3950 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 20:15:27.984390  3950 solver.cpp:266] Iteration 16100 (30.6002 iter/s, 3.26796s/100 iter), loss = 0.281586
I0109 20:15:27.984457  3950 solver.cpp:285]     Train net output #0: loss = 0.281586 (* 1 = 0.281586 loss)
I0109 20:15:27.984469  3950 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 20:15:31.252784  3950 solver.cpp:266] Iteration 16200 (30.5964 iter/s, 3.26836s/100 iter), loss = 0.156381
I0109 20:15:31.252851  3950 solver.cpp:285]     Train net output #0: loss = 0.156381 (* 1 = 0.156381 loss)
I0109 20:15:31.252861  3950 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 20:15:34.521430  3950 solver.cpp:266] Iteration 16300 (30.594 iter/s, 3.26861s/100 iter), loss = 0.137733
I0109 20:15:34.521628  3950 solver.cpp:285]     Train net output #0: loss = 0.137733 (* 1 = 0.137733 loss)
I0109 20:15:34.521643  3950 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 20:15:37.790367  3950 solver.cpp:266] Iteration 16400 (30.5925 iter/s, 3.26877s/100 iter), loss = 0.177457
I0109 20:15:37.790436  3950 solver.cpp:285]     Train net output #0: loss = 0.177457 (* 1 = 0.177457 loss)
I0109 20:15:37.790448  3950 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 20:15:41.059092  3950 solver.cpp:266] Iteration 16500 (30.5936 iter/s, 3.26866s/100 iter), loss = 0.134965
I0109 20:15:41.059154  3950 solver.cpp:285]     Train net output #0: loss = 0.134965 (* 1 = 0.134965 loss)
I0109 20:15:41.059168  3950 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 20:15:44.331190  3950 solver.cpp:266] Iteration 16600 (30.5618 iter/s, 3.27206s/100 iter), loss = 0.123672
I0109 20:15:44.331259  3950 solver.cpp:285]     Train net output #0: loss = 0.123672 (* 1 = 0.123672 loss)
I0109 20:15:44.331272  3950 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 20:15:47.600257  3950 solver.cpp:266] Iteration 16700 (30.5901 iter/s, 3.26903s/100 iter), loss = 0.164302
I0109 20:15:47.600319  3950 solver.cpp:285]     Train net output #0: loss = 0.164302 (* 1 = 0.164302 loss)
I0109 20:15:47.600332  3950 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 20:15:50.869146  3950 solver.cpp:266] Iteration 16800 (30.592 iter/s, 3.26883s/100 iter), loss = 0.155441
I0109 20:15:50.869210  3950 solver.cpp:285]     Train net output #0: loss = 0.155441 (* 1 = 0.155441 loss)
I0109 20:15:50.869222  3950 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 20:15:54.139236  3950 solver.cpp:266] Iteration 16900 (30.5805 iter/s, 3.27006s/100 iter), loss = 0.175352
I0109 20:15:54.139305  3950 solver.cpp:285]     Train net output #0: loss = 0.175352 (* 1 = 0.175352 loss)
I0109 20:15:54.139318  3950 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 20:15:57.376890  3950 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 20:15:58.197062  3950 solver.cpp:517]     Test net output #0: loss = 0.454029 (* 1 = 0.454029 loss)
I0109 20:15:58.197100  3950 solver.cpp:517]     Test net output #1: top-1 = 0.857111
I0109 20:15:58.197110  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 20:15:58.227998  3950 solver.cpp:266] Iteration 17000 (24.4574 iter/s, 4.08873s/100 iter), loss = 0.181929
I0109 20:15:58.228049  3950 solver.cpp:285]     Train net output #0: loss = 0.181929 (* 1 = 0.181929 loss)
I0109 20:15:58.228063  3950 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 20:16:01.499842  3950 solver.cpp:266] Iteration 17100 (30.564 iter/s, 3.27182s/100 iter), loss = 0.11187
I0109 20:16:01.499915  3950 solver.cpp:285]     Train net output #0: loss = 0.11187 (* 1 = 0.11187 loss)
I0109 20:16:01.499928  3950 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 20:16:04.772140  3950 solver.cpp:266] Iteration 17200 (30.56 iter/s, 3.27225s/100 iter), loss = 0.157318
I0109 20:16:04.772320  3950 solver.cpp:285]     Train net output #0: loss = 0.157318 (* 1 = 0.157318 loss)
I0109 20:16:04.772339  3950 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 20:16:08.043548  3950 solver.cpp:266] Iteration 17300 (30.5695 iter/s, 3.27124s/100 iter), loss = 0.124407
I0109 20:16:08.043613  3950 solver.cpp:285]     Train net output #0: loss = 0.124407 (* 1 = 0.124407 loss)
I0109 20:16:08.043627  3950 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 20:16:11.313313  3950 solver.cpp:266] Iteration 17400 (30.5836 iter/s, 3.26972s/100 iter), loss = 0.120951
I0109 20:16:11.313401  3950 solver.cpp:285]     Train net output #0: loss = 0.120951 (* 1 = 0.120951 loss)
I0109 20:16:11.313416  3950 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 20:16:14.586738  3950 solver.cpp:266] Iteration 17500 (30.5496 iter/s, 3.27337s/100 iter), loss = 0.133116
I0109 20:16:14.586798  3950 solver.cpp:285]     Train net output #0: loss = 0.133116 (* 1 = 0.133116 loss)
I0109 20:16:14.586810  3950 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 20:16:17.855762  3950 solver.cpp:266] Iteration 17600 (30.5905 iter/s, 3.26899s/100 iter), loss = 0.147235
I0109 20:16:17.855823  3950 solver.cpp:285]     Train net output #0: loss = 0.147235 (* 1 = 0.147235 loss)
I0109 20:16:17.855835  3950 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 20:16:21.128131  3950 solver.cpp:266] Iteration 17700 (30.5594 iter/s, 3.27231s/100 iter), loss = 0.142469
I0109 20:16:21.128206  3950 solver.cpp:285]     Train net output #0: loss = 0.142469 (* 1 = 0.142469 loss)
I0109 20:16:21.128226  3950 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 20:16:24.399065  3950 solver.cpp:266] Iteration 17800 (30.5727 iter/s, 3.2709s/100 iter), loss = 0.11244
I0109 20:16:24.399133  3950 solver.cpp:285]     Train net output #0: loss = 0.11244 (* 1 = 0.11244 loss)
I0109 20:16:24.399152  3950 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 20:16:27.669059  3950 solver.cpp:266] Iteration 17900 (30.5814 iter/s, 3.26996s/100 iter), loss = 0.165669
I0109 20:16:27.669128  3950 solver.cpp:285]     Train net output #0: loss = 0.165669 (* 1 = 0.165669 loss)
I0109 20:16:27.669147  3950 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 20:16:30.908933  3950 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 20:16:31.725986  3950 solver.cpp:517]     Test net output #0: loss = 0.458148 (* 1 = 0.458148 loss)
I0109 20:16:31.726027  3950 solver.cpp:517]     Test net output #1: top-1 = 0.857667
I0109 20:16:31.726039  3950 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 20:16:31.756953  3950 solver.cpp:266] Iteration 18000 (24.4626 iter/s, 4.08787s/100 iter), loss = 0.168741
I0109 20:16:31.757009  3950 solver.cpp:285]     Train net output #0: loss = 0.168741 (* 1 = 0.168741 loss)
I0109 20:16:31.757031  3950 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 20:16:35.026631  3950 solver.cpp:266] Iteration 18100 (30.5845 iter/s, 3.26963s/100 iter), loss = 0.279585
I0109 20:16:35.026813  3950 solver.cpp:285]     Train net output #0: loss = 0.279585 (* 1 = 0.279585 loss)
I0109 20:16:35.026829  3950 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 20:16:38.295684  3950 solver.cpp:266] Iteration 18200 (30.5913 iter/s, 3.2689s/100 iter), loss = 0.133173
I0109 20:16:38.295747  3950 solver.cpp:285]     Train net output #0: loss = 0.133173 (* 1 = 0.133173 loss)
I0109 20:16:38.295758  3950 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 20:16:41.565508  3950 solver.cpp:266] Iteration 18300 (30.583 iter/s, 3.26979s/100 iter), loss = 0.192662
I0109 20:16:41.565572  3950 solver.cpp:285]     Train net output #0: loss = 0.192662 (* 1 = 0.192662 loss)
I0109 20:16:41.565583  3950 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 20:16:44.835193  3950 solver.cpp:266] Iteration 18400 (30.5843 iter/s, 3.26965s/100 iter), loss = 0.154191
I0109 20:16:44.835258  3950 solver.cpp:285]     Train net output #0: loss = 0.154191 (* 1 = 0.154191 loss)
I0109 20:16:44.835269  3950 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 20:16:48.104277  3950 solver.cpp:266] Iteration 18500 (30.5902 iter/s, 3.26902s/100 iter), loss = 0.197221
I0109 20:16:48.104339  3950 solver.cpp:285]     Train net output #0: loss = 0.197221 (* 1 = 0.197221 loss)
I0109 20:16:48.104352  3950 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 20:16:51.371273  3950 solver.cpp:266] Iteration 18600 (30.6095 iter/s, 3.26696s/100 iter), loss = 0.141564
I0109 20:16:51.371336  3950 solver.cpp:285]     Train net output #0: loss = 0.141564 (* 1 = 0.141564 loss)
I0109 20:16:51.371349  3950 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 20:16:54.639457  3950 solver.cpp:266] Iteration 18700 (30.5983 iter/s, 3.26815s/100 iter), loss = 0.178501
I0109 20:16:54.639523  3950 solver.cpp:285]     Train net output #0: loss = 0.178501 (* 1 = 0.178501 loss)
I0109 20:16:54.639535  3950 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 20:16:57.907173  3950 solver.cpp:266] Iteration 18800 (30.603 iter/s, 3.26765s/100 iter), loss = 0.153472
I0109 20:16:57.907236  3950 solver.cpp:285]     Train net output #0: loss = 0.153472 (* 1 = 0.153472 loss)
I0109 20:16:57.907248  3950 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 20:17:01.176652  3950 solver.cpp:266] Iteration 18900 (30.5862 iter/s, 3.26945s/100 iter), loss = 0.117495
I0109 20:17:01.176717  3950 solver.cpp:285]     Train net output #0: loss = 0.117495 (* 1 = 0.117495 loss)
I0109 20:17:01.176729  3950 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 20:17:04.413736  3950 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 20:17:05.231101  3950 solver.cpp:517]     Test net output #0: loss = 0.452732 (* 1 = 0.452732 loss)
I0109 20:17:05.231262  3950 solver.cpp:517]     Test net output #1: top-1 = 0.856889
I0109 20:17:05.231274  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992444
I0109 20:17:05.262217  3950 solver.cpp:266] Iteration 19000 (24.4765 iter/s, 4.08554s/100 iter), loss = 0.153082
I0109 20:17:05.262253  3950 solver.cpp:285]     Train net output #0: loss = 0.153081 (* 1 = 0.153081 loss)
I0109 20:17:05.262269  3950 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 20:17:08.527156  3950 solver.cpp:266] Iteration 19100 (30.6285 iter/s, 3.26493s/100 iter), loss = 0.156259
I0109 20:17:08.527221  3950 solver.cpp:285]     Train net output #0: loss = 0.156259 (* 1 = 0.156259 loss)
I0109 20:17:08.527235  3950 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 20:17:11.791903  3950 solver.cpp:266] Iteration 19200 (30.6306 iter/s, 3.26471s/100 iter), loss = 0.22084
I0109 20:17:11.791970  3950 solver.cpp:285]     Train net output #0: loss = 0.22084 (* 1 = 0.22084 loss)
I0109 20:17:11.791982  3950 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 20:17:15.059931  3950 solver.cpp:266] Iteration 19300 (30.6001 iter/s, 3.26796s/100 iter), loss = 0.128481
I0109 20:17:15.059995  3950 solver.cpp:285]     Train net output #0: loss = 0.128481 (* 1 = 0.128481 loss)
I0109 20:17:15.060009  3950 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 20:17:18.327834  3950 solver.cpp:266] Iteration 19400 (30.601 iter/s, 3.26787s/100 iter), loss = 0.127254
I0109 20:17:18.327894  3950 solver.cpp:285]     Train net output #0: loss = 0.127254 (* 1 = 0.127254 loss)
I0109 20:17:18.327906  3950 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 20:17:21.596086  3950 solver.cpp:266] Iteration 19500 (30.5977 iter/s, 3.26822s/100 iter), loss = 0.0674766
I0109 20:17:21.596146  3950 solver.cpp:285]     Train net output #0: loss = 0.0674765 (* 1 = 0.0674765 loss)
I0109 20:17:21.596158  3950 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 20:17:24.862926  3950 solver.cpp:266] Iteration 19600 (30.6109 iter/s, 3.26681s/100 iter), loss = 0.148363
I0109 20:17:24.862987  3950 solver.cpp:285]     Train net output #0: loss = 0.148363 (* 1 = 0.148363 loss)
I0109 20:17:24.862999  3950 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 20:17:28.130640  3950 solver.cpp:266] Iteration 19700 (30.603 iter/s, 3.26766s/100 iter), loss = 0.109995
I0109 20:17:28.130702  3950 solver.cpp:285]     Train net output #0: loss = 0.109995 (* 1 = 0.109995 loss)
I0109 20:17:28.130715  3950 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 20:17:31.399060  3950 solver.cpp:266] Iteration 19800 (30.5961 iter/s, 3.26839s/100 iter), loss = 0.177322
I0109 20:17:31.399123  3950 solver.cpp:285]     Train net output #0: loss = 0.177322 (* 1 = 0.177322 loss)
I0109 20:17:31.399135  3950 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 20:17:34.666626  3950 solver.cpp:266] Iteration 19900 (30.6041 iter/s, 3.26753s/100 iter), loss = 0.219206
I0109 20:17:34.666687  3950 solver.cpp:285]     Train net output #0: loss = 0.219206 (* 1 = 0.219206 loss)
I0109 20:17:34.666698  3950 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 20:17:37.903019  3950 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/_iter_20000.caffemodel
I0109 20:17:38.003302  3950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/_iter_20000.solverstate
I0109 20:17:38.029352  3950 solver.cpp:378] Iteration 20000, loss = 0.0178456
I0109 20:17:38.029389  3950 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 20:17:38.843730  3950 solver.cpp:517]     Test net output #0: loss = 0.449383 (* 1 = 0.449383 loss)
I0109 20:17:38.843768  3950 solver.cpp:517]     Test net output #1: top-1 = 0.858667
I0109 20:17:38.843776  3950 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 20:17:38.843783  3950 solver.cpp:386] Optimization Done (29.9741 iter/s).
I0109 20:17:38.843789  3950 caffe_interface.cpp:530] Optimization Done.
I0109 20:17:39.005091  3991 pruning_runner.cpp:190] Sens info found, use it.
I0109 20:17:39.050498  3991 pruning_runner.cpp:217] Start compressing, please wait...
I0109 20:17:40.091809  3991 pruning_runner.cpp:264] Compression complete 0.000236749%
I0109 20:17:40.406791  3991 pruning_runner.cpp:264] Compression complete 50.0001%
I0109 20:17:40.717113  3991 pruning_runner.cpp:264] Compression complete 66.6668%
I0109 20:17:41.021266  3991 pruning_runner.cpp:264] Compression complete 80.0001%
I0109 20:17:41.337756  3991 pruning_runner.cpp:264] Compression complete 88.8889%
I0109 20:17:41.651660  3991 pruning_runner.cpp:264] Compression complete 94.4445%
I0109 20:17:41.959911  3991 pruning_runner.cpp:264] Compression complete 97.1429%
I0109 20:17:42.269451  3991 pruning_runner.cpp:264] Compression complete 99.6429%
I0109 20:17:42.580587  3991 pruning_runner.cpp:264] Compression complete 99.9552%
I0109 20:17:42.897826  3991 pruning_runner.cpp:264] Compression complete 99.9776%
I0109 20:17:43.207832  3991 pruning_runner.cpp:264] Compression complete 99.9993%
I0109 20:17:43.517452  3991 pruning_runner.cpp:264] Compression complete 99.9996%
I0109 20:17:43.831979  3991 pruning_runner.cpp:264] Compression complete 100%
I0109 20:17:44.142990  3991 pruning_runner.cpp:264] Compression complete 100%
I0109 20:17:44.454277  3991 pruning_runner.cpp:264] Compression complete 100%
I0109 20:17:44.765123  3991 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 20:17:44.765455  3991 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 20:17:44.765877  3991 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:17:44.766115  3991 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:17:44.766290  3991 layer_factory.hpp:77] Creating layer data
I0109 20:17:44.766376  3991 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:44.766495  3991 net.cpp:94] Creating Layer data
I0109 20:17:44.766531  3991 net.cpp:409] data -> data
I0109 20:17:44.766564  3991 net.cpp:409] data -> label
I0109 20:17:44.767294  4288 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:17:44.767331  4288 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:17:44.767632  3991 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:17:44.767768  3991 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:17:44.773980  3991 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:44.774042  3991 net.cpp:144] Setting up data
I0109 20:17:44.774080  3991 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:17:44.774093  3991 net.cpp:151] Top shape: 50 (50)
I0109 20:17:44.774098  3991 net.cpp:159] Memory required for data: 614600
I0109 20:17:44.774106  3991 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:17:44.774133  3991 net.cpp:94] Creating Layer label_data_1_split
I0109 20:17:44.774154  3991 net.cpp:435] label_data_1_split <- label
I0109 20:17:44.774183  3991 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:17:44.774211  3991 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:17:44.774235  3991 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:17:44.774308  3991 net.cpp:144] Setting up label_data_1_split
I0109 20:17:44.774327  3991 net.cpp:151] Top shape: 50 (50)
I0109 20:17:44.774334  3991 net.cpp:151] Top shape: 50 (50)
I0109 20:17:44.774343  3991 net.cpp:151] Top shape: 50 (50)
I0109 20:17:44.774348  3991 net.cpp:159] Memory required for data: 615200
I0109 20:17:44.774368  3991 layer_factory.hpp:77] Creating layer conv1
I0109 20:17:44.774397  3991 net.cpp:94] Creating Layer conv1
I0109 20:17:44.774421  3991 net.cpp:435] conv1 <- data
I0109 20:17:44.774446  3991 net.cpp:409] conv1 -> conv1
I0109 20:17:44.775341  3991 net.cpp:144] Setting up conv1
I0109 20:17:44.775362  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.775370  3991 net.cpp:159] Memory required for data: 7168800
I0109 20:17:44.775385  3991 layer_factory.hpp:77] Creating layer bn1
I0109 20:17:44.775415  3991 net.cpp:94] Creating Layer bn1
I0109 20:17:44.775437  3991 net.cpp:435] bn1 <- conv1
I0109 20:17:44.775465  3991 net.cpp:409] bn1 -> scale1
I0109 20:17:44.776438  3991 net.cpp:144] Setting up bn1
I0109 20:17:44.776458  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.776465  3991 net.cpp:159] Memory required for data: 13722400
I0109 20:17:44.776486  3991 layer_factory.hpp:77] Creating layer relu1
I0109 20:17:44.776515  3991 net.cpp:94] Creating Layer relu1
I0109 20:17:44.776535  3991 net.cpp:435] relu1 <- scale1
I0109 20:17:44.776561  3991 net.cpp:409] relu1 -> relu1
I0109 20:17:44.776614  3991 net.cpp:144] Setting up relu1
I0109 20:17:44.776633  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.776641  3991 net.cpp:159] Memory required for data: 20276000
I0109 20:17:44.776648  3991 layer_factory.hpp:77] Creating layer conv2
I0109 20:17:44.776664  3991 net.cpp:94] Creating Layer conv2
I0109 20:17:44.776684  3991 net.cpp:435] conv2 <- relu1
I0109 20:17:44.776707  3991 net.cpp:409] conv2 -> conv2
I0109 20:17:44.777570  3991 net.cpp:144] Setting up conv2
I0109 20:17:44.777640  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.777685  3991 net.cpp:159] Memory required for data: 26829600
I0109 20:17:44.777721  3991 layer_factory.hpp:77] Creating layer bn2
I0109 20:17:44.777768  3991 net.cpp:94] Creating Layer bn2
I0109 20:17:44.777782  3991 net.cpp:435] bn2 <- conv2
I0109 20:17:44.777794  3991 net.cpp:409] bn2 -> scale2
I0109 20:17:44.778506  3991 net.cpp:144] Setting up bn2
I0109 20:17:44.778528  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.778537  3991 net.cpp:159] Memory required for data: 33383200
I0109 20:17:44.778553  3991 layer_factory.hpp:77] Creating layer relu2
I0109 20:17:44.778578  3991 net.cpp:94] Creating Layer relu2
I0109 20:17:44.778599  3991 net.cpp:435] relu2 <- scale2
I0109 20:17:44.778643  3991 net.cpp:409] relu2 -> relu2
I0109 20:17:44.778709  3991 net.cpp:144] Setting up relu2
I0109 20:17:44.778731  3991 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:44.778739  3991 net.cpp:159] Memory required for data: 39936800
I0109 20:17:44.778774  3991 layer_factory.hpp:77] Creating layer pool1
I0109 20:17:44.778800  3991 net.cpp:94] Creating Layer pool1
I0109 20:17:44.778841  3991 net.cpp:435] pool1 <- relu2
I0109 20:17:44.778870  3991 net.cpp:409] pool1 -> pool1
I0109 20:17:44.778956  3991 net.cpp:144] Setting up pool1
I0109 20:17:44.778977  3991 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:17:44.778985  3991 net.cpp:159] Memory required for data: 41575200
I0109 20:17:44.778990  3991 layer_factory.hpp:77] Creating layer drop1
I0109 20:17:44.779001  3991 net.cpp:94] Creating Layer drop1
I0109 20:17:44.779021  3991 net.cpp:435] drop1 <- pool1
I0109 20:17:44.779044  3991 net.cpp:409] drop1 -> drop1
I0109 20:17:44.779238  3991 net.cpp:144] Setting up drop1
I0109 20:17:44.779259  3991 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:17:44.779266  3991 net.cpp:159] Memory required for data: 43213600
I0109 20:17:44.779273  3991 layer_factory.hpp:77] Creating layer conv3
I0109 20:17:44.779286  3991 net.cpp:94] Creating Layer conv3
I0109 20:17:44.779309  3991 net.cpp:435] conv3 <- drop1
I0109 20:17:44.779332  3991 net.cpp:409] conv3 -> conv3
I0109 20:17:44.780334  3991 net.cpp:144] Setting up conv3
I0109 20:17:44.780356  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.780366  3991 net.cpp:159] Memory required for data: 46490400
I0109 20:17:44.780377  3991 layer_factory.hpp:77] Creating layer bn3
I0109 20:17:44.780390  3991 net.cpp:94] Creating Layer bn3
I0109 20:17:44.780413  3991 net.cpp:435] bn3 <- conv3
I0109 20:17:44.780441  3991 net.cpp:409] bn3 -> scale3
I0109 20:17:44.781203  3991 net.cpp:144] Setting up bn3
I0109 20:17:44.781225  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.781234  3991 net.cpp:159] Memory required for data: 49767200
I0109 20:17:44.781255  3991 layer_factory.hpp:77] Creating layer relu3
I0109 20:17:44.781292  3991 net.cpp:94] Creating Layer relu3
I0109 20:17:44.781335  3991 net.cpp:435] relu3 <- scale3
I0109 20:17:44.781361  3991 net.cpp:409] relu3 -> relu3
I0109 20:17:44.781455  3991 net.cpp:144] Setting up relu3
I0109 20:17:44.781474  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.781481  3991 net.cpp:159] Memory required for data: 53044000
I0109 20:17:44.781489  3991 layer_factory.hpp:77] Creating layer conv4
I0109 20:17:44.781505  3991 net.cpp:94] Creating Layer conv4
I0109 20:17:44.781527  3991 net.cpp:435] conv4 <- relu3
I0109 20:17:44.781551  3991 net.cpp:409] conv4 -> conv4
I0109 20:17:44.782107  3991 net.cpp:144] Setting up conv4
I0109 20:17:44.782172  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.782191  3991 net.cpp:159] Memory required for data: 56320800
I0109 20:17:44.782203  3991 layer_factory.hpp:77] Creating layer bn4
I0109 20:17:44.782279  3991 net.cpp:94] Creating Layer bn4
I0109 20:17:44.782300  3991 net.cpp:435] bn4 <- conv4
I0109 20:17:44.782339  3991 net.cpp:409] bn4 -> scale4
I0109 20:17:44.783458  3991 net.cpp:144] Setting up bn4
I0109 20:17:44.783483  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.783530  3991 net.cpp:159] Memory required for data: 59597600
I0109 20:17:44.783558  3991 layer_factory.hpp:77] Creating layer relu4
I0109 20:17:44.783609  3991 net.cpp:94] Creating Layer relu4
I0109 20:17:44.783658  3991 net.cpp:435] relu4 <- scale4
I0109 20:17:44.783730  3991 net.cpp:409] relu4 -> relu4
I0109 20:17:44.783831  3991 net.cpp:144] Setting up relu4
I0109 20:17:44.783877  3991 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:44.783915  3991 net.cpp:159] Memory required for data: 62874400
I0109 20:17:44.783955  3991 layer_factory.hpp:77] Creating layer pool2
I0109 20:17:44.783999  3991 net.cpp:94] Creating Layer pool2
I0109 20:17:44.784040  3991 net.cpp:435] pool2 <- relu4
I0109 20:17:44.784083  3991 net.cpp:409] pool2 -> pool2
I0109 20:17:44.784178  3991 net.cpp:144] Setting up pool2
I0109 20:17:44.784221  3991 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:17:44.784260  3991 net.cpp:159] Memory required for data: 63693600
I0109 20:17:44.784305  3991 layer_factory.hpp:77] Creating layer drop2
I0109 20:17:44.784361  3991 net.cpp:94] Creating Layer drop2
I0109 20:17:44.784402  3991 net.cpp:435] drop2 <- pool2
I0109 20:17:44.784451  3991 net.cpp:409] drop2 -> drop2
I0109 20:17:44.784543  3991 net.cpp:144] Setting up drop2
I0109 20:17:44.784590  3991 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:17:44.784628  3991 net.cpp:159] Memory required for data: 64512800
I0109 20:17:44.784672  3991 layer_factory.hpp:77] Creating layer fc1
I0109 20:17:44.784718  3991 net.cpp:94] Creating Layer fc1
I0109 20:17:44.784763  3991 net.cpp:435] fc1 <- drop2
I0109 20:17:44.784806  3991 net.cpp:409] fc1 -> fc1
I0109 20:17:44.804930  3991 net.cpp:144] Setting up fc1
I0109 20:17:44.804955  3991 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:44.804960  3991 net.cpp:159] Memory required for data: 64615200
I0109 20:17:44.804970  3991 layer_factory.hpp:77] Creating layer bn5
I0109 20:17:44.804994  3991 net.cpp:94] Creating Layer bn5
I0109 20:17:44.805009  3991 net.cpp:435] bn5 <- fc1
I0109 20:17:44.805023  3991 net.cpp:409] bn5 -> scale5
I0109 20:17:44.805660  3991 net.cpp:144] Setting up bn5
I0109 20:17:44.805680  3991 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:44.805685  3991 net.cpp:159] Memory required for data: 64717600
I0109 20:17:44.805706  3991 layer_factory.hpp:77] Creating layer relu5
I0109 20:17:44.805724  3991 net.cpp:94] Creating Layer relu5
I0109 20:17:44.805742  3991 net.cpp:435] relu5 <- scale5
I0109 20:17:44.805752  3991 net.cpp:409] relu5 -> relu5
I0109 20:17:44.805799  3991 net.cpp:144] Setting up relu5
I0109 20:17:44.805817  3991 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:44.805824  3991 net.cpp:159] Memory required for data: 64820000
I0109 20:17:44.805827  3991 layer_factory.hpp:77] Creating layer drop3
I0109 20:17:44.805837  3991 net.cpp:94] Creating Layer drop3
I0109 20:17:44.805851  3991 net.cpp:435] drop3 <- relu5
I0109 20:17:44.805861  3991 net.cpp:409] drop3 -> drop3
I0109 20:17:44.805918  3991 net.cpp:144] Setting up drop3
I0109 20:17:44.805935  3991 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:44.805939  3991 net.cpp:159] Memory required for data: 64922400
I0109 20:17:44.805943  3991 layer_factory.hpp:77] Creating layer fc2
I0109 20:17:44.805953  3991 net.cpp:94] Creating Layer fc2
I0109 20:17:44.805968  3991 net.cpp:435] fc2 <- drop3
I0109 20:17:44.805979  3991 net.cpp:409] fc2 -> fc2
I0109 20:17:44.806149  3991 net.cpp:144] Setting up fc2
I0109 20:17:44.806164  3991 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:44.806169  3991 net.cpp:159] Memory required for data: 64924400
I0109 20:17:44.806177  3991 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:17:44.806195  3991 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:17:44.806210  3991 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:17:44.806221  3991 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:17:44.806241  3991 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:17:44.806252  3991 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:17:44.806324  3991 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:17:44.806339  3991 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:44.806344  3991 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:44.806347  3991 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:44.806351  3991 net.cpp:159] Memory required for data: 64930400
I0109 20:17:44.806366  3991 layer_factory.hpp:77] Creating layer loss
I0109 20:17:44.806378  3991 net.cpp:94] Creating Layer loss
I0109 20:17:44.806391  3991 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:17:44.806399  3991 net.cpp:435] loss <- label_data_1_split_0
I0109 20:17:44.806416  3991 net.cpp:409] loss -> loss
I0109 20:17:44.806430  3991 layer_factory.hpp:77] Creating layer loss
I0109 20:17:44.806542  3991 net.cpp:144] Setting up loss
I0109 20:17:44.806556  3991 net.cpp:151] Top shape: (1)
I0109 20:17:44.806561  3991 net.cpp:154]     with loss weight 1
I0109 20:17:44.806581  3991 net.cpp:159] Memory required for data: 64930404
I0109 20:17:44.806587  3991 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:17:44.806622  3991 net.cpp:94] Creating Layer accuracy-top1
I0109 20:17:44.806638  3991 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:17:44.806646  3991 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:17:44.806655  3991 net.cpp:409] accuracy-top1 -> top-1
I0109 20:17:44.806670  3991 net.cpp:144] Setting up accuracy-top1
I0109 20:17:44.806686  3991 net.cpp:151] Top shape: (1)
I0109 20:17:44.806692  3991 net.cpp:159] Memory required for data: 64930408
I0109 20:17:44.806699  3991 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:17:44.806707  3991 net.cpp:94] Creating Layer accuracy-top5
I0109 20:17:44.806723  3991 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:17:44.806732  3991 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:17:44.806749  3991 net.cpp:409] accuracy-top5 -> top-5
I0109 20:17:44.806761  3991 net.cpp:144] Setting up accuracy-top5
I0109 20:17:44.806778  3991 net.cpp:151] Top shape: (1)
I0109 20:17:44.806784  3991 net.cpp:159] Memory required for data: 64930412
I0109 20:17:44.806790  3991 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:17:44.806805  3991 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:17:44.806814  3991 net.cpp:220] loss needs backward computation.
I0109 20:17:44.806828  3991 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:17:44.806835  3991 net.cpp:220] fc2 needs backward computation.
I0109 20:17:44.806841  3991 net.cpp:220] drop3 needs backward computation.
I0109 20:17:44.806856  3991 net.cpp:220] relu5 needs backward computation.
I0109 20:17:44.806862  3991 net.cpp:220] bn5 needs backward computation.
I0109 20:17:44.806869  3991 net.cpp:220] fc1 needs backward computation.
I0109 20:17:44.806882  3991 net.cpp:220] drop2 needs backward computation.
I0109 20:17:44.806890  3991 net.cpp:220] pool2 needs backward computation.
I0109 20:17:44.806897  3991 net.cpp:220] relu4 needs backward computation.
I0109 20:17:44.806910  3991 net.cpp:220] bn4 needs backward computation.
I0109 20:17:44.806917  3991 net.cpp:220] conv4 needs backward computation.
I0109 20:17:44.806931  3991 net.cpp:220] relu3 needs backward computation.
I0109 20:17:44.806937  3991 net.cpp:220] bn3 needs backward computation.
I0109 20:17:44.806943  3991 net.cpp:220] conv3 needs backward computation.
I0109 20:17:44.806949  3991 net.cpp:220] drop1 needs backward computation.
I0109 20:17:44.806963  3991 net.cpp:220] pool1 needs backward computation.
I0109 20:17:44.806970  3991 net.cpp:220] relu2 needs backward computation.
I0109 20:17:44.806977  3991 net.cpp:220] bn2 needs backward computation.
I0109 20:17:44.806989  3991 net.cpp:220] conv2 needs backward computation.
I0109 20:17:44.806998  3991 net.cpp:220] relu1 needs backward computation.
I0109 20:17:44.807011  3991 net.cpp:220] bn1 needs backward computation.
I0109 20:17:44.807018  3991 net.cpp:220] conv1 needs backward computation.
I0109 20:17:44.807025  3991 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:17:44.807040  3991 net.cpp:222] data does not need backward computation.
I0109 20:17:44.807046  3991 net.cpp:264] This network produces output loss
I0109 20:17:44.807052  3991 net.cpp:264] This network produces output top-1
I0109 20:17:44.807060  3991 net.cpp:264] This network produces output top-5
I0109 20:17:44.807096  3991 net.cpp:284] Network initialization done.
I0109 20:17:44.810770  3991 caffe_interface.cpp:363] Running for 180 iterations.
I0109 20:17:44.822603  3991 caffe_interface.cpp:125] Batch 0, loss = 1.05146
I0109 20:17:44.822633  3991 caffe_interface.cpp:125] Batch 0, top-1 = 0.86
I0109 20:17:44.822640  3991 caffe_interface.cpp:125] Batch 0, top-5 = 0.96
I0109 20:17:44.828950  3991 caffe_interface.cpp:125] Batch 1, loss = 0.821505
I0109 20:17:44.828975  3991 caffe_interface.cpp:125] Batch 1, top-1 = 0.78
I0109 20:17:44.828981  3991 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0109 20:17:44.835297  3991 caffe_interface.cpp:125] Batch 2, loss = 1.42923
I0109 20:17:44.835322  3991 caffe_interface.cpp:125] Batch 2, top-1 = 0.66
I0109 20:17:44.835343  3991 caffe_interface.cpp:125] Batch 2, top-5 = 0.96
I0109 20:17:44.841673  3991 caffe_interface.cpp:125] Batch 3, loss = 0.569563
I0109 20:17:44.841698  3991 caffe_interface.cpp:125] Batch 3, top-1 = 0.76
I0109 20:17:44.841704  3991 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0109 20:17:44.848006  3991 caffe_interface.cpp:125] Batch 4, loss = 0.565923
I0109 20:17:44.848031  3991 caffe_interface.cpp:125] Batch 4, top-1 = 0.86
I0109 20:17:44.848037  3991 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0109 20:17:44.854326  3991 caffe_interface.cpp:125] Batch 5, loss = 0.723289
I0109 20:17:44.854351  3991 caffe_interface.cpp:125] Batch 5, top-1 = 0.8
I0109 20:17:44.854357  3991 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0109 20:17:44.860625  3991 caffe_interface.cpp:125] Batch 6, loss = 1.04348
I0109 20:17:44.860651  3991 caffe_interface.cpp:125] Batch 6, top-1 = 0.74
I0109 20:17:44.860656  3991 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0109 20:17:44.866935  3991 caffe_interface.cpp:125] Batch 7, loss = 0.716187
I0109 20:17:44.866961  3991 caffe_interface.cpp:125] Batch 7, top-1 = 0.74
I0109 20:17:44.866966  3991 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0109 20:17:44.873224  3991 caffe_interface.cpp:125] Batch 8, loss = 1.20129
I0109 20:17:44.873247  3991 caffe_interface.cpp:125] Batch 8, top-1 = 0.66
I0109 20:17:44.873255  3991 caffe_interface.cpp:125] Batch 8, top-5 = 0.92
I0109 20:17:44.879643  3991 caffe_interface.cpp:125] Batch 9, loss = 1.06171
I0109 20:17:44.879668  3991 caffe_interface.cpp:125] Batch 9, top-1 = 0.66
I0109 20:17:44.879674  3991 caffe_interface.cpp:125] Batch 9, top-5 = 0.96
I0109 20:17:44.885967  3991 caffe_interface.cpp:125] Batch 10, loss = 0.60724
I0109 20:17:44.885989  3991 caffe_interface.cpp:125] Batch 10, top-1 = 0.78
I0109 20:17:44.885994  3991 caffe_interface.cpp:125] Batch 10, top-5 = 1
I0109 20:17:44.892258  3991 caffe_interface.cpp:125] Batch 11, loss = 0.654124
I0109 20:17:44.892283  3991 caffe_interface.cpp:125] Batch 11, top-1 = 0.84
I0109 20:17:44.892289  3991 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0109 20:17:44.898547  3991 caffe_interface.cpp:125] Batch 12, loss = 0.669043
I0109 20:17:44.898571  3991 caffe_interface.cpp:125] Batch 12, top-1 = 0.8
I0109 20:17:44.898577  3991 caffe_interface.cpp:125] Batch 12, top-5 = 0.98
I0109 20:17:44.904847  3991 caffe_interface.cpp:125] Batch 13, loss = 0.921014
I0109 20:17:44.904872  3991 caffe_interface.cpp:125] Batch 13, top-1 = 0.7
I0109 20:17:44.904877  3991 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0109 20:17:44.911159  3991 caffe_interface.cpp:125] Batch 14, loss = 0.495712
I0109 20:17:44.911183  3991 caffe_interface.cpp:125] Batch 14, top-1 = 0.8
I0109 20:17:44.911188  3991 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0109 20:17:44.917464  3991 caffe_interface.cpp:125] Batch 15, loss = 0.724393
I0109 20:17:44.917488  3991 caffe_interface.cpp:125] Batch 15, top-1 = 0.78
I0109 20:17:44.917495  3991 caffe_interface.cpp:125] Batch 15, top-5 = 0.98
I0109 20:17:44.923770  3991 caffe_interface.cpp:125] Batch 16, loss = 0.698469
I0109 20:17:44.923795  3991 caffe_interface.cpp:125] Batch 16, top-1 = 0.8
I0109 20:17:44.923802  3991 caffe_interface.cpp:125] Batch 16, top-5 = 0.94
I0109 20:17:44.930083  3991 caffe_interface.cpp:125] Batch 17, loss = 1.3685
I0109 20:17:44.930106  3991 caffe_interface.cpp:125] Batch 17, top-1 = 0.74
I0109 20:17:44.930112  3991 caffe_interface.cpp:125] Batch 17, top-5 = 0.94
I0109 20:17:44.936367  3991 caffe_interface.cpp:125] Batch 18, loss = 0.706044
I0109 20:17:44.936390  3991 caffe_interface.cpp:125] Batch 18, top-1 = 0.78
I0109 20:17:44.936396  3991 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0109 20:17:44.942669  3991 caffe_interface.cpp:125] Batch 19, loss = 0.68278
I0109 20:17:44.942694  3991 caffe_interface.cpp:125] Batch 19, top-1 = 0.78
I0109 20:17:44.942700  3991 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0109 20:17:44.948947  3991 caffe_interface.cpp:125] Batch 20, loss = 0.666635
I0109 20:17:44.948971  3991 caffe_interface.cpp:125] Batch 20, top-1 = 0.82
I0109 20:17:44.948993  3991 caffe_interface.cpp:125] Batch 20, top-5 = 0.98
I0109 20:17:44.955268  3991 caffe_interface.cpp:125] Batch 21, loss = 1.51799
I0109 20:17:44.955292  3991 caffe_interface.cpp:125] Batch 21, top-1 = 0.6
I0109 20:17:44.955298  3991 caffe_interface.cpp:125] Batch 21, top-5 = 0.96
I0109 20:17:44.961572  3991 caffe_interface.cpp:125] Batch 22, loss = 0.608012
I0109 20:17:44.961622  3991 caffe_interface.cpp:125] Batch 22, top-1 = 0.82
I0109 20:17:44.961630  3991 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0109 20:17:44.969023  3991 caffe_interface.cpp:125] Batch 23, loss = 0.82366
I0109 20:17:44.969051  3991 caffe_interface.cpp:125] Batch 23, top-1 = 0.72
I0109 20:17:44.969058  3991 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0109 20:17:44.974689  3991 caffe_interface.cpp:125] Batch 24, loss = 0.910201
I0109 20:17:44.974714  3991 caffe_interface.cpp:125] Batch 24, top-1 = 0.76
I0109 20:17:44.974721  3991 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0109 20:17:44.980363  3991 caffe_interface.cpp:125] Batch 25, loss = 1.06106
I0109 20:17:44.980388  3991 caffe_interface.cpp:125] Batch 25, top-1 = 0.68
I0109 20:17:44.980394  3991 caffe_interface.cpp:125] Batch 25, top-5 = 0.96
I0109 20:17:44.986060  3991 caffe_interface.cpp:125] Batch 26, loss = 1.15241
I0109 20:17:44.986084  3991 caffe_interface.cpp:125] Batch 26, top-1 = 0.78
I0109 20:17:44.986090  3991 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0109 20:17:44.991770  3991 caffe_interface.cpp:125] Batch 27, loss = 0.382077
I0109 20:17:44.991796  3991 caffe_interface.cpp:125] Batch 27, top-1 = 0.86
I0109 20:17:44.991802  3991 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0109 20:17:44.997474  3991 caffe_interface.cpp:125] Batch 28, loss = 0.511248
I0109 20:17:44.997499  3991 caffe_interface.cpp:125] Batch 28, top-1 = 0.76
I0109 20:17:44.997506  3991 caffe_interface.cpp:125] Batch 28, top-5 = 1
I0109 20:17:45.003160  3991 caffe_interface.cpp:125] Batch 29, loss = 0.633534
I0109 20:17:45.003185  3991 caffe_interface.cpp:125] Batch 29, top-1 = 0.82
I0109 20:17:45.003190  3991 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0109 20:17:45.008893  3991 caffe_interface.cpp:125] Batch 30, loss = 0.578684
I0109 20:17:45.008916  3991 caffe_interface.cpp:125] Batch 30, top-1 = 0.86
I0109 20:17:45.008921  3991 caffe_interface.cpp:125] Batch 30, top-5 = 0.96
I0109 20:17:45.014575  3991 caffe_interface.cpp:125] Batch 31, loss = 0.842123
I0109 20:17:45.014598  3991 caffe_interface.cpp:125] Batch 31, top-1 = 0.76
I0109 20:17:45.014605  3991 caffe_interface.cpp:125] Batch 31, top-5 = 0.98
I0109 20:17:45.020259  3991 caffe_interface.cpp:125] Batch 32, loss = 1.08819
I0109 20:17:45.020283  3991 caffe_interface.cpp:125] Batch 32, top-1 = 0.76
I0109 20:17:45.020290  3991 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0109 20:17:45.025970  3991 caffe_interface.cpp:125] Batch 33, loss = 0.89293
I0109 20:17:45.025993  3991 caffe_interface.cpp:125] Batch 33, top-1 = 0.76
I0109 20:17:45.025998  3991 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0109 20:17:45.031644  3991 caffe_interface.cpp:125] Batch 34, loss = 0.967592
I0109 20:17:45.031668  3991 caffe_interface.cpp:125] Batch 34, top-1 = 0.8
I0109 20:17:45.031673  3991 caffe_interface.cpp:125] Batch 34, top-5 = 0.96
I0109 20:17:45.037322  3991 caffe_interface.cpp:125] Batch 35, loss = 0.808693
I0109 20:17:45.037346  3991 caffe_interface.cpp:125] Batch 35, top-1 = 0.76
I0109 20:17:45.037351  3991 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0109 20:17:45.042999  3991 caffe_interface.cpp:125] Batch 36, loss = 1.0276
I0109 20:17:45.043021  3991 caffe_interface.cpp:125] Batch 36, top-1 = 0.76
I0109 20:17:45.043028  3991 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0109 20:17:45.048708  3991 caffe_interface.cpp:125] Batch 37, loss = 1.0981
I0109 20:17:45.048732  3991 caffe_interface.cpp:125] Batch 37, top-1 = 0.7
I0109 20:17:45.048738  3991 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0109 20:17:45.054399  3991 caffe_interface.cpp:125] Batch 38, loss = 1.04522
I0109 20:17:45.054424  3991 caffe_interface.cpp:125] Batch 38, top-1 = 0.72
I0109 20:17:45.054443  3991 caffe_interface.cpp:125] Batch 38, top-5 = 0.98
I0109 20:17:45.060107  3991 caffe_interface.cpp:125] Batch 39, loss = 0.53875
I0109 20:17:45.060132  3991 caffe_interface.cpp:125] Batch 39, top-1 = 0.82
I0109 20:17:45.060137  3991 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0109 20:17:45.065794  3991 caffe_interface.cpp:125] Batch 40, loss = 0.803589
I0109 20:17:45.065819  3991 caffe_interface.cpp:125] Batch 40, top-1 = 0.82
I0109 20:17:45.065824  3991 caffe_interface.cpp:125] Batch 40, top-5 = 0.98
I0109 20:17:45.071483  3991 caffe_interface.cpp:125] Batch 41, loss = 0.527543
I0109 20:17:45.071507  3991 caffe_interface.cpp:125] Batch 41, top-1 = 0.74
I0109 20:17:45.071513  3991 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0109 20:17:45.077169  3991 caffe_interface.cpp:125] Batch 42, loss = 1.35189
I0109 20:17:45.077193  3991 caffe_interface.cpp:125] Batch 42, top-1 = 0.72
I0109 20:17:45.077199  3991 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0109 20:17:45.083042  3991 caffe_interface.cpp:125] Batch 43, loss = 0.811612
I0109 20:17:45.083068  3991 caffe_interface.cpp:125] Batch 43, top-1 = 0.82
I0109 20:17:45.083075  3991 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0109 20:17:45.088706  3991 caffe_interface.cpp:125] Batch 44, loss = 0.824329
I0109 20:17:45.088730  3991 caffe_interface.cpp:125] Batch 44, top-1 = 0.76
I0109 20:17:45.088737  3991 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0109 20:17:45.094380  3991 caffe_interface.cpp:125] Batch 45, loss = 0.96174
I0109 20:17:45.094405  3991 caffe_interface.cpp:125] Batch 45, top-1 = 0.82
I0109 20:17:45.094410  3991 caffe_interface.cpp:125] Batch 45, top-5 = 0.94
I0109 20:17:45.100075  3991 caffe_interface.cpp:125] Batch 46, loss = 1.03972
I0109 20:17:45.100098  3991 caffe_interface.cpp:125] Batch 46, top-1 = 0.76
I0109 20:17:45.100103  3991 caffe_interface.cpp:125] Batch 46, top-5 = 0.94
I0109 20:17:45.105772  3991 caffe_interface.cpp:125] Batch 47, loss = 1.3015
I0109 20:17:45.105795  3991 caffe_interface.cpp:125] Batch 47, top-1 = 0.76
I0109 20:17:45.105801  3991 caffe_interface.cpp:125] Batch 47, top-5 = 0.96
I0109 20:17:45.111485  3991 caffe_interface.cpp:125] Batch 48, loss = 0.666744
I0109 20:17:45.111508  3991 caffe_interface.cpp:125] Batch 48, top-1 = 0.76
I0109 20:17:45.111515  3991 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0109 20:17:45.117178  3991 caffe_interface.cpp:125] Batch 49, loss = 0.828136
I0109 20:17:45.117202  3991 caffe_interface.cpp:125] Batch 49, top-1 = 0.84
I0109 20:17:45.117208  3991 caffe_interface.cpp:125] Batch 49, top-5 = 0.94
I0109 20:17:45.122915  3991 caffe_interface.cpp:125] Batch 50, loss = 0.659264
I0109 20:17:45.122941  3991 caffe_interface.cpp:125] Batch 50, top-1 = 0.78
I0109 20:17:45.122946  3991 caffe_interface.cpp:125] Batch 50, top-5 = 0.98
I0109 20:17:45.128617  3991 caffe_interface.cpp:125] Batch 51, loss = 1.1281
I0109 20:17:45.128643  3991 caffe_interface.cpp:125] Batch 51, top-1 = 0.66
I0109 20:17:45.128648  3991 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0109 20:17:45.135234  3991 caffe_interface.cpp:125] Batch 52, loss = 0.897941
I0109 20:17:45.135260  3991 caffe_interface.cpp:125] Batch 52, top-1 = 0.74
I0109 20:17:45.135267  3991 caffe_interface.cpp:125] Batch 52, top-5 = 1
I0109 20:17:45.140442  3991 caffe_interface.cpp:125] Batch 53, loss = 1.03142
I0109 20:17:45.140468  3991 caffe_interface.cpp:125] Batch 53, top-1 = 0.72
I0109 20:17:45.140475  3991 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0109 20:17:45.145676  3991 caffe_interface.cpp:125] Batch 54, loss = 0.927784
I0109 20:17:45.145699  3991 caffe_interface.cpp:125] Batch 54, top-1 = 0.72
I0109 20:17:45.145705  3991 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0109 20:17:45.150889  3991 caffe_interface.cpp:125] Batch 55, loss = 1.0745
I0109 20:17:45.150914  3991 caffe_interface.cpp:125] Batch 55, top-1 = 0.72
I0109 20:17:45.150920  3991 caffe_interface.cpp:125] Batch 55, top-5 = 0.96
I0109 20:17:45.156100  3991 caffe_interface.cpp:125] Batch 56, loss = 0.624826
I0109 20:17:45.156139  3991 caffe_interface.cpp:125] Batch 56, top-1 = 0.84
I0109 20:17:45.156147  3991 caffe_interface.cpp:125] Batch 56, top-5 = 0.96
I0109 20:17:45.161331  3991 caffe_interface.cpp:125] Batch 57, loss = 0.948872
I0109 20:17:45.161355  3991 caffe_interface.cpp:125] Batch 57, top-1 = 0.74
I0109 20:17:45.161361  3991 caffe_interface.cpp:125] Batch 57, top-5 = 0.98
I0109 20:17:45.166529  3991 caffe_interface.cpp:125] Batch 58, loss = 0.953045
I0109 20:17:45.166553  3991 caffe_interface.cpp:125] Batch 58, top-1 = 0.72
I0109 20:17:45.166558  3991 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0109 20:17:45.171741  3991 caffe_interface.cpp:125] Batch 59, loss = 0.959339
I0109 20:17:45.171766  3991 caffe_interface.cpp:125] Batch 59, top-1 = 0.72
I0109 20:17:45.171772  3991 caffe_interface.cpp:125] Batch 59, top-5 = 0.98
I0109 20:17:45.176903  3991 caffe_interface.cpp:125] Batch 60, loss = 0.924663
I0109 20:17:45.176928  3991 caffe_interface.cpp:125] Batch 60, top-1 = 0.76
I0109 20:17:45.176934  3991 caffe_interface.cpp:125] Batch 60, top-5 = 0.96
I0109 20:17:45.182126  3991 caffe_interface.cpp:125] Batch 61, loss = 0.825737
I0109 20:17:45.182150  3991 caffe_interface.cpp:125] Batch 61, top-1 = 0.82
I0109 20:17:45.182157  3991 caffe_interface.cpp:125] Batch 61, top-5 = 0.98
I0109 20:17:45.187350  3991 caffe_interface.cpp:125] Batch 62, loss = 0.543537
I0109 20:17:45.187374  3991 caffe_interface.cpp:125] Batch 62, top-1 = 0.86
I0109 20:17:45.187381  3991 caffe_interface.cpp:125] Batch 62, top-5 = 0.98
I0109 20:17:45.192581  3991 caffe_interface.cpp:125] Batch 63, loss = 1.08355
I0109 20:17:45.192606  3991 caffe_interface.cpp:125] Batch 63, top-1 = 0.8
I0109 20:17:45.192612  3991 caffe_interface.cpp:125] Batch 63, top-5 = 0.94
I0109 20:17:45.197800  3991 caffe_interface.cpp:125] Batch 64, loss = 0.573696
I0109 20:17:45.197824  3991 caffe_interface.cpp:125] Batch 64, top-1 = 0.76
I0109 20:17:45.197830  3991 caffe_interface.cpp:125] Batch 64, top-5 = 0.98
I0109 20:17:45.203018  3991 caffe_interface.cpp:125] Batch 65, loss = 0.673367
I0109 20:17:45.203042  3991 caffe_interface.cpp:125] Batch 65, top-1 = 0.74
I0109 20:17:45.203047  3991 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0109 20:17:45.208261  3991 caffe_interface.cpp:125] Batch 66, loss = 0.738107
I0109 20:17:45.208286  3991 caffe_interface.cpp:125] Batch 66, top-1 = 0.78
I0109 20:17:45.208292  3991 caffe_interface.cpp:125] Batch 66, top-5 = 0.94
I0109 20:17:45.213472  3991 caffe_interface.cpp:125] Batch 67, loss = 0.891061
I0109 20:17:45.213497  3991 caffe_interface.cpp:125] Batch 67, top-1 = 0.78
I0109 20:17:45.213503  3991 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0109 20:17:45.218708  3991 caffe_interface.cpp:125] Batch 68, loss = 0.991566
I0109 20:17:45.218731  3991 caffe_interface.cpp:125] Batch 68, top-1 = 0.68
I0109 20:17:45.218739  3991 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0109 20:17:45.223909  3991 caffe_interface.cpp:125] Batch 69, loss = 1.38384
I0109 20:17:45.223933  3991 caffe_interface.cpp:125] Batch 69, top-1 = 0.66
I0109 20:17:45.223938  3991 caffe_interface.cpp:125] Batch 69, top-5 = 0.94
I0109 20:17:45.229113  3991 caffe_interface.cpp:125] Batch 70, loss = 0.865557
I0109 20:17:45.229140  3991 caffe_interface.cpp:125] Batch 70, top-1 = 0.8
I0109 20:17:45.229144  3991 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0109 20:17:45.234335  3991 caffe_interface.cpp:125] Batch 71, loss = 0.637262
I0109 20:17:45.234360  3991 caffe_interface.cpp:125] Batch 71, top-1 = 0.84
I0109 20:17:45.234365  3991 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0109 20:17:45.239534  3991 caffe_interface.cpp:125] Batch 72, loss = 0.545824
I0109 20:17:45.239558  3991 caffe_interface.cpp:125] Batch 72, top-1 = 0.8
I0109 20:17:45.239564  3991 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0109 20:17:45.244769  3991 caffe_interface.cpp:125] Batch 73, loss = 1.38365
I0109 20:17:45.244793  3991 caffe_interface.cpp:125] Batch 73, top-1 = 0.74
I0109 20:17:45.244799  3991 caffe_interface.cpp:125] Batch 73, top-5 = 0.94
I0109 20:17:45.249971  3991 caffe_interface.cpp:125] Batch 74, loss = 0.692749
I0109 20:17:45.250010  3991 caffe_interface.cpp:125] Batch 74, top-1 = 0.86
I0109 20:17:45.250016  3991 caffe_interface.cpp:125] Batch 74, top-5 = 1
I0109 20:17:45.255211  3991 caffe_interface.cpp:125] Batch 75, loss = 0.580485
I0109 20:17:45.255235  3991 caffe_interface.cpp:125] Batch 75, top-1 = 0.84
I0109 20:17:45.255240  3991 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0109 20:17:45.260403  3991 caffe_interface.cpp:125] Batch 76, loss = 0.702542
I0109 20:17:45.260427  3991 caffe_interface.cpp:125] Batch 76, top-1 = 0.76
I0109 20:17:45.260434  3991 caffe_interface.cpp:125] Batch 76, top-5 = 0.98
I0109 20:17:45.265626  3991 caffe_interface.cpp:125] Batch 77, loss = 0.899753
I0109 20:17:45.265650  3991 caffe_interface.cpp:125] Batch 77, top-1 = 0.78
I0109 20:17:45.265655  3991 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0109 20:17:45.270867  3991 caffe_interface.cpp:125] Batch 78, loss = 0.977128
I0109 20:17:45.270891  3991 caffe_interface.cpp:125] Batch 78, top-1 = 0.76
I0109 20:17:45.270897  3991 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0109 20:17:45.276057  3991 caffe_interface.cpp:125] Batch 79, loss = 1.26079
I0109 20:17:45.276082  3991 caffe_interface.cpp:125] Batch 79, top-1 = 0.8
I0109 20:17:45.276088  3991 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0109 20:17:45.281265  3991 caffe_interface.cpp:125] Batch 80, loss = 1.11827
I0109 20:17:45.281289  3991 caffe_interface.cpp:125] Batch 80, top-1 = 0.76
I0109 20:17:45.281296  3991 caffe_interface.cpp:125] Batch 80, top-5 = 0.98
I0109 20:17:45.286458  3991 caffe_interface.cpp:125] Batch 81, loss = 1.20795
I0109 20:17:45.286481  3991 caffe_interface.cpp:125] Batch 81, top-1 = 0.64
I0109 20:17:45.286486  3991 caffe_interface.cpp:125] Batch 81, top-5 = 0.94
I0109 20:17:45.291680  3991 caffe_interface.cpp:125] Batch 82, loss = 0.990144
I0109 20:17:45.291704  3991 caffe_interface.cpp:125] Batch 82, top-1 = 0.74
I0109 20:17:45.291709  3991 caffe_interface.cpp:125] Batch 82, top-5 = 0.94
I0109 20:17:45.296910  3991 caffe_interface.cpp:125] Batch 83, loss = 0.71785
I0109 20:17:45.296934  3991 caffe_interface.cpp:125] Batch 83, top-1 = 0.8
I0109 20:17:45.296941  3991 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0109 20:17:45.303771  3991 caffe_interface.cpp:125] Batch 84, loss = 1.28979
I0109 20:17:45.303795  3991 caffe_interface.cpp:125] Batch 84, top-1 = 0.74
I0109 20:17:45.303802  3991 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0109 20:17:45.308588  3991 caffe_interface.cpp:125] Batch 85, loss = 0.889779
I0109 20:17:45.308612  3991 caffe_interface.cpp:125] Batch 85, top-1 = 0.74
I0109 20:17:45.308619  3991 caffe_interface.cpp:125] Batch 85, top-5 = 0.98
I0109 20:17:45.313428  3991 caffe_interface.cpp:125] Batch 86, loss = 0.720352
I0109 20:17:45.313453  3991 caffe_interface.cpp:125] Batch 86, top-1 = 0.72
I0109 20:17:45.313458  3991 caffe_interface.cpp:125] Batch 86, top-5 = 0.98
I0109 20:17:45.318250  3991 caffe_interface.cpp:125] Batch 87, loss = 0.71878
I0109 20:17:45.318274  3991 caffe_interface.cpp:125] Batch 87, top-1 = 0.74
I0109 20:17:45.318280  3991 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0109 20:17:45.323062  3991 caffe_interface.cpp:125] Batch 88, loss = 1.15103
I0109 20:17:45.323086  3991 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0109 20:17:45.323092  3991 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0109 20:17:45.327895  3991 caffe_interface.cpp:125] Batch 89, loss = 1.17677
I0109 20:17:45.327920  3991 caffe_interface.cpp:125] Batch 89, top-1 = 0.6
I0109 20:17:45.327925  3991 caffe_interface.cpp:125] Batch 89, top-5 = 0.98
I0109 20:17:45.332711  3991 caffe_interface.cpp:125] Batch 90, loss = 0.597419
I0109 20:17:45.332736  3991 caffe_interface.cpp:125] Batch 90, top-1 = 0.82
I0109 20:17:45.332741  3991 caffe_interface.cpp:125] Batch 90, top-5 = 0.98
I0109 20:17:45.337536  3991 caffe_interface.cpp:125] Batch 91, loss = 0.616398
I0109 20:17:45.337563  3991 caffe_interface.cpp:125] Batch 91, top-1 = 0.84
I0109 20:17:45.337568  3991 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0109 20:17:45.342381  3991 caffe_interface.cpp:125] Batch 92, loss = 0.679434
I0109 20:17:45.342406  3991 caffe_interface.cpp:125] Batch 92, top-1 = 0.72
I0109 20:17:45.342412  3991 caffe_interface.cpp:125] Batch 92, top-5 = 1
I0109 20:17:45.347198  3991 caffe_interface.cpp:125] Batch 93, loss = 0.91585
I0109 20:17:45.347223  3991 caffe_interface.cpp:125] Batch 93, top-1 = 0.72
I0109 20:17:45.347227  3991 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0109 20:17:45.352015  3991 caffe_interface.cpp:125] Batch 94, loss = 0.923038
I0109 20:17:45.352039  3991 caffe_interface.cpp:125] Batch 94, top-1 = 0.72
I0109 20:17:45.352044  3991 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0109 20:17:45.356819  3991 caffe_interface.cpp:125] Batch 95, loss = 0.748364
I0109 20:17:45.356844  3991 caffe_interface.cpp:125] Batch 95, top-1 = 0.78
I0109 20:17:45.356850  3991 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0109 20:17:45.361657  3991 caffe_interface.cpp:125] Batch 96, loss = 0.654563
I0109 20:17:45.361680  3991 caffe_interface.cpp:125] Batch 96, top-1 = 0.78
I0109 20:17:45.361686  3991 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0109 20:17:45.366500  3991 caffe_interface.cpp:125] Batch 97, loss = 1.02128
I0109 20:17:45.366524  3991 caffe_interface.cpp:125] Batch 97, top-1 = 0.78
I0109 20:17:45.366530  3991 caffe_interface.cpp:125] Batch 97, top-5 = 0.98
I0109 20:17:45.371330  3991 caffe_interface.cpp:125] Batch 98, loss = 0.703548
I0109 20:17:45.371354  3991 caffe_interface.cpp:125] Batch 98, top-1 = 0.76
I0109 20:17:45.371359  3991 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0109 20:17:45.376121  3991 caffe_interface.cpp:125] Batch 99, loss = 1.09911
I0109 20:17:45.376145  3991 caffe_interface.cpp:125] Batch 99, top-1 = 0.74
I0109 20:17:45.376150  3991 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0109 20:17:45.380960  3991 caffe_interface.cpp:125] Batch 100, loss = 0.82523
I0109 20:17:45.380986  3991 caffe_interface.cpp:125] Batch 100, top-1 = 0.76
I0109 20:17:45.380992  3991 caffe_interface.cpp:125] Batch 100, top-5 = 0.98
I0109 20:17:45.385797  3991 caffe_interface.cpp:125] Batch 101, loss = 0.711311
I0109 20:17:45.385820  3991 caffe_interface.cpp:125] Batch 101, top-1 = 0.78
I0109 20:17:45.385826  3991 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0109 20:17:45.390628  3991 caffe_interface.cpp:125] Batch 102, loss = 1.01962
I0109 20:17:45.390651  3991 caffe_interface.cpp:125] Batch 102, top-1 = 0.76
I0109 20:17:45.390657  3991 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0109 20:17:45.395443  3991 caffe_interface.cpp:125] Batch 103, loss = 0.689395
I0109 20:17:45.395468  3991 caffe_interface.cpp:125] Batch 103, top-1 = 0.82
I0109 20:17:45.395473  3991 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0109 20:17:45.400251  3991 caffe_interface.cpp:125] Batch 104, loss = 0.760516
I0109 20:17:45.400275  3991 caffe_interface.cpp:125] Batch 104, top-1 = 0.76
I0109 20:17:45.400282  3991 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0109 20:17:45.405057  3991 caffe_interface.cpp:125] Batch 105, loss = 0.85493
I0109 20:17:45.405081  3991 caffe_interface.cpp:125] Batch 105, top-1 = 0.74
I0109 20:17:45.405086  3991 caffe_interface.cpp:125] Batch 105, top-5 = 0.96
I0109 20:17:45.409893  3991 caffe_interface.cpp:125] Batch 106, loss = 0.925332
I0109 20:17:45.409916  3991 caffe_interface.cpp:125] Batch 106, top-1 = 0.78
I0109 20:17:45.409922  3991 caffe_interface.cpp:125] Batch 106, top-5 = 0.98
I0109 20:17:45.414716  3991 caffe_interface.cpp:125] Batch 107, loss = 0.96492
I0109 20:17:45.414741  3991 caffe_interface.cpp:125] Batch 107, top-1 = 0.74
I0109 20:17:45.414746  3991 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0109 20:17:45.419548  3991 caffe_interface.cpp:125] Batch 108, loss = 0.968255
I0109 20:17:45.419571  3991 caffe_interface.cpp:125] Batch 108, top-1 = 0.76
I0109 20:17:45.419579  3991 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0109 20:17:45.424396  3991 caffe_interface.cpp:125] Batch 109, loss = 0.787113
I0109 20:17:45.424419  3991 caffe_interface.cpp:125] Batch 109, top-1 = 0.76
I0109 20:17:45.424425  3991 caffe_interface.cpp:125] Batch 109, top-5 = 0.98
I0109 20:17:45.429252  3991 caffe_interface.cpp:125] Batch 110, loss = 0.498382
I0109 20:17:45.429277  3991 caffe_interface.cpp:125] Batch 110, top-1 = 0.82
I0109 20:17:45.429282  3991 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0109 20:17:45.434087  3991 caffe_interface.cpp:125] Batch 111, loss = 0.315983
I0109 20:17:45.434111  3991 caffe_interface.cpp:125] Batch 111, top-1 = 0.9
I0109 20:17:45.434118  3991 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0109 20:17:45.438910  3991 caffe_interface.cpp:125] Batch 112, loss = 1.1966
I0109 20:17:45.438935  3991 caffe_interface.cpp:125] Batch 112, top-1 = 0.68
I0109 20:17:45.438941  3991 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0109 20:17:45.443723  3991 caffe_interface.cpp:125] Batch 113, loss = 1.13006
I0109 20:17:45.443747  3991 caffe_interface.cpp:125] Batch 113, top-1 = 0.64
I0109 20:17:45.443753  3991 caffe_interface.cpp:125] Batch 113, top-5 = 0.94
I0109 20:17:45.448539  3991 caffe_interface.cpp:125] Batch 114, loss = 0.944895
I0109 20:17:45.448563  3991 caffe_interface.cpp:125] Batch 114, top-1 = 0.8
I0109 20:17:45.448568  3991 caffe_interface.cpp:125] Batch 114, top-5 = 0.96
I0109 20:17:45.453961  3991 caffe_interface.cpp:125] Batch 115, loss = 0.84829
I0109 20:17:45.453987  3991 caffe_interface.cpp:125] Batch 115, top-1 = 0.78
I0109 20:17:45.453994  3991 caffe_interface.cpp:125] Batch 115, top-5 = 0.98
I0109 20:17:45.458776  3991 caffe_interface.cpp:125] Batch 116, loss = 0.854438
I0109 20:17:45.458799  3991 caffe_interface.cpp:125] Batch 116, top-1 = 0.78
I0109 20:17:45.458806  3991 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0109 20:17:45.463589  3991 caffe_interface.cpp:125] Batch 117, loss = 0.699787
I0109 20:17:45.463613  3991 caffe_interface.cpp:125] Batch 117, top-1 = 0.78
I0109 20:17:45.463619  3991 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0109 20:17:45.470180  3991 caffe_interface.cpp:125] Batch 118, loss = 0.887912
I0109 20:17:45.470206  3991 caffe_interface.cpp:125] Batch 118, top-1 = 0.78
I0109 20:17:45.470212  3991 caffe_interface.cpp:125] Batch 118, top-5 = 0.98
I0109 20:17:45.474669  3991 caffe_interface.cpp:125] Batch 119, loss = 0.444222
I0109 20:17:45.474694  3991 caffe_interface.cpp:125] Batch 119, top-1 = 0.86
I0109 20:17:45.474699  3991 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0109 20:17:45.479218  3991 caffe_interface.cpp:125] Batch 120, loss = 0.601573
I0109 20:17:45.479243  3991 caffe_interface.cpp:125] Batch 120, top-1 = 0.78
I0109 20:17:45.479249  3991 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0109 20:17:45.483733  3991 caffe_interface.cpp:125] Batch 121, loss = 0.948211
I0109 20:17:45.483757  3991 caffe_interface.cpp:125] Batch 121, top-1 = 0.76
I0109 20:17:45.483763  3991 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0109 20:17:45.488250  3991 caffe_interface.cpp:125] Batch 122, loss = 0.991841
I0109 20:17:45.488274  3991 caffe_interface.cpp:125] Batch 122, top-1 = 0.72
I0109 20:17:45.488281  3991 caffe_interface.cpp:125] Batch 122, top-5 = 0.98
I0109 20:17:45.492767  3991 caffe_interface.cpp:125] Batch 123, loss = 1.14736
I0109 20:17:45.492794  3991 caffe_interface.cpp:125] Batch 123, top-1 = 0.68
I0109 20:17:45.492799  3991 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0109 20:17:45.497269  3991 caffe_interface.cpp:125] Batch 124, loss = 0.666125
I0109 20:17:45.497294  3991 caffe_interface.cpp:125] Batch 124, top-1 = 0.76
I0109 20:17:45.497301  3991 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0109 20:17:45.501822  3991 caffe_interface.cpp:125] Batch 125, loss = 0.905949
I0109 20:17:45.501847  3991 caffe_interface.cpp:125] Batch 125, top-1 = 0.78
I0109 20:17:45.501852  3991 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0109 20:17:45.506337  3991 caffe_interface.cpp:125] Batch 126, loss = 0.764628
I0109 20:17:45.506361  3991 caffe_interface.cpp:125] Batch 126, top-1 = 0.7
I0109 20:17:45.506367  3991 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0109 20:17:45.510856  3991 caffe_interface.cpp:125] Batch 127, loss = 0.719411
I0109 20:17:45.510895  3991 caffe_interface.cpp:125] Batch 127, top-1 = 0.8
I0109 20:17:45.510902  3991 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0109 20:17:45.515398  3991 caffe_interface.cpp:125] Batch 128, loss = 1.2922
I0109 20:17:45.515420  3991 caffe_interface.cpp:125] Batch 128, top-1 = 0.68
I0109 20:17:45.515427  3991 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0109 20:17:45.519959  3991 caffe_interface.cpp:125] Batch 129, loss = 0.72339
I0109 20:17:45.519984  3991 caffe_interface.cpp:125] Batch 129, top-1 = 0.74
I0109 20:17:45.519990  3991 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0109 20:17:45.524514  3991 caffe_interface.cpp:125] Batch 130, loss = 0.430058
I0109 20:17:45.524538  3991 caffe_interface.cpp:125] Batch 130, top-1 = 0.82
I0109 20:17:45.524544  3991 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0109 20:17:45.529039  3991 caffe_interface.cpp:125] Batch 131, loss = 1.07868
I0109 20:17:45.529063  3991 caffe_interface.cpp:125] Batch 131, top-1 = 0.7
I0109 20:17:45.529069  3991 caffe_interface.cpp:125] Batch 131, top-5 = 0.98
I0109 20:17:45.533547  3991 caffe_interface.cpp:125] Batch 132, loss = 0.632935
I0109 20:17:45.533571  3991 caffe_interface.cpp:125] Batch 132, top-1 = 0.84
I0109 20:17:45.533578  3991 caffe_interface.cpp:125] Batch 132, top-5 = 0.98
I0109 20:17:45.538084  3991 caffe_interface.cpp:125] Batch 133, loss = 0.801202
I0109 20:17:45.538108  3991 caffe_interface.cpp:125] Batch 133, top-1 = 0.78
I0109 20:17:45.538115  3991 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0109 20:17:45.542588  3991 caffe_interface.cpp:125] Batch 134, loss = 1.10105
I0109 20:17:45.542611  3991 caffe_interface.cpp:125] Batch 134, top-1 = 0.7
I0109 20:17:45.542616  3991 caffe_interface.cpp:125] Batch 134, top-5 = 0.98
I0109 20:17:45.547108  3991 caffe_interface.cpp:125] Batch 135, loss = 0.905894
I0109 20:17:45.547132  3991 caffe_interface.cpp:125] Batch 135, top-1 = 0.74
I0109 20:17:45.547138  3991 caffe_interface.cpp:125] Batch 135, top-5 = 1
I0109 20:17:45.551633  3991 caffe_interface.cpp:125] Batch 136, loss = 0.522252
I0109 20:17:45.551656  3991 caffe_interface.cpp:125] Batch 136, top-1 = 0.88
I0109 20:17:45.551662  3991 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0109 20:17:45.556143  3991 caffe_interface.cpp:125] Batch 137, loss = 0.826477
I0109 20:17:45.556165  3991 caffe_interface.cpp:125] Batch 137, top-1 = 0.76
I0109 20:17:45.556171  3991 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0109 20:17:45.560662  3991 caffe_interface.cpp:125] Batch 138, loss = 0.887473
I0109 20:17:45.560686  3991 caffe_interface.cpp:125] Batch 138, top-1 = 0.78
I0109 20:17:45.560693  3991 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0109 20:17:45.565186  3991 caffe_interface.cpp:125] Batch 139, loss = 0.927041
I0109 20:17:45.565210  3991 caffe_interface.cpp:125] Batch 139, top-1 = 0.72
I0109 20:17:45.565215  3991 caffe_interface.cpp:125] Batch 139, top-5 = 0.98
I0109 20:17:45.569736  3991 caffe_interface.cpp:125] Batch 140, loss = 1.28144
I0109 20:17:45.569759  3991 caffe_interface.cpp:125] Batch 140, top-1 = 0.72
I0109 20:17:45.569766  3991 caffe_interface.cpp:125] Batch 140, top-5 = 0.94
I0109 20:17:45.574234  3991 caffe_interface.cpp:125] Batch 141, loss = 0.399621
I0109 20:17:45.574257  3991 caffe_interface.cpp:125] Batch 141, top-1 = 0.88
I0109 20:17:45.574263  3991 caffe_interface.cpp:125] Batch 141, top-5 = 1
I0109 20:17:45.578770  3991 caffe_interface.cpp:125] Batch 142, loss = 0.684642
I0109 20:17:45.578799  3991 caffe_interface.cpp:125] Batch 142, top-1 = 0.78
I0109 20:17:45.578804  3991 caffe_interface.cpp:125] Batch 142, top-5 = 0.98
I0109 20:17:45.583328  3991 caffe_interface.cpp:125] Batch 143, loss = 0.655647
I0109 20:17:45.583353  3991 caffe_interface.cpp:125] Batch 143, top-1 = 0.8
I0109 20:17:45.583359  3991 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0109 20:17:45.587877  3991 caffe_interface.cpp:125] Batch 144, loss = 0.996146
I0109 20:17:45.587900  3991 caffe_interface.cpp:125] Batch 144, top-1 = 0.78
I0109 20:17:45.587906  3991 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0109 20:17:45.592461  3991 caffe_interface.cpp:125] Batch 145, loss = 0.766308
I0109 20:17:45.592485  3991 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0109 20:17:45.592491  3991 caffe_interface.cpp:125] Batch 145, top-5 = 0.96
I0109 20:17:45.596998  3991 caffe_interface.cpp:125] Batch 146, loss = 0.825618
I0109 20:17:45.597023  3991 caffe_interface.cpp:125] Batch 146, top-1 = 0.78
I0109 20:17:45.597028  3991 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0109 20:17:45.601528  3991 caffe_interface.cpp:125] Batch 147, loss = 0.68291
I0109 20:17:45.601553  3991 caffe_interface.cpp:125] Batch 147, top-1 = 0.74
I0109 20:17:45.601559  3991 caffe_interface.cpp:125] Batch 147, top-5 = 0.98
I0109 20:17:45.606070  3991 caffe_interface.cpp:125] Batch 148, loss = 1.12032
I0109 20:17:45.606093  3991 caffe_interface.cpp:125] Batch 148, top-1 = 0.74
I0109 20:17:45.606101  3991 caffe_interface.cpp:125] Batch 148, top-5 = 0.98
I0109 20:17:45.610600  3991 caffe_interface.cpp:125] Batch 149, loss = 0.883776
I0109 20:17:45.610625  3991 caffe_interface.cpp:125] Batch 149, top-1 = 0.72
I0109 20:17:45.610630  3991 caffe_interface.cpp:125] Batch 149, top-5 = 0.98
I0109 20:17:45.615123  3991 caffe_interface.cpp:125] Batch 150, loss = 0.626371
I0109 20:17:45.615145  3991 caffe_interface.cpp:125] Batch 150, top-1 = 0.8
I0109 20:17:45.615151  3991 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0109 20:17:45.619638  3991 caffe_interface.cpp:125] Batch 151, loss = 0.499972
I0109 20:17:45.619663  3991 caffe_interface.cpp:125] Batch 151, top-1 = 0.84
I0109 20:17:45.619668  3991 caffe_interface.cpp:125] Batch 151, top-5 = 0.98
I0109 20:17:45.624155  3991 caffe_interface.cpp:125] Batch 152, loss = 0.668523
I0109 20:17:45.624179  3991 caffe_interface.cpp:125] Batch 152, top-1 = 0.82
I0109 20:17:45.624186  3991 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0109 20:17:45.628686  3991 caffe_interface.cpp:125] Batch 153, loss = 0.538876
I0109 20:17:45.628710  3991 caffe_interface.cpp:125] Batch 153, top-1 = 0.82
I0109 20:17:45.628716  3991 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0109 20:17:45.633203  3991 caffe_interface.cpp:125] Batch 154, loss = 0.312293
I0109 20:17:45.633227  3991 caffe_interface.cpp:125] Batch 154, top-1 = 0.92
I0109 20:17:45.633234  3991 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0109 20:17:45.639731  3991 caffe_interface.cpp:125] Batch 155, loss = 0.614799
I0109 20:17:45.639758  3991 caffe_interface.cpp:125] Batch 155, top-1 = 0.82
I0109 20:17:45.639765  3991 caffe_interface.cpp:125] Batch 155, top-5 = 0.98
I0109 20:17:45.644173  3991 caffe_interface.cpp:125] Batch 156, loss = 0.454447
I0109 20:17:45.644196  3991 caffe_interface.cpp:125] Batch 156, top-1 = 0.82
I0109 20:17:45.644203  3991 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0109 20:17:45.648638  3991 caffe_interface.cpp:125] Batch 157, loss = 0.703465
I0109 20:17:45.648663  3991 caffe_interface.cpp:125] Batch 157, top-1 = 0.76
I0109 20:17:45.648669  3991 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0109 20:17:45.653120  3991 caffe_interface.cpp:125] Batch 158, loss = 0.87163
I0109 20:17:45.653143  3991 caffe_interface.cpp:125] Batch 158, top-1 = 0.74
I0109 20:17:45.653149  3991 caffe_interface.cpp:125] Batch 158, top-5 = 0.96
I0109 20:17:45.657553  3991 caffe_interface.cpp:125] Batch 159, loss = 1.1363
I0109 20:17:45.657578  3991 caffe_interface.cpp:125] Batch 159, top-1 = 0.76
I0109 20:17:45.657584  3991 caffe_interface.cpp:125] Batch 159, top-5 = 0.94
I0109 20:17:45.662058  3991 caffe_interface.cpp:125] Batch 160, loss = 0.731425
I0109 20:17:45.662082  3991 caffe_interface.cpp:125] Batch 160, top-1 = 0.76
I0109 20:17:45.662089  3991 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0109 20:17:45.666507  3991 caffe_interface.cpp:125] Batch 161, loss = 0.420712
I0109 20:17:45.666530  3991 caffe_interface.cpp:125] Batch 161, top-1 = 0.82
I0109 20:17:45.666537  3991 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0109 20:17:45.670961  3991 caffe_interface.cpp:125] Batch 162, loss = 1.14485
I0109 20:17:45.670985  3991 caffe_interface.cpp:125] Batch 162, top-1 = 0.7
I0109 20:17:45.671017  3991 caffe_interface.cpp:125] Batch 162, top-5 = 0.96
I0109 20:17:45.675405  3991 caffe_interface.cpp:125] Batch 163, loss = 0.939289
I0109 20:17:45.675431  3991 caffe_interface.cpp:125] Batch 163, top-1 = 0.82
I0109 20:17:45.675436  3991 caffe_interface.cpp:125] Batch 163, top-5 = 0.98
I0109 20:17:45.679911  3991 caffe_interface.cpp:125] Batch 164, loss = 1.38132
I0109 20:17:45.679936  3991 caffe_interface.cpp:125] Batch 164, top-1 = 0.58
I0109 20:17:45.679944  3991 caffe_interface.cpp:125] Batch 164, top-5 = 0.96
I0109 20:17:45.684361  3991 caffe_interface.cpp:125] Batch 165, loss = 0.53986
I0109 20:17:45.684386  3991 caffe_interface.cpp:125] Batch 165, top-1 = 0.8
I0109 20:17:45.684391  3991 caffe_interface.cpp:125] Batch 165, top-5 = 1
I0109 20:17:45.688796  3991 caffe_interface.cpp:125] Batch 166, loss = 0.542664
I0109 20:17:45.688820  3991 caffe_interface.cpp:125] Batch 166, top-1 = 0.86
I0109 20:17:45.688827  3991 caffe_interface.cpp:125] Batch 166, top-5 = 0.96
I0109 20:17:45.693256  3991 caffe_interface.cpp:125] Batch 167, loss = 0.952974
I0109 20:17:45.693281  3991 caffe_interface.cpp:125] Batch 167, top-1 = 0.78
I0109 20:17:45.693286  3991 caffe_interface.cpp:125] Batch 167, top-5 = 0.96
I0109 20:17:45.697734  3991 caffe_interface.cpp:125] Batch 168, loss = 0.784138
I0109 20:17:45.697758  3991 caffe_interface.cpp:125] Batch 168, top-1 = 0.74
I0109 20:17:45.697764  3991 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0109 20:17:45.702172  3991 caffe_interface.cpp:125] Batch 169, loss = 1.72942
I0109 20:17:45.702194  3991 caffe_interface.cpp:125] Batch 169, top-1 = 0.62
I0109 20:17:45.702199  3991 caffe_interface.cpp:125] Batch 169, top-5 = 1
I0109 20:17:45.706615  3991 caffe_interface.cpp:125] Batch 170, loss = 0.479415
I0109 20:17:45.706638  3991 caffe_interface.cpp:125] Batch 170, top-1 = 0.86
I0109 20:17:45.706645  3991 caffe_interface.cpp:125] Batch 170, top-5 = 0.98
I0109 20:17:45.711046  3991 caffe_interface.cpp:125] Batch 171, loss = 0.426648
I0109 20:17:45.711071  3991 caffe_interface.cpp:125] Batch 171, top-1 = 0.84
I0109 20:17:45.711076  3991 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0109 20:17:45.715500  3991 caffe_interface.cpp:125] Batch 172, loss = 0.523256
I0109 20:17:45.715523  3991 caffe_interface.cpp:125] Batch 172, top-1 = 0.8
I0109 20:17:45.715528  3991 caffe_interface.cpp:125] Batch 172, top-5 = 1
I0109 20:17:45.719955  3991 caffe_interface.cpp:125] Batch 173, loss = 1.08076
I0109 20:17:45.719977  3991 caffe_interface.cpp:125] Batch 173, top-1 = 0.72
I0109 20:17:45.719982  3991 caffe_interface.cpp:125] Batch 173, top-5 = 0.96
I0109 20:17:45.724383  3991 caffe_interface.cpp:125] Batch 174, loss = 1.10563
I0109 20:17:45.724406  3991 caffe_interface.cpp:125] Batch 174, top-1 = 0.76
I0109 20:17:45.724411  3991 caffe_interface.cpp:125] Batch 174, top-5 = 0.98
I0109 20:17:45.728811  3991 caffe_interface.cpp:125] Batch 175, loss = 0.50237
I0109 20:17:45.728835  3991 caffe_interface.cpp:125] Batch 175, top-1 = 0.82
I0109 20:17:45.728840  3991 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0109 20:17:45.733242  3991 caffe_interface.cpp:125] Batch 176, loss = 0.793101
I0109 20:17:45.733265  3991 caffe_interface.cpp:125] Batch 176, top-1 = 0.78
I0109 20:17:45.733271  3991 caffe_interface.cpp:125] Batch 176, top-5 = 0.98
I0109 20:17:45.737716  3991 caffe_interface.cpp:125] Batch 177, loss = 0.668891
I0109 20:17:45.737738  3991 caffe_interface.cpp:125] Batch 177, top-1 = 0.76
I0109 20:17:45.737745  3991 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0109 20:17:45.742152  3991 caffe_interface.cpp:125] Batch 178, loss = 0.65725
I0109 20:17:45.742174  3991 caffe_interface.cpp:125] Batch 178, top-1 = 0.8
I0109 20:17:45.742179  3991 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0109 20:17:45.746589  3991 caffe_interface.cpp:125] Batch 179, loss = 1.12485
I0109 20:17:45.746613  3991 caffe_interface.cpp:125] Batch 179, top-1 = 0.64
I0109 20:17:45.746618  3991 caffe_interface.cpp:125] Batch 179, top-5 = 0.96
I0109 20:17:45.746623  3991 caffe_interface.cpp:130] Loss: 0.843083
I0109 20:17:45.746651  3991 caffe_interface.cpp:142] loss = 0.843083 (* 1 = 0.843083 loss)
I0109 20:17:45.746671  3991 caffe_interface.cpp:142] top-1 = 0.767
I0109 20:17:45.746681  3991 caffe_interface.cpp:142] top-5 = 0.982445
I0109 20:17:45.761056  3991 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/sparse.caffemodel
I0109 20:17:45.761093  3991 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.6:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.766999841    | -0.09799999    |
+-------------------------------------------------------------------+
| Weights        | 68389          | 32571          | -52.3739204%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 20594176       | -58.0170746%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config6.prototxt
I0109 20:17:45.876713  4296 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0109 20:17:45.988973  4296 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 20:17:45.989513  4296 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 20:17:45.989539  4296 caffe_interface.cpp:493] Using GPUs 0
I0109 20:17:45.989822  4296 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 20:17:46.654188  4296 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt"
type: "SGD"
I0109 20:17:46.654355  4296 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0109 20:17:46.654709  4296 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 20:17:46.654747  4296 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 20:17:46.654755  4296 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 20:17:46.654954  4296 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 20:17:46.655074  4296 layer_factory.hpp:77] Creating layer data
I0109 20:17:46.655225  4296 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:46.656080  4296 net.cpp:94] Creating Layer data
I0109 20:17:46.656116  4296 net.cpp:409] data -> data
I0109 20:17:46.656138  4296 net.cpp:409] data -> label
I0109 20:17:46.657650  4307 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 20:17:46.657702  4307 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 20:17:46.657833  4296 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 20:17:46.657959  4296 data_layer.cpp:83] output data size: 128,3,32,32
I0109 20:17:46.668941  4296 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:46.669011  4296 net.cpp:144] Setting up data
I0109 20:17:46.669059  4296 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 20:17:46.669090  4296 net.cpp:151] Top shape: 128 (128)
I0109 20:17:46.669123  4296 net.cpp:159] Memory required for data: 1573376
I0109 20:17:46.669142  4296 layer_factory.hpp:77] Creating layer conv1
I0109 20:17:46.669168  4296 net.cpp:94] Creating Layer conv1
I0109 20:17:46.669196  4296 net.cpp:435] conv1 <- data
I0109 20:17:46.669251  4296 net.cpp:409] conv1 -> conv1
I0109 20:17:46.670761  4296 net.cpp:144] Setting up conv1
I0109 20:17:46.670789  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.670797  4296 net.cpp:159] Memory required for data: 18350592
I0109 20:17:46.670826  4296 layer_factory.hpp:77] Creating layer bn1
I0109 20:17:46.670867  4296 net.cpp:94] Creating Layer bn1
I0109 20:17:46.670903  4296 net.cpp:435] bn1 <- conv1
I0109 20:17:46.670928  4296 net.cpp:409] bn1 -> scale1
I0109 20:17:46.671840  4296 net.cpp:144] Setting up bn1
I0109 20:17:46.671862  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.671871  4296 net.cpp:159] Memory required for data: 35127808
I0109 20:17:46.671892  4296 layer_factory.hpp:77] Creating layer relu1
I0109 20:17:46.671926  4296 net.cpp:94] Creating Layer relu1
I0109 20:17:46.671969  4296 net.cpp:435] relu1 <- scale1
I0109 20:17:46.672017  4296 net.cpp:409] relu1 -> relu1
I0109 20:17:46.672106  4296 net.cpp:144] Setting up relu1
I0109 20:17:46.672129  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.672137  4296 net.cpp:159] Memory required for data: 51905024
I0109 20:17:46.672204  4296 layer_factory.hpp:77] Creating layer conv2
I0109 20:17:46.672252  4296 net.cpp:94] Creating Layer conv2
I0109 20:17:46.672302  4296 net.cpp:435] conv2 <- relu1
I0109 20:17:46.672354  4296 net.cpp:409] conv2 -> conv2
I0109 20:17:46.673454  4296 net.cpp:144] Setting up conv2
I0109 20:17:46.673481  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.673492  4296 net.cpp:159] Memory required for data: 68682240
I0109 20:17:46.673616  4296 layer_factory.hpp:77] Creating layer bn2
I0109 20:17:46.673666  4296 net.cpp:94] Creating Layer bn2
I0109 20:17:46.673710  4296 net.cpp:435] bn2 <- conv2
I0109 20:17:46.673743  4296 net.cpp:409] bn2 -> scale2
I0109 20:17:46.674829  4296 net.cpp:144] Setting up bn2
I0109 20:17:46.674849  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.674857  4296 net.cpp:159] Memory required for data: 85459456
I0109 20:17:46.674938  4296 layer_factory.hpp:77] Creating layer relu2
I0109 20:17:46.674962  4296 net.cpp:94] Creating Layer relu2
I0109 20:17:46.674969  4296 net.cpp:435] relu2 <- scale2
I0109 20:17:46.675052  4296 net.cpp:409] relu2 -> relu2
I0109 20:17:46.675141  4296 net.cpp:144] Setting up relu2
I0109 20:17:46.675163  4296 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:17:46.675169  4296 net.cpp:159] Memory required for data: 102236672
I0109 20:17:46.675236  4296 layer_factory.hpp:77] Creating layer pool1
I0109 20:17:46.675276  4296 net.cpp:94] Creating Layer pool1
I0109 20:17:46.675315  4296 net.cpp:435] pool1 <- relu2
I0109 20:17:46.675356  4296 net.cpp:409] pool1 -> pool1
I0109 20:17:46.675715  4296 net.cpp:144] Setting up pool1
I0109 20:17:46.675736  4296 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:17:46.675743  4296 net.cpp:159] Memory required for data: 106430976
I0109 20:17:46.675828  4296 layer_factory.hpp:77] Creating layer drop1
I0109 20:17:46.675873  4296 net.cpp:94] Creating Layer drop1
I0109 20:17:46.675932  4296 net.cpp:435] drop1 <- pool1
I0109 20:17:46.675971  4296 net.cpp:409] drop1 -> drop1
I0109 20:17:46.676061  4296 net.cpp:144] Setting up drop1
I0109 20:17:46.676081  4296 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:17:46.676090  4296 net.cpp:159] Memory required for data: 110625280
I0109 20:17:46.676156  4296 layer_factory.hpp:77] Creating layer conv3
I0109 20:17:46.676201  4296 net.cpp:94] Creating Layer conv3
I0109 20:17:46.676241  4296 net.cpp:435] conv3 <- drop1
I0109 20:17:46.676285  4296 net.cpp:409] conv3 -> conv3
I0109 20:17:46.677656  4296 net.cpp:144] Setting up conv3
I0109 20:17:46.677678  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.677686  4296 net.cpp:159] Memory required for data: 119013888
I0109 20:17:46.677772  4296 layer_factory.hpp:77] Creating layer bn3
I0109 20:17:46.677801  4296 net.cpp:94] Creating Layer bn3
I0109 20:17:46.677866  4296 net.cpp:435] bn3 <- conv3
I0109 20:17:46.677909  4296 net.cpp:409] bn3 -> scale3
I0109 20:17:46.678776  4296 net.cpp:144] Setting up bn3
I0109 20:17:46.678802  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.678810  4296 net.cpp:159] Memory required for data: 127402496
I0109 20:17:46.678911  4296 layer_factory.hpp:77] Creating layer relu3
I0109 20:17:46.678937  4296 net.cpp:94] Creating Layer relu3
I0109 20:17:46.678946  4296 net.cpp:435] relu3 <- scale3
I0109 20:17:46.679024  4296 net.cpp:409] relu3 -> relu3
I0109 20:17:46.679118  4296 net.cpp:144] Setting up relu3
I0109 20:17:46.679150  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.679193  4296 net.cpp:159] Memory required for data: 135791104
I0109 20:17:46.679214  4296 layer_factory.hpp:77] Creating layer conv4
I0109 20:17:46.679289  4296 net.cpp:94] Creating Layer conv4
I0109 20:17:46.679308  4296 net.cpp:435] conv4 <- relu3
I0109 20:17:46.679353  4296 net.cpp:409] conv4 -> conv4
I0109 20:17:46.679883  4296 net.cpp:144] Setting up conv4
I0109 20:17:46.679906  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.679914  4296 net.cpp:159] Memory required for data: 144179712
I0109 20:17:46.679989  4296 layer_factory.hpp:77] Creating layer bn4
I0109 20:17:46.680018  4296 net.cpp:94] Creating Layer bn4
I0109 20:17:46.680083  4296 net.cpp:435] bn4 <- conv4
I0109 20:17:46.680128  4296 net.cpp:409] bn4 -> scale4
I0109 20:17:46.681002  4296 net.cpp:144] Setting up bn4
I0109 20:17:46.681026  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.681033  4296 net.cpp:159] Memory required for data: 152568320
I0109 20:17:46.681115  4296 layer_factory.hpp:77] Creating layer relu4
I0109 20:17:46.681138  4296 net.cpp:94] Creating Layer relu4
I0109 20:17:46.681146  4296 net.cpp:435] relu4 <- scale4
I0109 20:17:46.681226  4296 net.cpp:409] relu4 -> relu4
I0109 20:17:46.681303  4296 net.cpp:144] Setting up relu4
I0109 20:17:46.681325  4296 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:17:46.681332  4296 net.cpp:159] Memory required for data: 160956928
I0109 20:17:46.681339  4296 layer_factory.hpp:77] Creating layer pool2
I0109 20:17:46.681368  4296 net.cpp:94] Creating Layer pool2
I0109 20:17:46.681390  4296 net.cpp:435] pool2 <- relu4
I0109 20:17:46.681432  4296 net.cpp:409] pool2 -> pool2
I0109 20:17:46.681546  4296 net.cpp:144] Setting up pool2
I0109 20:17:46.681568  4296 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:17:46.681576  4296 net.cpp:159] Memory required for data: 163054080
I0109 20:17:46.681659  4296 layer_factory.hpp:77] Creating layer drop2
I0109 20:17:46.681705  4296 net.cpp:94] Creating Layer drop2
I0109 20:17:46.681747  4296 net.cpp:435] drop2 <- pool2
I0109 20:17:46.681784  4296 net.cpp:409] drop2 -> drop2
I0109 20:17:46.681885  4296 net.cpp:144] Setting up drop2
I0109 20:17:46.681905  4296 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:17:46.681912  4296 net.cpp:159] Memory required for data: 165151232
I0109 20:17:46.681980  4296 layer_factory.hpp:77] Creating layer fc1
I0109 20:17:46.682029  4296 net.cpp:94] Creating Layer fc1
I0109 20:17:46.682077  4296 net.cpp:435] fc1 <- drop2
I0109 20:17:46.682116  4296 net.cpp:409] fc1 -> fc1
I0109 20:17:46.705219  4296 net.cpp:144] Setting up fc1
I0109 20:17:46.705248  4296 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:17:46.705255  4296 net.cpp:159] Memory required for data: 165413376
I0109 20:17:46.705265  4296 layer_factory.hpp:77] Creating layer bn5
I0109 20:17:46.705286  4296 net.cpp:94] Creating Layer bn5
I0109 20:17:46.705317  4296 net.cpp:435] bn5 <- fc1
I0109 20:17:46.705343  4296 net.cpp:409] bn5 -> scale5
I0109 20:17:46.706009  4296 net.cpp:144] Setting up bn5
I0109 20:17:46.706030  4296 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:17:46.706037  4296 net.cpp:159] Memory required for data: 165675520
I0109 20:17:46.706066  4296 layer_factory.hpp:77] Creating layer relu5
I0109 20:17:46.706084  4296 net.cpp:94] Creating Layer relu5
I0109 20:17:46.706092  4296 net.cpp:435] relu5 <- scale5
I0109 20:17:46.706105  4296 net.cpp:409] relu5 -> relu5
I0109 20:17:46.706159  4296 net.cpp:144] Setting up relu5
I0109 20:17:46.706176  4296 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:17:46.706180  4296 net.cpp:159] Memory required for data: 165937664
I0109 20:17:46.706185  4296 layer_factory.hpp:77] Creating layer drop3
I0109 20:17:46.706198  4296 net.cpp:94] Creating Layer drop3
I0109 20:17:46.706214  4296 net.cpp:435] drop3 <- relu5
I0109 20:17:46.706224  4296 net.cpp:409] drop3 -> drop3
I0109 20:17:46.706281  4296 net.cpp:144] Setting up drop3
I0109 20:17:46.706298  4296 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:17:46.706303  4296 net.cpp:159] Memory required for data: 166199808
I0109 20:17:46.706308  4296 layer_factory.hpp:77] Creating layer fc2
I0109 20:17:46.706323  4296 net.cpp:94] Creating Layer fc2
I0109 20:17:46.706337  4296 net.cpp:435] fc2 <- drop3
I0109 20:17:46.706353  4296 net.cpp:409] fc2 -> fc2
I0109 20:17:46.706526  4296 net.cpp:144] Setting up fc2
I0109 20:17:46.706542  4296 net.cpp:151] Top shape: 128 10 (1280)
I0109 20:17:46.706547  4296 net.cpp:159] Memory required for data: 166204928
I0109 20:17:46.706554  4296 layer_factory.hpp:77] Creating layer loss
I0109 20:17:46.706568  4296 net.cpp:94] Creating Layer loss
I0109 20:17:46.706594  4296 net.cpp:435] loss <- fc2
I0109 20:17:46.706612  4296 net.cpp:435] loss <- label
I0109 20:17:46.706625  4296 net.cpp:409] loss -> loss
I0109 20:17:46.706640  4296 layer_factory.hpp:77] Creating layer loss
I0109 20:17:46.707463  4296 net.cpp:144] Setting up loss
I0109 20:17:46.707487  4296 net.cpp:151] Top shape: (1)
I0109 20:17:46.707496  4296 net.cpp:154]     with loss weight 1
I0109 20:17:46.707523  4296 net.cpp:159] Memory required for data: 166204932
I0109 20:17:46.707532  4296 net.cpp:220] loss needs backward computation.
I0109 20:17:46.707561  4296 net.cpp:220] fc2 needs backward computation.
I0109 20:17:46.707571  4296 net.cpp:220] drop3 needs backward computation.
I0109 20:17:46.707577  4296 net.cpp:220] relu5 needs backward computation.
I0109 20:17:46.707599  4296 net.cpp:220] bn5 needs backward computation.
I0109 20:17:46.707617  4296 net.cpp:220] fc1 needs backward computation.
I0109 20:17:46.707625  4296 net.cpp:220] drop2 needs backward computation.
I0109 20:17:46.707633  4296 net.cpp:220] pool2 needs backward computation.
I0109 20:17:46.707643  4296 net.cpp:220] relu4 needs backward computation.
I0109 20:17:46.707649  4296 net.cpp:220] bn4 needs backward computation.
I0109 20:17:46.707656  4296 net.cpp:220] conv4 needs backward computation.
I0109 20:17:46.707664  4296 net.cpp:220] relu3 needs backward computation.
I0109 20:17:46.707671  4296 net.cpp:220] bn3 needs backward computation.
I0109 20:17:46.707679  4296 net.cpp:220] conv3 needs backward computation.
I0109 20:17:46.707689  4296 net.cpp:220] drop1 needs backward computation.
I0109 20:17:46.707695  4296 net.cpp:220] pool1 needs backward computation.
I0109 20:17:46.707701  4296 net.cpp:220] relu2 needs backward computation.
I0109 20:17:46.707710  4296 net.cpp:220] bn2 needs backward computation.
I0109 20:17:46.707717  4296 net.cpp:220] conv2 needs backward computation.
I0109 20:17:46.707725  4296 net.cpp:220] relu1 needs backward computation.
I0109 20:17:46.707759  4296 net.cpp:220] bn1 needs backward computation.
I0109 20:17:46.707767  4296 net.cpp:220] conv1 needs backward computation.
I0109 20:17:46.707774  4296 net.cpp:222] data does not need backward computation.
I0109 20:17:46.707782  4296 net.cpp:264] This network produces output loss
I0109 20:17:46.707824  4296 net.cpp:284] Network initialization done.
I0109 20:17:46.708204  4296 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0109 20:17:46.708266  4296 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:17:46.708492  4296 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:17:46.708659  4296 layer_factory.hpp:77] Creating layer data
I0109 20:17:46.708753  4296 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:46.709168  4296 net.cpp:94] Creating Layer data
I0109 20:17:46.709187  4296 net.cpp:409] data -> data
I0109 20:17:46.709201  4296 net.cpp:409] data -> label
I0109 20:17:46.710566  4313 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:17:46.710611  4313 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:17:46.711362  4296 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:17:46.711495  4296 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:17:46.719442  4296 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:17:46.719508  4296 net.cpp:144] Setting up data
I0109 20:17:46.719547  4296 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:17:46.719565  4296 net.cpp:151] Top shape: 50 (50)
I0109 20:17:46.719573  4296 net.cpp:159] Memory required for data: 614600
I0109 20:17:46.719581  4296 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:17:46.719607  4296 net.cpp:94] Creating Layer label_data_1_split
I0109 20:17:46.719628  4296 net.cpp:435] label_data_1_split <- label
I0109 20:17:46.719656  4296 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:17:46.719681  4296 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:17:46.719696  4296 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:17:46.719789  4296 net.cpp:144] Setting up label_data_1_split
I0109 20:17:46.719810  4296 net.cpp:151] Top shape: 50 (50)
I0109 20:17:46.719818  4296 net.cpp:151] Top shape: 50 (50)
I0109 20:17:46.719825  4296 net.cpp:151] Top shape: 50 (50)
I0109 20:17:46.719844  4296 net.cpp:159] Memory required for data: 615200
I0109 20:17:46.719864  4296 layer_factory.hpp:77] Creating layer conv1
I0109 20:17:46.719902  4296 net.cpp:94] Creating Layer conv1
I0109 20:17:46.719918  4296 net.cpp:435] conv1 <- data
I0109 20:17:46.719933  4296 net.cpp:409] conv1 -> conv1
I0109 20:17:46.720271  4296 net.cpp:144] Setting up conv1
I0109 20:17:46.720295  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.720304  4296 net.cpp:159] Memory required for data: 7168800
I0109 20:17:46.720319  4296 layer_factory.hpp:77] Creating layer bn1
I0109 20:17:46.720350  4296 net.cpp:94] Creating Layer bn1
I0109 20:17:46.720373  4296 net.cpp:435] bn1 <- conv1
I0109 20:17:46.720398  4296 net.cpp:409] bn1 -> scale1
I0109 20:17:46.721691  4296 net.cpp:144] Setting up bn1
I0109 20:17:46.721711  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.721720  4296 net.cpp:159] Memory required for data: 13722400
I0109 20:17:46.721740  4296 layer_factory.hpp:77] Creating layer relu1
I0109 20:17:46.721768  4296 net.cpp:94] Creating Layer relu1
I0109 20:17:46.721789  4296 net.cpp:435] relu1 <- scale1
I0109 20:17:46.721814  4296 net.cpp:409] relu1 -> relu1
I0109 20:17:46.721874  4296 net.cpp:144] Setting up relu1
I0109 20:17:46.721890  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.721897  4296 net.cpp:159] Memory required for data: 20276000
I0109 20:17:46.721904  4296 layer_factory.hpp:77] Creating layer conv2
I0109 20:17:46.721921  4296 net.cpp:94] Creating Layer conv2
I0109 20:17:46.721943  4296 net.cpp:435] conv2 <- relu1
I0109 20:17:46.721972  4296 net.cpp:409] conv2 -> conv2
I0109 20:17:46.722611  4296 net.cpp:144] Setting up conv2
I0109 20:17:46.722635  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.722642  4296 net.cpp:159] Memory required for data: 26829600
I0109 20:17:46.722656  4296 layer_factory.hpp:77] Creating layer bn2
I0109 20:17:46.722690  4296 net.cpp:94] Creating Layer bn2
I0109 20:17:46.722714  4296 net.cpp:435] bn2 <- conv2
I0109 20:17:46.722743  4296 net.cpp:409] bn2 -> scale2
I0109 20:17:46.723862  4296 net.cpp:144] Setting up bn2
I0109 20:17:46.723883  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.723891  4296 net.cpp:159] Memory required for data: 33383200
I0109 20:17:46.723907  4296 layer_factory.hpp:77] Creating layer relu2
I0109 20:17:46.723938  4296 net.cpp:94] Creating Layer relu2
I0109 20:17:46.723980  4296 net.cpp:435] relu2 <- scale2
I0109 20:17:46.723997  4296 net.cpp:409] relu2 -> relu2
I0109 20:17:46.724112  4296 net.cpp:144] Setting up relu2
I0109 20:17:46.724161  4296 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:17:46.724205  4296 net.cpp:159] Memory required for data: 39936800
I0109 20:17:46.724246  4296 layer_factory.hpp:77] Creating layer pool1
I0109 20:17:46.724313  4296 net.cpp:94] Creating Layer pool1
I0109 20:17:46.724364  4296 net.cpp:435] pool1 <- relu2
I0109 20:17:46.724423  4296 net.cpp:409] pool1 -> pool1
I0109 20:17:46.724539  4296 net.cpp:144] Setting up pool1
I0109 20:17:46.724591  4296 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:17:46.724623  4296 net.cpp:159] Memory required for data: 41575200
I0109 20:17:46.724647  4296 layer_factory.hpp:77] Creating layer drop1
I0109 20:17:46.724689  4296 net.cpp:94] Creating Layer drop1
I0109 20:17:46.724705  4296 net.cpp:435] drop1 <- pool1
I0109 20:17:46.724745  4296 net.cpp:409] drop1 -> drop1
I0109 20:17:46.724850  4296 net.cpp:144] Setting up drop1
I0109 20:17:46.724905  4296 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:17:46.724946  4296 net.cpp:159] Memory required for data: 43213600
I0109 20:17:46.724984  4296 layer_factory.hpp:77] Creating layer conv3
I0109 20:17:46.725036  4296 net.cpp:94] Creating Layer conv3
I0109 20:17:46.725085  4296 net.cpp:435] conv3 <- drop1
I0109 20:17:46.725142  4296 net.cpp:409] conv3 -> conv3
I0109 20:17:46.725862  4296 net.cpp:144] Setting up conv3
I0109 20:17:46.725930  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.725970  4296 net.cpp:159] Memory required for data: 46490400
I0109 20:17:46.726012  4296 layer_factory.hpp:77] Creating layer bn3
I0109 20:17:46.726061  4296 net.cpp:94] Creating Layer bn3
I0109 20:17:46.726102  4296 net.cpp:435] bn3 <- conv3
I0109 20:17:46.726151  4296 net.cpp:409] bn3 -> scale3
I0109 20:17:46.727288  4296 net.cpp:144] Setting up bn3
I0109 20:17:46.727308  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.727318  4296 net.cpp:159] Memory required for data: 49767200
I0109 20:17:46.727367  4296 layer_factory.hpp:77] Creating layer relu3
I0109 20:17:46.727416  4296 net.cpp:94] Creating Layer relu3
I0109 20:17:46.727463  4296 net.cpp:435] relu3 <- scale3
I0109 20:17:46.727511  4296 net.cpp:409] relu3 -> relu3
I0109 20:17:46.727592  4296 net.cpp:144] Setting up relu3
I0109 20:17:46.727641  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.727682  4296 net.cpp:159] Memory required for data: 53044000
I0109 20:17:46.727717  4296 layer_factory.hpp:77] Creating layer conv4
I0109 20:17:46.727772  4296 net.cpp:94] Creating Layer conv4
I0109 20:17:46.727816  4296 net.cpp:435] conv4 <- relu3
I0109 20:17:46.727869  4296 net.cpp:409] conv4 -> conv4
I0109 20:17:46.728690  4296 net.cpp:144] Setting up conv4
I0109 20:17:46.728754  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.728797  4296 net.cpp:159] Memory required for data: 56320800
I0109 20:17:46.728830  4296 layer_factory.hpp:77] Creating layer bn4
I0109 20:17:46.728915  4296 net.cpp:94] Creating Layer bn4
I0109 20:17:46.728960  4296 net.cpp:435] bn4 <- conv4
I0109 20:17:46.729014  4296 net.cpp:409] bn4 -> scale4
I0109 20:17:46.730199  4296 net.cpp:144] Setting up bn4
I0109 20:17:46.730222  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.730231  4296 net.cpp:159] Memory required for data: 59597600
I0109 20:17:46.730281  4296 layer_factory.hpp:77] Creating layer relu4
I0109 20:17:46.730332  4296 net.cpp:94] Creating Layer relu4
I0109 20:17:46.730373  4296 net.cpp:435] relu4 <- scale4
I0109 20:17:46.730422  4296 net.cpp:409] relu4 -> relu4
I0109 20:17:46.730504  4296 net.cpp:144] Setting up relu4
I0109 20:17:46.730554  4296 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:17:46.730594  4296 net.cpp:159] Memory required for data: 62874400
I0109 20:17:46.730635  4296 layer_factory.hpp:77] Creating layer pool2
I0109 20:17:46.730661  4296 net.cpp:94] Creating Layer pool2
I0109 20:17:46.730732  4296 net.cpp:435] pool2 <- relu4
I0109 20:17:46.730792  4296 net.cpp:409] pool2 -> pool2
I0109 20:17:46.730901  4296 net.cpp:144] Setting up pool2
I0109 20:17:46.730947  4296 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:17:46.730984  4296 net.cpp:159] Memory required for data: 63693600
I0109 20:17:46.731015  4296 layer_factory.hpp:77] Creating layer drop2
I0109 20:17:46.731048  4296 net.cpp:94] Creating Layer drop2
I0109 20:17:46.731093  4296 net.cpp:435] drop2 <- pool2
I0109 20:17:46.731130  4296 net.cpp:409] drop2 -> drop2
I0109 20:17:46.731279  4296 net.cpp:144] Setting up drop2
I0109 20:17:46.731313  4296 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:17:46.731321  4296 net.cpp:159] Memory required for data: 64512800
I0109 20:17:46.731328  4296 layer_factory.hpp:77] Creating layer fc1
I0109 20:17:46.731359  4296 net.cpp:94] Creating Layer fc1
I0109 20:17:46.731387  4296 net.cpp:435] fc1 <- drop2
I0109 20:17:46.731400  4296 net.cpp:409] fc1 -> fc1
I0109 20:17:46.752852  4296 net.cpp:144] Setting up fc1
I0109 20:17:46.752880  4296 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:46.752888  4296 net.cpp:159] Memory required for data: 64615200
I0109 20:17:46.752900  4296 layer_factory.hpp:77] Creating layer bn5
I0109 20:17:46.752918  4296 net.cpp:94] Creating Layer bn5
I0109 20:17:46.752929  4296 net.cpp:435] bn5 <- fc1
I0109 20:17:46.752943  4296 net.cpp:409] bn5 -> scale5
I0109 20:17:46.753628  4296 net.cpp:144] Setting up bn5
I0109 20:17:46.753648  4296 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:46.753655  4296 net.cpp:159] Memory required for data: 64717600
I0109 20:17:46.753681  4296 layer_factory.hpp:77] Creating layer relu5
I0109 20:17:46.753700  4296 net.cpp:94] Creating Layer relu5
I0109 20:17:46.753710  4296 net.cpp:435] relu5 <- scale5
I0109 20:17:46.753720  4296 net.cpp:409] relu5 -> relu5
I0109 20:17:46.753777  4296 net.cpp:144] Setting up relu5
I0109 20:17:46.753795  4296 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:46.753803  4296 net.cpp:159] Memory required for data: 64820000
I0109 20:17:46.753810  4296 layer_factory.hpp:77] Creating layer drop3
I0109 20:17:46.753823  4296 net.cpp:94] Creating Layer drop3
I0109 20:17:46.753832  4296 net.cpp:435] drop3 <- relu5
I0109 20:17:46.753841  4296 net.cpp:409] drop3 -> drop3
I0109 20:17:46.753896  4296 net.cpp:144] Setting up drop3
I0109 20:17:46.753912  4296 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:17:46.753919  4296 net.cpp:159] Memory required for data: 64922400
I0109 20:17:46.753926  4296 layer_factory.hpp:77] Creating layer fc2
I0109 20:17:46.753939  4296 net.cpp:94] Creating Layer fc2
I0109 20:17:46.753947  4296 net.cpp:435] fc2 <- drop3
I0109 20:17:46.753959  4296 net.cpp:409] fc2 -> fc2
I0109 20:17:46.754140  4296 net.cpp:144] Setting up fc2
I0109 20:17:46.754158  4296 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:46.754164  4296 net.cpp:159] Memory required for data: 64924400
I0109 20:17:46.754174  4296 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:17:46.754190  4296 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:17:46.754204  4296 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:17:46.754215  4296 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:17:46.754227  4296 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:17:46.754246  4296 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:17:46.754315  4296 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:17:46.754329  4296 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:46.754333  4296 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:46.754338  4296 net.cpp:151] Top shape: 50 10 (500)
I0109 20:17:46.754343  4296 net.cpp:159] Memory required for data: 64930400
I0109 20:17:46.754356  4296 layer_factory.hpp:77] Creating layer loss
I0109 20:17:46.754370  4296 net.cpp:94] Creating Layer loss
I0109 20:17:46.754384  4296 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:17:46.754395  4296 net.cpp:435] loss <- label_data_1_split_0
I0109 20:17:46.754405  4296 net.cpp:409] loss -> loss
I0109 20:17:46.754426  4296 layer_factory.hpp:77] Creating layer loss
I0109 20:17:46.754559  4296 net.cpp:144] Setting up loss
I0109 20:17:46.754576  4296 net.cpp:151] Top shape: (1)
I0109 20:17:46.754580  4296 net.cpp:154]     with loss weight 1
I0109 20:17:46.754603  4296 net.cpp:159] Memory required for data: 64930404
I0109 20:17:46.754611  4296 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:17:46.754629  4296 net.cpp:94] Creating Layer accuracy-top1
I0109 20:17:46.754637  4296 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:17:46.754652  4296 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:17:46.754683  4296 net.cpp:409] accuracy-top1 -> top-1
I0109 20:17:46.754709  4296 net.cpp:144] Setting up accuracy-top1
I0109 20:17:46.754725  4296 net.cpp:151] Top shape: (1)
I0109 20:17:46.754731  4296 net.cpp:159] Memory required for data: 64930408
I0109 20:17:46.754737  4296 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:17:46.754770  4296 net.cpp:94] Creating Layer accuracy-top5
I0109 20:17:46.754792  4296 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:17:46.754813  4296 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:17:46.754832  4296 net.cpp:409] accuracy-top5 -> top-5
I0109 20:17:46.754848  4296 net.cpp:144] Setting up accuracy-top5
I0109 20:17:46.754856  4296 net.cpp:151] Top shape: (1)
I0109 20:17:46.754865  4296 net.cpp:159] Memory required for data: 64930412
I0109 20:17:46.754871  4296 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:17:46.754881  4296 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:17:46.754889  4296 net.cpp:220] loss needs backward computation.
I0109 20:17:46.754896  4296 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:17:46.754902  4296 net.cpp:220] fc2 needs backward computation.
I0109 20:17:46.754910  4296 net.cpp:220] drop3 needs backward computation.
I0109 20:17:46.754916  4296 net.cpp:220] relu5 needs backward computation.
I0109 20:17:46.754925  4296 net.cpp:220] bn5 needs backward computation.
I0109 20:17:46.754930  4296 net.cpp:220] fc1 needs backward computation.
I0109 20:17:46.754937  4296 net.cpp:220] drop2 needs backward computation.
I0109 20:17:46.754943  4296 net.cpp:220] pool2 needs backward computation.
I0109 20:17:46.754951  4296 net.cpp:220] relu4 needs backward computation.
I0109 20:17:46.754957  4296 net.cpp:220] bn4 needs backward computation.
I0109 20:17:46.754964  4296 net.cpp:220] conv4 needs backward computation.
I0109 20:17:46.754971  4296 net.cpp:220] relu3 needs backward computation.
I0109 20:17:46.754977  4296 net.cpp:220] bn3 needs backward computation.
I0109 20:17:46.754983  4296 net.cpp:220] conv3 needs backward computation.
I0109 20:17:46.754992  4296 net.cpp:220] drop1 needs backward computation.
I0109 20:17:46.754998  4296 net.cpp:220] pool1 needs backward computation.
I0109 20:17:46.755008  4296 net.cpp:220] relu2 needs backward computation.
I0109 20:17:46.755017  4296 net.cpp:220] bn2 needs backward computation.
I0109 20:17:46.755023  4296 net.cpp:220] conv2 needs backward computation.
I0109 20:17:46.755029  4296 net.cpp:220] relu1 needs backward computation.
I0109 20:17:46.755036  4296 net.cpp:220] bn1 needs backward computation.
I0109 20:17:46.755043  4296 net.cpp:220] conv1 needs backward computation.
I0109 20:17:46.755051  4296 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:17:46.755060  4296 net.cpp:222] data does not need backward computation.
I0109 20:17:46.755066  4296 net.cpp:264] This network produces output loss
I0109 20:17:46.755072  4296 net.cpp:264] This network produces output top-1
I0109 20:17:46.755081  4296 net.cpp:264] This network produces output top-5
I0109 20:17:46.755115  4296 net.cpp:284] Network initialization done.
I0109 20:17:46.755220  4296 solver.cpp:63] Solver scaffolding done.
I0109 20:17:46.756532  4296 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/sparse.caffemodel
I0109 20:17:46.820284  4296 caffe_interface.cpp:527] Starting Optimization
I0109 20:17:46.820334  4296 solver.cpp:335] Solving 
I0109 20:17:46.820343  4296 solver.cpp:336] Learning Rate Policy: poly
I0109 20:17:46.821713  4296 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 20:17:47.632877  4296 solver.cpp:517]     Test net output #0: loss = 0.843083 (* 1 = 0.843083 loss)
I0109 20:17:47.632927  4296 solver.cpp:517]     Test net output #1: top-1 = 0.767
I0109 20:17:47.632941  4296 solver.cpp:517]     Test net output #2: top-5 = 0.982445
I0109 20:17:47.679440  4296 solver.cpp:266] Iteration 0 (0 iter/s, 0.859045s/100 iter), loss = 0.294564
I0109 20:17:47.679486  4296 solver.cpp:285]     Train net output #0: loss = 0.294564 (* 1 = 0.294564 loss)
I0109 20:17:47.679549  4296 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 20:17:50.923565  4296 solver.cpp:266] Iteration 100 (30.8254 iter/s, 3.24408s/100 iter), loss = 0.275014
I0109 20:17:50.923635  4296 solver.cpp:285]     Train net output #0: loss = 0.275014 (* 1 = 0.275014 loss)
I0109 20:17:50.923655  4296 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 20:17:54.176966  4296 solver.cpp:266] Iteration 200 (30.7374 iter/s, 3.25336s/100 iter), loss = 0.205748
I0109 20:17:54.177033  4296 solver.cpp:285]     Train net output #0: loss = 0.205748 (* 1 = 0.205748 loss)
I0109 20:17:54.177045  4296 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 20:17:57.428542  4296 solver.cpp:266] Iteration 300 (30.7547 iter/s, 3.25154s/100 iter), loss = 0.374203
I0109 20:17:57.428607  4296 solver.cpp:285]     Train net output #0: loss = 0.374203 (* 1 = 0.374203 loss)
I0109 20:17:57.428620  4296 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 20:18:00.679005  4296 solver.cpp:266] Iteration 400 (30.7652 iter/s, 3.25043s/100 iter), loss = 0.292194
I0109 20:18:00.679071  4296 solver.cpp:285]     Train net output #0: loss = 0.292194 (* 1 = 0.292194 loss)
I0109 20:18:00.679083  4296 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 20:18:03.937348  4296 solver.cpp:266] Iteration 500 (30.6911 iter/s, 3.25828s/100 iter), loss = 0.341481
I0109 20:18:03.937413  4296 solver.cpp:285]     Train net output #0: loss = 0.341481 (* 1 = 0.341481 loss)
I0109 20:18:03.937427  4296 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 20:18:07.198958  4296 solver.cpp:266] Iteration 600 (30.66 iter/s, 3.26157s/100 iter), loss = 0.383027
I0109 20:18:07.199023  4296 solver.cpp:285]     Train net output #0: loss = 0.383027 (* 1 = 0.383027 loss)
I0109 20:18:07.199035  4296 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 20:18:10.462108  4296 solver.cpp:266] Iteration 700 (30.6456 iter/s, 3.26311s/100 iter), loss = 0.423813
I0109 20:18:10.462185  4296 solver.cpp:285]     Train net output #0: loss = 0.423813 (* 1 = 0.423813 loss)
I0109 20:18:10.462200  4296 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 20:18:13.728725  4296 solver.cpp:266] Iteration 800 (30.6131 iter/s, 3.26657s/100 iter), loss = 0.268049
I0109 20:18:13.728791  4296 solver.cpp:285]     Train net output #0: loss = 0.268049 (* 1 = 0.268049 loss)
I0109 20:18:13.728804  4296 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 20:18:16.984787  4296 solver.cpp:266] Iteration 900 (30.7125 iter/s, 3.256s/100 iter), loss = 0.321521
I0109 20:18:16.984975  4296 solver.cpp:285]     Train net output #0: loss = 0.321521 (* 1 = 0.321521 loss)
I0109 20:18:16.984992  4296 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 20:18:20.217438  4296 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 20:18:21.029356  4296 solver.cpp:517]     Test net output #0: loss = 0.780461 (* 1 = 0.780461 loss)
I0109 20:18:21.029393  4296 solver.cpp:517]     Test net output #1: top-1 = 0.797444
I0109 20:18:21.029403  4296 solver.cpp:517]     Test net output #2: top-5 = 0.983333
I0109 20:18:21.060252  4296 solver.cpp:266] Iteration 1000 (24.5379 iter/s, 4.07532s/100 iter), loss = 0.229039
I0109 20:18:21.060289  4296 solver.cpp:285]     Train net output #0: loss = 0.229039 (* 1 = 0.229039 loss)
I0109 20:18:21.060304  4296 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 20:18:24.317075  4296 solver.cpp:266] Iteration 1100 (30.7048 iter/s, 3.25681s/100 iter), loss = 0.219738
I0109 20:18:24.317142  4296 solver.cpp:285]     Train net output #0: loss = 0.219738 (* 1 = 0.219738 loss)
I0109 20:18:24.317154  4296 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 20:18:27.579944  4296 solver.cpp:266] Iteration 1200 (30.6482 iter/s, 3.26283s/100 iter), loss = 0.221525
I0109 20:18:27.580008  4296 solver.cpp:285]     Train net output #0: loss = 0.221525 (* 1 = 0.221525 loss)
I0109 20:18:27.580020  4296 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 20:18:30.849794  4296 solver.cpp:266] Iteration 1300 (30.5828 iter/s, 3.26982s/100 iter), loss = 0.212972
I0109 20:18:30.849858  4296 solver.cpp:285]     Train net output #0: loss = 0.212972 (* 1 = 0.212972 loss)
I0109 20:18:30.849872  4296 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 20:18:34.116117  4296 solver.cpp:266] Iteration 1400 (30.616 iter/s, 3.26626s/100 iter), loss = 0.279563
I0109 20:18:34.116180  4296 solver.cpp:285]     Train net output #0: loss = 0.279563 (* 1 = 0.279563 loss)
I0109 20:18:34.116192  4296 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 20:18:37.386924  4296 solver.cpp:266] Iteration 1500 (30.5738 iter/s, 3.27077s/100 iter), loss = 0.264776
I0109 20:18:37.386986  4296 solver.cpp:285]     Train net output #0: loss = 0.264776 (* 1 = 0.264776 loss)
I0109 20:18:37.386998  4296 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 20:18:40.651341  4296 solver.cpp:266] Iteration 1600 (30.6337 iter/s, 3.26438s/100 iter), loss = 0.272475
I0109 20:18:40.651417  4296 solver.cpp:285]     Train net output #0: loss = 0.272475 (* 1 = 0.272475 loss)
I0109 20:18:40.651432  4296 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 20:18:43.920065  4296 solver.cpp:266] Iteration 1700 (30.5937 iter/s, 3.26865s/100 iter), loss = 0.291043
I0109 20:18:43.920131  4296 solver.cpp:285]     Train net output #0: loss = 0.291043 (* 1 = 0.291043 loss)
I0109 20:18:43.920147  4296 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 20:18:47.184985  4296 solver.cpp:266] Iteration 1800 (30.629 iter/s, 3.26488s/100 iter), loss = 0.415209
I0109 20:18:47.185176  4296 solver.cpp:285]     Train net output #0: loss = 0.415209 (* 1 = 0.415209 loss)
I0109 20:18:47.185190  4296 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 20:18:50.453616  4296 solver.cpp:266] Iteration 1900 (30.5954 iter/s, 3.26846s/100 iter), loss = 0.305931
I0109 20:18:50.453681  4296 solver.cpp:285]     Train net output #0: loss = 0.305931 (* 1 = 0.305931 loss)
I0109 20:18:50.453693  4296 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 20:18:53.689608  4296 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 20:18:54.506510  4296 solver.cpp:517]     Test net output #0: loss = 0.662431 (* 1 = 0.662431 loss)
I0109 20:18:54.506547  4296 solver.cpp:517]     Test net output #1: top-1 = 0.813
I0109 20:18:54.506554  4296 solver.cpp:517]     Test net output #2: top-5 = 0.985778
I0109 20:18:54.537436  4296 solver.cpp:266] Iteration 2000 (24.487 iter/s, 4.0838s/100 iter), loss = 0.222139
I0109 20:18:54.537480  4296 solver.cpp:285]     Train net output #0: loss = 0.222139 (* 1 = 0.222139 loss)
I0109 20:18:54.537495  4296 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 20:18:57.801807  4296 solver.cpp:266] Iteration 2100 (30.6339 iter/s, 3.26435s/100 iter), loss = 0.297212
I0109 20:18:57.801870  4296 solver.cpp:285]     Train net output #0: loss = 0.297212 (* 1 = 0.297212 loss)
I0109 20:18:57.801883  4296 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 20:19:01.069208  4296 solver.cpp:266] Iteration 2200 (30.6059 iter/s, 3.26734s/100 iter), loss = 0.202002
I0109 20:19:01.069273  4296 solver.cpp:285]     Train net output #0: loss = 0.202002 (* 1 = 0.202002 loss)
I0109 20:19:01.069285  4296 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 20:19:04.336989  4296 solver.cpp:266] Iteration 2300 (30.6021 iter/s, 3.26775s/100 iter), loss = 0.37301
I0109 20:19:04.337056  4296 solver.cpp:285]     Train net output #0: loss = 0.37301 (* 1 = 0.37301 loss)
I0109 20:19:04.337069  4296 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 20:19:07.602916  4296 solver.cpp:266] Iteration 2400 (30.6196 iter/s, 3.26589s/100 iter), loss = 0.183546
I0109 20:19:07.602998  4296 solver.cpp:285]     Train net output #0: loss = 0.183546 (* 1 = 0.183546 loss)
I0109 20:19:07.603015  4296 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 20:19:10.871047  4296 solver.cpp:266] Iteration 2500 (30.599 iter/s, 3.26808s/100 iter), loss = 0.291508
I0109 20:19:10.871109  4296 solver.cpp:285]     Train net output #0: loss = 0.291508 (* 1 = 0.291508 loss)
I0109 20:19:10.871122  4296 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 20:19:14.134483  4296 solver.cpp:266] Iteration 2600 (30.6431 iter/s, 3.26338s/100 iter), loss = 0.221952
I0109 20:19:14.134543  4296 solver.cpp:285]     Train net output #0: loss = 0.221952 (* 1 = 0.221952 loss)
I0109 20:19:14.134555  4296 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 20:19:17.403095  4296 solver.cpp:266] Iteration 2700 (30.5943 iter/s, 3.26858s/100 iter), loss = 0.344664
I0109 20:19:17.403283  4296 solver.cpp:285]     Train net output #0: loss = 0.344664 (* 1 = 0.344664 loss)
I0109 20:19:17.403298  4296 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 20:19:20.672478  4296 solver.cpp:266] Iteration 2800 (30.5883 iter/s, 3.26923s/100 iter), loss = 0.475899
I0109 20:19:20.672541  4296 solver.cpp:285]     Train net output #0: loss = 0.475899 (* 1 = 0.475899 loss)
I0109 20:19:20.672554  4296 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 20:19:23.939932  4296 solver.cpp:266] Iteration 2900 (30.6054 iter/s, 3.26739s/100 iter), loss = 0.25508
I0109 20:19:23.939992  4296 solver.cpp:285]     Train net output #0: loss = 0.25508 (* 1 = 0.25508 loss)
I0109 20:19:23.940004  4296 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 20:19:27.173661  4296 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 20:19:27.987130  4296 solver.cpp:517]     Test net output #0: loss = 0.658601 (* 1 = 0.658601 loss)
I0109 20:19:27.987169  4296 solver.cpp:517]     Test net output #1: top-1 = 0.806333
I0109 20:19:27.987179  4296 solver.cpp:517]     Test net output #2: top-5 = 0.981889
I0109 20:19:28.018074  4296 solver.cpp:266] Iteration 3000 (24.5211 iter/s, 4.07813s/100 iter), loss = 0.285596
I0109 20:19:28.018110  4296 solver.cpp:285]     Train net output #0: loss = 0.285596 (* 1 = 0.285596 loss)
I0109 20:19:28.018123  4296 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 20:19:31.285825  4296 solver.cpp:266] Iteration 3100 (30.6021 iter/s, 3.26774s/100 iter), loss = 0.262998
I0109 20:19:31.285888  4296 solver.cpp:285]     Train net output #0: loss = 0.262998 (* 1 = 0.262998 loss)
I0109 20:19:31.285900  4296 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 20:19:34.549861  4296 solver.cpp:266] Iteration 3200 (30.6372 iter/s, 3.26401s/100 iter), loss = 0.277863
I0109 20:19:34.549924  4296 solver.cpp:285]     Train net output #0: loss = 0.277863 (* 1 = 0.277863 loss)
I0109 20:19:34.549937  4296 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 20:19:37.817240  4296 solver.cpp:266] Iteration 3300 (30.6059 iter/s, 3.26734s/100 iter), loss = 0.283746
I0109 20:19:37.817307  4296 solver.cpp:285]     Train net output #0: loss = 0.283746 (* 1 = 0.283746 loss)
I0109 20:19:37.817322  4296 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 20:19:41.083829  4296 solver.cpp:266] Iteration 3400 (30.6136 iter/s, 3.26652s/100 iter), loss = 0.155227
I0109 20:19:41.083914  4296 solver.cpp:285]     Train net output #0: loss = 0.155227 (* 1 = 0.155227 loss)
I0109 20:19:41.083928  4296 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 20:19:44.352186  4296 solver.cpp:266] Iteration 3500 (30.5969 iter/s, 3.26831s/100 iter), loss = 0.289826
I0109 20:19:44.352246  4296 solver.cpp:285]     Train net output #0: loss = 0.289826 (* 1 = 0.289826 loss)
I0109 20:19:44.352259  4296 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 20:19:47.620683  4296 solver.cpp:266] Iteration 3600 (30.5954 iter/s, 3.26847s/100 iter), loss = 0.208231
I0109 20:19:47.620849  4296 solver.cpp:285]     Train net output #0: loss = 0.208231 (* 1 = 0.208231 loss)
I0109 20:19:47.620864  4296 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 20:19:50.887817  4296 solver.cpp:266] Iteration 3700 (30.6094 iter/s, 3.26697s/100 iter), loss = 0.240483
I0109 20:19:50.887883  4296 solver.cpp:285]     Train net output #0: loss = 0.240483 (* 1 = 0.240483 loss)
I0109 20:19:50.887897  4296 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 20:19:54.151041  4296 solver.cpp:266] Iteration 3800 (30.6449 iter/s, 3.26319s/100 iter), loss = 0.242566
I0109 20:19:54.151103  4296 solver.cpp:285]     Train net output #0: loss = 0.242566 (* 1 = 0.242566 loss)
I0109 20:19:54.151115  4296 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 20:19:57.418365  4296 solver.cpp:266] Iteration 3900 (30.6064 iter/s, 3.26729s/100 iter), loss = 0.265996
I0109 20:19:57.418427  4296 solver.cpp:285]     Train net output #0: loss = 0.265996 (* 1 = 0.265996 loss)
I0109 20:19:57.418439  4296 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 20:20:00.650424  4296 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 20:20:01.464951  4296 solver.cpp:517]     Test net output #0: loss = 0.544023 (* 1 = 0.544023 loss)
I0109 20:20:01.464990  4296 solver.cpp:517]     Test net output #1: top-1 = 0.827445
I0109 20:20:01.464999  4296 solver.cpp:517]     Test net output #2: top-5 = 0.990555
I0109 20:20:01.495820  4296 solver.cpp:266] Iteration 4000 (24.5252 iter/s, 4.07743s/100 iter), loss = 0.2251
I0109 20:20:01.495864  4296 solver.cpp:285]     Train net output #0: loss = 0.2251 (* 1 = 0.2251 loss)
I0109 20:20:01.495879  4296 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 20:20:04.760303  4296 solver.cpp:266] Iteration 4100 (30.6329 iter/s, 3.26447s/100 iter), loss = 0.213346
I0109 20:20:04.760370  4296 solver.cpp:285]     Train net output #0: loss = 0.213346 (* 1 = 0.213346 loss)
I0109 20:20:04.760383  4296 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 20:20:08.024374  4296 solver.cpp:266] Iteration 4200 (30.6372 iter/s, 3.26401s/100 iter), loss = 0.215524
I0109 20:20:08.024435  4296 solver.cpp:285]     Train net output #0: loss = 0.215524 (* 1 = 0.215524 loss)
I0109 20:20:08.024447  4296 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 20:20:11.288836  4296 solver.cpp:266] Iteration 4300 (30.6332 iter/s, 3.26443s/100 iter), loss = 0.344144
I0109 20:20:11.288898  4296 solver.cpp:285]     Train net output #0: loss = 0.344144 (* 1 = 0.344144 loss)
I0109 20:20:11.288910  4296 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 20:20:14.552093  4296 solver.cpp:266] Iteration 4400 (30.6445 iter/s, 3.26323s/100 iter), loss = 0.230349
I0109 20:20:14.552155  4296 solver.cpp:285]     Train net output #0: loss = 0.230349 (* 1 = 0.230349 loss)
I0109 20:20:14.552166  4296 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 20:20:17.811451  4296 solver.cpp:266] Iteration 4500 (30.6812 iter/s, 3.25933s/100 iter), loss = 0.296164
I0109 20:20:17.811632  4296 solver.cpp:285]     Train net output #0: loss = 0.296164 (* 1 = 0.296164 loss)
I0109 20:20:17.811648  4296 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 20:20:21.078982  4296 solver.cpp:266] Iteration 4600 (30.6058 iter/s, 3.26736s/100 iter), loss = 0.372353
I0109 20:20:21.079044  4296 solver.cpp:285]     Train net output #0: loss = 0.372353 (* 1 = 0.372353 loss)
I0109 20:20:21.079056  4296 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 20:20:24.341761  4296 solver.cpp:266] Iteration 4700 (30.649 iter/s, 3.26275s/100 iter), loss = 0.255785
I0109 20:20:24.341836  4296 solver.cpp:285]     Train net output #0: loss = 0.255785 (* 1 = 0.255785 loss)
I0109 20:20:24.341852  4296 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 20:20:27.610671  4296 solver.cpp:266] Iteration 4800 (30.5916 iter/s, 3.26887s/100 iter), loss = 0.310525
I0109 20:20:27.610734  4296 solver.cpp:285]     Train net output #0: loss = 0.310525 (* 1 = 0.310525 loss)
I0109 20:20:27.610746  4296 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 20:20:30.876758  4296 solver.cpp:266] Iteration 4900 (30.618 iter/s, 3.26606s/100 iter), loss = 0.239089
I0109 20:20:30.876822  4296 solver.cpp:285]     Train net output #0: loss = 0.239089 (* 1 = 0.239089 loss)
I0109 20:20:30.876835  4296 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 20:20:34.111608  4296 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 20:20:34.926741  4296 solver.cpp:517]     Test net output #0: loss = 0.604513 (* 1 = 0.604513 loss)
I0109 20:20:34.926781  4296 solver.cpp:517]     Test net output #1: top-1 = 0.817667
I0109 20:20:34.926789  4296 solver.cpp:517]     Test net output #2: top-5 = 0.989222
I0109 20:20:34.957623  4296 solver.cpp:266] Iteration 5000 (24.5049 iter/s, 4.08082s/100 iter), loss = 0.1745
I0109 20:20:34.957664  4296 solver.cpp:285]     Train net output #0: loss = 0.1745 (* 1 = 0.1745 loss)
I0109 20:20:34.957679  4296 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 20:20:38.222920  4296 solver.cpp:266] Iteration 5100 (30.6252 iter/s, 3.26528s/100 iter), loss = 0.191456
I0109 20:20:38.223003  4296 solver.cpp:285]     Train net output #0: loss = 0.191456 (* 1 = 0.191456 loss)
I0109 20:20:38.223019  4296 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 20:20:41.489925  4296 solver.cpp:266] Iteration 5200 (30.6095 iter/s, 3.26696s/100 iter), loss = 0.305635
I0109 20:20:41.489989  4296 solver.cpp:285]     Train net output #0: loss = 0.305635 (* 1 = 0.305635 loss)
I0109 20:20:41.490001  4296 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 20:20:44.753952  4296 solver.cpp:266] Iteration 5300 (30.6373 iter/s, 3.26399s/100 iter), loss = 0.262419
I0109 20:20:44.754019  4296 solver.cpp:285]     Train net output #0: loss = 0.262419 (* 1 = 0.262419 loss)
I0109 20:20:44.754034  4296 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 20:20:48.019989  4296 solver.cpp:266] Iteration 5400 (30.6187 iter/s, 3.26598s/100 iter), loss = 0.202059
I0109 20:20:48.020211  4296 solver.cpp:285]     Train net output #0: loss = 0.202059 (* 1 = 0.202059 loss)
I0109 20:20:48.020227  4296 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 20:20:51.289746  4296 solver.cpp:266] Iteration 5500 (30.5851 iter/s, 3.26957s/100 iter), loss = 0.325749
I0109 20:20:51.289822  4296 solver.cpp:285]     Train net output #0: loss = 0.325749 (* 1 = 0.325749 loss)
I0109 20:20:51.289839  4296 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 20:20:54.556273  4296 solver.cpp:266] Iteration 5600 (30.614 iter/s, 3.26648s/100 iter), loss = 0.27622
I0109 20:20:54.556350  4296 solver.cpp:285]     Train net output #0: loss = 0.27622 (* 1 = 0.27622 loss)
I0109 20:20:54.556365  4296 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 20:20:57.817252  4296 solver.cpp:266] Iteration 5700 (30.6661 iter/s, 3.26093s/100 iter), loss = 0.171024
I0109 20:20:57.817314  4296 solver.cpp:285]     Train net output #0: loss = 0.171024 (* 1 = 0.171024 loss)
I0109 20:20:57.817327  4296 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 20:21:01.087911  4296 solver.cpp:266] Iteration 5800 (30.5754 iter/s, 3.2706s/100 iter), loss = 0.246669
I0109 20:21:01.087991  4296 solver.cpp:285]     Train net output #0: loss = 0.246669 (* 1 = 0.246669 loss)
I0109 20:21:01.088007  4296 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 20:21:04.355883  4296 solver.cpp:266] Iteration 5900 (30.6005 iter/s, 3.26793s/100 iter), loss = 0.300526
I0109 20:21:04.355944  4296 solver.cpp:285]     Train net output #0: loss = 0.300526 (* 1 = 0.300526 loss)
I0109 20:21:04.355957  4296 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 20:21:07.593109  4296 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 20:21:08.407302  4296 solver.cpp:517]     Test net output #0: loss = 0.524556 (* 1 = 0.524556 loss)
I0109 20:21:08.407342  4296 solver.cpp:517]     Test net output #1: top-1 = 0.833
I0109 20:21:08.407351  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0109 20:21:08.438138  4296 solver.cpp:266] Iteration 6000 (24.4964 iter/s, 4.08224s/100 iter), loss = 0.305357
I0109 20:21:08.438174  4296 solver.cpp:285]     Train net output #0: loss = 0.305357 (* 1 = 0.305357 loss)
I0109 20:21:08.438189  4296 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 20:21:11.696439  4296 solver.cpp:266] Iteration 6100 (30.6909 iter/s, 3.25829s/100 iter), loss = 0.336008
I0109 20:21:11.696501  4296 solver.cpp:285]     Train net output #0: loss = 0.336008 (* 1 = 0.336008 loss)
I0109 20:21:11.696514  4296 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 20:21:14.963778  4296 solver.cpp:266] Iteration 6200 (30.6065 iter/s, 3.26728s/100 iter), loss = 0.248722
I0109 20:21:14.963841  4296 solver.cpp:285]     Train net output #0: loss = 0.248722 (* 1 = 0.248722 loss)
I0109 20:21:14.963855  4296 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 20:21:18.236899  4296 solver.cpp:266] Iteration 6300 (30.5522 iter/s, 3.27309s/100 iter), loss = 0.186422
I0109 20:21:18.237118  4296 solver.cpp:285]     Train net output #0: loss = 0.186422 (* 1 = 0.186422 loss)
I0109 20:21:18.237143  4296 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 20:21:21.506100  4296 solver.cpp:266] Iteration 6400 (30.5902 iter/s, 3.26902s/100 iter), loss = 0.267838
I0109 20:21:21.506183  4296 solver.cpp:285]     Train net output #0: loss = 0.267838 (* 1 = 0.267838 loss)
I0109 20:21:21.506204  4296 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 20:21:24.778604  4296 solver.cpp:266] Iteration 6500 (30.5581 iter/s, 3.27246s/100 iter), loss = 0.272611
I0109 20:21:24.778673  4296 solver.cpp:285]     Train net output #0: loss = 0.27261 (* 1 = 0.27261 loss)
I0109 20:21:24.778687  4296 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 20:21:28.051249  4296 solver.cpp:266] Iteration 6600 (30.5569 iter/s, 3.27258s/100 iter), loss = 0.175548
I0109 20:21:28.051321  4296 solver.cpp:285]     Train net output #0: loss = 0.175548 (* 1 = 0.175548 loss)
I0109 20:21:28.051342  4296 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 20:21:31.318233  4296 solver.cpp:266] Iteration 6700 (30.6096 iter/s, 3.26695s/100 iter), loss = 0.245849
I0109 20:21:31.318301  4296 solver.cpp:285]     Train net output #0: loss = 0.245848 (* 1 = 0.245848 loss)
I0109 20:21:31.318320  4296 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 20:21:34.583395  4296 solver.cpp:266] Iteration 6800 (30.6267 iter/s, 3.26513s/100 iter), loss = 0.284536
I0109 20:21:34.583462  4296 solver.cpp:285]     Train net output #0: loss = 0.284536 (* 1 = 0.284536 loss)
I0109 20:21:34.583479  4296 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 20:21:37.846905  4296 solver.cpp:266] Iteration 6900 (30.6422 iter/s, 3.26348s/100 iter), loss = 0.20111
I0109 20:21:37.846974  4296 solver.cpp:285]     Train net output #0: loss = 0.201109 (* 1 = 0.201109 loss)
I0109 20:21:37.846993  4296 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 20:21:41.082142  4296 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 20:21:41.897030  4296 solver.cpp:517]     Test net output #0: loss = 0.505916 (* 1 = 0.505916 loss)
I0109 20:21:41.897073  4296 solver.cpp:517]     Test net output #1: top-1 = 0.836444
I0109 20:21:41.897086  4296 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0109 20:21:41.927928  4296 solver.cpp:266] Iteration 7000 (24.5039 iter/s, 4.08098s/100 iter), loss = 0.153619
I0109 20:21:41.927973  4296 solver.cpp:285]     Train net output #0: loss = 0.153619 (* 1 = 0.153619 loss)
I0109 20:21:41.927994  4296 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 20:21:45.193573  4296 solver.cpp:266] Iteration 7100 (30.6219 iter/s, 3.26563s/100 iter), loss = 0.264397
I0109 20:21:45.193652  4296 solver.cpp:285]     Train net output #0: loss = 0.264397 (* 1 = 0.264397 loss)
I0109 20:21:45.193670  4296 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 20:21:48.457916  4296 solver.cpp:266] Iteration 7200 (30.6345 iter/s, 3.2643s/100 iter), loss = 0.266484
I0109 20:21:48.458062  4296 solver.cpp:285]     Train net output #0: loss = 0.266484 (* 1 = 0.266484 loss)
I0109 20:21:48.458082  4296 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 20:21:51.725503  4296 solver.cpp:266] Iteration 7300 (30.6047 iter/s, 3.26747s/100 iter), loss = 0.230538
I0109 20:21:51.725628  4296 solver.cpp:285]     Train net output #0: loss = 0.230538 (* 1 = 0.230538 loss)
I0109 20:21:51.725652  4296 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 20:21:54.984927  4296 solver.cpp:266] Iteration 7400 (30.6811 iter/s, 3.25934s/100 iter), loss = 0.253727
I0109 20:21:54.984997  4296 solver.cpp:285]     Train net output #0: loss = 0.253727 (* 1 = 0.253727 loss)
I0109 20:21:54.985015  4296 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 20:21:58.253607  4296 solver.cpp:266] Iteration 7500 (30.5938 iter/s, 3.26863s/100 iter), loss = 0.145382
I0109 20:21:58.253675  4296 solver.cpp:285]     Train net output #0: loss = 0.145382 (* 1 = 0.145382 loss)
I0109 20:21:58.253695  4296 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 20:22:01.524266  4296 solver.cpp:266] Iteration 7600 (30.5752 iter/s, 3.27062s/100 iter), loss = 0.293913
I0109 20:22:01.524330  4296 solver.cpp:285]     Train net output #0: loss = 0.293913 (* 1 = 0.293913 loss)
I0109 20:22:01.524343  4296 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 20:22:04.791551  4296 solver.cpp:266] Iteration 7700 (30.6068 iter/s, 3.26725s/100 iter), loss = 0.24175
I0109 20:22:04.791613  4296 solver.cpp:285]     Train net output #0: loss = 0.24175 (* 1 = 0.24175 loss)
I0109 20:22:04.791626  4296 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 20:22:08.060169  4296 solver.cpp:266] Iteration 7800 (30.5945 iter/s, 3.26856s/100 iter), loss = 0.235707
I0109 20:22:08.060240  4296 solver.cpp:285]     Train net output #0: loss = 0.235706 (* 1 = 0.235706 loss)
I0109 20:22:08.060251  4296 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 20:22:11.327821  4296 solver.cpp:266] Iteration 7900 (30.6034 iter/s, 3.26761s/100 iter), loss = 0.164248
I0109 20:22:11.327879  4296 solver.cpp:285]     Train net output #0: loss = 0.164247 (* 1 = 0.164247 loss)
I0109 20:22:11.327891  4296 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 20:22:14.562258  4296 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 20:22:15.378276  4296 solver.cpp:517]     Test net output #0: loss = 0.512819 (* 1 = 0.512819 loss)
I0109 20:22:15.378314  4296 solver.cpp:517]     Test net output #1: top-1 = 0.835778
I0109 20:22:15.378322  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 20:22:15.409222  4296 solver.cpp:266] Iteration 8000 (24.5015 iter/s, 4.08139s/100 iter), loss = 0.287606
I0109 20:22:15.409258  4296 solver.cpp:285]     Train net output #0: loss = 0.287605 (* 1 = 0.287605 loss)
I0109 20:22:15.409272  4296 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 20:22:18.676846  4296 solver.cpp:266] Iteration 8100 (30.6033 iter/s, 3.26762s/100 iter), loss = 0.282043
I0109 20:22:18.677012  4296 solver.cpp:285]     Train net output #0: loss = 0.282043 (* 1 = 0.282043 loss)
I0109 20:22:18.677028  4296 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 20:22:21.940340  4296 solver.cpp:266] Iteration 8200 (30.6435 iter/s, 3.26333s/100 iter), loss = 0.192635
I0109 20:22:21.940399  4296 solver.cpp:285]     Train net output #0: loss = 0.192634 (* 1 = 0.192634 loss)
I0109 20:22:21.940412  4296 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 20:22:25.201951  4296 solver.cpp:266] Iteration 8300 (30.66 iter/s, 3.26158s/100 iter), loss = 0.322411
I0109 20:22:25.202011  4296 solver.cpp:285]     Train net output #0: loss = 0.322411 (* 1 = 0.322411 loss)
I0109 20:22:25.202023  4296 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 20:22:28.464799  4296 solver.cpp:266] Iteration 8400 (30.6483 iter/s, 3.26282s/100 iter), loss = 0.198458
I0109 20:22:28.464862  4296 solver.cpp:285]     Train net output #0: loss = 0.198458 (* 1 = 0.198458 loss)
I0109 20:22:28.464874  4296 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 20:22:31.730100  4296 solver.cpp:266] Iteration 8500 (30.6254 iter/s, 3.26527s/100 iter), loss = 0.236376
I0109 20:22:31.730162  4296 solver.cpp:285]     Train net output #0: loss = 0.236376 (* 1 = 0.236376 loss)
I0109 20:22:31.730175  4296 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 20:22:34.997776  4296 solver.cpp:266] Iteration 8600 (30.6033 iter/s, 3.26762s/100 iter), loss = 0.292535
I0109 20:22:34.997844  4296 solver.cpp:285]     Train net output #0: loss = 0.292535 (* 1 = 0.292535 loss)
I0109 20:22:34.997864  4296 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 20:22:38.265720  4296 solver.cpp:266] Iteration 8700 (30.6006 iter/s, 3.26791s/100 iter), loss = 0.17541
I0109 20:22:38.265799  4296 solver.cpp:285]     Train net output #0: loss = 0.17541 (* 1 = 0.17541 loss)
I0109 20:22:38.265815  4296 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 20:22:41.525671  4296 solver.cpp:266] Iteration 8800 (30.6757 iter/s, 3.25991s/100 iter), loss = 0.216659
I0109 20:22:41.525734  4296 solver.cpp:285]     Train net output #0: loss = 0.216658 (* 1 = 0.216658 loss)
I0109 20:22:41.525748  4296 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 20:22:44.791538  4296 solver.cpp:266] Iteration 8900 (30.6201 iter/s, 3.26583s/100 iter), loss = 0.262035
I0109 20:22:44.791602  4296 solver.cpp:285]     Train net output #0: loss = 0.262035 (* 1 = 0.262035 loss)
I0109 20:22:44.791616  4296 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 20:22:48.027431  4296 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 20:22:48.843374  4296 solver.cpp:517]     Test net output #0: loss = 0.500722 (* 1 = 0.500722 loss)
I0109 20:22:48.843554  4296 solver.cpp:517]     Test net output #1: top-1 = 0.841667
I0109 20:22:48.843565  4296 solver.cpp:517]     Test net output #2: top-5 = 0.990555
I0109 20:22:48.874392  4296 solver.cpp:266] Iteration 9000 (24.4928 iter/s, 4.08284s/100 iter), loss = 0.233078
I0109 20:22:48.874429  4296 solver.cpp:285]     Train net output #0: loss = 0.233078 (* 1 = 0.233078 loss)
I0109 20:22:48.874444  4296 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 20:22:52.142072  4296 solver.cpp:266] Iteration 9100 (30.6031 iter/s, 3.26764s/100 iter), loss = 0.170184
I0109 20:22:52.142132  4296 solver.cpp:285]     Train net output #0: loss = 0.170183 (* 1 = 0.170183 loss)
I0109 20:22:52.142143  4296 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 20:22:55.406682  4296 solver.cpp:266] Iteration 9200 (30.6318 iter/s, 3.26458s/100 iter), loss = 0.221937
I0109 20:22:55.406744  4296 solver.cpp:285]     Train net output #0: loss = 0.221937 (* 1 = 0.221937 loss)
I0109 20:22:55.406756  4296 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 20:22:58.671983  4296 solver.cpp:266] Iteration 9300 (30.6254 iter/s, 3.26527s/100 iter), loss = 0.151969
I0109 20:22:58.672047  4296 solver.cpp:285]     Train net output #0: loss = 0.151969 (* 1 = 0.151969 loss)
I0109 20:22:58.672062  4296 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 20:23:01.939036  4296 solver.cpp:266] Iteration 9400 (30.6092 iter/s, 3.26699s/100 iter), loss = 0.239423
I0109 20:23:01.939103  4296 solver.cpp:285]     Train net output #0: loss = 0.239422 (* 1 = 0.239422 loss)
I0109 20:23:01.939116  4296 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 20:23:05.201215  4296 solver.cpp:266] Iteration 9500 (30.6547 iter/s, 3.26214s/100 iter), loss = 0.259518
I0109 20:23:05.201279  4296 solver.cpp:285]     Train net output #0: loss = 0.259518 (* 1 = 0.259518 loss)
I0109 20:23:05.201292  4296 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 20:23:08.466295  4296 solver.cpp:266] Iteration 9600 (30.6274 iter/s, 3.26505s/100 iter), loss = 0.154196
I0109 20:23:08.466359  4296 solver.cpp:285]     Train net output #0: loss = 0.154196 (* 1 = 0.154196 loss)
I0109 20:23:08.466372  4296 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 20:23:11.731631  4296 solver.cpp:266] Iteration 9700 (30.625 iter/s, 3.2653s/100 iter), loss = 0.191993
I0109 20:23:11.731693  4296 solver.cpp:285]     Train net output #0: loss = 0.191993 (* 1 = 0.191993 loss)
I0109 20:23:11.731706  4296 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 20:23:14.994443  4296 solver.cpp:266] Iteration 9800 (30.649 iter/s, 3.26275s/100 iter), loss = 0.203833
I0109 20:23:14.994505  4296 solver.cpp:285]     Train net output #0: loss = 0.203832 (* 1 = 0.203832 loss)
I0109 20:23:14.994518  4296 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 20:23:18.262820  4296 solver.cpp:266] Iteration 9900 (30.5965 iter/s, 3.26834s/100 iter), loss = 0.141854
I0109 20:23:18.262881  4296 solver.cpp:285]     Train net output #0: loss = 0.141854 (* 1 = 0.141854 loss)
I0109 20:23:18.262892  4296 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 20:23:21.489943  4296 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 20:23:22.306820  4296 solver.cpp:517]     Test net output #0: loss = 0.501439 (* 1 = 0.501439 loss)
I0109 20:23:22.306859  4296 solver.cpp:517]     Test net output #1: top-1 = 0.839333
I0109 20:23:22.306867  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 20:23:22.337671  4296 solver.cpp:266] Iteration 10000 (24.5409 iter/s, 4.07483s/100 iter), loss = 0.315849
I0109 20:23:22.337714  4296 solver.cpp:285]     Train net output #0: loss = 0.315849 (* 1 = 0.315849 loss)
I0109 20:23:22.337729  4296 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 20:23:25.601382  4296 solver.cpp:266] Iteration 10100 (30.6401 iter/s, 3.2637s/100 iter), loss = 0.154817
I0109 20:23:25.601445  4296 solver.cpp:285]     Train net output #0: loss = 0.154816 (* 1 = 0.154816 loss)
I0109 20:23:25.601459  4296 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 20:23:28.869035  4296 solver.cpp:266] Iteration 10200 (30.6033 iter/s, 3.26762s/100 iter), loss = 0.187269
I0109 20:23:28.869098  4296 solver.cpp:285]     Train net output #0: loss = 0.187269 (* 1 = 0.187269 loss)
I0109 20:23:28.869110  4296 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 20:23:32.135711  4296 solver.cpp:266] Iteration 10300 (30.6127 iter/s, 3.26662s/100 iter), loss = 0.191845
I0109 20:23:32.135773  4296 solver.cpp:285]     Train net output #0: loss = 0.191845 (* 1 = 0.191845 loss)
I0109 20:23:32.135785  4296 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 20:23:35.402362  4296 solver.cpp:266] Iteration 10400 (30.6127 iter/s, 3.26662s/100 iter), loss = 0.317364
I0109 20:23:35.402429  4296 solver.cpp:285]     Train net output #0: loss = 0.317364 (* 1 = 0.317364 loss)
I0109 20:23:35.402441  4296 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 20:23:38.665271  4296 solver.cpp:266] Iteration 10500 (30.6478 iter/s, 3.26287s/100 iter), loss = 0.322325
I0109 20:23:38.665343  4296 solver.cpp:285]     Train net output #0: loss = 0.322325 (* 1 = 0.322325 loss)
I0109 20:23:38.665361  4296 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 20:23:41.932783  4296 solver.cpp:266] Iteration 10600 (30.6049 iter/s, 3.26745s/100 iter), loss = 0.247344
I0109 20:23:41.932850  4296 solver.cpp:285]     Train net output #0: loss = 0.247344 (* 1 = 0.247344 loss)
I0109 20:23:41.932863  4296 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 20:23:45.197710  4296 solver.cpp:266] Iteration 10700 (30.6289 iter/s, 3.26489s/100 iter), loss = 0.177689
I0109 20:23:45.197772  4296 solver.cpp:285]     Train net output #0: loss = 0.177689 (* 1 = 0.177689 loss)
I0109 20:23:45.197783  4296 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 20:23:48.467703  4296 solver.cpp:266] Iteration 10800 (30.5814 iter/s, 3.26996s/100 iter), loss = 0.241236
I0109 20:23:48.467793  4296 solver.cpp:285]     Train net output #0: loss = 0.241235 (* 1 = 0.241235 loss)
I0109 20:23:48.467813  4296 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 20:23:51.733330  4296 solver.cpp:266] Iteration 10900 (30.6225 iter/s, 3.26557s/100 iter), loss = 0.175677
I0109 20:23:51.733429  4296 solver.cpp:285]     Train net output #0: loss = 0.175677 (* 1 = 0.175677 loss)
I0109 20:23:51.733446  4296 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 20:23:54.967746  4296 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 20:23:55.783468  4296 solver.cpp:517]     Test net output #0: loss = 0.467859 (* 1 = 0.467859 loss)
I0109 20:23:55.783507  4296 solver.cpp:517]     Test net output #1: top-1 = 0.844666
I0109 20:23:55.783515  4296 solver.cpp:517]     Test net output #2: top-5 = 0.994
I0109 20:23:55.814559  4296 solver.cpp:266] Iteration 11000 (24.5027 iter/s, 4.08118s/100 iter), loss = 0.242156
I0109 20:23:55.814596  4296 solver.cpp:285]     Train net output #0: loss = 0.242156 (* 1 = 0.242156 loss)
I0109 20:23:55.814612  4296 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 20:23:59.083472  4296 solver.cpp:266] Iteration 11100 (30.5915 iter/s, 3.26888s/100 iter), loss = 0.170255
I0109 20:23:59.083534  4296 solver.cpp:285]     Train net output #0: loss = 0.170255 (* 1 = 0.170255 loss)
I0109 20:23:59.083546  4296 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 20:24:02.355427  4296 solver.cpp:266] Iteration 11200 (30.5631 iter/s, 3.27192s/100 iter), loss = 0.203468
I0109 20:24:02.355490  4296 solver.cpp:285]     Train net output #0: loss = 0.203468 (* 1 = 0.203468 loss)
I0109 20:24:02.355502  4296 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 20:24:05.617655  4296 solver.cpp:266] Iteration 11300 (30.6542 iter/s, 3.26219s/100 iter), loss = 0.160408
I0109 20:24:05.617717  4296 solver.cpp:285]     Train net output #0: loss = 0.160408 (* 1 = 0.160408 loss)
I0109 20:24:05.617730  4296 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 20:24:08.892983  4296 solver.cpp:266] Iteration 11400 (30.5316 iter/s, 3.2753s/100 iter), loss = 0.278575
I0109 20:24:08.893051  4296 solver.cpp:285]     Train net output #0: loss = 0.278575 (* 1 = 0.278575 loss)
I0109 20:24:08.893065  4296 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 20:24:12.161675  4296 solver.cpp:266] Iteration 11500 (30.5939 iter/s, 3.26863s/100 iter), loss = 0.255558
I0109 20:24:12.161737  4296 solver.cpp:285]     Train net output #0: loss = 0.255558 (* 1 = 0.255558 loss)
I0109 20:24:12.161749  4296 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 20:24:15.421828  4296 solver.cpp:266] Iteration 11600 (30.6737 iter/s, 3.26012s/100 iter), loss = 0.227112
I0109 20:24:15.421914  4296 solver.cpp:285]     Train net output #0: loss = 0.227112 (* 1 = 0.227112 loss)
I0109 20:24:15.421931  4296 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 20:24:18.690299  4296 solver.cpp:266] Iteration 11700 (30.5959 iter/s, 3.26842s/100 iter), loss = 0.200861
I0109 20:24:18.690363  4296 solver.cpp:285]     Train net output #0: loss = 0.200861 (* 1 = 0.200861 loss)
I0109 20:24:18.690376  4296 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 20:24:21.961016  4296 solver.cpp:266] Iteration 11800 (30.5749 iter/s, 3.27065s/100 iter), loss = 0.178699
I0109 20:24:21.961232  4296 solver.cpp:285]     Train net output #0: loss = 0.178699 (* 1 = 0.178699 loss)
I0109 20:24:21.961251  4296 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 20:24:25.233224  4296 solver.cpp:266] Iteration 11900 (30.5621 iter/s, 3.27203s/100 iter), loss = 0.175853
I0109 20:24:25.233289  4296 solver.cpp:285]     Train net output #0: loss = 0.175853 (* 1 = 0.175853 loss)
I0109 20:24:25.233301  4296 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 20:24:28.469260  4296 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 20:24:29.283870  4296 solver.cpp:517]     Test net output #0: loss = 0.468898 (* 1 = 0.468898 loss)
I0109 20:24:29.283912  4296 solver.cpp:517]     Test net output #1: top-1 = 0.851889
I0109 20:24:29.283921  4296 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 20:24:29.314858  4296 solver.cpp:266] Iteration 12000 (24.5002 iter/s, 4.08161s/100 iter), loss = 0.218758
I0109 20:24:29.314921  4296 solver.cpp:285]     Train net output #0: loss = 0.218758 (* 1 = 0.218758 loss)
I0109 20:24:29.314937  4296 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 20:24:32.580971  4296 solver.cpp:266] Iteration 12100 (30.6177 iter/s, 3.26608s/100 iter), loss = 0.32409
I0109 20:24:32.581032  4296 solver.cpp:285]     Train net output #0: loss = 0.32409 (* 1 = 0.32409 loss)
I0109 20:24:32.581045  4296 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 20:24:35.847388  4296 solver.cpp:266] Iteration 12200 (30.6149 iter/s, 3.26639s/100 iter), loss = 0.198702
I0109 20:24:35.847450  4296 solver.cpp:285]     Train net output #0: loss = 0.198702 (* 1 = 0.198702 loss)
I0109 20:24:35.847461  4296 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 20:24:39.115486  4296 solver.cpp:266] Iteration 12300 (30.5994 iter/s, 3.26804s/100 iter), loss = 0.268706
I0109 20:24:39.115548  4296 solver.cpp:285]     Train net output #0: loss = 0.268706 (* 1 = 0.268706 loss)
I0109 20:24:39.115561  4296 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 20:24:42.379945  4296 solver.cpp:266] Iteration 12400 (30.6333 iter/s, 3.26443s/100 iter), loss = 0.157915
I0109 20:24:42.380010  4296 solver.cpp:285]     Train net output #0: loss = 0.157915 (* 1 = 0.157915 loss)
I0109 20:24:42.380023  4296 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 20:24:45.645025  4296 solver.cpp:266] Iteration 12500 (30.6274 iter/s, 3.26505s/100 iter), loss = 0.1739
I0109 20:24:45.645087  4296 solver.cpp:285]     Train net output #0: loss = 0.1739 (* 1 = 0.1739 loss)
I0109 20:24:45.645102  4296 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 20:24:48.906661  4296 solver.cpp:266] Iteration 12600 (30.6598 iter/s, 3.2616s/100 iter), loss = 0.162908
I0109 20:24:48.906724  4296 solver.cpp:285]     Train net output #0: loss = 0.162908 (* 1 = 0.162908 loss)
I0109 20:24:48.906738  4296 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 20:24:52.171145  4296 solver.cpp:266] Iteration 12700 (30.6333 iter/s, 3.26442s/100 iter), loss = 0.23948
I0109 20:24:52.171341  4296 solver.cpp:285]     Train net output #0: loss = 0.23948 (* 1 = 0.23948 loss)
I0109 20:24:52.171356  4296 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 20:24:55.440042  4296 solver.cpp:266] Iteration 12800 (30.5929 iter/s, 3.26873s/100 iter), loss = 0.147037
I0109 20:24:55.440126  4296 solver.cpp:285]     Train net output #0: loss = 0.147037 (* 1 = 0.147037 loss)
I0109 20:24:55.440142  4296 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 20:24:58.710484  4296 solver.cpp:266] Iteration 12900 (30.5774 iter/s, 3.27039s/100 iter), loss = 0.263487
I0109 20:24:58.710554  4296 solver.cpp:285]     Train net output #0: loss = 0.263487 (* 1 = 0.263487 loss)
I0109 20:24:58.710567  4296 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 20:25:01.944564  4296 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 20:25:02.759119  4296 solver.cpp:517]     Test net output #0: loss = 0.486645 (* 1 = 0.486645 loss)
I0109 20:25:02.759160  4296 solver.cpp:517]     Test net output #1: top-1 = 0.846445
I0109 20:25:02.759171  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0109 20:25:02.790187  4296 solver.cpp:266] Iteration 13000 (24.5118 iter/s, 4.07967s/100 iter), loss = 0.18573
I0109 20:25:02.790237  4296 solver.cpp:285]     Train net output #0: loss = 0.18573 (* 1 = 0.18573 loss)
I0109 20:25:02.790254  4296 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 20:25:06.051517  4296 solver.cpp:266] Iteration 13100 (30.6628 iter/s, 3.26128s/100 iter), loss = 0.211304
I0109 20:25:06.051581  4296 solver.cpp:285]     Train net output #0: loss = 0.211304 (* 1 = 0.211304 loss)
I0109 20:25:06.051597  4296 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 20:25:09.317435  4296 solver.cpp:266] Iteration 13200 (30.6196 iter/s, 3.26589s/100 iter), loss = 0.302868
I0109 20:25:09.317502  4296 solver.cpp:285]     Train net output #0: loss = 0.302868 (* 1 = 0.302868 loss)
I0109 20:25:09.317513  4296 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 20:25:12.586005  4296 solver.cpp:266] Iteration 13300 (30.5947 iter/s, 3.26854s/100 iter), loss = 0.181416
I0109 20:25:12.586066  4296 solver.cpp:285]     Train net output #0: loss = 0.181416 (* 1 = 0.181416 loss)
I0109 20:25:12.586079  4296 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 20:25:15.849335  4296 solver.cpp:266] Iteration 13400 (30.6438 iter/s, 3.2633s/100 iter), loss = 0.290026
I0109 20:25:15.849398  4296 solver.cpp:285]     Train net output #0: loss = 0.290026 (* 1 = 0.290026 loss)
I0109 20:25:15.849409  4296 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 20:25:19.117038  4296 solver.cpp:266] Iteration 13500 (30.6031 iter/s, 3.26765s/100 iter), loss = 0.156734
I0109 20:25:19.117100  4296 solver.cpp:285]     Train net output #0: loss = 0.156734 (* 1 = 0.156734 loss)
I0109 20:25:19.117112  4296 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 20:25:22.381567  4296 solver.cpp:266] Iteration 13600 (30.6326 iter/s, 3.26449s/100 iter), loss = 0.174708
I0109 20:25:22.381726  4296 solver.cpp:285]     Train net output #0: loss = 0.174708 (* 1 = 0.174708 loss)
I0109 20:25:22.381741  4296 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 20:25:25.646456  4296 solver.cpp:266] Iteration 13700 (30.6301 iter/s, 3.26476s/100 iter), loss = 0.176358
I0109 20:25:25.646519  4296 solver.cpp:285]     Train net output #0: loss = 0.176358 (* 1 = 0.176358 loss)
I0109 20:25:25.646533  4296 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 20:25:28.912976  4296 solver.cpp:266] Iteration 13800 (30.6139 iter/s, 3.26649s/100 iter), loss = 0.172338
I0109 20:25:28.913041  4296 solver.cpp:285]     Train net output #0: loss = 0.172338 (* 1 = 0.172338 loss)
I0109 20:25:28.913053  4296 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 20:25:32.179553  4296 solver.cpp:266] Iteration 13900 (30.6137 iter/s, 3.26652s/100 iter), loss = 0.190655
I0109 20:25:32.179615  4296 solver.cpp:285]     Train net output #0: loss = 0.190655 (* 1 = 0.190655 loss)
I0109 20:25:32.179626  4296 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 20:25:35.414573  4296 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 20:25:36.236274  4296 solver.cpp:517]     Test net output #0: loss = 0.520429 (* 1 = 0.520429 loss)
I0109 20:25:36.236313  4296 solver.cpp:517]     Test net output #1: top-1 = 0.832889
I0109 20:25:36.236325  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0109 20:25:36.267172  4296 solver.cpp:266] Iteration 14000 (24.4642 iter/s, 4.0876s/100 iter), loss = 0.24502
I0109 20:25:36.267220  4296 solver.cpp:285]     Train net output #0: loss = 0.24502 (* 1 = 0.24502 loss)
I0109 20:25:36.267241  4296 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 20:25:39.536423  4296 solver.cpp:266] Iteration 14100 (30.5882 iter/s, 3.26924s/100 iter), loss = 0.209135
I0109 20:25:39.536494  4296 solver.cpp:285]     Train net output #0: loss = 0.209135 (* 1 = 0.209135 loss)
I0109 20:25:39.536512  4296 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 20:25:42.800772  4296 solver.cpp:266] Iteration 14200 (30.6343 iter/s, 3.26431s/100 iter), loss = 0.217574
I0109 20:25:42.800835  4296 solver.cpp:285]     Train net output #0: loss = 0.217574 (* 1 = 0.217574 loss)
I0109 20:25:42.800848  4296 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 20:25:46.062503  4296 solver.cpp:266] Iteration 14300 (30.6591 iter/s, 3.26167s/100 iter), loss = 0.339036
I0109 20:25:46.062564  4296 solver.cpp:285]     Train net output #0: loss = 0.339036 (* 1 = 0.339036 loss)
I0109 20:25:46.062577  4296 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 20:25:49.320838  4296 solver.cpp:266] Iteration 14400 (30.6908 iter/s, 3.2583s/100 iter), loss = 0.113973
I0109 20:25:49.320900  4296 solver.cpp:285]     Train net output #0: loss = 0.113972 (* 1 = 0.113972 loss)
I0109 20:25:49.320912  4296 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 20:25:52.589488  4296 solver.cpp:266] Iteration 14500 (30.594 iter/s, 3.26862s/100 iter), loss = 0.121145
I0109 20:25:52.589622  4296 solver.cpp:285]     Train net output #0: loss = 0.121145 (* 1 = 0.121145 loss)
I0109 20:25:52.589637  4296 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 20:25:55.859812  4296 solver.cpp:266] Iteration 14600 (30.5789 iter/s, 3.27022s/100 iter), loss = 0.186773
I0109 20:25:55.859874  4296 solver.cpp:285]     Train net output #0: loss = 0.186773 (* 1 = 0.186773 loss)
I0109 20:25:55.859886  4296 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 20:25:59.124764  4296 solver.cpp:266] Iteration 14700 (30.6289 iter/s, 3.26489s/100 iter), loss = 0.187436
I0109 20:25:59.124830  4296 solver.cpp:285]     Train net output #0: loss = 0.187436 (* 1 = 0.187436 loss)
I0109 20:25:59.124842  4296 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 20:26:02.393836  4296 solver.cpp:266] Iteration 14800 (30.5901 iter/s, 3.26904s/100 iter), loss = 0.263755
I0109 20:26:02.393900  4296 solver.cpp:285]     Train net output #0: loss = 0.263755 (* 1 = 0.263755 loss)
I0109 20:26:02.393914  4296 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 20:26:05.661679  4296 solver.cpp:266] Iteration 14900 (30.6015 iter/s, 3.26781s/100 iter), loss = 0.257087
I0109 20:26:05.661743  4296 solver.cpp:285]     Train net output #0: loss = 0.257087 (* 1 = 0.257087 loss)
I0109 20:26:05.661757  4296 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 20:26:08.900426  4296 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 20:26:09.721922  4296 solver.cpp:517]     Test net output #0: loss = 0.513878 (* 1 = 0.513878 loss)
I0109 20:26:09.721961  4296 solver.cpp:517]     Test net output #1: top-1 = 0.840667
I0109 20:26:09.721969  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 20:26:09.752820  4296 solver.cpp:266] Iteration 15000 (24.4432 iter/s, 4.09112s/100 iter), loss = 0.179903
I0109 20:26:09.752858  4296 solver.cpp:285]     Train net output #0: loss = 0.179903 (* 1 = 0.179903 loss)
I0109 20:26:09.752874  4296 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 20:26:13.020165  4296 solver.cpp:266] Iteration 15100 (30.6062 iter/s, 3.26731s/100 iter), loss = 0.137962
I0109 20:26:13.020228  4296 solver.cpp:285]     Train net output #0: loss = 0.137962 (* 1 = 0.137962 loss)
I0109 20:26:13.020241  4296 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 20:26:16.287066  4296 solver.cpp:266] Iteration 15200 (30.6103 iter/s, 3.26687s/100 iter), loss = 0.27472
I0109 20:26:16.287129  4296 solver.cpp:285]     Train net output #0: loss = 0.274719 (* 1 = 0.274719 loss)
I0109 20:26:16.287142  4296 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 20:26:19.554914  4296 solver.cpp:266] Iteration 15300 (30.6015 iter/s, 3.26781s/100 iter), loss = 0.361731
I0109 20:26:19.554978  4296 solver.cpp:285]     Train net output #0: loss = 0.361731 (* 1 = 0.361731 loss)
I0109 20:26:19.554991  4296 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 20:26:22.823910  4296 solver.cpp:266] Iteration 15400 (30.5908 iter/s, 3.26896s/100 iter), loss = 0.22973
I0109 20:26:22.824107  4296 solver.cpp:285]     Train net output #0: loss = 0.229729 (* 1 = 0.229729 loss)
I0109 20:26:22.824127  4296 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 20:26:26.087020  4296 solver.cpp:266] Iteration 15500 (30.6474 iter/s, 3.26292s/100 iter), loss = 0.203991
I0109 20:26:26.087105  4296 solver.cpp:285]     Train net output #0: loss = 0.203991 (* 1 = 0.203991 loss)
I0109 20:26:26.087121  4296 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 20:26:29.346468  4296 solver.cpp:266] Iteration 15600 (30.6805 iter/s, 3.25939s/100 iter), loss = 0.14315
I0109 20:26:29.346542  4296 solver.cpp:285]     Train net output #0: loss = 0.14315 (* 1 = 0.14315 loss)
I0109 20:26:29.346557  4296 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 20:26:32.597717  4296 solver.cpp:266] Iteration 15700 (30.7578 iter/s, 3.2512s/100 iter), loss = 0.222065
I0109 20:26:32.597779  4296 solver.cpp:285]     Train net output #0: loss = 0.222065 (* 1 = 0.222065 loss)
I0109 20:26:32.597792  4296 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 20:26:35.850272  4296 solver.cpp:266] Iteration 15800 (30.7454 iter/s, 3.25252s/100 iter), loss = 0.14843
I0109 20:26:35.850358  4296 solver.cpp:285]     Train net output #0: loss = 0.14843 (* 1 = 0.14843 loss)
I0109 20:26:35.850373  4296 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 20:26:39.097200  4296 solver.cpp:266] Iteration 15900 (30.7991 iter/s, 3.24685s/100 iter), loss = 0.212439
I0109 20:26:39.097266  4296 solver.cpp:285]     Train net output #0: loss = 0.212439 (* 1 = 0.212439 loss)
I0109 20:26:39.097278  4296 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 20:26:42.309995  4296 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 20:26:43.118690  4296 solver.cpp:517]     Test net output #0: loss = 0.451226 (* 1 = 0.451226 loss)
I0109 20:26:43.118731  4296 solver.cpp:517]     Test net output #1: top-1 = 0.854333
I0109 20:26:43.118738  4296 solver.cpp:517]     Test net output #2: top-5 = 0.993444
I0109 20:26:43.149179  4296 solver.cpp:266] Iteration 16000 (24.6794 iter/s, 4.05195s/100 iter), loss = 0.293193
I0109 20:26:43.149220  4296 solver.cpp:285]     Train net output #0: loss = 0.293193 (* 1 = 0.293193 loss)
I0109 20:26:43.149235  4296 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 20:26:46.398080  4296 solver.cpp:266] Iteration 16100 (30.7798 iter/s, 3.24889s/100 iter), loss = 0.27962
I0109 20:26:46.398144  4296 solver.cpp:285]     Train net output #0: loss = 0.27962 (* 1 = 0.27962 loss)
I0109 20:26:46.398157  4296 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 20:26:49.644366  4296 solver.cpp:266] Iteration 16200 (30.8048 iter/s, 3.24625s/100 iter), loss = 0.16344
I0109 20:26:49.644443  4296 solver.cpp:285]     Train net output #0: loss = 0.16344 (* 1 = 0.16344 loss)
I0109 20:26:49.644465  4296 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 20:26:52.893630  4296 solver.cpp:266] Iteration 16300 (30.7767 iter/s, 3.24922s/100 iter), loss = 0.191334
I0109 20:26:52.893851  4296 solver.cpp:285]     Train net output #0: loss = 0.191334 (* 1 = 0.191334 loss)
I0109 20:26:52.893868  4296 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 20:26:56.141970  4296 solver.cpp:266] Iteration 16400 (30.787 iter/s, 3.24812s/100 iter), loss = 0.200156
I0109 20:26:56.142045  4296 solver.cpp:285]     Train net output #0: loss = 0.200156 (* 1 = 0.200156 loss)
I0109 20:26:56.142060  4296 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 20:26:59.388622  4296 solver.cpp:266] Iteration 16500 (30.8014 iter/s, 3.24661s/100 iter), loss = 0.147461
I0109 20:26:59.388684  4296 solver.cpp:285]     Train net output #0: loss = 0.147461 (* 1 = 0.147461 loss)
I0109 20:26:59.388698  4296 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 20:27:02.638154  4296 solver.cpp:266] Iteration 16600 (30.774 iter/s, 3.2495s/100 iter), loss = 0.160566
I0109 20:27:02.638227  4296 solver.cpp:285]     Train net output #0: loss = 0.160566 (* 1 = 0.160566 loss)
I0109 20:27:02.638242  4296 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 20:27:05.887840  4296 solver.cpp:266] Iteration 16700 (30.7726 iter/s, 3.24964s/100 iter), loss = 0.165089
I0109 20:27:05.887905  4296 solver.cpp:285]     Train net output #0: loss = 0.165088 (* 1 = 0.165088 loss)
I0109 20:27:05.887919  4296 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 20:27:09.140166  4296 solver.cpp:266] Iteration 16800 (30.7478 iter/s, 3.25226s/100 iter), loss = 0.188234
I0109 20:27:09.140239  4296 solver.cpp:285]     Train net output #0: loss = 0.188234 (* 1 = 0.188234 loss)
I0109 20:27:09.140251  4296 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 20:27:12.391783  4296 solver.cpp:266] Iteration 16900 (30.7543 iter/s, 3.25157s/100 iter), loss = 0.171462
I0109 20:27:12.391847  4296 solver.cpp:285]     Train net output #0: loss = 0.171462 (* 1 = 0.171462 loss)
I0109 20:27:12.391860  4296 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 20:27:15.608549  4296 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 20:27:16.422677  4296 solver.cpp:517]     Test net output #0: loss = 0.460024 (* 1 = 0.460024 loss)
I0109 20:27:16.422713  4296 solver.cpp:517]     Test net output #1: top-1 = 0.849889
I0109 20:27:16.422721  4296 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0109 20:27:16.453444  4296 solver.cpp:266] Iteration 17000 (24.6206 iter/s, 4.06164s/100 iter), loss = 0.190192
I0109 20:27:16.453480  4296 solver.cpp:285]     Train net output #0: loss = 0.190192 (* 1 = 0.190192 loss)
I0109 20:27:16.453495  4296 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 20:27:19.702698  4296 solver.cpp:266] Iteration 17100 (30.7764 iter/s, 3.24925s/100 iter), loss = 0.142398
I0109 20:27:19.702759  4296 solver.cpp:285]     Train net output #0: loss = 0.142398 (* 1 = 0.142398 loss)
I0109 20:27:19.702771  4296 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 20:27:22.955160  4296 solver.cpp:266] Iteration 17200 (30.7465 iter/s, 3.2524s/100 iter), loss = 0.192806
I0109 20:27:22.955299  4296 solver.cpp:285]     Train net output #0: loss = 0.192806 (* 1 = 0.192806 loss)
I0109 20:27:22.955313  4296 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 20:27:26.220199  4296 solver.cpp:266] Iteration 17300 (30.6285 iter/s, 3.26493s/100 iter), loss = 0.203988
I0109 20:27:26.220265  4296 solver.cpp:285]     Train net output #0: loss = 0.203988 (* 1 = 0.203988 loss)
I0109 20:27:26.220278  4296 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 20:27:29.486861  4296 solver.cpp:266] Iteration 17400 (30.6126 iter/s, 3.26663s/100 iter), loss = 0.195227
I0109 20:27:29.486919  4296 solver.cpp:285]     Train net output #0: loss = 0.195227 (* 1 = 0.195227 loss)
I0109 20:27:29.486932  4296 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 20:27:32.752339  4296 solver.cpp:266] Iteration 17500 (30.6237 iter/s, 3.26545s/100 iter), loss = 0.140286
I0109 20:27:32.752403  4296 solver.cpp:285]     Train net output #0: loss = 0.140286 (* 1 = 0.140286 loss)
I0109 20:27:32.752415  4296 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 20:27:36.018934  4296 solver.cpp:266] Iteration 17600 (30.6135 iter/s, 3.26653s/100 iter), loss = 0.159278
I0109 20:27:36.018996  4296 solver.cpp:285]     Train net output #0: loss = 0.159278 (* 1 = 0.159278 loss)
I0109 20:27:36.019009  4296 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 20:27:39.283299  4296 solver.cpp:266] Iteration 17700 (30.6341 iter/s, 3.26433s/100 iter), loss = 0.166418
I0109 20:27:39.283360  4296 solver.cpp:285]     Train net output #0: loss = 0.166418 (* 1 = 0.166418 loss)
I0109 20:27:39.283375  4296 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 20:27:42.543772  4296 solver.cpp:266] Iteration 17800 (30.6707 iter/s, 3.26044s/100 iter), loss = 0.111989
I0109 20:27:42.543834  4296 solver.cpp:285]     Train net output #0: loss = 0.111989 (* 1 = 0.111989 loss)
I0109 20:27:42.543848  4296 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 20:27:45.809849  4296 solver.cpp:266] Iteration 17900 (30.6181 iter/s, 3.26605s/100 iter), loss = 0.223452
I0109 20:27:45.809913  4296 solver.cpp:285]     Train net output #0: loss = 0.223452 (* 1 = 0.223452 loss)
I0109 20:27:45.809926  4296 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 20:27:49.045370  4296 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 20:27:49.860626  4296 solver.cpp:517]     Test net output #0: loss = 0.460389 (* 1 = 0.460389 loss)
I0109 20:27:49.860662  4296 solver.cpp:517]     Test net output #1: top-1 = 0.850222
I0109 20:27:49.860669  4296 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 20:27:49.891526  4296 solver.cpp:266] Iteration 18000 (24.4999 iter/s, 4.08166s/100 iter), loss = 0.169387
I0109 20:27:49.891562  4296 solver.cpp:285]     Train net output #0: loss = 0.169387 (* 1 = 0.169387 loss)
I0109 20:27:49.891577  4296 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 20:27:53.156469  4296 solver.cpp:266] Iteration 18100 (30.6287 iter/s, 3.26491s/100 iter), loss = 0.253428
I0109 20:27:53.156641  4296 solver.cpp:285]     Train net output #0: loss = 0.253428 (* 1 = 0.253428 loss)
I0109 20:27:53.156656  4296 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 20:27:56.422228  4296 solver.cpp:266] Iteration 18200 (30.622 iter/s, 3.26562s/100 iter), loss = 0.128059
I0109 20:27:56.422288  4296 solver.cpp:285]     Train net output #0: loss = 0.128059 (* 1 = 0.128059 loss)
I0109 20:27:56.422302  4296 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 20:27:59.688923  4296 solver.cpp:266] Iteration 18300 (30.6123 iter/s, 3.26667s/100 iter), loss = 0.20294
I0109 20:27:59.688985  4296 solver.cpp:285]     Train net output #0: loss = 0.20294 (* 1 = 0.20294 loss)
I0109 20:27:59.688998  4296 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 20:28:02.953054  4296 solver.cpp:266] Iteration 18400 (30.6366 iter/s, 3.26407s/100 iter), loss = 0.191537
I0109 20:28:02.953117  4296 solver.cpp:285]     Train net output #0: loss = 0.191537 (* 1 = 0.191537 loss)
I0109 20:28:02.953130  4296 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 20:28:06.219770  4296 solver.cpp:266] Iteration 18500 (30.6121 iter/s, 3.26668s/100 iter), loss = 0.246389
I0109 20:28:06.219832  4296 solver.cpp:285]     Train net output #0: loss = 0.246389 (* 1 = 0.246389 loss)
I0109 20:28:06.219846  4296 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 20:28:09.480073  4296 solver.cpp:266] Iteration 18600 (30.6723 iter/s, 3.26027s/100 iter), loss = 0.156176
I0109 20:28:09.480139  4296 solver.cpp:285]     Train net output #0: loss = 0.156176 (* 1 = 0.156176 loss)
I0109 20:28:09.480151  4296 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 20:28:12.748263  4296 solver.cpp:266] Iteration 18700 (30.5983 iter/s, 3.26816s/100 iter), loss = 0.158833
I0109 20:28:12.748324  4296 solver.cpp:285]     Train net output #0: loss = 0.158833 (* 1 = 0.158833 loss)
I0109 20:28:12.748337  4296 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 20:28:16.014835  4296 solver.cpp:266] Iteration 18800 (30.6137 iter/s, 3.26652s/100 iter), loss = 0.168459
I0109 20:28:16.014895  4296 solver.cpp:285]     Train net output #0: loss = 0.168459 (* 1 = 0.168459 loss)
I0109 20:28:16.014909  4296 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 20:28:19.275739  4296 solver.cpp:266] Iteration 18900 (30.6666 iter/s, 3.26087s/100 iter), loss = 0.138303
I0109 20:28:19.275800  4296 solver.cpp:285]     Train net output #0: loss = 0.138303 (* 1 = 0.138303 loss)
I0109 20:28:19.275813  4296 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 20:28:22.506887  4296 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 20:28:23.323087  4296 solver.cpp:517]     Test net output #0: loss = 0.447963 (* 1 = 0.447963 loss)
I0109 20:28:23.323237  4296 solver.cpp:517]     Test net output #1: top-1 = 0.855556
I0109 20:28:23.323248  4296 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0109 20:28:23.354192  4296 solver.cpp:266] Iteration 19000 (24.5192 iter/s, 4.07843s/100 iter), loss = 0.150045
I0109 20:28:23.354226  4296 solver.cpp:285]     Train net output #0: loss = 0.150045 (* 1 = 0.150045 loss)
I0109 20:28:23.354241  4296 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 20:28:26.620126  4296 solver.cpp:266] Iteration 19100 (30.6192 iter/s, 3.26593s/100 iter), loss = 0.198326
I0109 20:28:26.620187  4296 solver.cpp:285]     Train net output #0: loss = 0.198326 (* 1 = 0.198326 loss)
I0109 20:28:26.620199  4296 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 20:28:29.882908  4296 solver.cpp:266] Iteration 19200 (30.649 iter/s, 3.26275s/100 iter), loss = 0.213689
I0109 20:28:29.882971  4296 solver.cpp:285]     Train net output #0: loss = 0.213689 (* 1 = 0.213689 loss)
I0109 20:28:29.882982  4296 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 20:28:33.149857  4296 solver.cpp:266] Iteration 19300 (30.6102 iter/s, 3.26689s/100 iter), loss = 0.182435
I0109 20:28:33.149921  4296 solver.cpp:285]     Train net output #0: loss = 0.182435 (* 1 = 0.182435 loss)
I0109 20:28:33.149935  4296 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 20:28:36.415887  4296 solver.cpp:266] Iteration 19400 (30.6185 iter/s, 3.266s/100 iter), loss = 0.168927
I0109 20:28:36.415947  4296 solver.cpp:285]     Train net output #0: loss = 0.168927 (* 1 = 0.168927 loss)
I0109 20:28:36.415959  4296 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 20:28:39.675032  4296 solver.cpp:266] Iteration 19500 (30.6832 iter/s, 3.25911s/100 iter), loss = 0.125329
I0109 20:28:39.675091  4296 solver.cpp:285]     Train net output #0: loss = 0.125329 (* 1 = 0.125329 loss)
I0109 20:28:39.675103  4296 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 20:28:42.941501  4296 solver.cpp:266] Iteration 19600 (30.6146 iter/s, 3.26642s/100 iter), loss = 0.147848
I0109 20:28:42.941565  4296 solver.cpp:285]     Train net output #0: loss = 0.147848 (* 1 = 0.147848 loss)
I0109 20:28:42.941578  4296 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 20:28:46.204767  4296 solver.cpp:266] Iteration 19700 (30.6445 iter/s, 3.26323s/100 iter), loss = 0.194979
I0109 20:28:46.204829  4296 solver.cpp:285]     Train net output #0: loss = 0.194979 (* 1 = 0.194979 loss)
I0109 20:28:46.204843  4296 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 20:28:49.470204  4296 solver.cpp:266] Iteration 19800 (30.6241 iter/s, 3.2654s/100 iter), loss = 0.191852
I0109 20:28:49.470270  4296 solver.cpp:285]     Train net output #0: loss = 0.191852 (* 1 = 0.191852 loss)
I0109 20:28:49.470283  4296 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 20:28:52.734235  4296 solver.cpp:266] Iteration 19900 (30.6373 iter/s, 3.26399s/100 iter), loss = 0.187077
I0109 20:28:52.734297  4296 solver.cpp:285]     Train net output #0: loss = 0.187077 (* 1 = 0.187077 loss)
I0109 20:28:52.734309  4296 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 20:28:55.969970  4296 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/_iter_20000.caffemodel
I0109 20:28:56.072041  4296 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/_iter_20000.solverstate
I0109 20:28:56.098177  4296 solver.cpp:378] Iteration 20000, loss = 0.0290947
I0109 20:28:56.098217  4296 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 20:28:56.903234  4296 solver.cpp:517]     Test net output #0: loss = 0.450035 (* 1 = 0.450035 loss)
I0109 20:28:56.903275  4296 solver.cpp:517]     Test net output #1: top-1 = 0.856778
I0109 20:28:56.903281  4296 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 20:28:56.903287  4296 solver.cpp:386] Optimization Done (30.0344 iter/s).
I0109 20:28:56.903295  4296 caffe_interface.cpp:530] Optimization Done.
I0109 20:28:57.065064  4323 pruning_runner.cpp:190] Sens info found, use it.
I0109 20:28:57.111299  4323 pruning_runner.cpp:217] Start compressing, please wait...
I0109 20:28:58.166371  4323 pruning_runner.cpp:264] Compression complete 0.000184767%
I0109 20:28:58.493429  4323 pruning_runner.cpp:264] Compression complete 50.0001%
I0109 20:28:58.816615  4323 pruning_runner.cpp:264] Compression complete 66.6668%
I0109 20:28:59.138882  4323 pruning_runner.cpp:264] Compression complete 80.0001%
I0109 20:28:59.467052  4323 pruning_runner.cpp:264] Compression complete 94.1177%
I0109 20:28:59.798363  4323 pruning_runner.cpp:264] Compression complete 97.0588%
I0109 20:29:00.133533  4323 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 20:29:00.134485  4323 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 20:29:00.134894  4323 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:29:00.135128  4323 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:29:00.135293  4323 layer_factory.hpp:77] Creating layer data
I0109 20:29:00.135378  4323 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:00.135504  4323 net.cpp:94] Creating Layer data
I0109 20:29:00.135534  4323 net.cpp:409] data -> data
I0109 20:29:00.135561  4323 net.cpp:409] data -> label
I0109 20:29:00.136637  4458 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:29:00.136677  4458 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:29:00.136765  4323 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:29:00.136955  4323 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:29:00.143700  4323 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:00.143759  4323 net.cpp:144] Setting up data
I0109 20:29:00.143779  4323 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:29:00.143791  4323 net.cpp:151] Top shape: 50 (50)
I0109 20:29:00.143797  4323 net.cpp:159] Memory required for data: 614600
I0109 20:29:00.143805  4323 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:29:00.143846  4323 net.cpp:94] Creating Layer label_data_1_split
I0109 20:29:00.143885  4323 net.cpp:435] label_data_1_split <- label
I0109 20:29:00.143909  4323 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:29:00.143925  4323 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:29:00.143952  4323 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:29:00.144043  4323 net.cpp:144] Setting up label_data_1_split
I0109 20:29:00.144062  4323 net.cpp:151] Top shape: 50 (50)
I0109 20:29:00.144071  4323 net.cpp:151] Top shape: 50 (50)
I0109 20:29:00.144078  4323 net.cpp:151] Top shape: 50 (50)
I0109 20:29:00.144093  4323 net.cpp:159] Memory required for data: 615200
I0109 20:29:00.144099  4323 layer_factory.hpp:77] Creating layer conv1
I0109 20:29:00.144119  4323 net.cpp:94] Creating Layer conv1
I0109 20:29:00.144134  4323 net.cpp:435] conv1 <- data
I0109 20:29:00.144148  4323 net.cpp:409] conv1 -> conv1
I0109 20:29:00.145184  4323 net.cpp:144] Setting up conv1
I0109 20:29:00.145205  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.145213  4323 net.cpp:159] Memory required for data: 7168800
I0109 20:29:00.145232  4323 layer_factory.hpp:77] Creating layer bn1
I0109 20:29:00.145256  4323 net.cpp:94] Creating Layer bn1
I0109 20:29:00.145264  4323 net.cpp:435] bn1 <- conv1
I0109 20:29:00.145275  4323 net.cpp:409] bn1 -> scale1
I0109 20:29:00.146415  4323 net.cpp:144] Setting up bn1
I0109 20:29:00.146435  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.146443  4323 net.cpp:159] Memory required for data: 13722400
I0109 20:29:00.146464  4323 layer_factory.hpp:77] Creating layer relu1
I0109 20:29:00.146499  4323 net.cpp:94] Creating Layer relu1
I0109 20:29:00.146514  4323 net.cpp:435] relu1 <- scale1
I0109 20:29:00.146526  4323 net.cpp:409] relu1 -> relu1
I0109 20:29:00.146598  4323 net.cpp:144] Setting up relu1
I0109 20:29:00.146653  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.146694  4323 net.cpp:159] Memory required for data: 20276000
I0109 20:29:00.146733  4323 layer_factory.hpp:77] Creating layer conv2
I0109 20:29:00.146778  4323 net.cpp:94] Creating Layer conv2
I0109 20:29:00.146821  4323 net.cpp:435] conv2 <- relu1
I0109 20:29:00.146845  4323 net.cpp:409] conv2 -> conv2
I0109 20:29:00.148118  4323 net.cpp:144] Setting up conv2
I0109 20:29:00.148169  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.148216  4323 net.cpp:159] Memory required for data: 26829600
I0109 20:29:00.148267  4323 layer_factory.hpp:77] Creating layer bn2
I0109 20:29:00.148327  4323 net.cpp:94] Creating Layer bn2
I0109 20:29:00.148378  4323 net.cpp:435] bn2 <- conv2
I0109 20:29:00.148429  4323 net.cpp:409] bn2 -> scale2
I0109 20:29:00.149446  4323 net.cpp:144] Setting up bn2
I0109 20:29:00.149466  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.149473  4323 net.cpp:159] Memory required for data: 33383200
I0109 20:29:00.149544  4323 layer_factory.hpp:77] Creating layer relu2
I0109 20:29:00.149585  4323 net.cpp:94] Creating Layer relu2
I0109 20:29:00.149628  4323 net.cpp:435] relu2 <- scale2
I0109 20:29:00.149674  4323 net.cpp:409] relu2 -> relu2
I0109 20:29:00.149758  4323 net.cpp:144] Setting up relu2
I0109 20:29:00.149809  4323 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:00.149853  4323 net.cpp:159] Memory required for data: 39936800
I0109 20:29:00.149896  4323 layer_factory.hpp:77] Creating layer pool1
I0109 20:29:00.149945  4323 net.cpp:94] Creating Layer pool1
I0109 20:29:00.149987  4323 net.cpp:435] pool1 <- relu2
I0109 20:29:00.150032  4323 net.cpp:409] pool1 -> pool1
I0109 20:29:00.150126  4323 net.cpp:144] Setting up pool1
I0109 20:29:00.150176  4323 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:29:00.150216  4323 net.cpp:159] Memory required for data: 41575200
I0109 20:29:00.150257  4323 layer_factory.hpp:77] Creating layer drop1
I0109 20:29:00.150303  4323 net.cpp:94] Creating Layer drop1
I0109 20:29:00.150323  4323 net.cpp:435] drop1 <- pool1
I0109 20:29:00.150336  4323 net.cpp:409] drop1 -> drop1
I0109 20:29:00.150406  4323 net.cpp:144] Setting up drop1
I0109 20:29:00.150434  4323 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:29:00.150439  4323 net.cpp:159] Memory required for data: 43213600
I0109 20:29:00.150444  4323 layer_factory.hpp:77] Creating layer conv3
I0109 20:29:00.150458  4323 net.cpp:94] Creating Layer conv3
I0109 20:29:00.150475  4323 net.cpp:435] conv3 <- drop1
I0109 20:29:00.150488  4323 net.cpp:409] conv3 -> conv3
I0109 20:29:00.151916  4323 net.cpp:144] Setting up conv3
I0109 20:29:00.151935  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.151939  4323 net.cpp:159] Memory required for data: 46490400
I0109 20:29:00.151948  4323 layer_factory.hpp:77] Creating layer bn3
I0109 20:29:00.151962  4323 net.cpp:94] Creating Layer bn3
I0109 20:29:00.151979  4323 net.cpp:435] bn3 <- conv3
I0109 20:29:00.151993  4323 net.cpp:409] bn3 -> scale3
I0109 20:29:00.153033  4323 net.cpp:144] Setting up bn3
I0109 20:29:00.153049  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.153054  4323 net.cpp:159] Memory required for data: 49767200
I0109 20:29:00.153071  4323 layer_factory.hpp:77] Creating layer relu3
I0109 20:29:00.153095  4323 net.cpp:94] Creating Layer relu3
I0109 20:29:00.153102  4323 net.cpp:435] relu3 <- scale3
I0109 20:29:00.153120  4323 net.cpp:409] relu3 -> relu3
I0109 20:29:00.153208  4323 net.cpp:144] Setting up relu3
I0109 20:29:00.153380  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.153415  4323 net.cpp:159] Memory required for data: 53044000
I0109 20:29:00.153437  4323 layer_factory.hpp:77] Creating layer conv4
I0109 20:29:00.153471  4323 net.cpp:94] Creating Layer conv4
I0109 20:29:00.153496  4323 net.cpp:435] conv4 <- relu3
I0109 20:29:00.153523  4323 net.cpp:409] conv4 -> conv4
I0109 20:29:00.154343  4323 net.cpp:144] Setting up conv4
I0109 20:29:00.154368  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.154378  4323 net.cpp:159] Memory required for data: 56320800
I0109 20:29:00.154390  4323 layer_factory.hpp:77] Creating layer bn4
I0109 20:29:00.154422  4323 net.cpp:94] Creating Layer bn4
I0109 20:29:00.154443  4323 net.cpp:435] bn4 <- conv4
I0109 20:29:00.154494  4323 net.cpp:409] bn4 -> scale4
I0109 20:29:00.156008  4323 net.cpp:144] Setting up bn4
I0109 20:29:00.156033  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.156040  4323 net.cpp:159] Memory required for data: 59597600
I0109 20:29:00.156136  4323 layer_factory.hpp:77] Creating layer relu4
I0109 20:29:00.156159  4323 net.cpp:94] Creating Layer relu4
I0109 20:29:00.156208  4323 net.cpp:435] relu4 <- scale4
I0109 20:29:00.156281  4323 net.cpp:409] relu4 -> relu4
I0109 20:29:00.156415  4323 net.cpp:144] Setting up relu4
I0109 20:29:00.156471  4323 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:00.156517  4323 net.cpp:159] Memory required for data: 62874400
I0109 20:29:00.156556  4323 layer_factory.hpp:77] Creating layer pool2
I0109 20:29:00.156602  4323 net.cpp:94] Creating Layer pool2
I0109 20:29:00.156641  4323 net.cpp:435] pool2 <- relu4
I0109 20:29:00.156684  4323 net.cpp:409] pool2 -> pool2
I0109 20:29:00.156775  4323 net.cpp:144] Setting up pool2
I0109 20:29:00.156822  4323 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:29:00.156847  4323 net.cpp:159] Memory required for data: 63693600
I0109 20:29:00.156868  4323 layer_factory.hpp:77] Creating layer drop2
I0109 20:29:00.156893  4323 net.cpp:94] Creating Layer drop2
I0109 20:29:00.156910  4323 net.cpp:435] drop2 <- pool2
I0109 20:29:00.156924  4323 net.cpp:409] drop2 -> drop2
I0109 20:29:00.156991  4323 net.cpp:144] Setting up drop2
I0109 20:29:00.157028  4323 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:29:00.157063  4323 net.cpp:159] Memory required for data: 64512800
I0109 20:29:00.157088  4323 layer_factory.hpp:77] Creating layer fc1
I0109 20:29:00.157124  4323 net.cpp:94] Creating Layer fc1
I0109 20:29:00.157140  4323 net.cpp:435] fc1 <- drop2
I0109 20:29:00.157155  4323 net.cpp:409] fc1 -> fc1
I0109 20:29:00.178819  4323 net.cpp:144] Setting up fc1
I0109 20:29:00.178865  4323 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:00.178905  4323 net.cpp:159] Memory required for data: 64615200
I0109 20:29:00.178925  4323 layer_factory.hpp:77] Creating layer bn5
I0109 20:29:00.178954  4323 net.cpp:94] Creating Layer bn5
I0109 20:29:00.178966  4323 net.cpp:435] bn5 <- fc1
I0109 20:29:00.178983  4323 net.cpp:409] bn5 -> scale5
I0109 20:29:00.179615  4323 net.cpp:144] Setting up bn5
I0109 20:29:00.179636  4323 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:00.179642  4323 net.cpp:159] Memory required for data: 64717600
I0109 20:29:00.179672  4323 layer_factory.hpp:77] Creating layer relu5
I0109 20:29:00.179692  4323 net.cpp:94] Creating Layer relu5
I0109 20:29:00.179700  4323 net.cpp:435] relu5 <- scale5
I0109 20:29:00.179711  4323 net.cpp:409] relu5 -> relu5
I0109 20:29:00.179769  4323 net.cpp:144] Setting up relu5
I0109 20:29:00.179787  4323 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:00.179791  4323 net.cpp:159] Memory required for data: 64820000
I0109 20:29:00.179795  4323 layer_factory.hpp:77] Creating layer drop3
I0109 20:29:00.179807  4323 net.cpp:94] Creating Layer drop3
I0109 20:29:00.179822  4323 net.cpp:435] drop3 <- relu5
I0109 20:29:00.179833  4323 net.cpp:409] drop3 -> drop3
I0109 20:29:00.179899  4323 net.cpp:144] Setting up drop3
I0109 20:29:00.179914  4323 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:00.179919  4323 net.cpp:159] Memory required for data: 64922400
I0109 20:29:00.179924  4323 layer_factory.hpp:77] Creating layer fc2
I0109 20:29:00.179939  4323 net.cpp:94] Creating Layer fc2
I0109 20:29:00.179952  4323 net.cpp:435] fc2 <- drop3
I0109 20:29:00.179966  4323 net.cpp:409] fc2 -> fc2
I0109 20:29:00.180147  4323 net.cpp:144] Setting up fc2
I0109 20:29:00.180162  4323 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:00.180166  4323 net.cpp:159] Memory required for data: 64924400
I0109 20:29:00.180176  4323 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:29:00.180196  4323 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:29:00.180210  4323 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:29:00.180223  4323 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:29:00.180244  4323 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:29:00.180258  4323 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:29:00.180332  4323 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:29:00.180348  4323 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:00.180353  4323 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:00.180358  4323 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:00.180364  4323 net.cpp:159] Memory required for data: 64930400
I0109 20:29:00.180378  4323 layer_factory.hpp:77] Creating layer loss
I0109 20:29:00.180393  4323 net.cpp:94] Creating Layer loss
I0109 20:29:00.180408  4323 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:29:00.180416  4323 net.cpp:435] loss <- label_data_1_split_0
I0109 20:29:00.180435  4323 net.cpp:409] loss -> loss
I0109 20:29:00.180452  4323 layer_factory.hpp:77] Creating layer loss
I0109 20:29:00.180568  4323 net.cpp:144] Setting up loss
I0109 20:29:00.180583  4323 net.cpp:151] Top shape: (1)
I0109 20:29:00.180586  4323 net.cpp:154]     with loss weight 1
I0109 20:29:00.180615  4323 net.cpp:159] Memory required for data: 64930404
I0109 20:29:00.180622  4323 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:29:00.180642  4323 net.cpp:94] Creating Layer accuracy-top1
I0109 20:29:00.180650  4323 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:29:00.180667  4323 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:29:00.180681  4323 net.cpp:409] accuracy-top1 -> top-1
I0109 20:29:00.180716  4323 net.cpp:144] Setting up accuracy-top1
I0109 20:29:00.180732  4323 net.cpp:151] Top shape: (1)
I0109 20:29:00.180739  4323 net.cpp:159] Memory required for data: 64930408
I0109 20:29:00.180745  4323 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:29:00.180757  4323 net.cpp:94] Creating Layer accuracy-top5
I0109 20:29:00.180766  4323 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:29:00.180789  4323 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:29:00.180801  4323 net.cpp:409] accuracy-top5 -> top-5
I0109 20:29:00.180815  4323 net.cpp:144] Setting up accuracy-top5
I0109 20:29:00.180825  4323 net.cpp:151] Top shape: (1)
I0109 20:29:00.180832  4323 net.cpp:159] Memory required for data: 64930412
I0109 20:29:00.180840  4323 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:29:00.180847  4323 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:29:00.180855  4323 net.cpp:220] loss needs backward computation.
I0109 20:29:00.180864  4323 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:29:00.180872  4323 net.cpp:220] fc2 needs backward computation.
I0109 20:29:00.180878  4323 net.cpp:220] drop3 needs backward computation.
I0109 20:29:00.180886  4323 net.cpp:220] relu5 needs backward computation.
I0109 20:29:00.180893  4323 net.cpp:220] bn5 needs backward computation.
I0109 20:29:00.180900  4323 net.cpp:220] fc1 needs backward computation.
I0109 20:29:00.180907  4323 net.cpp:220] drop2 needs backward computation.
I0109 20:29:00.180913  4323 net.cpp:220] pool2 needs backward computation.
I0109 20:29:00.180922  4323 net.cpp:220] relu4 needs backward computation.
I0109 20:29:00.180930  4323 net.cpp:220] bn4 needs backward computation.
I0109 20:29:00.180938  4323 net.cpp:220] conv4 needs backward computation.
I0109 20:29:00.180945  4323 net.cpp:220] relu3 needs backward computation.
I0109 20:29:00.180954  4323 net.cpp:220] bn3 needs backward computation.
I0109 20:29:00.180963  4323 net.cpp:220] conv3 needs backward computation.
I0109 20:29:00.180969  4323 net.cpp:220] drop1 needs backward computation.
I0109 20:29:00.180977  4323 net.cpp:220] pool1 needs backward computation.
I0109 20:29:00.180985  4323 net.cpp:220] relu2 needs backward computation.
I0109 20:29:00.180992  4323 net.cpp:220] bn2 needs backward computation.
I0109 20:29:00.180999  4323 net.cpp:220] conv2 needs backward computation.
I0109 20:29:00.181007  4323 net.cpp:220] relu1 needs backward computation.
I0109 20:29:00.181015  4323 net.cpp:220] bn1 needs backward computation.
I0109 20:29:00.181025  4323 net.cpp:220] conv1 needs backward computation.
I0109 20:29:00.181032  4323 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:29:00.181041  4323 net.cpp:222] data does not need backward computation.
I0109 20:29:00.181049  4323 net.cpp:264] This network produces output loss
I0109 20:29:00.181057  4323 net.cpp:264] This network produces output top-1
I0109 20:29:00.181063  4323 net.cpp:264] This network produces output top-5
I0109 20:29:00.181099  4323 net.cpp:284] Network initialization done.
I0109 20:29:00.185556  4323 caffe_interface.cpp:363] Running for 180 iterations.
I0109 20:29:00.197615  4323 caffe_interface.cpp:125] Batch 0, loss = 4.21796
I0109 20:29:00.197650  4323 caffe_interface.cpp:125] Batch 0, top-1 = 0.22
I0109 20:29:00.197664  4323 caffe_interface.cpp:125] Batch 0, top-5 = 0.66
I0109 20:29:00.203959  4323 caffe_interface.cpp:125] Batch 1, loss = 4.28693
I0109 20:29:00.203986  4323 caffe_interface.cpp:125] Batch 1, top-1 = 0.26
I0109 20:29:00.203996  4323 caffe_interface.cpp:125] Batch 1, top-5 = 0.66
I0109 20:29:00.210273  4323 caffe_interface.cpp:125] Batch 2, loss = 4.4085
I0109 20:29:00.210299  4323 caffe_interface.cpp:125] Batch 2, top-1 = 0.18
I0109 20:29:00.210309  4323 caffe_interface.cpp:125] Batch 2, top-5 = 0.74
I0109 20:29:00.216588  4323 caffe_interface.cpp:125] Batch 3, loss = 4.92253
I0109 20:29:00.216612  4323 caffe_interface.cpp:125] Batch 3, top-1 = 0.2
I0109 20:29:00.216624  4323 caffe_interface.cpp:125] Batch 3, top-5 = 0.52
I0109 20:29:00.222905  4323 caffe_interface.cpp:125] Batch 4, loss = 3.86182
I0109 20:29:00.222929  4323 caffe_interface.cpp:125] Batch 4, top-1 = 0.18
I0109 20:29:00.222939  4323 caffe_interface.cpp:125] Batch 4, top-5 = 0.7
I0109 20:29:00.229224  4323 caffe_interface.cpp:125] Batch 5, loss = 4.72287
I0109 20:29:00.229249  4323 caffe_interface.cpp:125] Batch 5, top-1 = 0.32
I0109 20:29:00.229259  4323 caffe_interface.cpp:125] Batch 5, top-5 = 0.7
I0109 20:29:00.235568  4323 caffe_interface.cpp:125] Batch 6, loss = 3.97465
I0109 20:29:00.235592  4323 caffe_interface.cpp:125] Batch 6, top-1 = 0.2
I0109 20:29:00.235601  4323 caffe_interface.cpp:125] Batch 6, top-5 = 0.7
I0109 20:29:00.241842  4323 caffe_interface.cpp:125] Batch 7, loss = 4.10384
I0109 20:29:00.241864  4323 caffe_interface.cpp:125] Batch 7, top-1 = 0.24
I0109 20:29:00.241875  4323 caffe_interface.cpp:125] Batch 7, top-5 = 0.68
I0109 20:29:00.248131  4323 caffe_interface.cpp:125] Batch 8, loss = 3.89395
I0109 20:29:00.248154  4323 caffe_interface.cpp:125] Batch 8, top-1 = 0.26
I0109 20:29:00.248165  4323 caffe_interface.cpp:125] Batch 8, top-5 = 0.7
I0109 20:29:00.254423  4323 caffe_interface.cpp:125] Batch 9, loss = 4.8808
I0109 20:29:00.254447  4323 caffe_interface.cpp:125] Batch 9, top-1 = 0.2
I0109 20:29:00.254456  4323 caffe_interface.cpp:125] Batch 9, top-5 = 0.66
I0109 20:29:00.260716  4323 caffe_interface.cpp:125] Batch 10, loss = 4.40834
I0109 20:29:00.260740  4323 caffe_interface.cpp:125] Batch 10, top-1 = 0.16
I0109 20:29:00.260751  4323 caffe_interface.cpp:125] Batch 10, top-5 = 0.72
I0109 20:29:00.267027  4323 caffe_interface.cpp:125] Batch 11, loss = 5.12334
I0109 20:29:00.267050  4323 caffe_interface.cpp:125] Batch 11, top-1 = 0.16
I0109 20:29:00.267061  4323 caffe_interface.cpp:125] Batch 11, top-5 = 0.52
I0109 20:29:00.273305  4323 caffe_interface.cpp:125] Batch 12, loss = 4.26569
I0109 20:29:00.273330  4323 caffe_interface.cpp:125] Batch 12, top-1 = 0.26
I0109 20:29:00.273340  4323 caffe_interface.cpp:125] Batch 12, top-5 = 0.72
I0109 20:29:00.279605  4323 caffe_interface.cpp:125] Batch 13, loss = 3.24334
I0109 20:29:00.279630  4323 caffe_interface.cpp:125] Batch 13, top-1 = 0.36
I0109 20:29:00.279640  4323 caffe_interface.cpp:125] Batch 13, top-5 = 0.78
I0109 20:29:00.285930  4323 caffe_interface.cpp:125] Batch 14, loss = 4.4727
I0109 20:29:00.285955  4323 caffe_interface.cpp:125] Batch 14, top-1 = 0.16
I0109 20:29:00.285965  4323 caffe_interface.cpp:125] Batch 14, top-5 = 0.68
I0109 20:29:00.292234  4323 caffe_interface.cpp:125] Batch 15, loss = 3.76398
I0109 20:29:00.292258  4323 caffe_interface.cpp:125] Batch 15, top-1 = 0.28
I0109 20:29:00.292270  4323 caffe_interface.cpp:125] Batch 15, top-5 = 0.74
I0109 20:29:00.298527  4323 caffe_interface.cpp:125] Batch 16, loss = 4.39369
I0109 20:29:00.298550  4323 caffe_interface.cpp:125] Batch 16, top-1 = 0.18
I0109 20:29:00.298560  4323 caffe_interface.cpp:125] Batch 16, top-5 = 0.64
I0109 20:29:00.304872  4323 caffe_interface.cpp:125] Batch 17, loss = 4.96969
I0109 20:29:00.304895  4323 caffe_interface.cpp:125] Batch 17, top-1 = 0.2
I0109 20:29:00.304904  4323 caffe_interface.cpp:125] Batch 17, top-5 = 0.58
I0109 20:29:00.311170  4323 caffe_interface.cpp:125] Batch 18, loss = 3.68914
I0109 20:29:00.311195  4323 caffe_interface.cpp:125] Batch 18, top-1 = 0.22
I0109 20:29:00.311206  4323 caffe_interface.cpp:125] Batch 18, top-5 = 0.68
I0109 20:29:00.317456  4323 caffe_interface.cpp:125] Batch 19, loss = 4.32321
I0109 20:29:00.317479  4323 caffe_interface.cpp:125] Batch 19, top-1 = 0.22
I0109 20:29:00.317490  4323 caffe_interface.cpp:125] Batch 19, top-5 = 0.6
I0109 20:29:00.323788  4323 caffe_interface.cpp:125] Batch 20, loss = 4.16345
I0109 20:29:00.323812  4323 caffe_interface.cpp:125] Batch 20, top-1 = 0.26
I0109 20:29:00.323822  4323 caffe_interface.cpp:125] Batch 20, top-5 = 0.64
I0109 20:29:00.330099  4323 caffe_interface.cpp:125] Batch 21, loss = 4.94766
I0109 20:29:00.330122  4323 caffe_interface.cpp:125] Batch 21, top-1 = 0.26
I0109 20:29:00.330132  4323 caffe_interface.cpp:125] Batch 21, top-5 = 0.68
I0109 20:29:00.336387  4323 caffe_interface.cpp:125] Batch 22, loss = 4.57766
I0109 20:29:00.336411  4323 caffe_interface.cpp:125] Batch 22, top-1 = 0.22
I0109 20:29:00.336421  4323 caffe_interface.cpp:125] Batch 22, top-5 = 0.66
I0109 20:29:00.342639  4323 caffe_interface.cpp:125] Batch 23, loss = 3.80792
I0109 20:29:00.342664  4323 caffe_interface.cpp:125] Batch 23, top-1 = 0.28
I0109 20:29:00.342702  4323 caffe_interface.cpp:125] Batch 23, top-5 = 0.7
I0109 20:29:00.348975  4323 caffe_interface.cpp:125] Batch 24, loss = 3.67023
I0109 20:29:00.349001  4323 caffe_interface.cpp:125] Batch 24, top-1 = 0.3
I0109 20:29:00.349011  4323 caffe_interface.cpp:125] Batch 24, top-5 = 0.76
I0109 20:29:00.355283  4323 caffe_interface.cpp:125] Batch 25, loss = 4.10301
I0109 20:29:00.355306  4323 caffe_interface.cpp:125] Batch 25, top-1 = 0.32
I0109 20:29:00.355315  4323 caffe_interface.cpp:125] Batch 25, top-5 = 0.68
I0109 20:29:00.361609  4323 caffe_interface.cpp:125] Batch 26, loss = 4.20964
I0109 20:29:00.361631  4323 caffe_interface.cpp:125] Batch 26, top-1 = 0.2
I0109 20:29:00.361641  4323 caffe_interface.cpp:125] Batch 26, top-5 = 0.64
I0109 20:29:00.367934  4323 caffe_interface.cpp:125] Batch 27, loss = 4.2851
I0109 20:29:00.367961  4323 caffe_interface.cpp:125] Batch 27, top-1 = 0.22
I0109 20:29:00.367972  4323 caffe_interface.cpp:125] Batch 27, top-5 = 0.66
I0109 20:29:00.374239  4323 caffe_interface.cpp:125] Batch 28, loss = 3.58195
I0109 20:29:00.374264  4323 caffe_interface.cpp:125] Batch 28, top-1 = 0.32
I0109 20:29:00.374274  4323 caffe_interface.cpp:125] Batch 28, top-5 = 0.76
I0109 20:29:00.380554  4323 caffe_interface.cpp:125] Batch 29, loss = 3.21701
I0109 20:29:00.380581  4323 caffe_interface.cpp:125] Batch 29, top-1 = 0.3
I0109 20:29:00.380592  4323 caffe_interface.cpp:125] Batch 29, top-5 = 0.76
I0109 20:29:00.386878  4323 caffe_interface.cpp:125] Batch 30, loss = 4.13384
I0109 20:29:00.386904  4323 caffe_interface.cpp:125] Batch 30, top-1 = 0.26
I0109 20:29:00.386914  4323 caffe_interface.cpp:125] Batch 30, top-5 = 0.64
I0109 20:29:00.393194  4323 caffe_interface.cpp:125] Batch 31, loss = 3.72952
I0109 20:29:00.393218  4323 caffe_interface.cpp:125] Batch 31, top-1 = 0.26
I0109 20:29:00.393230  4323 caffe_interface.cpp:125] Batch 31, top-5 = 0.7
I0109 20:29:00.399549  4323 caffe_interface.cpp:125] Batch 32, loss = 2.94645
I0109 20:29:00.399572  4323 caffe_interface.cpp:125] Batch 32, top-1 = 0.38
I0109 20:29:00.399582  4323 caffe_interface.cpp:125] Batch 32, top-5 = 0.76
I0109 20:29:00.405879  4323 caffe_interface.cpp:125] Batch 33, loss = 4.5641
I0109 20:29:00.405903  4323 caffe_interface.cpp:125] Batch 33, top-1 = 0.22
I0109 20:29:00.405913  4323 caffe_interface.cpp:125] Batch 33, top-5 = 0.66
I0109 20:29:00.412190  4323 caffe_interface.cpp:125] Batch 34, loss = 3.88818
I0109 20:29:00.412214  4323 caffe_interface.cpp:125] Batch 34, top-1 = 0.28
I0109 20:29:00.412223  4323 caffe_interface.cpp:125] Batch 34, top-5 = 0.7
I0109 20:29:00.418509  4323 caffe_interface.cpp:125] Batch 35, loss = 3.74426
I0109 20:29:00.418534  4323 caffe_interface.cpp:125] Batch 35, top-1 = 0.28
I0109 20:29:00.418545  4323 caffe_interface.cpp:125] Batch 35, top-5 = 0.72
I0109 20:29:00.424823  4323 caffe_interface.cpp:125] Batch 36, loss = 3.54846
I0109 20:29:00.424847  4323 caffe_interface.cpp:125] Batch 36, top-1 = 0.32
I0109 20:29:00.424856  4323 caffe_interface.cpp:125] Batch 36, top-5 = 0.72
I0109 20:29:00.431134  4323 caffe_interface.cpp:125] Batch 37, loss = 4.32087
I0109 20:29:00.431159  4323 caffe_interface.cpp:125] Batch 37, top-1 = 0.2
I0109 20:29:00.431169  4323 caffe_interface.cpp:125] Batch 37, top-5 = 0.64
I0109 20:29:00.437455  4323 caffe_interface.cpp:125] Batch 38, loss = 3.98245
I0109 20:29:00.437479  4323 caffe_interface.cpp:125] Batch 38, top-1 = 0.28
I0109 20:29:00.437489  4323 caffe_interface.cpp:125] Batch 38, top-5 = 0.74
I0109 20:29:00.443732  4323 caffe_interface.cpp:125] Batch 39, loss = 4.03367
I0109 20:29:00.443758  4323 caffe_interface.cpp:125] Batch 39, top-1 = 0.16
I0109 20:29:00.443768  4323 caffe_interface.cpp:125] Batch 39, top-5 = 0.7
I0109 20:29:00.449448  4323 caffe_interface.cpp:125] Batch 40, loss = 4.58835
I0109 20:29:00.449474  4323 caffe_interface.cpp:125] Batch 40, top-1 = 0.28
I0109 20:29:00.449486  4323 caffe_interface.cpp:125] Batch 40, top-5 = 0.64
I0109 20:29:00.455153  4323 caffe_interface.cpp:125] Batch 41, loss = 5.18845
I0109 20:29:00.455178  4323 caffe_interface.cpp:125] Batch 41, top-1 = 0.2
I0109 20:29:00.455215  4323 caffe_interface.cpp:125] Batch 41, top-5 = 0.56
I0109 20:29:00.460886  4323 caffe_interface.cpp:125] Batch 42, loss = 4.04964
I0109 20:29:00.460912  4323 caffe_interface.cpp:125] Batch 42, top-1 = 0.24
I0109 20:29:00.460922  4323 caffe_interface.cpp:125] Batch 42, top-5 = 0.68
I0109 20:29:00.466591  4323 caffe_interface.cpp:125] Batch 43, loss = 3.65841
I0109 20:29:00.466616  4323 caffe_interface.cpp:125] Batch 43, top-1 = 0.24
I0109 20:29:00.466627  4323 caffe_interface.cpp:125] Batch 43, top-5 = 0.76
I0109 20:29:00.472308  4323 caffe_interface.cpp:125] Batch 44, loss = 4.57073
I0109 20:29:00.472332  4323 caffe_interface.cpp:125] Batch 44, top-1 = 0.3
I0109 20:29:00.472342  4323 caffe_interface.cpp:125] Batch 44, top-5 = 0.62
I0109 20:29:00.478044  4323 caffe_interface.cpp:125] Batch 45, loss = 4.79709
I0109 20:29:00.478068  4323 caffe_interface.cpp:125] Batch 45, top-1 = 0.22
I0109 20:29:00.478078  4323 caffe_interface.cpp:125] Batch 45, top-5 = 0.58
I0109 20:29:00.483764  4323 caffe_interface.cpp:125] Batch 46, loss = 3.5367
I0109 20:29:00.483789  4323 caffe_interface.cpp:125] Batch 46, top-1 = 0.26
I0109 20:29:00.483800  4323 caffe_interface.cpp:125] Batch 46, top-5 = 0.76
I0109 20:29:00.489449  4323 caffe_interface.cpp:125] Batch 47, loss = 4.36486
I0109 20:29:00.489475  4323 caffe_interface.cpp:125] Batch 47, top-1 = 0.28
I0109 20:29:00.489485  4323 caffe_interface.cpp:125] Batch 47, top-5 = 0.64
I0109 20:29:00.495138  4323 caffe_interface.cpp:125] Batch 48, loss = 4.46106
I0109 20:29:00.495162  4323 caffe_interface.cpp:125] Batch 48, top-1 = 0.22
I0109 20:29:00.495173  4323 caffe_interface.cpp:125] Batch 48, top-5 = 0.66
I0109 20:29:00.500824  4323 caffe_interface.cpp:125] Batch 49, loss = 4.82331
I0109 20:29:00.500849  4323 caffe_interface.cpp:125] Batch 49, top-1 = 0.2
I0109 20:29:00.500859  4323 caffe_interface.cpp:125] Batch 49, top-5 = 0.62
I0109 20:29:00.506542  4323 caffe_interface.cpp:125] Batch 50, loss = 4.87071
I0109 20:29:00.506567  4323 caffe_interface.cpp:125] Batch 50, top-1 = 0.22
I0109 20:29:00.506577  4323 caffe_interface.cpp:125] Batch 50, top-5 = 0.58
I0109 20:29:00.512243  4323 caffe_interface.cpp:125] Batch 51, loss = 4.22542
I0109 20:29:00.512270  4323 caffe_interface.cpp:125] Batch 51, top-1 = 0.18
I0109 20:29:00.512281  4323 caffe_interface.cpp:125] Batch 51, top-5 = 0.6
I0109 20:29:00.517925  4323 caffe_interface.cpp:125] Batch 52, loss = 3.74022
I0109 20:29:00.517951  4323 caffe_interface.cpp:125] Batch 52, top-1 = 0.16
I0109 20:29:00.517961  4323 caffe_interface.cpp:125] Batch 52, top-5 = 0.7
I0109 20:29:00.523653  4323 caffe_interface.cpp:125] Batch 53, loss = 3.97559
I0109 20:29:00.523679  4323 caffe_interface.cpp:125] Batch 53, top-1 = 0.28
I0109 20:29:00.523689  4323 caffe_interface.cpp:125] Batch 53, top-5 = 0.72
I0109 20:29:00.529340  4323 caffe_interface.cpp:125] Batch 54, loss = 4.41205
I0109 20:29:00.529364  4323 caffe_interface.cpp:125] Batch 54, top-1 = 0.22
I0109 20:29:00.529374  4323 caffe_interface.cpp:125] Batch 54, top-5 = 0.64
I0109 20:29:00.535004  4323 caffe_interface.cpp:125] Batch 55, loss = 3.57517
I0109 20:29:00.535028  4323 caffe_interface.cpp:125] Batch 55, top-1 = 0.18
I0109 20:29:00.535040  4323 caffe_interface.cpp:125] Batch 55, top-5 = 0.76
I0109 20:29:00.540665  4323 caffe_interface.cpp:125] Batch 56, loss = 4.02211
I0109 20:29:00.540689  4323 caffe_interface.cpp:125] Batch 56, top-1 = 0.22
I0109 20:29:00.540699  4323 caffe_interface.cpp:125] Batch 56, top-5 = 0.7
I0109 20:29:00.546350  4323 caffe_interface.cpp:125] Batch 57, loss = 4.74006
I0109 20:29:00.546373  4323 caffe_interface.cpp:125] Batch 57, top-1 = 0.32
I0109 20:29:00.546383  4323 caffe_interface.cpp:125] Batch 57, top-5 = 0.64
I0109 20:29:00.552052  4323 caffe_interface.cpp:125] Batch 58, loss = 3.73193
I0109 20:29:00.552076  4323 caffe_interface.cpp:125] Batch 58, top-1 = 0.3
I0109 20:29:00.552088  4323 caffe_interface.cpp:125] Batch 58, top-5 = 0.66
I0109 20:29:00.557744  4323 caffe_interface.cpp:125] Batch 59, loss = 4.43051
I0109 20:29:00.557785  4323 caffe_interface.cpp:125] Batch 59, top-1 = 0.22
I0109 20:29:00.557797  4323 caffe_interface.cpp:125] Batch 59, top-5 = 0.64
I0109 20:29:00.563463  4323 caffe_interface.cpp:125] Batch 60, loss = 3.83814
I0109 20:29:00.563488  4323 caffe_interface.cpp:125] Batch 60, top-1 = 0.3
I0109 20:29:00.563499  4323 caffe_interface.cpp:125] Batch 60, top-5 = 0.66
I0109 20:29:00.569133  4323 caffe_interface.cpp:125] Batch 61, loss = 4.6372
I0109 20:29:00.569157  4323 caffe_interface.cpp:125] Batch 61, top-1 = 0.22
I0109 20:29:00.569167  4323 caffe_interface.cpp:125] Batch 61, top-5 = 0.58
I0109 20:29:00.574791  4323 caffe_interface.cpp:125] Batch 62, loss = 4.7367
I0109 20:29:00.574816  4323 caffe_interface.cpp:125] Batch 62, top-1 = 0.28
I0109 20:29:00.574826  4323 caffe_interface.cpp:125] Batch 62, top-5 = 0.6
I0109 20:29:00.580482  4323 caffe_interface.cpp:125] Batch 63, loss = 4.16524
I0109 20:29:00.580507  4323 caffe_interface.cpp:125] Batch 63, top-1 = 0.26
I0109 20:29:00.580518  4323 caffe_interface.cpp:125] Batch 63, top-5 = 0.7
I0109 20:29:00.586166  4323 caffe_interface.cpp:125] Batch 64, loss = 4.67055
I0109 20:29:00.586190  4323 caffe_interface.cpp:125] Batch 64, top-1 = 0.24
I0109 20:29:00.586200  4323 caffe_interface.cpp:125] Batch 64, top-5 = 0.54
I0109 20:29:00.591886  4323 caffe_interface.cpp:125] Batch 65, loss = 4.28741
I0109 20:29:00.591910  4323 caffe_interface.cpp:125] Batch 65, top-1 = 0.22
I0109 20:29:00.591920  4323 caffe_interface.cpp:125] Batch 65, top-5 = 0.64
I0109 20:29:00.597554  4323 caffe_interface.cpp:125] Batch 66, loss = 3.74171
I0109 20:29:00.597579  4323 caffe_interface.cpp:125] Batch 66, top-1 = 0.22
I0109 20:29:00.597609  4323 caffe_interface.cpp:125] Batch 66, top-5 = 0.72
I0109 20:29:00.603262  4323 caffe_interface.cpp:125] Batch 67, loss = 3.93377
I0109 20:29:00.603286  4323 caffe_interface.cpp:125] Batch 67, top-1 = 0.28
I0109 20:29:00.603298  4323 caffe_interface.cpp:125] Batch 67, top-5 = 0.72
I0109 20:29:00.608948  4323 caffe_interface.cpp:125] Batch 68, loss = 5.06997
I0109 20:29:00.608973  4323 caffe_interface.cpp:125] Batch 68, top-1 = 0.2
I0109 20:29:00.608983  4323 caffe_interface.cpp:125] Batch 68, top-5 = 0.56
I0109 20:29:00.614410  4323 caffe_interface.cpp:125] Batch 69, loss = 3.45837
I0109 20:29:00.614434  4323 caffe_interface.cpp:125] Batch 69, top-1 = 0.26
I0109 20:29:00.614445  4323 caffe_interface.cpp:125] Batch 69, top-5 = 0.8
I0109 20:29:00.619645  4323 caffe_interface.cpp:125] Batch 70, loss = 4.30659
I0109 20:29:00.619671  4323 caffe_interface.cpp:125] Batch 70, top-1 = 0.24
I0109 20:29:00.619681  4323 caffe_interface.cpp:125] Batch 70, top-5 = 0.62
I0109 20:29:00.624874  4323 caffe_interface.cpp:125] Batch 71, loss = 3.74967
I0109 20:29:00.624899  4323 caffe_interface.cpp:125] Batch 71, top-1 = 0.28
I0109 20:29:00.624910  4323 caffe_interface.cpp:125] Batch 71, top-5 = 0.68
I0109 20:29:00.630112  4323 caffe_interface.cpp:125] Batch 72, loss = 4.4178
I0109 20:29:00.630136  4323 caffe_interface.cpp:125] Batch 72, top-1 = 0.14
I0109 20:29:00.630147  4323 caffe_interface.cpp:125] Batch 72, top-5 = 0.68
I0109 20:29:00.635340  4323 caffe_interface.cpp:125] Batch 73, loss = 4.43538
I0109 20:29:00.635365  4323 caffe_interface.cpp:125] Batch 73, top-1 = 0.2
I0109 20:29:00.635375  4323 caffe_interface.cpp:125] Batch 73, top-5 = 0.68
I0109 20:29:00.640527  4323 caffe_interface.cpp:125] Batch 74, loss = 4.41318
I0109 20:29:00.640552  4323 caffe_interface.cpp:125] Batch 74, top-1 = 0.2
I0109 20:29:00.640563  4323 caffe_interface.cpp:125] Batch 74, top-5 = 0.64
I0109 20:29:00.645750  4323 caffe_interface.cpp:125] Batch 75, loss = 4.14462
I0109 20:29:00.645774  4323 caffe_interface.cpp:125] Batch 75, top-1 = 0.16
I0109 20:29:00.645786  4323 caffe_interface.cpp:125] Batch 75, top-5 = 0.76
I0109 20:29:00.650995  4323 caffe_interface.cpp:125] Batch 76, loss = 4.36791
I0109 20:29:00.651019  4323 caffe_interface.cpp:125] Batch 76, top-1 = 0.2
I0109 20:29:00.651031  4323 caffe_interface.cpp:125] Batch 76, top-5 = 0.64
I0109 20:29:00.656224  4323 caffe_interface.cpp:125] Batch 77, loss = 4.94352
I0109 20:29:00.656266  4323 caffe_interface.cpp:125] Batch 77, top-1 = 0.22
I0109 20:29:00.656278  4323 caffe_interface.cpp:125] Batch 77, top-5 = 0.6
I0109 20:29:00.661468  4323 caffe_interface.cpp:125] Batch 78, loss = 4.41647
I0109 20:29:00.661494  4323 caffe_interface.cpp:125] Batch 78, top-1 = 0.22
I0109 20:29:00.661504  4323 caffe_interface.cpp:125] Batch 78, top-5 = 0.66
I0109 20:29:00.666723  4323 caffe_interface.cpp:125] Batch 79, loss = 5.57758
I0109 20:29:00.666746  4323 caffe_interface.cpp:125] Batch 79, top-1 = 0.14
I0109 20:29:00.666759  4323 caffe_interface.cpp:125] Batch 79, top-5 = 0.5
I0109 20:29:00.671952  4323 caffe_interface.cpp:125] Batch 80, loss = 4.24673
I0109 20:29:00.671975  4323 caffe_interface.cpp:125] Batch 80, top-1 = 0.26
I0109 20:29:00.671985  4323 caffe_interface.cpp:125] Batch 80, top-5 = 0.7
I0109 20:29:00.677194  4323 caffe_interface.cpp:125] Batch 81, loss = 3.98262
I0109 20:29:00.677219  4323 caffe_interface.cpp:125] Batch 81, top-1 = 0.28
I0109 20:29:00.677229  4323 caffe_interface.cpp:125] Batch 81, top-5 = 0.6
I0109 20:29:00.682418  4323 caffe_interface.cpp:125] Batch 82, loss = 3.89124
I0109 20:29:00.682442  4323 caffe_interface.cpp:125] Batch 82, top-1 = 0.3
I0109 20:29:00.682452  4323 caffe_interface.cpp:125] Batch 82, top-5 = 0.66
I0109 20:29:00.687629  4323 caffe_interface.cpp:125] Batch 83, loss = 3.84756
I0109 20:29:00.687656  4323 caffe_interface.cpp:125] Batch 83, top-1 = 0.24
I0109 20:29:00.687667  4323 caffe_interface.cpp:125] Batch 83, top-5 = 0.74
I0109 20:29:00.692853  4323 caffe_interface.cpp:125] Batch 84, loss = 3.95121
I0109 20:29:00.692878  4323 caffe_interface.cpp:125] Batch 84, top-1 = 0.24
I0109 20:29:00.692888  4323 caffe_interface.cpp:125] Batch 84, top-5 = 0.68
I0109 20:29:00.698076  4323 caffe_interface.cpp:125] Batch 85, loss = 5.11995
I0109 20:29:00.698099  4323 caffe_interface.cpp:125] Batch 85, top-1 = 0.14
I0109 20:29:00.698109  4323 caffe_interface.cpp:125] Batch 85, top-5 = 0.54
I0109 20:29:00.703299  4323 caffe_interface.cpp:125] Batch 86, loss = 3.87904
I0109 20:29:00.703323  4323 caffe_interface.cpp:125] Batch 86, top-1 = 0.28
I0109 20:29:00.703335  4323 caffe_interface.cpp:125] Batch 86, top-5 = 0.72
I0109 20:29:00.708515  4323 caffe_interface.cpp:125] Batch 87, loss = 4.49331
I0109 20:29:00.708540  4323 caffe_interface.cpp:125] Batch 87, top-1 = 0.24
I0109 20:29:00.708552  4323 caffe_interface.cpp:125] Batch 87, top-5 = 0.62
I0109 20:29:00.713735  4323 caffe_interface.cpp:125] Batch 88, loss = 4.17649
I0109 20:29:00.713759  4323 caffe_interface.cpp:125] Batch 88, top-1 = 0.28
I0109 20:29:00.713769  4323 caffe_interface.cpp:125] Batch 88, top-5 = 0.6
I0109 20:29:00.718964  4323 caffe_interface.cpp:125] Batch 89, loss = 4.31313
I0109 20:29:00.718988  4323 caffe_interface.cpp:125] Batch 89, top-1 = 0.28
I0109 20:29:00.718999  4323 caffe_interface.cpp:125] Batch 89, top-5 = 0.66
I0109 20:29:00.724174  4323 caffe_interface.cpp:125] Batch 90, loss = 3.24856
I0109 20:29:00.724200  4323 caffe_interface.cpp:125] Batch 90, top-1 = 0.36
I0109 20:29:00.724210  4323 caffe_interface.cpp:125] Batch 90, top-5 = 0.76
I0109 20:29:00.729423  4323 caffe_interface.cpp:125] Batch 91, loss = 3.96008
I0109 20:29:00.729447  4323 caffe_interface.cpp:125] Batch 91, top-1 = 0.28
I0109 20:29:00.729460  4323 caffe_interface.cpp:125] Batch 91, top-5 = 0.66
I0109 20:29:00.734655  4323 caffe_interface.cpp:125] Batch 92, loss = 3.16313
I0109 20:29:00.734680  4323 caffe_interface.cpp:125] Batch 92, top-1 = 0.32
I0109 20:29:00.734690  4323 caffe_interface.cpp:125] Batch 92, top-5 = 0.84
I0109 20:29:00.739893  4323 caffe_interface.cpp:125] Batch 93, loss = 3.93154
I0109 20:29:00.739917  4323 caffe_interface.cpp:125] Batch 93, top-1 = 0.24
I0109 20:29:00.739928  4323 caffe_interface.cpp:125] Batch 93, top-5 = 0.7
I0109 20:29:00.745100  4323 caffe_interface.cpp:125] Batch 94, loss = 4.02229
I0109 20:29:00.745124  4323 caffe_interface.cpp:125] Batch 94, top-1 = 0.22
I0109 20:29:00.745134  4323 caffe_interface.cpp:125] Batch 94, top-5 = 0.68
I0109 20:29:00.750339  4323 caffe_interface.cpp:125] Batch 95, loss = 3.87658
I0109 20:29:00.750362  4323 caffe_interface.cpp:125] Batch 95, top-1 = 0.3
I0109 20:29:00.750373  4323 caffe_interface.cpp:125] Batch 95, top-5 = 0.7
I0109 20:29:00.755556  4323 caffe_interface.cpp:125] Batch 96, loss = 4.18393
I0109 20:29:00.755580  4323 caffe_interface.cpp:125] Batch 96, top-1 = 0.28
I0109 20:29:00.755590  4323 caffe_interface.cpp:125] Batch 96, top-5 = 0.6
I0109 20:29:00.760771  4323 caffe_interface.cpp:125] Batch 97, loss = 4.15251
I0109 20:29:00.760795  4323 caffe_interface.cpp:125] Batch 97, top-1 = 0.2
I0109 20:29:00.760807  4323 caffe_interface.cpp:125] Batch 97, top-5 = 0.66
I0109 20:29:00.766002  4323 caffe_interface.cpp:125] Batch 98, loss = 4.18177
I0109 20:29:00.766027  4323 caffe_interface.cpp:125] Batch 98, top-1 = 0.2
I0109 20:29:00.766037  4323 caffe_interface.cpp:125] Batch 98, top-5 = 0.66
I0109 20:29:00.771204  4323 caffe_interface.cpp:125] Batch 99, loss = 5.01173
I0109 20:29:00.771229  4323 caffe_interface.cpp:125] Batch 99, top-1 = 0.3
I0109 20:29:00.771239  4323 caffe_interface.cpp:125] Batch 99, top-5 = 0.56
I0109 20:29:00.776422  4323 caffe_interface.cpp:125] Batch 100, loss = 3.79528
I0109 20:29:00.776445  4323 caffe_interface.cpp:125] Batch 100, top-1 = 0.2
I0109 20:29:00.776455  4323 caffe_interface.cpp:125] Batch 100, top-5 = 0.7
I0109 20:29:00.781453  4323 caffe_interface.cpp:125] Batch 101, loss = 4.25851
I0109 20:29:00.781476  4323 caffe_interface.cpp:125] Batch 101, top-1 = 0.26
I0109 20:29:00.781487  4323 caffe_interface.cpp:125] Batch 101, top-5 = 0.6
I0109 20:29:00.786265  4323 caffe_interface.cpp:125] Batch 102, loss = 3.61284
I0109 20:29:00.786289  4323 caffe_interface.cpp:125] Batch 102, top-1 = 0.26
I0109 20:29:00.786299  4323 caffe_interface.cpp:125] Batch 102, top-5 = 0.76
I0109 20:29:00.791110  4323 caffe_interface.cpp:125] Batch 103, loss = 3.70799
I0109 20:29:00.791132  4323 caffe_interface.cpp:125] Batch 103, top-1 = 0.22
I0109 20:29:00.791143  4323 caffe_interface.cpp:125] Batch 103, top-5 = 0.74
I0109 20:29:00.795938  4323 caffe_interface.cpp:125] Batch 104, loss = 4.91856
I0109 20:29:00.795963  4323 caffe_interface.cpp:125] Batch 104, top-1 = 0.22
I0109 20:29:00.795972  4323 caffe_interface.cpp:125] Batch 104, top-5 = 0.58
I0109 20:29:00.800760  4323 caffe_interface.cpp:125] Batch 105, loss = 4.38419
I0109 20:29:00.800783  4323 caffe_interface.cpp:125] Batch 105, top-1 = 0.2
I0109 20:29:00.800792  4323 caffe_interface.cpp:125] Batch 105, top-5 = 0.64
I0109 20:29:00.805559  4323 caffe_interface.cpp:125] Batch 106, loss = 4.61999
I0109 20:29:00.805584  4323 caffe_interface.cpp:125] Batch 106, top-1 = 0.16
I0109 20:29:00.805608  4323 caffe_interface.cpp:125] Batch 106, top-5 = 0.64
I0109 20:29:00.810401  4323 caffe_interface.cpp:125] Batch 107, loss = 4.33972
I0109 20:29:00.810425  4323 caffe_interface.cpp:125] Batch 107, top-1 = 0.1
I0109 20:29:00.810436  4323 caffe_interface.cpp:125] Batch 107, top-5 = 0.72
I0109 20:29:00.815219  4323 caffe_interface.cpp:125] Batch 108, loss = 3.65836
I0109 20:29:00.815243  4323 caffe_interface.cpp:125] Batch 108, top-1 = 0.24
I0109 20:29:00.815253  4323 caffe_interface.cpp:125] Batch 108, top-5 = 0.82
I0109 20:29:00.820041  4323 caffe_interface.cpp:125] Batch 109, loss = 4.66468
I0109 20:29:00.820063  4323 caffe_interface.cpp:125] Batch 109, top-1 = 0.26
I0109 20:29:00.820073  4323 caffe_interface.cpp:125] Batch 109, top-5 = 0.56
I0109 20:29:00.824862  4323 caffe_interface.cpp:125] Batch 110, loss = 4.86621
I0109 20:29:00.824887  4323 caffe_interface.cpp:125] Batch 110, top-1 = 0.24
I0109 20:29:00.824898  4323 caffe_interface.cpp:125] Batch 110, top-5 = 0.6
I0109 20:29:00.829680  4323 caffe_interface.cpp:125] Batch 111, loss = 3.69997
I0109 20:29:00.829701  4323 caffe_interface.cpp:125] Batch 111, top-1 = 0.24
I0109 20:29:00.829713  4323 caffe_interface.cpp:125] Batch 111, top-5 = 0.76
I0109 20:29:00.834498  4323 caffe_interface.cpp:125] Batch 112, loss = 5.05981
I0109 20:29:00.834522  4323 caffe_interface.cpp:125] Batch 112, top-1 = 0.2
I0109 20:29:00.834532  4323 caffe_interface.cpp:125] Batch 112, top-5 = 0.56
I0109 20:29:00.839337  4323 caffe_interface.cpp:125] Batch 113, loss = 4.13534
I0109 20:29:00.839360  4323 caffe_interface.cpp:125] Batch 113, top-1 = 0.22
I0109 20:29:00.839371  4323 caffe_interface.cpp:125] Batch 113, top-5 = 0.7
I0109 20:29:00.844122  4323 caffe_interface.cpp:125] Batch 114, loss = 4.78512
I0109 20:29:00.844146  4323 caffe_interface.cpp:125] Batch 114, top-1 = 0.2
I0109 20:29:00.844156  4323 caffe_interface.cpp:125] Batch 114, top-5 = 0.7
I0109 20:29:00.848945  4323 caffe_interface.cpp:125] Batch 115, loss = 5.22371
I0109 20:29:00.848969  4323 caffe_interface.cpp:125] Batch 115, top-1 = 0.22
I0109 20:29:00.848981  4323 caffe_interface.cpp:125] Batch 115, top-5 = 0.58
I0109 20:29:00.853780  4323 caffe_interface.cpp:125] Batch 116, loss = 4.67246
I0109 20:29:00.853802  4323 caffe_interface.cpp:125] Batch 116, top-1 = 0.18
I0109 20:29:00.853813  4323 caffe_interface.cpp:125] Batch 116, top-5 = 0.64
I0109 20:29:00.858595  4323 caffe_interface.cpp:125] Batch 117, loss = 3.64281
I0109 20:29:00.858618  4323 caffe_interface.cpp:125] Batch 117, top-1 = 0.3
I0109 20:29:00.858629  4323 caffe_interface.cpp:125] Batch 117, top-5 = 0.7
I0109 20:29:00.863405  4323 caffe_interface.cpp:125] Batch 118, loss = 4.98683
I0109 20:29:00.863430  4323 caffe_interface.cpp:125] Batch 118, top-1 = 0.14
I0109 20:29:00.863440  4323 caffe_interface.cpp:125] Batch 118, top-5 = 0.62
I0109 20:29:00.868211  4323 caffe_interface.cpp:125] Batch 119, loss = 3.88914
I0109 20:29:00.868234  4323 caffe_interface.cpp:125] Batch 119, top-1 = 0.26
I0109 20:29:00.868247  4323 caffe_interface.cpp:125] Batch 119, top-5 = 0.72
I0109 20:29:00.873020  4323 caffe_interface.cpp:125] Batch 120, loss = 3.9434
I0109 20:29:00.873044  4323 caffe_interface.cpp:125] Batch 120, top-1 = 0.26
I0109 20:29:00.873055  4323 caffe_interface.cpp:125] Batch 120, top-5 = 0.76
I0109 20:29:00.877837  4323 caffe_interface.cpp:125] Batch 121, loss = 4.45485
I0109 20:29:00.877861  4323 caffe_interface.cpp:125] Batch 121, top-1 = 0.18
I0109 20:29:00.877871  4323 caffe_interface.cpp:125] Batch 121, top-5 = 0.56
I0109 20:29:00.882644  4323 caffe_interface.cpp:125] Batch 122, loss = 3.56051
I0109 20:29:00.882668  4323 caffe_interface.cpp:125] Batch 122, top-1 = 0.3
I0109 20:29:00.882679  4323 caffe_interface.cpp:125] Batch 122, top-5 = 0.76
I0109 20:29:00.887452  4323 caffe_interface.cpp:125] Batch 123, loss = 4.23601
I0109 20:29:00.887477  4323 caffe_interface.cpp:125] Batch 123, top-1 = 0.2
I0109 20:29:00.887488  4323 caffe_interface.cpp:125] Batch 123, top-5 = 0.72
I0109 20:29:00.892252  4323 caffe_interface.cpp:125] Batch 124, loss = 4.28732
I0109 20:29:00.892276  4323 caffe_interface.cpp:125] Batch 124, top-1 = 0.28
I0109 20:29:00.892287  4323 caffe_interface.cpp:125] Batch 124, top-5 = 0.62
I0109 20:29:00.897094  4323 caffe_interface.cpp:125] Batch 125, loss = 4.67882
I0109 20:29:00.897119  4323 caffe_interface.cpp:125] Batch 125, top-1 = 0.22
I0109 20:29:00.897128  4323 caffe_interface.cpp:125] Batch 125, top-5 = 0.56
I0109 20:29:00.901901  4323 caffe_interface.cpp:125] Batch 126, loss = 4.46742
I0109 20:29:00.901924  4323 caffe_interface.cpp:125] Batch 126, top-1 = 0.24
I0109 20:29:00.901935  4323 caffe_interface.cpp:125] Batch 126, top-5 = 0.64
I0109 20:29:00.906747  4323 caffe_interface.cpp:125] Batch 127, loss = 4.76772
I0109 20:29:00.906772  4323 caffe_interface.cpp:125] Batch 127, top-1 = 0.18
I0109 20:29:00.906783  4323 caffe_interface.cpp:125] Batch 127, top-5 = 0.64
I0109 20:29:00.911551  4323 caffe_interface.cpp:125] Batch 128, loss = 3.76902
I0109 20:29:00.911576  4323 caffe_interface.cpp:125] Batch 128, top-1 = 0.2
I0109 20:29:00.911584  4323 caffe_interface.cpp:125] Batch 128, top-5 = 0.74
I0109 20:29:00.916357  4323 caffe_interface.cpp:125] Batch 129, loss = 4.51049
I0109 20:29:00.916380  4323 caffe_interface.cpp:125] Batch 129, top-1 = 0.28
I0109 20:29:00.916391  4323 caffe_interface.cpp:125] Batch 129, top-5 = 0.58
I0109 20:29:00.921195  4323 caffe_interface.cpp:125] Batch 130, loss = 5.10454
I0109 20:29:00.921237  4323 caffe_interface.cpp:125] Batch 130, top-1 = 0.2
I0109 20:29:00.921249  4323 caffe_interface.cpp:125] Batch 130, top-5 = 0.58
I0109 20:29:00.926056  4323 caffe_interface.cpp:125] Batch 131, loss = 3.55107
I0109 20:29:00.926081  4323 caffe_interface.cpp:125] Batch 131, top-1 = 0.2
I0109 20:29:00.926092  4323 caffe_interface.cpp:125] Batch 131, top-5 = 0.78
I0109 20:29:00.930898  4323 caffe_interface.cpp:125] Batch 132, loss = 3.82534
I0109 20:29:00.930922  4323 caffe_interface.cpp:125] Batch 132, top-1 = 0.24
I0109 20:29:00.930932  4323 caffe_interface.cpp:125] Batch 132, top-5 = 0.82
I0109 20:29:00.935761  4323 caffe_interface.cpp:125] Batch 133, loss = 3.27773
I0109 20:29:00.935784  4323 caffe_interface.cpp:125] Batch 133, top-1 = 0.4
I0109 20:29:00.935794  4323 caffe_interface.cpp:125] Batch 133, top-5 = 0.78
I0109 20:29:00.940596  4323 caffe_interface.cpp:125] Batch 134, loss = 3.99813
I0109 20:29:00.940621  4323 caffe_interface.cpp:125] Batch 134, top-1 = 0.32
I0109 20:29:00.940632  4323 caffe_interface.cpp:125] Batch 134, top-5 = 0.68
I0109 20:29:00.945380  4323 caffe_interface.cpp:125] Batch 135, loss = 4.74478
I0109 20:29:00.945403  4323 caffe_interface.cpp:125] Batch 135, top-1 = 0.14
I0109 20:29:00.945415  4323 caffe_interface.cpp:125] Batch 135, top-5 = 0.58
I0109 20:29:00.950075  4323 caffe_interface.cpp:125] Batch 136, loss = 4.7364
I0109 20:29:00.950101  4323 caffe_interface.cpp:125] Batch 136, top-1 = 0.16
I0109 20:29:00.950111  4323 caffe_interface.cpp:125] Batch 136, top-5 = 0.62
I0109 20:29:00.954610  4323 caffe_interface.cpp:125] Batch 137, loss = 5.29599
I0109 20:29:00.954635  4323 caffe_interface.cpp:125] Batch 137, top-1 = 0.16
I0109 20:29:00.954646  4323 caffe_interface.cpp:125] Batch 137, top-5 = 0.48
I0109 20:29:00.959149  4323 caffe_interface.cpp:125] Batch 138, loss = 4.09342
I0109 20:29:00.959174  4323 caffe_interface.cpp:125] Batch 138, top-1 = 0.28
I0109 20:29:00.959184  4323 caffe_interface.cpp:125] Batch 138, top-5 = 0.66
I0109 20:29:00.963686  4323 caffe_interface.cpp:125] Batch 139, loss = 4.35375
I0109 20:29:00.963711  4323 caffe_interface.cpp:125] Batch 139, top-1 = 0.18
I0109 20:29:00.963723  4323 caffe_interface.cpp:125] Batch 139, top-5 = 0.72
I0109 20:29:00.968215  4323 caffe_interface.cpp:125] Batch 140, loss = 4.42185
I0109 20:29:00.968240  4323 caffe_interface.cpp:125] Batch 140, top-1 = 0.22
I0109 20:29:00.968250  4323 caffe_interface.cpp:125] Batch 140, top-5 = 0.58
I0109 20:29:00.972753  4323 caffe_interface.cpp:125] Batch 141, loss = 3.93197
I0109 20:29:00.972777  4323 caffe_interface.cpp:125] Batch 141, top-1 = 0.32
I0109 20:29:00.972787  4323 caffe_interface.cpp:125] Batch 141, top-5 = 0.72
I0109 20:29:00.977313  4323 caffe_interface.cpp:125] Batch 142, loss = 4.24681
I0109 20:29:00.977339  4323 caffe_interface.cpp:125] Batch 142, top-1 = 0.2
I0109 20:29:00.977349  4323 caffe_interface.cpp:125] Batch 142, top-5 = 0.66
I0109 20:29:00.981844  4323 caffe_interface.cpp:125] Batch 143, loss = 4.4886
I0109 20:29:00.981868  4323 caffe_interface.cpp:125] Batch 143, top-1 = 0.2
I0109 20:29:00.981879  4323 caffe_interface.cpp:125] Batch 143, top-5 = 0.64
I0109 20:29:00.986385  4323 caffe_interface.cpp:125] Batch 144, loss = 3.91415
I0109 20:29:00.986410  4323 caffe_interface.cpp:125] Batch 144, top-1 = 0.28
I0109 20:29:00.986420  4323 caffe_interface.cpp:125] Batch 144, top-5 = 0.7
I0109 20:29:00.990914  4323 caffe_interface.cpp:125] Batch 145, loss = 4.27975
I0109 20:29:00.990938  4323 caffe_interface.cpp:125] Batch 145, top-1 = 0.2
I0109 20:29:00.990947  4323 caffe_interface.cpp:125] Batch 145, top-5 = 0.64
I0109 20:29:00.995470  4323 caffe_interface.cpp:125] Batch 146, loss = 3.35615
I0109 20:29:00.995496  4323 caffe_interface.cpp:125] Batch 146, top-1 = 0.26
I0109 20:29:00.995506  4323 caffe_interface.cpp:125] Batch 146, top-5 = 0.72
I0109 20:29:01.000005  4323 caffe_interface.cpp:125] Batch 147, loss = 4.17885
I0109 20:29:01.000030  4323 caffe_interface.cpp:125] Batch 147, top-1 = 0.2
I0109 20:29:01.000042  4323 caffe_interface.cpp:125] Batch 147, top-5 = 0.68
I0109 20:29:01.004590  4323 caffe_interface.cpp:125] Batch 148, loss = 3.37768
I0109 20:29:01.004616  4323 caffe_interface.cpp:125] Batch 148, top-1 = 0.32
I0109 20:29:01.004626  4323 caffe_interface.cpp:125] Batch 148, top-5 = 0.82
I0109 20:29:01.009124  4323 caffe_interface.cpp:125] Batch 149, loss = 3.62056
I0109 20:29:01.009150  4323 caffe_interface.cpp:125] Batch 149, top-1 = 0.32
I0109 20:29:01.009161  4323 caffe_interface.cpp:125] Batch 149, top-5 = 0.74
I0109 20:29:01.013669  4323 caffe_interface.cpp:125] Batch 150, loss = 4.17019
I0109 20:29:01.013692  4323 caffe_interface.cpp:125] Batch 150, top-1 = 0.26
I0109 20:29:01.013703  4323 caffe_interface.cpp:125] Batch 150, top-5 = 0.72
I0109 20:29:01.018185  4323 caffe_interface.cpp:125] Batch 151, loss = 4.95728
I0109 20:29:01.018210  4323 caffe_interface.cpp:125] Batch 151, top-1 = 0.14
I0109 20:29:01.018221  4323 caffe_interface.cpp:125] Batch 151, top-5 = 0.64
I0109 20:29:01.022728  4323 caffe_interface.cpp:125] Batch 152, loss = 3.92967
I0109 20:29:01.022753  4323 caffe_interface.cpp:125] Batch 152, top-1 = 0.26
I0109 20:29:01.022763  4323 caffe_interface.cpp:125] Batch 152, top-5 = 0.62
I0109 20:29:01.027297  4323 caffe_interface.cpp:125] Batch 153, loss = 4.51094
I0109 20:29:01.027320  4323 caffe_interface.cpp:125] Batch 153, top-1 = 0.24
I0109 20:29:01.027331  4323 caffe_interface.cpp:125] Batch 153, top-5 = 0.58
I0109 20:29:01.031841  4323 caffe_interface.cpp:125] Batch 154, loss = 4.09556
I0109 20:29:01.031867  4323 caffe_interface.cpp:125] Batch 154, top-1 = 0.24
I0109 20:29:01.031877  4323 caffe_interface.cpp:125] Batch 154, top-5 = 0.68
I0109 20:29:01.036376  4323 caffe_interface.cpp:125] Batch 155, loss = 4.88811
I0109 20:29:01.036401  4323 caffe_interface.cpp:125] Batch 155, top-1 = 0.22
I0109 20:29:01.036412  4323 caffe_interface.cpp:125] Batch 155, top-5 = 0.52
I0109 20:29:01.040912  4323 caffe_interface.cpp:125] Batch 156, loss = 3.88588
I0109 20:29:01.040938  4323 caffe_interface.cpp:125] Batch 156, top-1 = 0.3
I0109 20:29:01.040948  4323 caffe_interface.cpp:125] Batch 156, top-5 = 0.78
I0109 20:29:01.045415  4323 caffe_interface.cpp:125] Batch 157, loss = 4.31885
I0109 20:29:01.045439  4323 caffe_interface.cpp:125] Batch 157, top-1 = 0.24
I0109 20:29:01.045450  4323 caffe_interface.cpp:125] Batch 157, top-5 = 0.68
I0109 20:29:01.049918  4323 caffe_interface.cpp:125] Batch 158, loss = 4.25919
I0109 20:29:01.049945  4323 caffe_interface.cpp:125] Batch 158, top-1 = 0.14
I0109 20:29:01.049957  4323 caffe_interface.cpp:125] Batch 158, top-5 = 0.7
I0109 20:29:01.054447  4323 caffe_interface.cpp:125] Batch 159, loss = 4.32395
I0109 20:29:01.054473  4323 caffe_interface.cpp:125] Batch 159, top-1 = 0.18
I0109 20:29:01.054484  4323 caffe_interface.cpp:125] Batch 159, top-5 = 0.64
I0109 20:29:01.058991  4323 caffe_interface.cpp:125] Batch 160, loss = 4.88722
I0109 20:29:01.059016  4323 caffe_interface.cpp:125] Batch 160, top-1 = 0.24
I0109 20:29:01.059027  4323 caffe_interface.cpp:125] Batch 160, top-5 = 0.6
I0109 20:29:01.063513  4323 caffe_interface.cpp:125] Batch 161, loss = 3.44978
I0109 20:29:01.063539  4323 caffe_interface.cpp:125] Batch 161, top-1 = 0.2
I0109 20:29:01.063549  4323 caffe_interface.cpp:125] Batch 161, top-5 = 0.72
I0109 20:29:01.068073  4323 caffe_interface.cpp:125] Batch 162, loss = 4.69301
I0109 20:29:01.068099  4323 caffe_interface.cpp:125] Batch 162, top-1 = 0.22
I0109 20:29:01.068110  4323 caffe_interface.cpp:125] Batch 162, top-5 = 0.6
I0109 20:29:01.072579  4323 caffe_interface.cpp:125] Batch 163, loss = 4.59016
I0109 20:29:01.072605  4323 caffe_interface.cpp:125] Batch 163, top-1 = 0.18
I0109 20:29:01.072618  4323 caffe_interface.cpp:125] Batch 163, top-5 = 0.56
I0109 20:29:01.077121  4323 caffe_interface.cpp:125] Batch 164, loss = 4.82998
I0109 20:29:01.077145  4323 caffe_interface.cpp:125] Batch 164, top-1 = 0.16
I0109 20:29:01.077157  4323 caffe_interface.cpp:125] Batch 164, top-5 = 0.56
I0109 20:29:01.081682  4323 caffe_interface.cpp:125] Batch 165, loss = 4.75732
I0109 20:29:01.081704  4323 caffe_interface.cpp:125] Batch 165, top-1 = 0.2
I0109 20:29:01.081742  4323 caffe_interface.cpp:125] Batch 165, top-5 = 0.58
I0109 20:29:01.086233  4323 caffe_interface.cpp:125] Batch 166, loss = 4.1125
I0109 20:29:01.086258  4323 caffe_interface.cpp:125] Batch 166, top-1 = 0.26
I0109 20:29:01.086267  4323 caffe_interface.cpp:125] Batch 166, top-5 = 0.64
I0109 20:29:01.090740  4323 caffe_interface.cpp:125] Batch 167, loss = 3.94523
I0109 20:29:01.090764  4323 caffe_interface.cpp:125] Batch 167, top-1 = 0.24
I0109 20:29:01.090775  4323 caffe_interface.cpp:125] Batch 167, top-5 = 0.66
I0109 20:29:01.095283  4323 caffe_interface.cpp:125] Batch 168, loss = 3.99582
I0109 20:29:01.095309  4323 caffe_interface.cpp:125] Batch 168, top-1 = 0.3
I0109 20:29:01.095319  4323 caffe_interface.cpp:125] Batch 168, top-5 = 0.66
I0109 20:29:01.099833  4323 caffe_interface.cpp:125] Batch 169, loss = 4.46636
I0109 20:29:01.099858  4323 caffe_interface.cpp:125] Batch 169, top-1 = 0.3
I0109 20:29:01.099867  4323 caffe_interface.cpp:125] Batch 169, top-5 = 0.68
I0109 20:29:01.104372  4323 caffe_interface.cpp:125] Batch 170, loss = 4.1264
I0109 20:29:01.104398  4323 caffe_interface.cpp:125] Batch 170, top-1 = 0.3
I0109 20:29:01.104408  4323 caffe_interface.cpp:125] Batch 170, top-5 = 0.66
I0109 20:29:01.108920  4323 caffe_interface.cpp:125] Batch 171, loss = 4.81572
I0109 20:29:01.108945  4323 caffe_interface.cpp:125] Batch 171, top-1 = 0.26
I0109 20:29:01.108958  4323 caffe_interface.cpp:125] Batch 171, top-5 = 0.64
I0109 20:29:01.113461  4323 caffe_interface.cpp:125] Batch 172, loss = 3.9987
I0109 20:29:01.113487  4323 caffe_interface.cpp:125] Batch 172, top-1 = 0.16
I0109 20:29:01.113497  4323 caffe_interface.cpp:125] Batch 172, top-5 = 0.68
I0109 20:29:01.117950  4323 caffe_interface.cpp:125] Batch 173, loss = 3.99361
I0109 20:29:01.117975  4323 caffe_interface.cpp:125] Batch 173, top-1 = 0.2
I0109 20:29:01.117985  4323 caffe_interface.cpp:125] Batch 173, top-5 = 0.68
I0109 20:29:01.122437  4323 caffe_interface.cpp:125] Batch 174, loss = 4.02522
I0109 20:29:01.122462  4323 caffe_interface.cpp:125] Batch 174, top-1 = 0.36
I0109 20:29:01.122473  4323 caffe_interface.cpp:125] Batch 174, top-5 = 0.68
I0109 20:29:01.126890  4323 caffe_interface.cpp:125] Batch 175, loss = 4.26488
I0109 20:29:01.126915  4323 caffe_interface.cpp:125] Batch 175, top-1 = 0.24
I0109 20:29:01.126926  4323 caffe_interface.cpp:125] Batch 175, top-5 = 0.54
I0109 20:29:01.131363  4323 caffe_interface.cpp:125] Batch 176, loss = 4.21486
I0109 20:29:01.131387  4323 caffe_interface.cpp:125] Batch 176, top-1 = 0.24
I0109 20:29:01.131397  4323 caffe_interface.cpp:125] Batch 176, top-5 = 0.62
I0109 20:29:01.135838  4323 caffe_interface.cpp:125] Batch 177, loss = 4.09033
I0109 20:29:01.135864  4323 caffe_interface.cpp:125] Batch 177, top-1 = 0.24
I0109 20:29:01.135874  4323 caffe_interface.cpp:125] Batch 177, top-5 = 0.64
I0109 20:29:01.140269  4323 caffe_interface.cpp:125] Batch 178, loss = 3.79264
I0109 20:29:01.140295  4323 caffe_interface.cpp:125] Batch 178, top-1 = 0.3
I0109 20:29:01.140305  4323 caffe_interface.cpp:125] Batch 178, top-5 = 0.68
I0109 20:29:01.144704  4323 caffe_interface.cpp:125] Batch 179, loss = 4.24047
I0109 20:29:01.144729  4323 caffe_interface.cpp:125] Batch 179, top-1 = 0.24
I0109 20:29:01.144742  4323 caffe_interface.cpp:125] Batch 179, top-5 = 0.64
I0109 20:29:01.144748  4323 caffe_interface.cpp:130] Loss: 4.22841
I0109 20:29:01.144762  4323 caffe_interface.cpp:142] loss = 4.22841 (* 1 = 4.22841 loss)
I0109 20:29:01.144773  4323 caffe_interface.cpp:142] top-1 = 0.237556
I0109 20:29:01.144791  4323 caffe_interface.cpp:142] top-5 = 0.664333
I0109 20:29:01.159301  4323 pruning_runner.cpp:306] pruning done, output model: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/sparse.caffemodel
I0109 20:29:01.159350  4323 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.7:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.864999831    | 0.237555638    | -0.627444208   |
+-------------------------------------------------------------------+
| Weights        | 68389          | 25849          | -62.2029915%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 14499840       | -70.4408798%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config config7.prototxt
I0109 20:29:01.278772  4467 deephi_compress.cpp:236] /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0109 20:29:01.392046  4467 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 20:29:01.392671  4467 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 20:29:01.392704  4467 caffe_interface.cpp:493] Using GPUs 0
I0109 20:29:01.393007  4467 caffe_interface.cpp:498] GPU 0: Tesla K80
I0109 20:29:02.063366  4467 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "/home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt"
type: "SGD"
I0109 20:29:02.063576  4467 solver.cpp:99] Creating training net from net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0109 20:29:02.063958  4467 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0109 20:29:02.063995  4467 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0109 20:29:02.064004  4467 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0109 20:29:02.064210  4467 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0109 20:29:02.064338  4467 layer_factory.hpp:77] Creating layer data
I0109 20:29:02.064564  4467 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:02.065428  4467 net.cpp:94] Creating Layer data
I0109 20:29:02.065461  4467 net.cpp:409] data -> data
I0109 20:29:02.065485  4467 net.cpp:409] data -> label
I0109 20:29:02.066676  4478 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/train_lmdb
I0109 20:29:02.066726  4478 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0109 20:29:02.066860  4467 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0109 20:29:02.067018  4467 data_layer.cpp:83] output data size: 128,3,32,32
I0109 20:29:02.078830  4467 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:02.078904  4467 net.cpp:144] Setting up data
I0109 20:29:02.078927  4467 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0109 20:29:02.078938  4467 net.cpp:151] Top shape: 128 (128)
I0109 20:29:02.078944  4467 net.cpp:159] Memory required for data: 1573376
I0109 20:29:02.078953  4467 layer_factory.hpp:77] Creating layer conv1
I0109 20:29:02.078989  4467 net.cpp:94] Creating Layer conv1
I0109 20:29:02.079003  4467 net.cpp:435] conv1 <- data
I0109 20:29:02.079035  4467 net.cpp:409] conv1 -> conv1
I0109 20:29:02.080271  4467 net.cpp:144] Setting up conv1
I0109 20:29:02.080294  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.080302  4467 net.cpp:159] Memory required for data: 18350592
I0109 20:29:02.080333  4467 layer_factory.hpp:77] Creating layer bn1
I0109 20:29:02.080360  4467 net.cpp:94] Creating Layer bn1
I0109 20:29:02.080369  4467 net.cpp:435] bn1 <- conv1
I0109 20:29:02.080386  4467 net.cpp:409] bn1 -> scale1
I0109 20:29:02.081198  4467 net.cpp:144] Setting up bn1
I0109 20:29:02.081218  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.081224  4467 net.cpp:159] Memory required for data: 35127808
I0109 20:29:02.081248  4467 layer_factory.hpp:77] Creating layer relu1
I0109 20:29:02.081264  4467 net.cpp:94] Creating Layer relu1
I0109 20:29:02.081274  4467 net.cpp:435] relu1 <- scale1
I0109 20:29:02.081286  4467 net.cpp:409] relu1 -> relu1
I0109 20:29:02.081511  4467 net.cpp:144] Setting up relu1
I0109 20:29:02.081528  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.081535  4467 net.cpp:159] Memory required for data: 51905024
I0109 20:29:02.081542  4467 layer_factory.hpp:77] Creating layer conv2
I0109 20:29:02.081560  4467 net.cpp:94] Creating Layer conv2
I0109 20:29:02.081615  4467 net.cpp:435] conv2 <- relu1
I0109 20:29:02.081631  4467 net.cpp:409] conv2 -> conv2
I0109 20:29:02.083113  4467 net.cpp:144] Setting up conv2
I0109 20:29:02.083135  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.083144  4467 net.cpp:159] Memory required for data: 68682240
I0109 20:29:02.083161  4467 layer_factory.hpp:77] Creating layer bn2
I0109 20:29:02.083230  4467 net.cpp:94] Creating Layer bn2
I0109 20:29:02.083240  4467 net.cpp:435] bn2 <- conv2
I0109 20:29:02.083256  4467 net.cpp:409] bn2 -> scale2
I0109 20:29:02.084359  4467 net.cpp:144] Setting up bn2
I0109 20:29:02.084393  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.084401  4467 net.cpp:159] Memory required for data: 85459456
I0109 20:29:02.084420  4467 layer_factory.hpp:77] Creating layer relu2
I0109 20:29:02.084432  4467 net.cpp:94] Creating Layer relu2
I0109 20:29:02.084439  4467 net.cpp:435] relu2 <- scale2
I0109 20:29:02.084450  4467 net.cpp:409] relu2 -> relu2
I0109 20:29:02.084484  4467 net.cpp:144] Setting up relu2
I0109 20:29:02.084496  4467 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0109 20:29:02.084501  4467 net.cpp:159] Memory required for data: 102236672
I0109 20:29:02.084506  4467 layer_factory.hpp:77] Creating layer pool1
I0109 20:29:02.084519  4467 net.cpp:94] Creating Layer pool1
I0109 20:29:02.084527  4467 net.cpp:435] pool1 <- relu2
I0109 20:29:02.084535  4467 net.cpp:409] pool1 -> pool1
I0109 20:29:02.084595  4467 net.cpp:144] Setting up pool1
I0109 20:29:02.084607  4467 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:29:02.084614  4467 net.cpp:159] Memory required for data: 106430976
I0109 20:29:02.084619  4467 layer_factory.hpp:77] Creating layer drop1
I0109 20:29:02.084632  4467 net.cpp:94] Creating Layer drop1
I0109 20:29:02.084661  4467 net.cpp:435] drop1 <- pool1
I0109 20:29:02.084673  4467 net.cpp:409] drop1 -> drop1
I0109 20:29:02.084825  4467 net.cpp:144] Setting up drop1
I0109 20:29:02.084847  4467 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0109 20:29:02.084856  4467 net.cpp:159] Memory required for data: 110625280
I0109 20:29:02.084863  4467 layer_factory.hpp:77] Creating layer conv3
I0109 20:29:02.084937  4467 net.cpp:94] Creating Layer conv3
I0109 20:29:02.084956  4467 net.cpp:435] conv3 <- drop1
I0109 20:29:02.084972  4467 net.cpp:409] conv3 -> conv3
I0109 20:29:02.086427  4467 net.cpp:144] Setting up conv3
I0109 20:29:02.086450  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.086458  4467 net.cpp:159] Memory required for data: 119013888
I0109 20:29:02.086472  4467 layer_factory.hpp:77] Creating layer bn3
I0109 20:29:02.086551  4467 net.cpp:94] Creating Layer bn3
I0109 20:29:02.086570  4467 net.cpp:435] bn3 <- conv3
I0109 20:29:02.086586  4467 net.cpp:409] bn3 -> scale3
I0109 20:29:02.087584  4467 net.cpp:144] Setting up bn3
I0109 20:29:02.087623  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.087644  4467 net.cpp:159] Memory required for data: 127402496
I0109 20:29:02.087682  4467 layer_factory.hpp:77] Creating layer relu3
I0109 20:29:02.087752  4467 net.cpp:94] Creating Layer relu3
I0109 20:29:02.087771  4467 net.cpp:435] relu3 <- scale3
I0109 20:29:02.087785  4467 net.cpp:409] relu3 -> relu3
I0109 20:29:02.087862  4467 net.cpp:144] Setting up relu3
I0109 20:29:02.087883  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.087891  4467 net.cpp:159] Memory required for data: 135791104
I0109 20:29:02.087898  4467 layer_factory.hpp:77] Creating layer conv4
I0109 20:29:02.087914  4467 net.cpp:94] Creating Layer conv4
I0109 20:29:02.087970  4467 net.cpp:435] conv4 <- relu3
I0109 20:29:02.087985  4467 net.cpp:409] conv4 -> conv4
I0109 20:29:02.088804  4467 net.cpp:144] Setting up conv4
I0109 20:29:02.088840  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.088860  4467 net.cpp:159] Memory required for data: 144179712
I0109 20:29:02.088886  4467 layer_factory.hpp:77] Creating layer bn4
I0109 20:29:02.088914  4467 net.cpp:94] Creating Layer bn4
I0109 20:29:02.088935  4467 net.cpp:435] bn4 <- conv4
I0109 20:29:02.088961  4467 net.cpp:409] bn4 -> scale4
I0109 20:29:02.090199  4467 net.cpp:144] Setting up bn4
I0109 20:29:02.090222  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.090230  4467 net.cpp:159] Memory required for data: 152568320
I0109 20:29:02.090248  4467 layer_factory.hpp:77] Creating layer relu4
I0109 20:29:02.090262  4467 net.cpp:94] Creating Layer relu4
I0109 20:29:02.090270  4467 net.cpp:435] relu4 <- scale4
I0109 20:29:02.090281  4467 net.cpp:409] relu4 -> relu4
I0109 20:29:02.090427  4467 net.cpp:144] Setting up relu4
I0109 20:29:02.090454  4467 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0109 20:29:02.090461  4467 net.cpp:159] Memory required for data: 160956928
I0109 20:29:02.090473  4467 layer_factory.hpp:77] Creating layer pool2
I0109 20:29:02.090487  4467 net.cpp:94] Creating Layer pool2
I0109 20:29:02.090497  4467 net.cpp:435] pool2 <- relu4
I0109 20:29:02.090519  4467 net.cpp:409] pool2 -> pool2
I0109 20:29:02.090585  4467 net.cpp:144] Setting up pool2
I0109 20:29:02.090613  4467 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:29:02.090623  4467 net.cpp:159] Memory required for data: 163054080
I0109 20:29:02.090683  4467 layer_factory.hpp:77] Creating layer drop2
I0109 20:29:02.090703  4467 net.cpp:94] Creating Layer drop2
I0109 20:29:02.090714  4467 net.cpp:435] drop2 <- pool2
I0109 20:29:02.090725  4467 net.cpp:409] drop2 -> drop2
I0109 20:29:02.090785  4467 net.cpp:144] Setting up drop2
I0109 20:29:02.090800  4467 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0109 20:29:02.090806  4467 net.cpp:159] Memory required for data: 165151232
I0109 20:29:02.090812  4467 layer_factory.hpp:77] Creating layer fc1
I0109 20:29:02.090829  4467 net.cpp:94] Creating Layer fc1
I0109 20:29:02.090837  4467 net.cpp:435] fc1 <- drop2
I0109 20:29:02.090865  4467 net.cpp:409] fc1 -> fc1
I0109 20:29:02.114408  4467 net.cpp:144] Setting up fc1
I0109 20:29:02.114450  4467 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:29:02.114459  4467 net.cpp:159] Memory required for data: 165413376
I0109 20:29:02.114475  4467 layer_factory.hpp:77] Creating layer bn5
I0109 20:29:02.114495  4467 net.cpp:94] Creating Layer bn5
I0109 20:29:02.114503  4467 net.cpp:435] bn5 <- fc1
I0109 20:29:02.114524  4467 net.cpp:409] bn5 -> scale5
I0109 20:29:02.115177  4467 net.cpp:144] Setting up bn5
I0109 20:29:02.115198  4467 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:29:02.115206  4467 net.cpp:159] Memory required for data: 165675520
I0109 20:29:02.115233  4467 layer_factory.hpp:77] Creating layer relu5
I0109 20:29:02.115252  4467 net.cpp:94] Creating Layer relu5
I0109 20:29:02.115260  4467 net.cpp:435] relu5 <- scale5
I0109 20:29:02.115269  4467 net.cpp:409] relu5 -> relu5
I0109 20:29:02.115334  4467 net.cpp:144] Setting up relu5
I0109 20:29:02.115352  4467 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:29:02.115356  4467 net.cpp:159] Memory required for data: 165937664
I0109 20:29:02.115360  4467 layer_factory.hpp:77] Creating layer drop3
I0109 20:29:02.115370  4467 net.cpp:94] Creating Layer drop3
I0109 20:29:02.115386  4467 net.cpp:435] drop3 <- relu5
I0109 20:29:02.115399  4467 net.cpp:409] drop3 -> drop3
I0109 20:29:02.115468  4467 net.cpp:144] Setting up drop3
I0109 20:29:02.115486  4467 net.cpp:151] Top shape: 128 512 (65536)
I0109 20:29:02.115494  4467 net.cpp:159] Memory required for data: 166199808
I0109 20:29:02.115501  4467 layer_factory.hpp:77] Creating layer fc2
I0109 20:29:02.115514  4467 net.cpp:94] Creating Layer fc2
I0109 20:29:02.115522  4467 net.cpp:435] fc2 <- drop3
I0109 20:29:02.115536  4467 net.cpp:409] fc2 -> fc2
I0109 20:29:02.115730  4467 net.cpp:144] Setting up fc2
I0109 20:29:02.115746  4467 net.cpp:151] Top shape: 128 10 (1280)
I0109 20:29:02.115752  4467 net.cpp:159] Memory required for data: 166204928
I0109 20:29:02.115764  4467 layer_factory.hpp:77] Creating layer loss
I0109 20:29:02.115779  4467 net.cpp:94] Creating Layer loss
I0109 20:29:02.115792  4467 net.cpp:435] loss <- fc2
I0109 20:29:02.115801  4467 net.cpp:435] loss <- label
I0109 20:29:02.115813  4467 net.cpp:409] loss -> loss
I0109 20:29:02.115844  4467 layer_factory.hpp:77] Creating layer loss
I0109 20:29:02.116725  4467 net.cpp:144] Setting up loss
I0109 20:29:02.116747  4467 net.cpp:151] Top shape: (1)
I0109 20:29:02.116752  4467 net.cpp:154]     with loss weight 1
I0109 20:29:02.116781  4467 net.cpp:159] Memory required for data: 166204932
I0109 20:29:02.116791  4467 net.cpp:220] loss needs backward computation.
I0109 20:29:02.116827  4467 net.cpp:220] fc2 needs backward computation.
I0109 20:29:02.116837  4467 net.cpp:220] drop3 needs backward computation.
I0109 20:29:02.116843  4467 net.cpp:220] relu5 needs backward computation.
I0109 20:29:02.116849  4467 net.cpp:220] bn5 needs backward computation.
I0109 20:29:02.116855  4467 net.cpp:220] fc1 needs backward computation.
I0109 20:29:02.116864  4467 net.cpp:220] drop2 needs backward computation.
I0109 20:29:02.116871  4467 net.cpp:220] pool2 needs backward computation.
I0109 20:29:02.116879  4467 net.cpp:220] relu4 needs backward computation.
I0109 20:29:02.116886  4467 net.cpp:220] bn4 needs backward computation.
I0109 20:29:02.116895  4467 net.cpp:220] conv4 needs backward computation.
I0109 20:29:02.116902  4467 net.cpp:220] relu3 needs backward computation.
I0109 20:29:02.116909  4467 net.cpp:220] bn3 needs backward computation.
I0109 20:29:02.116916  4467 net.cpp:220] conv3 needs backward computation.
I0109 20:29:02.116925  4467 net.cpp:220] drop1 needs backward computation.
I0109 20:29:02.116932  4467 net.cpp:220] pool1 needs backward computation.
I0109 20:29:02.116938  4467 net.cpp:220] relu2 needs backward computation.
I0109 20:29:02.116946  4467 net.cpp:220] bn2 needs backward computation.
I0109 20:29:02.116955  4467 net.cpp:220] conv2 needs backward computation.
I0109 20:29:02.116961  4467 net.cpp:220] relu1 needs backward computation.
I0109 20:29:02.116991  4467 net.cpp:220] bn1 needs backward computation.
I0109 20:29:02.116999  4467 net.cpp:220] conv1 needs backward computation.
I0109 20:29:02.117008  4467 net.cpp:222] data does not need backward computation.
I0109 20:29:02.117014  4467 net.cpp:264] This network produces output loss
I0109 20:29:02.117048  4467 net.cpp:284] Network initialization done.
I0109 20:29:02.117432  4467 solver.cpp:189] Creating test net (#0) specified by net file: /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0109 20:29:02.117503  4467 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:29:02.117758  4467 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:29:02.117908  4467 layer_factory.hpp:77] Creating layer data
I0109 20:29:02.117980  4467 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:02.118244  4467 net.cpp:94] Creating Layer data
I0109 20:29:02.118266  4467 net.cpp:409] data -> data
I0109 20:29:02.118280  4467 net.cpp:409] data -> label
I0109 20:29:02.119493  4484 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:29:02.119534  4484 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:29:02.119673  4467 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:29:02.119827  4467 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:29:02.127813  4467 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:29:02.127885  4467 net.cpp:144] Setting up data
I0109 20:29:02.127945  4467 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:29:02.127959  4467 net.cpp:151] Top shape: 50 (50)
I0109 20:29:02.127964  4467 net.cpp:159] Memory required for data: 614600
I0109 20:29:02.127974  4467 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:29:02.127990  4467 net.cpp:94] Creating Layer label_data_1_split
I0109 20:29:02.127997  4467 net.cpp:435] label_data_1_split <- label
I0109 20:29:02.128013  4467 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:29:02.128029  4467 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:29:02.128041  4467 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:29:02.128172  4467 net.cpp:144] Setting up label_data_1_split
I0109 20:29:02.128191  4467 net.cpp:151] Top shape: 50 (50)
I0109 20:29:02.128201  4467 net.cpp:151] Top shape: 50 (50)
I0109 20:29:02.128208  4467 net.cpp:151] Top shape: 50 (50)
I0109 20:29:02.128213  4467 net.cpp:159] Memory required for data: 615200
I0109 20:29:02.128221  4467 layer_factory.hpp:77] Creating layer conv1
I0109 20:29:02.128244  4467 net.cpp:94] Creating Layer conv1
I0109 20:29:02.128253  4467 net.cpp:435] conv1 <- data
I0109 20:29:02.128268  4467 net.cpp:409] conv1 -> conv1
I0109 20:29:02.128599  4467 net.cpp:144] Setting up conv1
I0109 20:29:02.128620  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.128628  4467 net.cpp:159] Memory required for data: 7168800
I0109 20:29:02.128643  4467 layer_factory.hpp:77] Creating layer bn1
I0109 20:29:02.128661  4467 net.cpp:94] Creating Layer bn1
I0109 20:29:02.128670  4467 net.cpp:435] bn1 <- conv1
I0109 20:29:02.128682  4467 net.cpp:409] bn1 -> scale1
I0109 20:29:02.129789  4467 net.cpp:144] Setting up bn1
I0109 20:29:02.129808  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.129817  4467 net.cpp:159] Memory required for data: 13722400
I0109 20:29:02.129837  4467 layer_factory.hpp:77] Creating layer relu1
I0109 20:29:02.129870  4467 net.cpp:94] Creating Layer relu1
I0109 20:29:02.129884  4467 net.cpp:435] relu1 <- scale1
I0109 20:29:02.129894  4467 net.cpp:409] relu1 -> relu1
I0109 20:29:02.130096  4467 net.cpp:144] Setting up relu1
I0109 20:29:02.130116  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.130125  4467 net.cpp:159] Memory required for data: 20276000
I0109 20:29:02.130131  4467 layer_factory.hpp:77] Creating layer conv2
I0109 20:29:02.130148  4467 net.cpp:94] Creating Layer conv2
I0109 20:29:02.130157  4467 net.cpp:435] conv2 <- relu1
I0109 20:29:02.130169  4467 net.cpp:409] conv2 -> conv2
I0109 20:29:02.130669  4467 net.cpp:144] Setting up conv2
I0109 20:29:02.130688  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.130695  4467 net.cpp:159] Memory required for data: 26829600
I0109 20:29:02.130712  4467 layer_factory.hpp:77] Creating layer bn2
I0109 20:29:02.130731  4467 net.cpp:94] Creating Layer bn2
I0109 20:29:02.130739  4467 net.cpp:435] bn2 <- conv2
I0109 20:29:02.130753  4467 net.cpp:409] bn2 -> scale2
I0109 20:29:02.131944  4467 net.cpp:144] Setting up bn2
I0109 20:29:02.131965  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.131973  4467 net.cpp:159] Memory required for data: 33383200
I0109 20:29:02.132021  4467 layer_factory.hpp:77] Creating layer relu2
I0109 20:29:02.132071  4467 net.cpp:94] Creating Layer relu2
I0109 20:29:02.132120  4467 net.cpp:435] relu2 <- scale2
I0109 20:29:02.132186  4467 net.cpp:409] relu2 -> relu2
I0109 20:29:02.132275  4467 net.cpp:144] Setting up relu2
I0109 20:29:02.132326  4467 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:29:02.132369  4467 net.cpp:159] Memory required for data: 39936800
I0109 20:29:02.132411  4467 layer_factory.hpp:77] Creating layer pool1
I0109 20:29:02.132470  4467 net.cpp:94] Creating Layer pool1
I0109 20:29:02.132511  4467 net.cpp:435] pool1 <- relu2
I0109 20:29:02.132560  4467 net.cpp:409] pool1 -> pool1
I0109 20:29:02.132674  4467 net.cpp:144] Setting up pool1
I0109 20:29:02.132724  4467 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:29:02.132773  4467 net.cpp:159] Memory required for data: 41575200
I0109 20:29:02.132788  4467 layer_factory.hpp:77] Creating layer drop1
I0109 20:29:02.132804  4467 net.cpp:94] Creating Layer drop1
I0109 20:29:02.132846  4467 net.cpp:435] drop1 <- pool1
I0109 20:29:02.132860  4467 net.cpp:409] drop1 -> drop1
I0109 20:29:02.132977  4467 net.cpp:144] Setting up drop1
I0109 20:29:02.133044  4467 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:29:02.133085  4467 net.cpp:159] Memory required for data: 43213600
I0109 20:29:02.133122  4467 layer_factory.hpp:77] Creating layer conv3
I0109 20:29:02.133178  4467 net.cpp:94] Creating Layer conv3
I0109 20:29:02.133229  4467 net.cpp:435] conv3 <- drop1
I0109 20:29:02.133275  4467 net.cpp:409] conv3 -> conv3
I0109 20:29:02.133934  4467 net.cpp:144] Setting up conv3
I0109 20:29:02.133955  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.133962  4467 net.cpp:159] Memory required for data: 46490400
I0109 20:29:02.134006  4467 layer_factory.hpp:77] Creating layer bn3
I0109 20:29:02.134032  4467 net.cpp:94] Creating Layer bn3
I0109 20:29:02.134106  4467 net.cpp:435] bn3 <- conv3
I0109 20:29:02.134132  4467 net.cpp:409] bn3 -> scale3
I0109 20:29:02.135329  4467 net.cpp:144] Setting up bn3
I0109 20:29:02.135355  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.135365  4467 net.cpp:159] Memory required for data: 49767200
I0109 20:29:02.135383  4467 layer_factory.hpp:77] Creating layer relu3
I0109 20:29:02.135449  4467 net.cpp:94] Creating Layer relu3
I0109 20:29:02.135468  4467 net.cpp:435] relu3 <- scale3
I0109 20:29:02.135480  4467 net.cpp:409] relu3 -> relu3
I0109 20:29:02.135567  4467 net.cpp:144] Setting up relu3
I0109 20:29:02.135619  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.135664  4467 net.cpp:159] Memory required for data: 53044000
I0109 20:29:02.135709  4467 layer_factory.hpp:77] Creating layer conv4
I0109 20:29:02.135763  4467 net.cpp:94] Creating Layer conv4
I0109 20:29:02.135814  4467 net.cpp:435] conv4 <- relu3
I0109 20:29:02.135867  4467 net.cpp:409] conv4 -> conv4
I0109 20:29:02.136735  4467 net.cpp:144] Setting up conv4
I0109 20:29:02.136790  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.136833  4467 net.cpp:159] Memory required for data: 56320800
I0109 20:29:02.136879  4467 layer_factory.hpp:77] Creating layer bn4
I0109 20:29:02.136932  4467 net.cpp:94] Creating Layer bn4
I0109 20:29:02.136974  4467 net.cpp:435] bn4 <- conv4
I0109 20:29:02.137027  4467 net.cpp:409] bn4 -> scale4
I0109 20:29:02.138231  4467 net.cpp:144] Setting up bn4
I0109 20:29:02.138254  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.138263  4467 net.cpp:159] Memory required for data: 59597600
I0109 20:29:02.138312  4467 layer_factory.hpp:77] Creating layer relu4
I0109 20:29:02.138330  4467 net.cpp:94] Creating Layer relu4
I0109 20:29:02.138367  4467 net.cpp:435] relu4 <- scale4
I0109 20:29:02.138427  4467 net.cpp:409] relu4 -> relu4
I0109 20:29:02.138511  4467 net.cpp:144] Setting up relu4
I0109 20:29:02.138566  4467 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:29:02.138608  4467 net.cpp:159] Memory required for data: 62874400
I0109 20:29:02.138649  4467 layer_factory.hpp:77] Creating layer pool2
I0109 20:29:02.138703  4467 net.cpp:94] Creating Layer pool2
I0109 20:29:02.138746  4467 net.cpp:435] pool2 <- relu4
I0109 20:29:02.138792  4467 net.cpp:409] pool2 -> pool2
I0109 20:29:02.138900  4467 net.cpp:144] Setting up pool2
I0109 20:29:02.138948  4467 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:29:02.138988  4467 net.cpp:159] Memory required for data: 63693600
I0109 20:29:02.139029  4467 layer_factory.hpp:77] Creating layer drop2
I0109 20:29:02.139070  4467 net.cpp:94] Creating Layer drop2
I0109 20:29:02.139119  4467 net.cpp:435] drop2 <- pool2
I0109 20:29:02.139175  4467 net.cpp:409] drop2 -> drop2
I0109 20:29:02.139292  4467 net.cpp:144] Setting up drop2
I0109 20:29:02.139329  4467 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:29:02.139338  4467 net.cpp:159] Memory required for data: 64512800
I0109 20:29:02.139384  4467 layer_factory.hpp:77] Creating layer fc1
I0109 20:29:02.139462  4467 net.cpp:94] Creating Layer fc1
I0109 20:29:02.139508  4467 net.cpp:435] fc1 <- drop2
I0109 20:29:02.139551  4467 net.cpp:409] fc1 -> fc1
I0109 20:29:02.161855  4467 net.cpp:144] Setting up fc1
I0109 20:29:02.161900  4467 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:02.161906  4467 net.cpp:159] Memory required for data: 64615200
I0109 20:29:02.161918  4467 layer_factory.hpp:77] Creating layer bn5
I0109 20:29:02.161942  4467 net.cpp:94] Creating Layer bn5
I0109 20:29:02.161962  4467 net.cpp:435] bn5 <- fc1
I0109 20:29:02.161978  4467 net.cpp:409] bn5 -> scale5
I0109 20:29:02.162716  4467 net.cpp:144] Setting up bn5
I0109 20:29:02.162735  4467 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:02.162740  4467 net.cpp:159] Memory required for data: 64717600
I0109 20:29:02.162768  4467 layer_factory.hpp:77] Creating layer relu5
I0109 20:29:02.162788  4467 net.cpp:94] Creating Layer relu5
I0109 20:29:02.162796  4467 net.cpp:435] relu5 <- scale5
I0109 20:29:02.162807  4467 net.cpp:409] relu5 -> relu5
I0109 20:29:02.162881  4467 net.cpp:144] Setting up relu5
I0109 20:29:02.162897  4467 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:02.162901  4467 net.cpp:159] Memory required for data: 64820000
I0109 20:29:02.162906  4467 layer_factory.hpp:77] Creating layer drop3
I0109 20:29:02.162921  4467 net.cpp:94] Creating Layer drop3
I0109 20:29:02.162936  4467 net.cpp:435] drop3 <- relu5
I0109 20:29:02.162950  4467 net.cpp:409] drop3 -> drop3
I0109 20:29:02.163024  4467 net.cpp:144] Setting up drop3
I0109 20:29:02.163043  4467 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:29:02.163048  4467 net.cpp:159] Memory required for data: 64922400
I0109 20:29:02.163053  4467 layer_factory.hpp:77] Creating layer fc2
I0109 20:29:02.163067  4467 net.cpp:94] Creating Layer fc2
I0109 20:29:02.163082  4467 net.cpp:435] fc2 <- drop3
I0109 20:29:02.163096  4467 net.cpp:409] fc2 -> fc2
I0109 20:29:02.163302  4467 net.cpp:144] Setting up fc2
I0109 20:29:02.163319  4467 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:02.163323  4467 net.cpp:159] Memory required for data: 64924400
I0109 20:29:02.163331  4467 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:29:02.163342  4467 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:29:02.163357  4467 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:29:02.163372  4467 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:29:02.163406  4467 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:29:02.163422  4467 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:29:02.163498  4467 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:29:02.163514  4467 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:02.163523  4467 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:02.163529  4467 net.cpp:151] Top shape: 50 10 (500)
I0109 20:29:02.163537  4467 net.cpp:159] Memory required for data: 64930400
I0109 20:29:02.163542  4467 layer_factory.hpp:77] Creating layer loss
I0109 20:29:02.163560  4467 net.cpp:94] Creating Layer loss
I0109 20:29:02.163568  4467 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:29:02.163578  4467 net.cpp:435] loss <- label_data_1_split_0
I0109 20:29:02.163588  4467 net.cpp:409] loss -> loss
I0109 20:29:02.163601  4467 layer_factory.hpp:77] Creating layer loss
I0109 20:29:02.163730  4467 net.cpp:144] Setting up loss
I0109 20:29:02.163748  4467 net.cpp:151] Top shape: (1)
I0109 20:29:02.163755  4467 net.cpp:154]     with loss weight 1
I0109 20:29:02.163777  4467 net.cpp:159] Memory required for data: 64930404
I0109 20:29:02.163785  4467 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:29:02.163802  4467 net.cpp:94] Creating Layer accuracy-top1
I0109 20:29:02.163811  4467 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:29:02.163820  4467 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:29:02.163861  4467 net.cpp:409] accuracy-top1 -> top-1
I0109 20:29:02.163888  4467 net.cpp:144] Setting up accuracy-top1
I0109 20:29:02.163897  4467 net.cpp:151] Top shape: (1)
I0109 20:29:02.163903  4467 net.cpp:159] Memory required for data: 64930408
I0109 20:29:02.163909  4467 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:29:02.163926  4467 net.cpp:94] Creating Layer accuracy-top5
I0109 20:29:02.163935  4467 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:29:02.163942  4467 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:29:02.163951  4467 net.cpp:409] accuracy-top5 -> top-5
I0109 20:29:02.163981  4467 net.cpp:144] Setting up accuracy-top5
I0109 20:29:02.164005  4467 net.cpp:151] Top shape: (1)
I0109 20:29:02.164026  4467 net.cpp:159] Memory required for data: 64930412
I0109 20:29:02.164047  4467 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:29:02.164064  4467 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:29:02.164073  4467 net.cpp:220] loss needs backward computation.
I0109 20:29:02.164080  4467 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:29:02.164086  4467 net.cpp:220] fc2 needs backward computation.
I0109 20:29:02.164093  4467 net.cpp:220] drop3 needs backward computation.
I0109 20:29:02.164103  4467 net.cpp:220] relu5 needs backward computation.
I0109 20:29:02.164108  4467 net.cpp:220] bn5 needs backward computation.
I0109 20:29:02.164114  4467 net.cpp:220] fc1 needs backward computation.
I0109 20:29:02.164121  4467 net.cpp:220] drop2 needs backward computation.
I0109 20:29:02.164129  4467 net.cpp:220] pool2 needs backward computation.
I0109 20:29:02.164135  4467 net.cpp:220] relu4 needs backward computation.
I0109 20:29:02.164142  4467 net.cpp:220] bn4 needs backward computation.
I0109 20:29:02.164163  4467 net.cpp:220] conv4 needs backward computation.
I0109 20:29:02.164186  4467 net.cpp:220] relu3 needs backward computation.
I0109 20:29:02.164207  4467 net.cpp:220] bn3 needs backward computation.
I0109 20:29:02.164227  4467 net.cpp:220] conv3 needs backward computation.
I0109 20:29:02.164244  4467 net.cpp:220] drop1 needs backward computation.
I0109 20:29:02.164252  4467 net.cpp:220] pool1 needs backward computation.
I0109 20:29:02.164259  4467 net.cpp:220] relu2 needs backward computation.
I0109 20:29:02.164266  4467 net.cpp:220] bn2 needs backward computation.
I0109 20:29:02.164273  4467 net.cpp:220] conv2 needs backward computation.
I0109 20:29:02.164281  4467 net.cpp:220] relu1 needs backward computation.
I0109 20:29:02.164288  4467 net.cpp:220] bn1 needs backward computation.
I0109 20:29:02.164294  4467 net.cpp:220] conv1 needs backward computation.
I0109 20:29:02.164304  4467 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:29:02.164315  4467 net.cpp:222] data does not need backward computation.
I0109 20:29:02.164324  4467 net.cpp:264] This network produces output loss
I0109 20:29:02.164330  4467 net.cpp:264] This network produces output top-1
I0109 20:29:02.164337  4467 net.cpp:264] This network produces output top-5
I0109 20:29:02.164386  4467 net.cpp:284] Network initialization done.
I0109 20:29:02.164521  4467 solver.cpp:63] Solver scaffolding done.
I0109 20:29:02.165902  4467 caffe_interface.cpp:93] Finetuning from /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/sparse.caffemodel
I0109 20:29:02.235270  4467 caffe_interface.cpp:527] Starting Optimization
I0109 20:29:02.235327  4467 solver.cpp:335] Solving 
I0109 20:29:02.235332  4467 solver.cpp:336] Learning Rate Policy: poly
I0109 20:29:02.236716  4467 solver.cpp:418] Iteration 0, Testing net (#0)
I0109 20:29:03.044193  4467 solver.cpp:517]     Test net output #0: loss = 4.22841 (* 1 = 4.22841 loss)
I0109 20:29:03.044241  4467 solver.cpp:517]     Test net output #1: top-1 = 0.237556
I0109 20:29:03.044250  4467 solver.cpp:517]     Test net output #2: top-5 = 0.664333
I0109 20:29:03.090747  4467 solver.cpp:266] Iteration 0 (0 iter/s, 0.855355s/100 iter), loss = 1.16265
I0109 20:29:03.090818  4467 solver.cpp:285]     Train net output #0: loss = 1.16265 (* 1 = 1.16265 loss)
I0109 20:29:03.090890  4467 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0109 20:29:06.316013  4467 solver.cpp:266] Iteration 100 (31.0056 iter/s, 3.22522s/100 iter), loss = 0.427441
I0109 20:29:06.316079  4467 solver.cpp:285]     Train net output #0: loss = 0.427441 (* 1 = 0.427441 loss)
I0109 20:29:06.316093  4467 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0109 20:29:09.540530  4467 solver.cpp:266] Iteration 200 (31.0128 iter/s, 3.22448s/100 iter), loss = 0.258488
I0109 20:29:09.540596  4467 solver.cpp:285]     Train net output #0: loss = 0.258488 (* 1 = 0.258488 loss)
I0109 20:29:09.540609  4467 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0109 20:29:12.765132  4467 solver.cpp:266] Iteration 300 (31.0119 iter/s, 3.22456s/100 iter), loss = 0.275494
I0109 20:29:12.765202  4467 solver.cpp:285]     Train net output #0: loss = 0.275494 (* 1 = 0.275494 loss)
I0109 20:29:12.765213  4467 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0109 20:29:15.989341  4467 solver.cpp:266] Iteration 400 (31.016 iter/s, 3.22414s/100 iter), loss = 0.299914
I0109 20:29:15.989408  4467 solver.cpp:285]     Train net output #0: loss = 0.299914 (* 1 = 0.299914 loss)
I0109 20:29:15.989419  4467 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0109 20:29:19.212671  4467 solver.cpp:266] Iteration 500 (31.0242 iter/s, 3.22329s/100 iter), loss = 0.330377
I0109 20:29:19.212739  4467 solver.cpp:285]     Train net output #0: loss = 0.330377 (* 1 = 0.330377 loss)
I0109 20:29:19.212752  4467 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0109 20:29:22.436024  4467 solver.cpp:266] Iteration 600 (31.024 iter/s, 3.22331s/100 iter), loss = 0.391652
I0109 20:29:22.436089  4467 solver.cpp:285]     Train net output #0: loss = 0.391652 (* 1 = 0.391652 loss)
I0109 20:29:22.436101  4467 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0109 20:29:25.660455  4467 solver.cpp:266] Iteration 700 (31.0136 iter/s, 3.22439s/100 iter), loss = 0.423663
I0109 20:29:25.660522  4467 solver.cpp:285]     Train net output #0: loss = 0.423663 (* 1 = 0.423663 loss)
I0109 20:29:25.660534  4467 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0109 20:29:28.883989  4467 solver.cpp:266] Iteration 800 (31.0222 iter/s, 3.2235s/100 iter), loss = 0.308452
I0109 20:29:28.884054  4467 solver.cpp:285]     Train net output #0: loss = 0.308452 (* 1 = 0.308452 loss)
I0109 20:29:28.884066  4467 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0109 20:29:32.112362  4467 solver.cpp:266] Iteration 900 (30.976 iter/s, 3.22831s/100 iter), loss = 0.316661
I0109 20:29:32.112606  4467 solver.cpp:285]     Train net output #0: loss = 0.316661 (* 1 = 0.316661 loss)
I0109 20:29:32.112622  4467 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0109 20:29:35.306761  4467 solver.cpp:418] Iteration 1000, Testing net (#0)
I0109 20:29:36.113577  4467 solver.cpp:517]     Test net output #0: loss = 1.21994 (* 1 = 1.21994 loss)
I0109 20:29:36.113633  4467 solver.cpp:517]     Test net output #1: top-1 = 0.703778
I0109 20:29:36.113649  4467 solver.cpp:517]     Test net output #2: top-5 = 0.957556
I0109 20:29:36.144250  4467 solver.cpp:266] Iteration 1000 (24.8035 iter/s, 4.03169s/100 iter), loss = 0.277163
I0109 20:29:36.144295  4467 solver.cpp:285]     Train net output #0: loss = 0.277163 (* 1 = 0.277163 loss)
I0109 20:29:36.144316  4467 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0109 20:29:39.370551  4467 solver.cpp:266] Iteration 1100 (30.9954 iter/s, 3.22628s/100 iter), loss = 0.336563
I0109 20:29:39.370617  4467 solver.cpp:285]     Train net output #0: loss = 0.336563 (* 1 = 0.336563 loss)
I0109 20:29:39.370630  4467 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0109 20:29:42.593549  4467 solver.cpp:266] Iteration 1200 (31.0274 iter/s, 3.22296s/100 iter), loss = 0.52015
I0109 20:29:42.593628  4467 solver.cpp:285]     Train net output #0: loss = 0.52015 (* 1 = 0.52015 loss)
I0109 20:29:42.593641  4467 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0109 20:29:45.817008  4467 solver.cpp:266] Iteration 1300 (31.0231 iter/s, 3.22341s/100 iter), loss = 0.252455
I0109 20:29:45.817075  4467 solver.cpp:285]     Train net output #0: loss = 0.252455 (* 1 = 0.252455 loss)
I0109 20:29:45.817088  4467 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0109 20:29:49.039782  4467 solver.cpp:266] Iteration 1400 (31.0298 iter/s, 3.22271s/100 iter), loss = 0.286566
I0109 20:29:49.039846  4467 solver.cpp:285]     Train net output #0: loss = 0.286566 (* 1 = 0.286566 loss)
I0109 20:29:49.039857  4467 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0109 20:29:52.263578  4467 solver.cpp:266] Iteration 1500 (31.0197 iter/s, 3.22376s/100 iter), loss = 0.319505
I0109 20:29:52.263639  4467 solver.cpp:285]     Train net output #0: loss = 0.319505 (* 1 = 0.319505 loss)
I0109 20:29:52.263651  4467 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0109 20:29:55.487612  4467 solver.cpp:266] Iteration 1600 (31.0174 iter/s, 3.224s/100 iter), loss = 0.336457
I0109 20:29:55.487677  4467 solver.cpp:285]     Train net output #0: loss = 0.336457 (* 1 = 0.336457 loss)
I0109 20:29:55.487689  4467 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0109 20:29:58.711774  4467 solver.cpp:266] Iteration 1700 (31.0162 iter/s, 3.22412s/100 iter), loss = 0.230832
I0109 20:29:58.711839  4467 solver.cpp:285]     Train net output #0: loss = 0.230832 (* 1 = 0.230832 loss)
I0109 20:29:58.711853  4467 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0109 20:30:01.936168  4467 solver.cpp:266] Iteration 1800 (31.0139 iter/s, 3.22436s/100 iter), loss = 0.330046
I0109 20:30:01.936233  4467 solver.cpp:285]     Train net output #0: loss = 0.330046 (* 1 = 0.330046 loss)
I0109 20:30:01.936245  4467 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0109 20:30:05.161682  4467 solver.cpp:266] Iteration 1900 (31.0034 iter/s, 3.22545s/100 iter), loss = 0.3877
I0109 20:30:05.161881  4467 solver.cpp:285]     Train net output #0: loss = 0.3877 (* 1 = 0.3877 loss)
I0109 20:30:05.161895  4467 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0109 20:30:08.354332  4467 solver.cpp:418] Iteration 2000, Testing net (#0)
I0109 20:30:09.160859  4467 solver.cpp:517]     Test net output #0: loss = 0.810682 (* 1 = 0.810682 loss)
I0109 20:30:09.160897  4467 solver.cpp:517]     Test net output #1: top-1 = 0.773778
I0109 20:30:09.160905  4467 solver.cpp:517]     Test net output #2: top-5 = 0.983222
I0109 20:30:09.191273  4467 solver.cpp:266] Iteration 2000 (24.8174 iter/s, 4.02944s/100 iter), loss = 0.268815
I0109 20:30:09.191309  4467 solver.cpp:285]     Train net output #0: loss = 0.268815 (* 1 = 0.268815 loss)
I0109 20:30:09.191324  4467 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0109 20:30:12.414057  4467 solver.cpp:266] Iteration 2100 (31.0292 iter/s, 3.22277s/100 iter), loss = 0.248593
I0109 20:30:12.414120  4467 solver.cpp:285]     Train net output #0: loss = 0.248593 (* 1 = 0.248593 loss)
I0109 20:30:12.414134  4467 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0109 20:30:15.640609  4467 solver.cpp:266] Iteration 2200 (30.9932 iter/s, 3.22652s/100 iter), loss = 0.226059
I0109 20:30:15.640671  4467 solver.cpp:285]     Train net output #0: loss = 0.226059 (* 1 = 0.226059 loss)
I0109 20:30:15.640683  4467 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0109 20:30:18.864121  4467 solver.cpp:266] Iteration 2300 (31.0224 iter/s, 3.22348s/100 iter), loss = 0.388268
I0109 20:30:18.864182  4467 solver.cpp:285]     Train net output #0: loss = 0.388268 (* 1 = 0.388268 loss)
I0109 20:30:18.864194  4467 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0109 20:30:22.091845  4467 solver.cpp:266] Iteration 2400 (30.9822 iter/s, 3.22766s/100 iter), loss = 0.259878
I0109 20:30:22.091936  4467 solver.cpp:285]     Train net output #0: loss = 0.259878 (* 1 = 0.259878 loss)
I0109 20:30:22.091951  4467 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0109 20:30:25.319465  4467 solver.cpp:266] Iteration 2500 (30.9832 iter/s, 3.22756s/100 iter), loss = 0.344564
I0109 20:30:25.319530  4467 solver.cpp:285]     Train net output #0: loss = 0.344564 (* 1 = 0.344564 loss)
I0109 20:30:25.319543  4467 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0109 20:30:28.543802  4467 solver.cpp:266] Iteration 2600 (31.0145 iter/s, 3.2243s/100 iter), loss = 0.286166
I0109 20:30:28.543870  4467 solver.cpp:285]     Train net output #0: loss = 0.286166 (* 1 = 0.286166 loss)
I0109 20:30:28.543882  4467 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0109 20:30:31.769080  4467 solver.cpp:266] Iteration 2700 (31.0055 iter/s, 3.22524s/100 iter), loss = 0.352672
I0109 20:30:31.769151  4467 solver.cpp:285]     Train net output #0: loss = 0.352672 (* 1 = 0.352672 loss)
I0109 20:30:31.769163  4467 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0109 20:30:34.996942  4467 solver.cpp:266] Iteration 2800 (30.9809 iter/s, 3.22779s/100 iter), loss = 0.363583
I0109 20:30:34.997006  4467 solver.cpp:285]     Train net output #0: loss = 0.363583 (* 1 = 0.363583 loss)
I0109 20:30:34.997018  4467 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0109 20:30:38.222404  4467 solver.cpp:266] Iteration 2900 (31.0037 iter/s, 3.22542s/100 iter), loss = 0.270444
I0109 20:30:38.222606  4467 solver.cpp:285]     Train net output #0: loss = 0.270444 (* 1 = 0.270444 loss)
I0109 20:30:38.222621  4467 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0109 20:30:41.418515  4467 solver.cpp:418] Iteration 3000, Testing net (#0)
I0109 20:30:42.224350  4467 solver.cpp:517]     Test net output #0: loss = 0.565771 (* 1 = 0.565771 loss)
I0109 20:30:42.224386  4467 solver.cpp:517]     Test net output #1: top-1 = 0.820889
I0109 20:30:42.224395  4467 solver.cpp:517]     Test net output #2: top-5 = 0.989111
I0109 20:30:42.254777  4467 solver.cpp:266] Iteration 3000 (24.8003 iter/s, 4.03222s/100 iter), loss = 0.285683
I0109 20:30:42.254813  4467 solver.cpp:285]     Train net output #0: loss = 0.285683 (* 1 = 0.285683 loss)
I0109 20:30:42.254827  4467 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0109 20:30:45.479688  4467 solver.cpp:266] Iteration 3100 (31.0087 iter/s, 3.2249s/100 iter), loss = 0.351614
I0109 20:30:45.479753  4467 solver.cpp:285]     Train net output #0: loss = 0.351614 (* 1 = 0.351614 loss)
I0109 20:30:45.479763  4467 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0109 20:30:48.703582  4467 solver.cpp:266] Iteration 3200 (31.0188 iter/s, 3.22386s/100 iter), loss = 0.414259
I0109 20:30:48.703644  4467 solver.cpp:285]     Train net output #0: loss = 0.414259 (* 1 = 0.414259 loss)
I0109 20:30:48.703656  4467 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0109 20:30:51.928577  4467 solver.cpp:266] Iteration 3300 (31.0081 iter/s, 3.22496s/100 iter), loss = 0.324596
I0109 20:30:51.928642  4467 solver.cpp:285]     Train net output #0: loss = 0.324596 (* 1 = 0.324596 loss)
I0109 20:30:51.928655  4467 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0109 20:30:55.157953  4467 solver.cpp:266] Iteration 3400 (30.9663 iter/s, 3.22931s/100 iter), loss = 0.308626
I0109 20:30:55.158015  4467 solver.cpp:285]     Train net output #0: loss = 0.308626 (* 1 = 0.308626 loss)
I0109 20:30:55.158027  4467 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0109 20:30:58.394337  4467 solver.cpp:266] Iteration 3500 (30.899 iter/s, 3.23635s/100 iter), loss = 0.350511
I0109 20:30:58.394400  4467 solver.cpp:285]     Train net output #0: loss = 0.350511 (* 1 = 0.350511 loss)
I0109 20:30:58.394412  4467 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0109 20:31:01.626528  4467 solver.cpp:266] Iteration 3600 (30.9391 iter/s, 3.23216s/100 iter), loss = 0.228381
I0109 20:31:01.626593  4467 solver.cpp:285]     Train net output #0: loss = 0.228381 (* 1 = 0.228381 loss)
I0109 20:31:01.626605  4467 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0109 20:31:04.862452  4467 solver.cpp:266] Iteration 3700 (30.9034 iter/s, 3.23589s/100 iter), loss = 0.240239
I0109 20:31:04.862516  4467 solver.cpp:285]     Train net output #0: loss = 0.240239 (* 1 = 0.240239 loss)
I0109 20:31:04.862529  4467 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0109 20:31:08.092697  4467 solver.cpp:266] Iteration 3800 (30.958 iter/s, 3.23018s/100 iter), loss = 0.280067
I0109 20:31:08.092775  4467 solver.cpp:285]     Train net output #0: loss = 0.280067 (* 1 = 0.280067 loss)
I0109 20:31:08.092789  4467 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0109 20:31:11.320413  4467 solver.cpp:266] Iteration 3900 (30.9821 iter/s, 3.22767s/100 iter), loss = 0.269148
I0109 20:31:11.320621  4467 solver.cpp:285]     Train net output #0: loss = 0.269148 (* 1 = 0.269148 loss)
I0109 20:31:11.320636  4467 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0109 20:31:14.521431  4467 solver.cpp:418] Iteration 4000, Testing net (#0)
I0109 20:31:15.327937  4467 solver.cpp:517]     Test net output #0: loss = 0.543563 (* 1 = 0.543563 loss)
I0109 20:31:15.327975  4467 solver.cpp:517]     Test net output #1: top-1 = 0.828222
I0109 20:31:15.327981  4467 solver.cpp:517]     Test net output #2: top-5 = 0.990666
I0109 20:31:15.358418  4467 solver.cpp:266] Iteration 4000 (24.7657 iter/s, 4.03784s/100 iter), loss = 0.264711
I0109 20:31:15.358458  4467 solver.cpp:285]     Train net output #0: loss = 0.264711 (* 1 = 0.264711 loss)
I0109 20:31:15.358471  4467 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0109 20:31:18.590113  4467 solver.cpp:266] Iteration 4100 (30.9436 iter/s, 3.23168s/100 iter), loss = 0.254267
I0109 20:31:18.590173  4467 solver.cpp:285]     Train net output #0: loss = 0.254267 (* 1 = 0.254267 loss)
I0109 20:31:18.590186  4467 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0109 20:31:21.819906  4467 solver.cpp:266] Iteration 4200 (30.962 iter/s, 3.22976s/100 iter), loss = 0.293099
I0109 20:31:21.819965  4467 solver.cpp:285]     Train net output #0: loss = 0.293099 (* 1 = 0.293099 loss)
I0109 20:31:21.819977  4467 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0109 20:31:25.049432  4467 solver.cpp:266] Iteration 4300 (30.9648 iter/s, 3.22947s/100 iter), loss = 0.324582
I0109 20:31:25.049499  4467 solver.cpp:285]     Train net output #0: loss = 0.324582 (* 1 = 0.324582 loss)
I0109 20:31:25.049510  4467 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0109 20:31:28.273629  4467 solver.cpp:266] Iteration 4400 (31.0158 iter/s, 3.22416s/100 iter), loss = 0.254753
I0109 20:31:28.273694  4467 solver.cpp:285]     Train net output #0: loss = 0.254753 (* 1 = 0.254753 loss)
I0109 20:31:28.273706  4467 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0109 20:31:31.500677  4467 solver.cpp:266] Iteration 4500 (30.9884 iter/s, 3.22701s/100 iter), loss = 0.329822
I0109 20:31:31.500742  4467 solver.cpp:285]     Train net output #0: loss = 0.329822 (* 1 = 0.329822 loss)
I0109 20:31:31.500756  4467 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0109 20:31:34.726629  4467 solver.cpp:266] Iteration 4600 (30.999 iter/s, 3.22591s/100 iter), loss = 0.373255
I0109 20:31:34.726693  4467 solver.cpp:285]     Train net output #0: loss = 0.373255 (* 1 = 0.373255 loss)
I0109 20:31:34.726706  4467 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0109 20:31:37.954818  4467 solver.cpp:266] Iteration 4700 (30.9777 iter/s, 3.22813s/100 iter), loss = 0.26693
I0109 20:31:37.954879  4467 solver.cpp:285]     Train net output #0: loss = 0.26693 (* 1 = 0.26693 loss)
I0109 20:31:37.954890  4467 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0109 20:31:41.186465  4467 solver.cpp:266] Iteration 4800 (30.9443 iter/s, 3.23162s/100 iter), loss = 0.293454
I0109 20:31:41.186527  4467 solver.cpp:285]     Train net output #0: loss = 0.293454 (* 1 = 0.293454 loss)
I0109 20:31:41.186539  4467 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0109 20:31:44.418584  4467 solver.cpp:266] Iteration 4900 (30.9398 iter/s, 3.23208s/100 iter), loss = 0.273031
I0109 20:31:44.418742  4467 solver.cpp:285]     Train net output #0: loss = 0.273031 (* 1 = 0.273031 loss)
I0109 20:31:44.418758  4467 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0109 20:31:47.614112  4467 solver.cpp:418] Iteration 5000, Testing net (#0)
I0109 20:31:48.420878  4467 solver.cpp:517]     Test net output #0: loss = 0.59116 (* 1 = 0.59116 loss)
I0109 20:31:48.420917  4467 solver.cpp:517]     Test net output #1: top-1 = 0.812444
I0109 20:31:48.420928  4467 solver.cpp:517]     Test net output #2: top-5 = 0.988778
I0109 20:31:48.451200  4467 solver.cpp:266] Iteration 5000 (24.7985 iter/s, 4.0325s/100 iter), loss = 0.250826
I0109 20:31:48.451243  4467 solver.cpp:285]     Train net output #0: loss = 0.250826 (* 1 = 0.250826 loss)
I0109 20:31:48.451258  4467 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0109 20:31:51.675469  4467 solver.cpp:266] Iteration 5100 (31.0149 iter/s, 3.22425s/100 iter), loss = 0.205533
I0109 20:31:51.675535  4467 solver.cpp:285]     Train net output #0: loss = 0.205533 (* 1 = 0.205533 loss)
I0109 20:31:51.675547  4467 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0109 20:31:54.899866  4467 solver.cpp:266] Iteration 5200 (31.0139 iter/s, 3.22436s/100 iter), loss = 0.326986
I0109 20:31:54.899930  4467 solver.cpp:285]     Train net output #0: loss = 0.326986 (* 1 = 0.326986 loss)
I0109 20:31:54.899941  4467 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0109 20:31:58.127637  4467 solver.cpp:266] Iteration 5300 (30.9817 iter/s, 3.22771s/100 iter), loss = 0.227556
I0109 20:31:58.127703  4467 solver.cpp:285]     Train net output #0: loss = 0.227556 (* 1 = 0.227556 loss)
I0109 20:31:58.127717  4467 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0109 20:32:01.361788  4467 solver.cpp:266] Iteration 5400 (30.9204 iter/s, 3.23411s/100 iter), loss = 0.199838
I0109 20:32:01.361851  4467 solver.cpp:285]     Train net output #0: loss = 0.199838 (* 1 = 0.199838 loss)
I0109 20:32:01.361863  4467 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0109 20:32:04.596457  4467 solver.cpp:266] Iteration 5500 (30.9154 iter/s, 3.23464s/100 iter), loss = 0.306049
I0109 20:32:04.596520  4467 solver.cpp:285]     Train net output #0: loss = 0.306049 (* 1 = 0.306049 loss)
I0109 20:32:04.596532  4467 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0109 20:32:07.835435  4467 solver.cpp:266] Iteration 5600 (30.8743 iter/s, 3.23894s/100 iter), loss = 0.257335
I0109 20:32:07.835500  4467 solver.cpp:285]     Train net output #0: loss = 0.257335 (* 1 = 0.257335 loss)
I0109 20:32:07.835512  4467 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0109 20:32:11.064769  4467 solver.cpp:266] Iteration 5700 (30.9667 iter/s, 3.22927s/100 iter), loss = 0.264977
I0109 20:32:11.064833  4467 solver.cpp:285]     Train net output #0: loss = 0.264977 (* 1 = 0.264977 loss)
I0109 20:32:11.064846  4467 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0109 20:32:14.289718  4467 solver.cpp:266] Iteration 5800 (31.0086 iter/s, 3.22491s/100 iter), loss = 0.288926
I0109 20:32:14.289783  4467 solver.cpp:285]     Train net output #0: loss = 0.288926 (* 1 = 0.288926 loss)
I0109 20:32:14.289794  4467 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0109 20:32:17.519613  4467 solver.cpp:266] Iteration 5900 (30.9611 iter/s, 3.22986s/100 iter), loss = 0.284203
I0109 20:32:17.519790  4467 solver.cpp:285]     Train net output #0: loss = 0.284203 (* 1 = 0.284203 loss)
I0109 20:32:17.519805  4467 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0109 20:32:20.718607  4467 solver.cpp:418] Iteration 6000, Testing net (#0)
I0109 20:32:21.526397  4467 solver.cpp:517]     Test net output #0: loss = 0.569108 (* 1 = 0.569108 loss)
I0109 20:32:21.526434  4467 solver.cpp:517]     Test net output #1: top-1 = 0.822667
I0109 20:32:21.526443  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0109 20:32:21.556768  4467 solver.cpp:266] Iteration 6000 (24.7707 iter/s, 4.03702s/100 iter), loss = 0.3322
I0109 20:32:21.556807  4467 solver.cpp:285]     Train net output #0: loss = 0.3322 (* 1 = 0.3322 loss)
I0109 20:32:21.556820  4467 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0109 20:32:24.781229  4467 solver.cpp:266] Iteration 6100 (31.013 iter/s, 3.22445s/100 iter), loss = 0.464717
I0109 20:32:24.781296  4467 solver.cpp:285]     Train net output #0: loss = 0.464717 (* 1 = 0.464717 loss)
I0109 20:32:24.781309  4467 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0109 20:32:28.010067  4467 solver.cpp:266] Iteration 6200 (30.9715 iter/s, 3.22877s/100 iter), loss = 0.203166
I0109 20:32:28.010130  4467 solver.cpp:285]     Train net output #0: loss = 0.203166 (* 1 = 0.203166 loss)
I0109 20:32:28.010143  4467 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0109 20:32:31.236152  4467 solver.cpp:266] Iteration 6300 (30.9977 iter/s, 3.22605s/100 iter), loss = 0.237769
I0109 20:32:31.236215  4467 solver.cpp:285]     Train net output #0: loss = 0.237769 (* 1 = 0.237769 loss)
I0109 20:32:31.236227  4467 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0109 20:32:34.471614  4467 solver.cpp:266] Iteration 6400 (30.9078 iter/s, 3.23543s/100 iter), loss = 0.333357
I0109 20:32:34.471676  4467 solver.cpp:285]     Train net output #0: loss = 0.333357 (* 1 = 0.333357 loss)
I0109 20:32:34.471688  4467 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0109 20:32:37.697286  4467 solver.cpp:266] Iteration 6500 (31.0016 iter/s, 3.22564s/100 iter), loss = 0.332426
I0109 20:32:37.697350  4467 solver.cpp:285]     Train net output #0: loss = 0.332426 (* 1 = 0.332426 loss)
I0109 20:32:37.697361  4467 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0109 20:32:40.922045  4467 solver.cpp:266] Iteration 6600 (31.0104 iter/s, 3.22472s/100 iter), loss = 0.277187
I0109 20:32:40.922106  4467 solver.cpp:285]     Train net output #0: loss = 0.277187 (* 1 = 0.277187 loss)
I0109 20:32:40.922118  4467 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0109 20:32:44.146725  4467 solver.cpp:266] Iteration 6700 (31.0114 iter/s, 3.22462s/100 iter), loss = 0.283762
I0109 20:32:44.146790  4467 solver.cpp:285]     Train net output #0: loss = 0.283762 (* 1 = 0.283762 loss)
I0109 20:32:44.146800  4467 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0109 20:32:47.378443  4467 solver.cpp:266] Iteration 6800 (30.9436 iter/s, 3.23168s/100 iter), loss = 0.295398
I0109 20:32:47.378504  4467 solver.cpp:285]     Train net output #0: loss = 0.295398 (* 1 = 0.295398 loss)
I0109 20:32:47.378516  4467 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0109 20:32:50.611568  4467 solver.cpp:266] Iteration 6900 (30.9301 iter/s, 3.23309s/100 iter), loss = 0.226765
I0109 20:32:50.611760  4467 solver.cpp:285]     Train net output #0: loss = 0.226765 (* 1 = 0.226765 loss)
I0109 20:32:50.611778  4467 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0109 20:32:53.806362  4467 solver.cpp:418] Iteration 7000, Testing net (#0)
I0109 20:32:54.613997  4467 solver.cpp:517]     Test net output #0: loss = 0.522308 (* 1 = 0.522308 loss)
I0109 20:32:54.614039  4467 solver.cpp:517]     Test net output #1: top-1 = 0.829666
I0109 20:32:54.614048  4467 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0109 20:32:54.644569  4467 solver.cpp:266] Iteration 7000 (24.7963 iter/s, 4.03285s/100 iter), loss = 0.169729
I0109 20:32:54.644611  4467 solver.cpp:285]     Train net output #0: loss = 0.169729 (* 1 = 0.169729 loss)
I0109 20:32:54.644628  4467 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0109 20:32:57.876408  4467 solver.cpp:266] Iteration 7100 (30.9423 iter/s, 3.23182s/100 iter), loss = 0.299329
I0109 20:32:57.876480  4467 solver.cpp:285]     Train net output #0: loss = 0.299329 (* 1 = 0.299329 loss)
I0109 20:32:57.876494  4467 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0109 20:33:01.101178  4467 solver.cpp:266] Iteration 7200 (31.0106 iter/s, 3.2247s/100 iter), loss = 0.222711
I0109 20:33:01.101246  4467 solver.cpp:285]     Train net output #0: loss = 0.222711 (* 1 = 0.222711 loss)
I0109 20:33:01.101258  4467 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0109 20:33:04.333763  4467 solver.cpp:266] Iteration 7300 (30.9354 iter/s, 3.23255s/100 iter), loss = 0.196717
I0109 20:33:04.333832  4467 solver.cpp:285]     Train net output #0: loss = 0.196717 (* 1 = 0.196717 loss)
I0109 20:33:04.333845  4467 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0109 20:33:07.562871  4467 solver.cpp:266] Iteration 7400 (30.9687 iter/s, 3.22907s/100 iter), loss = 0.249404
I0109 20:33:07.562937  4467 solver.cpp:285]     Train net output #0: loss = 0.249404 (* 1 = 0.249404 loss)
I0109 20:33:07.562948  4467 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0109 20:33:10.797216  4467 solver.cpp:266] Iteration 7500 (30.9185 iter/s, 3.23431s/100 iter), loss = 0.171907
I0109 20:33:10.797282  4467 solver.cpp:285]     Train net output #0: loss = 0.171907 (* 1 = 0.171907 loss)
I0109 20:33:10.797294  4467 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0109 20:33:14.029937  4467 solver.cpp:266] Iteration 7600 (30.9343 iter/s, 3.23265s/100 iter), loss = 0.265724
I0109 20:33:14.030023  4467 solver.cpp:285]     Train net output #0: loss = 0.265724 (* 1 = 0.265724 loss)
I0109 20:33:14.030038  4467 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0109 20:33:17.260828  4467 solver.cpp:266] Iteration 7700 (30.9518 iter/s, 3.23083s/100 iter), loss = 0.340242
I0109 20:33:17.260892  4467 solver.cpp:285]     Train net output #0: loss = 0.340242 (* 1 = 0.340242 loss)
I0109 20:33:17.260905  4467 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0109 20:33:20.495301  4467 solver.cpp:266] Iteration 7800 (30.9173 iter/s, 3.23444s/100 iter), loss = 0.2634
I0109 20:33:20.495363  4467 solver.cpp:285]     Train net output #0: loss = 0.2634 (* 1 = 0.2634 loss)
I0109 20:33:20.495375  4467 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0109 20:33:23.733881  4467 solver.cpp:266] Iteration 7900 (30.878 iter/s, 3.23855s/100 iter), loss = 0.196722
I0109 20:33:23.734067  4467 solver.cpp:285]     Train net output #0: loss = 0.196722 (* 1 = 0.196722 loss)
I0109 20:33:23.734082  4467 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0109 20:33:26.932039  4467 solver.cpp:418] Iteration 8000, Testing net (#0)
I0109 20:33:27.738525  4467 solver.cpp:517]     Test net output #0: loss = 0.545882 (* 1 = 0.545882 loss)
I0109 20:33:27.738564  4467 solver.cpp:517]     Test net output #1: top-1 = 0.834222
I0109 20:33:27.738571  4467 solver.cpp:517]     Test net output #2: top-5 = 0.990555
I0109 20:33:27.768956  4467 solver.cpp:266] Iteration 8000 (24.7836 iter/s, 4.03493s/100 iter), loss = 0.361223
I0109 20:33:27.768996  4467 solver.cpp:285]     Train net output #0: loss = 0.361223 (* 1 = 0.361223 loss)
I0109 20:33:27.769011  4467 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0109 20:33:31.005517  4467 solver.cpp:266] Iteration 8100 (30.8973 iter/s, 3.23652s/100 iter), loss = 0.359328
I0109 20:33:31.005581  4467 solver.cpp:285]     Train net output #0: loss = 0.359328 (* 1 = 0.359328 loss)
I0109 20:33:31.005607  4467 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0109 20:33:34.244704  4467 solver.cpp:266] Iteration 8200 (30.8723 iter/s, 3.23915s/100 iter), loss = 0.25147
I0109 20:33:34.244767  4467 solver.cpp:285]     Train net output #0: loss = 0.25147 (* 1 = 0.25147 loss)
I0109 20:33:34.244779  4467 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0109 20:33:37.493903  4467 solver.cpp:266] Iteration 8300 (30.7771 iter/s, 3.24916s/100 iter), loss = 0.272248
I0109 20:33:37.493968  4467 solver.cpp:285]     Train net output #0: loss = 0.272248 (* 1 = 0.272248 loss)
I0109 20:33:37.493979  4467 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0109 20:33:40.741991  4467 solver.cpp:266] Iteration 8400 (30.7877 iter/s, 3.24805s/100 iter), loss = 0.245059
I0109 20:33:40.742058  4467 solver.cpp:285]     Train net output #0: loss = 0.245059 (* 1 = 0.245059 loss)
I0109 20:33:40.742070  4467 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0109 20:33:43.999622  4467 solver.cpp:266] Iteration 8500 (30.6978 iter/s, 3.25757s/100 iter), loss = 0.381623
I0109 20:33:43.999691  4467 solver.cpp:285]     Train net output #0: loss = 0.381623 (* 1 = 0.381623 loss)
I0109 20:33:43.999706  4467 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0109 20:33:47.260293  4467 solver.cpp:266] Iteration 8600 (30.6689 iter/s, 3.26063s/100 iter), loss = 0.311915
I0109 20:33:47.260360  4467 solver.cpp:285]     Train net output #0: loss = 0.311915 (* 1 = 0.311915 loss)
I0109 20:33:47.260372  4467 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0109 20:33:50.512645  4467 solver.cpp:266] Iteration 8700 (30.7473 iter/s, 3.25231s/100 iter), loss = 0.198899
I0109 20:33:50.512707  4467 solver.cpp:285]     Train net output #0: loss = 0.198899 (* 1 = 0.198899 loss)
I0109 20:33:50.512719  4467 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0109 20:33:53.764972  4467 solver.cpp:266] Iteration 8800 (30.7475 iter/s, 3.25229s/100 iter), loss = 0.278244
I0109 20:33:53.765180  4467 solver.cpp:285]     Train net output #0: loss = 0.278244 (* 1 = 0.278244 loss)
I0109 20:33:53.765195  4467 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0109 20:33:57.026723  4467 solver.cpp:266] Iteration 8900 (30.6603 iter/s, 3.26155s/100 iter), loss = 0.283919
I0109 20:33:57.026787  4467 solver.cpp:285]     Train net output #0: loss = 0.283919 (* 1 = 0.283919 loss)
I0109 20:33:57.026799  4467 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0109 20:34:00.246232  4467 solver.cpp:418] Iteration 9000, Testing net (#0)
I0109 20:34:01.059321  4467 solver.cpp:517]     Test net output #0: loss = 0.533927 (* 1 = 0.533927 loss)
I0109 20:34:01.059360  4467 solver.cpp:517]     Test net output #1: top-1 = 0.834
I0109 20:34:01.059367  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0109 20:34:01.089718  4467 solver.cpp:266] Iteration 9000 (24.6125 iter/s, 4.06298s/100 iter), loss = 0.238713
I0109 20:34:01.089756  4467 solver.cpp:285]     Train net output #0: loss = 0.238713 (* 1 = 0.238713 loss)
I0109 20:34:01.089771  4467 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0109 20:34:04.348384  4467 solver.cpp:266] Iteration 9100 (30.6875 iter/s, 3.25866s/100 iter), loss = 0.179909
I0109 20:34:04.348449  4467 solver.cpp:285]     Train net output #0: loss = 0.179909 (* 1 = 0.179909 loss)
I0109 20:34:04.348461  4467 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0109 20:34:07.608510  4467 solver.cpp:266] Iteration 9200 (30.674 iter/s, 3.26009s/100 iter), loss = 0.256537
I0109 20:34:07.608599  4467 solver.cpp:285]     Train net output #0: loss = 0.256537 (* 1 = 0.256537 loss)
I0109 20:34:07.608614  4467 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0109 20:34:10.866883  4467 solver.cpp:266] Iteration 9300 (30.6908 iter/s, 3.25831s/100 iter), loss = 0.205951
I0109 20:34:10.866964  4467 solver.cpp:285]     Train net output #0: loss = 0.205951 (* 1 = 0.205951 loss)
I0109 20:34:10.866981  4467 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0109 20:34:14.123553  4467 solver.cpp:266] Iteration 9400 (30.7069 iter/s, 3.25659s/100 iter), loss = 0.231069
I0109 20:34:14.123616  4467 solver.cpp:285]     Train net output #0: loss = 0.231069 (* 1 = 0.231069 loss)
I0109 20:34:14.123628  4467 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0109 20:34:17.376955  4467 solver.cpp:266] Iteration 9500 (30.7374 iter/s, 3.25337s/100 iter), loss = 0.273339
I0109 20:34:17.377023  4467 solver.cpp:285]     Train net output #0: loss = 0.273339 (* 1 = 0.273339 loss)
I0109 20:34:17.377035  4467 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0109 20:34:20.628372  4467 solver.cpp:266] Iteration 9600 (30.7562 iter/s, 3.25138s/100 iter), loss = 0.240032
I0109 20:34:20.628438  4467 solver.cpp:285]     Train net output #0: loss = 0.240032 (* 1 = 0.240032 loss)
I0109 20:34:20.628451  4467 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0109 20:34:23.888803  4467 solver.cpp:266] Iteration 9700 (30.6711 iter/s, 3.26039s/100 iter), loss = 0.248162
I0109 20:34:23.888949  4467 solver.cpp:285]     Train net output #0: loss = 0.248162 (* 1 = 0.248162 loss)
I0109 20:34:23.888964  4467 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0109 20:34:27.144094  4467 solver.cpp:266] Iteration 9800 (30.7205 iter/s, 3.25515s/100 iter), loss = 0.242458
I0109 20:34:27.144158  4467 solver.cpp:285]     Train net output #0: loss = 0.242458 (* 1 = 0.242458 loss)
I0109 20:34:27.144170  4467 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0109 20:34:30.403339  4467 solver.cpp:266] Iteration 9900 (30.6823 iter/s, 3.25921s/100 iter), loss = 0.177375
I0109 20:34:30.403409  4467 solver.cpp:285]     Train net output #0: loss = 0.177375 (* 1 = 0.177375 loss)
I0109 20:34:30.403426  4467 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0109 20:34:33.631348  4467 solver.cpp:418] Iteration 10000, Testing net (#0)
I0109 20:34:34.445117  4467 solver.cpp:517]     Test net output #0: loss = 0.531966 (* 1 = 0.531966 loss)
I0109 20:34:34.445159  4467 solver.cpp:517]     Test net output #1: top-1 = 0.828777
I0109 20:34:34.445171  4467 solver.cpp:517]     Test net output #2: top-5 = 0.990333
I0109 20:34:34.476089  4467 solver.cpp:266] Iteration 10000 (24.5536 iter/s, 4.07273s/100 iter), loss = 0.316456
I0109 20:34:34.476130  4467 solver.cpp:285]     Train net output #0: loss = 0.316456 (* 1 = 0.316456 loss)
I0109 20:34:34.476151  4467 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0109 20:34:37.731319  4467 solver.cpp:266] Iteration 10100 (30.7199 iter/s, 3.25522s/100 iter), loss = 0.187777
I0109 20:34:37.731389  4467 solver.cpp:285]     Train net output #0: loss = 0.187777 (* 1 = 0.187777 loss)
I0109 20:34:37.731408  4467 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0109 20:34:40.986454  4467 solver.cpp:266] Iteration 10200 (30.7213 iter/s, 3.25508s/100 iter), loss = 0.281677
I0109 20:34:40.986521  4467 solver.cpp:285]     Train net output #0: loss = 0.281677 (* 1 = 0.281677 loss)
I0109 20:34:40.986539  4467 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0109 20:34:44.246295  4467 solver.cpp:266] Iteration 10300 (30.6766 iter/s, 3.25981s/100 iter), loss = 0.223526
I0109 20:34:44.246364  4467 solver.cpp:285]     Train net output #0: loss = 0.223526 (* 1 = 0.223526 loss)
I0109 20:34:44.246382  4467 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0109 20:34:47.508777  4467 solver.cpp:266] Iteration 10400 (30.6518 iter/s, 3.26245s/100 iter), loss = 0.343316
I0109 20:34:47.508852  4467 solver.cpp:285]     Train net output #0: loss = 0.343316 (* 1 = 0.343316 loss)
I0109 20:34:47.508869  4467 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0109 20:34:50.769979  4467 solver.cpp:266] Iteration 10500 (30.6639 iter/s, 3.26117s/100 iter), loss = 0.305805
I0109 20:34:50.770051  4467 solver.cpp:285]     Train net output #0: loss = 0.305805 (* 1 = 0.305805 loss)
I0109 20:34:50.770071  4467 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0109 20:34:54.037498  4467 solver.cpp:266] Iteration 10600 (30.6048 iter/s, 3.26746s/100 iter), loss = 0.299053
I0109 20:34:54.037703  4467 solver.cpp:285]     Train net output #0: loss = 0.299053 (* 1 = 0.299053 loss)
I0109 20:34:54.037722  4467 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0109 20:34:57.302402  4467 solver.cpp:266] Iteration 10700 (30.6303 iter/s, 3.26474s/100 iter), loss = 0.218564
I0109 20:34:57.302470  4467 solver.cpp:285]     Train net output #0: loss = 0.218564 (* 1 = 0.218564 loss)
I0109 20:34:57.302487  4467 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0109 20:35:00.562515  4467 solver.cpp:266] Iteration 10800 (30.674 iter/s, 3.26008s/100 iter), loss = 0.274597
I0109 20:35:00.562587  4467 solver.cpp:285]     Train net output #0: loss = 0.274597 (* 1 = 0.274597 loss)
I0109 20:35:00.562604  4467 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0109 20:35:03.823997  4467 solver.cpp:266] Iteration 10900 (30.6612 iter/s, 3.26146s/100 iter), loss = 0.190871
I0109 20:35:03.824060  4467 solver.cpp:285]     Train net output #0: loss = 0.190871 (* 1 = 0.190871 loss)
I0109 20:35:03.824074  4467 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0109 20:35:07.055874  4467 solver.cpp:418] Iteration 11000, Testing net (#0)
I0109 20:35:07.864657  4467 solver.cpp:517]     Test net output #0: loss = 0.525627 (* 1 = 0.525627 loss)
I0109 20:35:07.864696  4467 solver.cpp:517]     Test net output #1: top-1 = 0.830111
I0109 20:35:07.864706  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0109 20:35:07.894956  4467 solver.cpp:266] Iteration 11000 (24.5643 iter/s, 4.07095s/100 iter), loss = 0.194233
I0109 20:35:07.894992  4467 solver.cpp:285]     Train net output #0: loss = 0.194233 (* 1 = 0.194233 loss)
I0109 20:35:07.895006  4467 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0109 20:35:11.150108  4467 solver.cpp:266] Iteration 11100 (30.7208 iter/s, 3.25513s/100 iter), loss = 0.260943
I0109 20:35:11.150176  4467 solver.cpp:285]     Train net output #0: loss = 0.260943 (* 1 = 0.260943 loss)
I0109 20:35:11.150187  4467 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0109 20:35:14.412091  4467 solver.cpp:266] Iteration 11200 (30.6565 iter/s, 3.26196s/100 iter), loss = 0.220649
I0109 20:35:14.412156  4467 solver.cpp:285]     Train net output #0: loss = 0.220649 (* 1 = 0.220649 loss)
I0109 20:35:14.412168  4467 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0109 20:35:17.671120  4467 solver.cpp:266] Iteration 11300 (30.6842 iter/s, 3.259s/100 iter), loss = 0.172802
I0109 20:35:17.671186  4467 solver.cpp:285]     Train net output #0: loss = 0.172802 (* 1 = 0.172802 loss)
I0109 20:35:17.671200  4467 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0109 20:35:20.928346  4467 solver.cpp:266] Iteration 11400 (30.7012 iter/s, 3.2572s/100 iter), loss = 0.322117
I0109 20:35:20.928408  4467 solver.cpp:285]     Train net output #0: loss = 0.322117 (* 1 = 0.322117 loss)
I0109 20:35:20.928421  4467 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0109 20:35:24.192497  4467 solver.cpp:266] Iteration 11500 (30.6363 iter/s, 3.2641s/100 iter), loss = 0.278312
I0109 20:35:24.192689  4467 solver.cpp:285]     Train net output #0: loss = 0.278312 (* 1 = 0.278312 loss)
I0109 20:35:24.192704  4467 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0109 20:35:27.447721  4467 solver.cpp:266] Iteration 11600 (30.7212 iter/s, 3.25508s/100 iter), loss = 0.207736
I0109 20:35:27.447784  4467 solver.cpp:285]     Train net output #0: loss = 0.207736 (* 1 = 0.207736 loss)
I0109 20:35:27.447796  4467 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0109 20:35:30.699431  4467 solver.cpp:266] Iteration 11700 (30.7533 iter/s, 3.25169s/100 iter), loss = 0.196315
I0109 20:35:30.699493  4467 solver.cpp:285]     Train net output #0: loss = 0.196315 (* 1 = 0.196315 loss)
I0109 20:35:30.699506  4467 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0109 20:35:33.962671  4467 solver.cpp:266] Iteration 11800 (30.6446 iter/s, 3.26322s/100 iter), loss = 0.176875
I0109 20:35:33.962741  4467 solver.cpp:285]     Train net output #0: loss = 0.176875 (* 1 = 0.176875 loss)
I0109 20:35:33.962754  4467 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0109 20:35:37.211076  4467 solver.cpp:266] Iteration 11900 (30.7849 iter/s, 3.24835s/100 iter), loss = 0.244162
I0109 20:35:37.211138  4467 solver.cpp:285]     Train net output #0: loss = 0.244162 (* 1 = 0.244162 loss)
I0109 20:35:37.211150  4467 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0109 20:35:40.427697  4467 solver.cpp:418] Iteration 12000, Testing net (#0)
I0109 20:35:41.234895  4467 solver.cpp:517]     Test net output #0: loss = 0.46978 (* 1 = 0.46978 loss)
I0109 20:35:41.234932  4467 solver.cpp:517]     Test net output #1: top-1 = 0.846778
I0109 20:35:41.234941  4467 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0109 20:35:41.265365  4467 solver.cpp:266] Iteration 12000 (24.6653 iter/s, 4.05428s/100 iter), loss = 0.293573
I0109 20:35:41.265403  4467 solver.cpp:285]     Train net output #0: loss = 0.293573 (* 1 = 0.293573 loss)
I0109 20:35:41.265417  4467 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0109 20:35:44.508867  4467 solver.cpp:266] Iteration 12100 (30.8309 iter/s, 3.2435s/100 iter), loss = 0.277665
I0109 20:35:44.508932  4467 solver.cpp:285]     Train net output #0: loss = 0.277665 (* 1 = 0.277665 loss)
I0109 20:35:44.508945  4467 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0109 20:35:47.738857  4467 solver.cpp:266] Iteration 12200 (30.9602 iter/s, 3.22996s/100 iter), loss = 0.297457
I0109 20:35:47.738920  4467 solver.cpp:285]     Train net output #0: loss = 0.297457 (* 1 = 0.297457 loss)
I0109 20:35:47.738934  4467 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0109 20:35:50.972544  4467 solver.cpp:266] Iteration 12300 (30.925 iter/s, 3.23363s/100 iter), loss = 0.240406
I0109 20:35:50.972606  4467 solver.cpp:285]     Train net output #0: loss = 0.240406 (* 1 = 0.240406 loss)
I0109 20:35:50.972618  4467 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0109 20:35:54.205255  4467 solver.cpp:266] Iteration 12400 (30.9341 iter/s, 3.23268s/100 iter), loss = 0.16461
I0109 20:35:54.205471  4467 solver.cpp:285]     Train net output #0: loss = 0.16461 (* 1 = 0.16461 loss)
I0109 20:35:54.205485  4467 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0109 20:35:57.428747  4467 solver.cpp:266] Iteration 12500 (31.024 iter/s, 3.22331s/100 iter), loss = 0.212512
I0109 20:35:57.428810  4467 solver.cpp:285]     Train net output #0: loss = 0.212512 (* 1 = 0.212512 loss)
I0109 20:35:57.428823  4467 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0109 20:36:00.661473  4467 solver.cpp:266] Iteration 12600 (30.934 iter/s, 3.23269s/100 iter), loss = 0.163978
I0109 20:36:00.661535  4467 solver.cpp:285]     Train net output #0: loss = 0.163978 (* 1 = 0.163978 loss)
I0109 20:36:00.661546  4467 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0109 20:36:03.889526  4467 solver.cpp:266] Iteration 12700 (30.9787 iter/s, 3.22802s/100 iter), loss = 0.221353
I0109 20:36:03.889601  4467 solver.cpp:285]     Train net output #0: loss = 0.221353 (* 1 = 0.221353 loss)
I0109 20:36:03.889614  4467 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0109 20:36:07.114461  4467 solver.cpp:266] Iteration 12800 (31.009 iter/s, 3.22487s/100 iter), loss = 0.236208
I0109 20:36:07.114526  4467 solver.cpp:285]     Train net output #0: loss = 0.236208 (* 1 = 0.236208 loss)
I0109 20:36:07.114537  4467 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0109 20:36:10.338330  4467 solver.cpp:266] Iteration 12900 (31.019 iter/s, 3.22383s/100 iter), loss = 0.281787
I0109 20:36:10.338395  4467 solver.cpp:285]     Train net output #0: loss = 0.281787 (* 1 = 0.281787 loss)
I0109 20:36:10.338408  4467 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0109 20:36:13.529601  4467 solver.cpp:418] Iteration 13000, Testing net (#0)
I0109 20:36:14.334004  4467 solver.cpp:517]     Test net output #0: loss = 0.511681 (* 1 = 0.511681 loss)
I0109 20:36:14.334041  4467 solver.cpp:517]     Test net output #1: top-1 = 0.838556
I0109 20:36:14.334049  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0109 20:36:14.364475  4467 solver.cpp:266] Iteration 13000 (24.8378 iter/s, 4.02612s/100 iter), loss = 0.3314
I0109 20:36:14.364509  4467 solver.cpp:285]     Train net output #0: loss = 0.3314 (* 1 = 0.3314 loss)
I0109 20:36:14.364523  4467 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0109 20:36:17.588522  4467 solver.cpp:266] Iteration 13100 (31.017 iter/s, 3.22404s/100 iter), loss = 0.192303
I0109 20:36:17.588584  4467 solver.cpp:285]     Train net output #0: loss = 0.192303 (* 1 = 0.192303 loss)
I0109 20:36:17.588596  4467 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0109 20:36:20.820838  4467 solver.cpp:266] Iteration 13200 (30.9379 iter/s, 3.23228s/100 iter), loss = 0.367451
I0109 20:36:20.820904  4467 solver.cpp:285]     Train net output #0: loss = 0.367451 (* 1 = 0.367451 loss)
I0109 20:36:20.820916  4467 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0109 20:36:24.045179  4467 solver.cpp:266] Iteration 13300 (31.0147 iter/s, 3.22428s/100 iter), loss = 0.25301
I0109 20:36:24.045243  4467 solver.cpp:285]     Train net output #0: loss = 0.25301 (* 1 = 0.25301 loss)
I0109 20:36:24.045255  4467 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0109 20:36:27.276751  4467 solver.cpp:266] Iteration 13400 (30.945 iter/s, 3.23154s/100 iter), loss = 0.357323
I0109 20:36:27.276899  4467 solver.cpp:285]     Train net output #0: loss = 0.357323 (* 1 = 0.357323 loss)
I0109 20:36:27.276913  4467 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0109 20:36:30.503104  4467 solver.cpp:266] Iteration 13500 (30.9959 iter/s, 3.22624s/100 iter), loss = 0.25278
I0109 20:36:30.503166  4467 solver.cpp:285]     Train net output #0: loss = 0.25278 (* 1 = 0.25278 loss)
I0109 20:36:30.503178  4467 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0109 20:36:33.737462  4467 solver.cpp:266] Iteration 13600 (30.9184 iter/s, 3.23432s/100 iter), loss = 0.154863
I0109 20:36:33.737525  4467 solver.cpp:285]     Train net output #0: loss = 0.154863 (* 1 = 0.154863 loss)
I0109 20:36:33.737536  4467 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0109 20:36:36.963740  4467 solver.cpp:266] Iteration 13700 (30.9958 iter/s, 3.22624s/100 iter), loss = 0.216283
I0109 20:36:36.963801  4467 solver.cpp:285]     Train net output #0: loss = 0.216283 (* 1 = 0.216283 loss)
I0109 20:36:36.963814  4467 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0109 20:36:40.193773  4467 solver.cpp:266] Iteration 13800 (30.9601 iter/s, 3.22997s/100 iter), loss = 0.176586
I0109 20:36:40.193851  4467 solver.cpp:285]     Train net output #0: loss = 0.176586 (* 1 = 0.176586 loss)
I0109 20:36:40.193866  4467 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0109 20:36:43.420318  4467 solver.cpp:266] Iteration 13900 (30.9934 iter/s, 3.2265s/100 iter), loss = 0.222502
I0109 20:36:43.420382  4467 solver.cpp:285]     Train net output #0: loss = 0.222502 (* 1 = 0.222502 loss)
I0109 20:36:43.420394  4467 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0109 20:36:46.622880  4467 solver.cpp:418] Iteration 14000, Testing net (#0)
I0109 20:36:47.426678  4467 solver.cpp:517]     Test net output #0: loss = 0.523518 (* 1 = 0.523518 loss)
I0109 20:36:47.426717  4467 solver.cpp:517]     Test net output #1: top-1 = 0.835555
I0109 20:36:47.426724  4467 solver.cpp:517]     Test net output #2: top-5 = 0.988555
I0109 20:36:47.457010  4467 solver.cpp:266] Iteration 14000 (24.7729 iter/s, 4.03667s/100 iter), loss = 0.289895
I0109 20:36:47.457046  4467 solver.cpp:285]     Train net output #0: loss = 0.289895 (* 1 = 0.289895 loss)
I0109 20:36:47.457062  4467 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0109 20:36:50.686388  4467 solver.cpp:266] Iteration 14100 (30.9658 iter/s, 3.22936s/100 iter), loss = 0.238971
I0109 20:36:50.686455  4467 solver.cpp:285]     Train net output #0: loss = 0.238971 (* 1 = 0.238971 loss)
I0109 20:36:50.686468  4467 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0109 20:36:53.915830  4467 solver.cpp:266] Iteration 14200 (30.9655 iter/s, 3.2294s/100 iter), loss = 0.241792
I0109 20:36:53.915899  4467 solver.cpp:285]     Train net output #0: loss = 0.241792 (* 1 = 0.241792 loss)
I0109 20:36:53.915911  4467 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0109 20:36:57.145061  4467 solver.cpp:266] Iteration 14300 (30.9678 iter/s, 3.22916s/100 iter), loss = 0.306008
I0109 20:36:57.145126  4467 solver.cpp:285]     Train net output #0: loss = 0.306008 (* 1 = 0.306008 loss)
I0109 20:36:57.145138  4467 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0109 20:37:00.370239  4467 solver.cpp:266] Iteration 14400 (31.0064 iter/s, 3.22514s/100 iter), loss = 0.186443
I0109 20:37:00.370470  4467 solver.cpp:285]     Train net output #0: loss = 0.186443 (* 1 = 0.186443 loss)
I0109 20:37:00.370487  4467 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0109 20:37:03.597246  4467 solver.cpp:266] Iteration 14500 (30.9904 iter/s, 3.22681s/100 iter), loss = 0.134705
I0109 20:37:03.597319  4467 solver.cpp:285]     Train net output #0: loss = 0.134705 (* 1 = 0.134705 loss)
I0109 20:37:03.597332  4467 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0109 20:37:06.825012  4467 solver.cpp:266] Iteration 14600 (30.9816 iter/s, 3.22772s/100 iter), loss = 0.197659
I0109 20:37:06.825078  4467 solver.cpp:285]     Train net output #0: loss = 0.197659 (* 1 = 0.197659 loss)
I0109 20:37:06.825090  4467 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0109 20:37:10.049863  4467 solver.cpp:266] Iteration 14700 (31.0098 iter/s, 3.22478s/100 iter), loss = 0.281372
I0109 20:37:10.049952  4467 solver.cpp:285]     Train net output #0: loss = 0.281372 (* 1 = 0.281372 loss)
I0109 20:37:10.049966  4467 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0109 20:37:13.274387  4467 solver.cpp:266] Iteration 14800 (31.0129 iter/s, 3.22447s/100 iter), loss = 0.281262
I0109 20:37:13.274452  4467 solver.cpp:285]     Train net output #0: loss = 0.281262 (* 1 = 0.281262 loss)
I0109 20:37:13.274464  4467 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0109 20:37:16.499524  4467 solver.cpp:266] Iteration 14900 (31.0068 iter/s, 3.2251s/100 iter), loss = 0.187516
I0109 20:37:16.499590  4467 solver.cpp:285]     Train net output #0: loss = 0.187516 (* 1 = 0.187516 loss)
I0109 20:37:16.499603  4467 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0109 20:37:19.694450  4467 solver.cpp:418] Iteration 15000, Testing net (#0)
I0109 20:37:20.500715  4467 solver.cpp:517]     Test net output #0: loss = 0.497175 (* 1 = 0.497175 loss)
I0109 20:37:20.500758  4467 solver.cpp:517]     Test net output #1: top-1 = 0.840778
I0109 20:37:20.500766  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0109 20:37:20.531128  4467 solver.cpp:266] Iteration 15000 (24.8042 iter/s, 4.03157s/100 iter), loss = 0.231316
I0109 20:37:20.531209  4467 solver.cpp:285]     Train net output #0: loss = 0.231316 (* 1 = 0.231316 loss)
I0109 20:37:20.531224  4467 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0109 20:37:23.755465  4467 solver.cpp:266] Iteration 15100 (31.0146 iter/s, 3.22428s/100 iter), loss = 0.21445
I0109 20:37:23.755529  4467 solver.cpp:285]     Train net output #0: loss = 0.21445 (* 1 = 0.21445 loss)
I0109 20:37:23.755542  4467 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0109 20:37:26.979007  4467 solver.cpp:266] Iteration 15200 (31.0221 iter/s, 3.2235s/100 iter), loss = 0.262303
I0109 20:37:26.979071  4467 solver.cpp:285]     Train net output #0: loss = 0.262303 (* 1 = 0.262303 loss)
I0109 20:37:26.979084  4467 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0109 20:37:30.202451  4467 solver.cpp:266] Iteration 15300 (31.0234 iter/s, 3.22338s/100 iter), loss = 0.36307
I0109 20:37:30.202536  4467 solver.cpp:285]     Train net output #0: loss = 0.36307 (* 1 = 0.36307 loss)
I0109 20:37:30.202551  4467 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0109 20:37:33.424945  4467 solver.cpp:266] Iteration 15400 (31.0324 iter/s, 3.22244s/100 iter), loss = 0.281388
I0109 20:37:33.425144  4467 solver.cpp:285]     Train net output #0: loss = 0.281388 (* 1 = 0.281388 loss)
I0109 20:37:33.425158  4467 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0109 20:37:36.647686  4467 solver.cpp:266] Iteration 15500 (31.0311 iter/s, 3.22257s/100 iter), loss = 0.235782
I0109 20:37:36.647753  4467 solver.cpp:285]     Train net output #0: loss = 0.235782 (* 1 = 0.235782 loss)
I0109 20:37:36.647763  4467 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0109 20:37:39.872185  4467 solver.cpp:266] Iteration 15600 (31.013 iter/s, 3.22445s/100 iter), loss = 0.138901
I0109 20:37:39.872270  4467 solver.cpp:285]     Train net output #0: loss = 0.138901 (* 1 = 0.138901 loss)
I0109 20:37:39.872285  4467 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0109 20:37:43.096822  4467 solver.cpp:266] Iteration 15700 (31.012 iter/s, 3.22455s/100 iter), loss = 0.31688
I0109 20:37:43.096887  4467 solver.cpp:285]     Train net output #0: loss = 0.31688 (* 1 = 0.31688 loss)
I0109 20:37:43.096899  4467 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0109 20:37:46.317669  4467 solver.cpp:266] Iteration 15800 (31.0481 iter/s, 3.2208s/100 iter), loss = 0.123777
I0109 20:37:46.317744  4467 solver.cpp:285]     Train net output #0: loss = 0.123777 (* 1 = 0.123777 loss)
I0109 20:37:46.317759  4467 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0109 20:37:49.537843  4467 solver.cpp:266] Iteration 15900 (31.0547 iter/s, 3.22012s/100 iter), loss = 0.321867
I0109 20:37:49.537909  4467 solver.cpp:285]     Train net output #0: loss = 0.321867 (* 1 = 0.321867 loss)
I0109 20:37:49.537919  4467 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0109 20:37:52.724812  4467 solver.cpp:418] Iteration 16000, Testing net (#0)
I0109 20:37:53.531301  4467 solver.cpp:517]     Test net output #0: loss = 0.480082 (* 1 = 0.480082 loss)
I0109 20:37:53.531338  4467 solver.cpp:517]     Test net output #1: top-1 = 0.846111
I0109 20:37:53.531347  4467 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0109 20:37:53.561929  4467 solver.cpp:266] Iteration 16000 (24.8505 iter/s, 4.02406s/100 iter), loss = 0.34119
I0109 20:37:53.561966  4467 solver.cpp:285]     Train net output #0: loss = 0.34119 (* 1 = 0.34119 loss)
I0109 20:37:53.561981  4467 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0109 20:37:56.779389  4467 solver.cpp:266] Iteration 16100 (31.0805 iter/s, 3.21745s/100 iter), loss = 0.238603
I0109 20:37:56.779449  4467 solver.cpp:285]     Train net output #0: loss = 0.238603 (* 1 = 0.238603 loss)
I0109 20:37:56.779461  4467 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0109 20:37:59.999562  4467 solver.cpp:266] Iteration 16200 (31.0548 iter/s, 3.22011s/100 iter), loss = 0.246522
I0109 20:37:59.999624  4467 solver.cpp:285]     Train net output #0: loss = 0.246522 (* 1 = 0.246522 loss)
I0109 20:37:59.999635  4467 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0109 20:38:03.219854  4467 solver.cpp:266] Iteration 16300 (31.0534 iter/s, 3.22025s/100 iter), loss = 0.283833
I0109 20:38:03.219918  4467 solver.cpp:285]     Train net output #0: loss = 0.283832 (* 1 = 0.283832 loss)
I0109 20:38:03.219929  4467 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0109 20:38:06.441895  4467 solver.cpp:266] Iteration 16400 (31.0366 iter/s, 3.222s/100 iter), loss = 0.251195
I0109 20:38:06.442114  4467 solver.cpp:285]     Train net output #0: loss = 0.251195 (* 1 = 0.251195 loss)
I0109 20:38:06.442131  4467 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0109 20:38:09.662544  4467 solver.cpp:266] Iteration 16500 (31.0515 iter/s, 3.22046s/100 iter), loss = 0.161652
I0109 20:38:09.662606  4467 solver.cpp:285]     Train net output #0: loss = 0.161652 (* 1 = 0.161652 loss)
I0109 20:38:09.662618  4467 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0109 20:38:12.884325  4467 solver.cpp:266] Iteration 16600 (31.0391 iter/s, 3.22174s/100 iter), loss = 0.188293
I0109 20:38:12.884385  4467 solver.cpp:285]     Train net output #0: loss = 0.188293 (* 1 = 0.188293 loss)
I0109 20:38:12.884397  4467 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0109 20:38:16.105856  4467 solver.cpp:266] Iteration 16700 (31.0417 iter/s, 3.22147s/100 iter), loss = 0.195528
I0109 20:38:16.105918  4467 solver.cpp:285]     Train net output #0: loss = 0.195528 (* 1 = 0.195528 loss)
I0109 20:38:16.105931  4467 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0109 20:38:19.325917  4467 solver.cpp:266] Iteration 16800 (31.0557 iter/s, 3.22002s/100 iter), loss = 0.237427
I0109 20:38:19.325979  4467 solver.cpp:285]     Train net output #0: loss = 0.237427 (* 1 = 0.237427 loss)
I0109 20:38:19.325990  4467 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0109 20:38:22.547222  4467 solver.cpp:266] Iteration 16900 (31.0437 iter/s, 3.22127s/100 iter), loss = 0.240131
I0109 20:38:22.547286  4467 solver.cpp:285]     Train net output #0: loss = 0.240131 (* 1 = 0.240131 loss)
I0109 20:38:22.547297  4467 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0109 20:38:25.740183  4467 solver.cpp:418] Iteration 17000, Testing net (#0)
I0109 20:38:26.547797  4467 solver.cpp:517]     Test net output #0: loss = 0.462786 (* 1 = 0.462786 loss)
I0109 20:38:26.547837  4467 solver.cpp:517]     Test net output #1: top-1 = 0.850667
I0109 20:38:26.547847  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0109 20:38:26.578248  4467 solver.cpp:266] Iteration 17000 (24.8078 iter/s, 4.03099s/100 iter), loss = 0.198684
I0109 20:38:26.578330  4467 solver.cpp:285]     Train net output #0: loss = 0.198684 (* 1 = 0.198684 loss)
I0109 20:38:26.578346  4467 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0109 20:38:29.800921  4467 solver.cpp:266] Iteration 17100 (31.0307 iter/s, 3.22262s/100 iter), loss = 0.217914
I0109 20:38:29.800985  4467 solver.cpp:285]     Train net output #0: loss = 0.217914 (* 1 = 0.217914 loss)
I0109 20:38:29.800997  4467 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0109 20:38:33.024729  4467 solver.cpp:266] Iteration 17200 (31.0198 iter/s, 3.22374s/100 iter), loss = 0.215588
I0109 20:38:33.024796  4467 solver.cpp:285]     Train net output #0: loss = 0.215588 (* 1 = 0.215588 loss)
I0109 20:38:33.024807  4467 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0109 20:38:36.249470  4467 solver.cpp:266] Iteration 17300 (31.0107 iter/s, 3.2247s/100 iter), loss = 0.205208
I0109 20:38:36.249547  4467 solver.cpp:285]     Train net output #0: loss = 0.205208 (* 1 = 0.205208 loss)
I0109 20:38:36.249562  4467 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0109 20:38:39.471585  4467 solver.cpp:266] Iteration 17400 (31.036 iter/s, 3.22207s/100 iter), loss = 0.228602
I0109 20:38:39.471784  4467 solver.cpp:285]     Train net output #0: loss = 0.228602 (* 1 = 0.228602 loss)
I0109 20:38:39.471798  4467 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0109 20:38:42.695307  4467 solver.cpp:266] Iteration 17500 (31.0217 iter/s, 3.22355s/100 iter), loss = 0.170553
I0109 20:38:42.695369  4467 solver.cpp:285]     Train net output #0: loss = 0.170553 (* 1 = 0.170553 loss)
I0109 20:38:42.695381  4467 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0109 20:38:45.919271  4467 solver.cpp:266] Iteration 17600 (31.0181 iter/s, 3.22393s/100 iter), loss = 0.152575
I0109 20:38:45.919342  4467 solver.cpp:285]     Train net output #0: loss = 0.152575 (* 1 = 0.152575 loss)
I0109 20:38:45.919360  4467 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0109 20:38:49.144771  4467 solver.cpp:266] Iteration 17700 (31.0036 iter/s, 3.22543s/100 iter), loss = 0.216055
I0109 20:38:49.144840  4467 solver.cpp:285]     Train net output #0: loss = 0.216055 (* 1 = 0.216055 loss)
I0109 20:38:49.144851  4467 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0109 20:38:52.371054  4467 solver.cpp:266] Iteration 17800 (30.9958 iter/s, 3.22624s/100 iter), loss = 0.160173
I0109 20:38:52.371121  4467 solver.cpp:285]     Train net output #0: loss = 0.160173 (* 1 = 0.160173 loss)
I0109 20:38:52.371134  4467 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0109 20:38:55.592566  4467 solver.cpp:266] Iteration 17900 (31.0417 iter/s, 3.22147s/100 iter), loss = 0.248115
I0109 20:38:55.592631  4467 solver.cpp:285]     Train net output #0: loss = 0.248115 (* 1 = 0.248115 loss)
I0109 20:38:55.592643  4467 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0109 20:38:58.786284  4467 solver.cpp:418] Iteration 18000, Testing net (#0)
I0109 20:38:59.591140  4467 solver.cpp:517]     Test net output #0: loss = 0.482581 (* 1 = 0.482581 loss)
I0109 20:38:59.591178  4467 solver.cpp:517]     Test net output #1: top-1 = 0.849555
I0109 20:38:59.591187  4467 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0109 20:38:59.621610  4467 solver.cpp:266] Iteration 18000 (24.82 iter/s, 4.02901s/100 iter), loss = 0.183657
I0109 20:38:59.621647  4467 solver.cpp:285]     Train net output #0: loss = 0.183657 (* 1 = 0.183657 loss)
I0109 20:38:59.621662  4467 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0109 20:39:02.849726  4467 solver.cpp:266] Iteration 18100 (30.978 iter/s, 3.2281s/100 iter), loss = 0.277166
I0109 20:39:02.849797  4467 solver.cpp:285]     Train net output #0: loss = 0.277166 (* 1 = 0.277166 loss)
I0109 20:39:02.849812  4467 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0109 20:39:06.075572  4467 solver.cpp:266] Iteration 18200 (31.0003 iter/s, 3.22577s/100 iter), loss = 0.143604
I0109 20:39:06.075641  4467 solver.cpp:285]     Train net output #0: loss = 0.143604 (* 1 = 0.143604 loss)
I0109 20:39:06.075654  4467 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0109 20:39:09.299393  4467 solver.cpp:266] Iteration 18300 (31.0195 iter/s, 3.22377s/100 iter), loss = 0.192129
I0109 20:39:09.299480  4467 solver.cpp:285]     Train net output #0: loss = 0.192129 (* 1 = 0.192129 loss)
I0109 20:39:09.299494  4467 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0109 20:39:12.522351  4467 solver.cpp:266] Iteration 18400 (31.028 iter/s, 3.2229s/100 iter), loss = 0.218403
I0109 20:39:12.522505  4467 solver.cpp:285]     Train net output #0: loss = 0.218403 (* 1 = 0.218403 loss)
I0109 20:39:12.522521  4467 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0109 20:39:15.743979  4467 solver.cpp:266] Iteration 18500 (31.0414 iter/s, 3.2215s/100 iter), loss = 0.277237
I0109 20:39:15.744040  4467 solver.cpp:285]     Train net output #0: loss = 0.277237 (* 1 = 0.277237 loss)
I0109 20:39:15.744053  4467 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0109 20:39:18.964826  4467 solver.cpp:266] Iteration 18600 (31.0481 iter/s, 3.22081s/100 iter), loss = 0.27563
I0109 20:39:18.964890  4467 solver.cpp:285]     Train net output #0: loss = 0.27563 (* 1 = 0.27563 loss)
I0109 20:39:18.964901  4467 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0109 20:39:22.186731  4467 solver.cpp:266] Iteration 18700 (31.0381 iter/s, 3.22184s/100 iter), loss = 0.201717
I0109 20:39:22.186792  4467 solver.cpp:285]     Train net output #0: loss = 0.201717 (* 1 = 0.201717 loss)
I0109 20:39:22.186805  4467 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0109 20:39:25.406999  4467 solver.cpp:266] Iteration 18800 (31.0536 iter/s, 3.22024s/100 iter), loss = 0.173048
I0109 20:39:25.407064  4467 solver.cpp:285]     Train net output #0: loss = 0.173048 (* 1 = 0.173048 loss)
I0109 20:39:25.407076  4467 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0109 20:39:28.626935  4467 solver.cpp:266] Iteration 18900 (31.0569 iter/s, 3.21989s/100 iter), loss = 0.145451
I0109 20:39:28.626997  4467 solver.cpp:285]     Train net output #0: loss = 0.145451 (* 1 = 0.145451 loss)
I0109 20:39:28.627009  4467 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0109 20:39:31.818017  4467 solver.cpp:418] Iteration 19000, Testing net (#0)
I0109 20:39:32.626353  4467 solver.cpp:517]     Test net output #0: loss = 0.456689 (* 1 = 0.456689 loss)
I0109 20:39:32.626392  4467 solver.cpp:517]     Test net output #1: top-1 = 0.853111
I0109 20:39:32.626400  4467 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0109 20:39:32.657209  4467 solver.cpp:266] Iteration 19000 (24.8123 iter/s, 4.03025s/100 iter), loss = 0.209503
I0109 20:39:32.657249  4467 solver.cpp:285]     Train net output #0: loss = 0.209502 (* 1 = 0.209502 loss)
I0109 20:39:32.657268  4467 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0109 20:39:35.878804  4467 solver.cpp:266] Iteration 19100 (31.0407 iter/s, 3.22158s/100 iter), loss = 0.180129
I0109 20:39:35.878868  4467 solver.cpp:285]     Train net output #0: loss = 0.180129 (* 1 = 0.180129 loss)
I0109 20:39:35.878880  4467 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0109 20:39:39.101410  4467 solver.cpp:266] Iteration 19200 (31.0314 iter/s, 3.22254s/100 iter), loss = 0.223424
I0109 20:39:39.101480  4467 solver.cpp:285]     Train net output #0: loss = 0.223423 (* 1 = 0.223423 loss)
I0109 20:39:39.101492  4467 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0109 20:39:42.323938  4467 solver.cpp:266] Iteration 19300 (31.0319 iter/s, 3.22249s/100 iter), loss = 0.234414
I0109 20:39:42.324003  4467 solver.cpp:285]     Train net output #0: loss = 0.234414 (* 1 = 0.234414 loss)
I0109 20:39:42.324017  4467 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0109 20:39:45.545931  4467 solver.cpp:266] Iteration 19400 (31.0371 iter/s, 3.22195s/100 iter), loss = 0.164345
I0109 20:39:45.546144  4467 solver.cpp:285]     Train net output #0: loss = 0.164345 (* 1 = 0.164345 loss)
I0109 20:39:45.546160  4467 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0109 20:39:48.767524  4467 solver.cpp:266] Iteration 19500 (31.0423 iter/s, 3.22141s/100 iter), loss = 0.18561
I0109 20:39:48.767590  4467 solver.cpp:285]     Train net output #0: loss = 0.18561 (* 1 = 0.18561 loss)
I0109 20:39:48.767601  4467 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0109 20:39:51.987812  4467 solver.cpp:266] Iteration 19600 (31.0535 iter/s, 3.22025s/100 iter), loss = 0.198879
I0109 20:39:51.987874  4467 solver.cpp:285]     Train net output #0: loss = 0.198879 (* 1 = 0.198879 loss)
I0109 20:39:51.987885  4467 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0109 20:39:55.209808  4467 solver.cpp:266] Iteration 19700 (31.0373 iter/s, 3.22193s/100 iter), loss = 0.224662
I0109 20:39:55.209873  4467 solver.cpp:285]     Train net output #0: loss = 0.224662 (* 1 = 0.224662 loss)
I0109 20:39:55.209887  4467 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0109 20:39:58.431563  4467 solver.cpp:266] Iteration 19800 (31.0394 iter/s, 3.22172s/100 iter), loss = 0.186119
I0109 20:39:58.431627  4467 solver.cpp:285]     Train net output #0: loss = 0.186119 (* 1 = 0.186119 loss)
I0109 20:39:58.431638  4467 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0109 20:40:01.654413  4467 solver.cpp:266] Iteration 19900 (31.0288 iter/s, 3.22281s/100 iter), loss = 0.245075
I0109 20:40:01.654476  4467 solver.cpp:285]     Train net output #0: loss = 0.245075 (* 1 = 0.245075 loss)
I0109 20:40:01.654487  4467 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0109 20:40:04.844513  4467 solver.cpp:929] Snapshotting to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/_iter_20000.caffemodel
I0109 20:40:04.946494  4467 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ML/cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/_iter_20000.solverstate
I0109 20:40:04.972524  4467 solver.cpp:378] Iteration 20000, loss = 0.0386015
I0109 20:40:04.972563  4467 solver.cpp:418] Iteration 20000, Testing net (#0)
I0109 20:40:05.776316  4467 solver.cpp:517]     Test net output #0: loss = 0.45866 (* 1 = 0.45866 loss)
I0109 20:40:05.776355  4467 solver.cpp:517]     Test net output #1: top-1 = 0.854889
I0109 20:40:05.776363  4467 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0109 20:40:05.776371  4467 solver.cpp:386] Optimization Done (30.3302 iter/s).
I0109 20:40:05.776377  4467 caffe_interface.cpp:530] Optimization Done.
I0109 20:40:06.077116  4493 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0109 20:40:06.077960  4493 gpu_memory.cpp:55] Total memory: 11996954624, Free: 11918311424, dev_info[0]: total=11996954624 free=11918311424
I0109 20:40:06.077978  4493 caffe_interface.cpp:66] Use GPU with device ID 0
I0109 20:40:06.078291  4493 caffe_interface.cpp:70] GPU device name: Tesla K80
I0109 20:40:06.745707  4493 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:40:06.746049  4493 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:40:06.746253  4493 layer_factory.hpp:77] Creating layer data
I0109 20:40:06.746345  4493 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:40:06.747061  4493 net.cpp:94] Creating Layer data
I0109 20:40:06.747082  4493 net.cpp:409] data -> data
I0109 20:40:06.747103  4493 net.cpp:409] data -> label
I0109 20:40:06.748075  4500 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:40:06.748121  4500 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:40:06.748246  4493 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:40:06.748414  4493 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:40:06.756439  4493 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:40:06.756512  4493 net.cpp:144] Setting up data
I0109 20:40:06.756536  4493 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:40:06.756547  4493 net.cpp:151] Top shape: 50 (50)
I0109 20:40:06.756553  4493 net.cpp:159] Memory required for data: 614600
I0109 20:40:06.756561  4493 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:40:06.756599  4493 net.cpp:94] Creating Layer label_data_1_split
I0109 20:40:06.756646  4493 net.cpp:435] label_data_1_split <- label
I0109 20:40:06.756705  4493 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:40:06.756757  4493 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:40:06.756815  4493 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:40:06.756953  4493 net.cpp:144] Setting up label_data_1_split
I0109 20:40:06.756988  4493 net.cpp:151] Top shape: 50 (50)
I0109 20:40:06.756997  4493 net.cpp:151] Top shape: 50 (50)
I0109 20:40:06.757005  4493 net.cpp:151] Top shape: 50 (50)
I0109 20:40:06.757053  4493 net.cpp:159] Memory required for data: 615200
I0109 20:40:06.757062  4493 layer_factory.hpp:77] Creating layer conv1
I0109 20:40:06.757112  4493 net.cpp:94] Creating Layer conv1
I0109 20:40:06.757158  4493 net.cpp:435] conv1 <- data
I0109 20:40:06.757211  4493 net.cpp:409] conv1 -> conv1
I0109 20:40:06.758414  4493 net.cpp:144] Setting up conv1
I0109 20:40:06.758438  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.758448  4493 net.cpp:159] Memory required for data: 7168800
I0109 20:40:06.758509  4493 layer_factory.hpp:77] Creating layer bn1
I0109 20:40:06.758574  4493 net.cpp:94] Creating Layer bn1
I0109 20:40:06.758621  4493 net.cpp:435] bn1 <- conv1
I0109 20:40:06.758679  4493 net.cpp:409] bn1 -> scale1
I0109 20:40:06.759793  4493 net.cpp:144] Setting up bn1
I0109 20:40:06.759814  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.759821  4493 net.cpp:159] Memory required for data: 13722400
I0109 20:40:06.759881  4493 layer_factory.hpp:77] Creating layer relu1
I0109 20:40:06.759937  4493 net.cpp:94] Creating Layer relu1
I0109 20:40:06.759981  4493 net.cpp:435] relu1 <- scale1
I0109 20:40:06.760031  4493 net.cpp:409] relu1 -> relu1
I0109 20:40:06.760131  4493 net.cpp:144] Setting up relu1
I0109 20:40:06.760206  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.760226  4493 net.cpp:159] Memory required for data: 20276000
I0109 20:40:06.760233  4493 layer_factory.hpp:77] Creating layer conv2
I0109 20:40:06.760321  4493 net.cpp:94] Creating Layer conv2
I0109 20:40:06.760367  4493 net.cpp:435] conv2 <- relu1
I0109 20:40:06.760423  4493 net.cpp:409] conv2 -> conv2
I0109 20:40:06.761543  4493 net.cpp:144] Setting up conv2
I0109 20:40:06.761569  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.761579  4493 net.cpp:159] Memory required for data: 26829600
I0109 20:40:06.761651  4493 layer_factory.hpp:77] Creating layer bn2
I0109 20:40:06.761704  4493 net.cpp:94] Creating Layer bn2
I0109 20:40:06.761721  4493 net.cpp:435] bn2 <- conv2
I0109 20:40:06.761736  4493 net.cpp:409] bn2 -> scale2
I0109 20:40:06.762498  4493 net.cpp:144] Setting up bn2
I0109 20:40:06.762523  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.762532  4493 net.cpp:159] Memory required for data: 33383200
I0109 20:40:06.762590  4493 layer_factory.hpp:77] Creating layer relu2
I0109 20:40:06.762627  4493 net.cpp:94] Creating Layer relu2
I0109 20:40:06.762650  4493 net.cpp:435] relu2 <- scale2
I0109 20:40:06.762698  4493 net.cpp:409] relu2 -> relu2
I0109 20:40:06.762778  4493 net.cpp:144] Setting up relu2
I0109 20:40:06.762796  4493 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:06.762804  4493 net.cpp:159] Memory required for data: 39936800
I0109 20:40:06.762811  4493 layer_factory.hpp:77] Creating layer pool1
I0109 20:40:06.762874  4493 net.cpp:94] Creating Layer pool1
I0109 20:40:06.762892  4493 net.cpp:435] pool1 <- relu2
I0109 20:40:06.762905  4493 net.cpp:409] pool1 -> pool1
I0109 20:40:06.762980  4493 net.cpp:144] Setting up pool1
I0109 20:40:06.763005  4493 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:40:06.763016  4493 net.cpp:159] Memory required for data: 41575200
I0109 20:40:06.763022  4493 layer_factory.hpp:77] Creating layer drop1
I0109 20:40:06.763036  4493 net.cpp:94] Creating Layer drop1
I0109 20:40:06.763057  4493 net.cpp:435] drop1 <- pool1
I0109 20:40:06.763089  4493 net.cpp:409] drop1 -> drop1
I0109 20:40:06.763183  4493 net.cpp:144] Setting up drop1
I0109 20:40:06.763206  4493 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:40:06.763212  4493 net.cpp:159] Memory required for data: 43213600
I0109 20:40:06.763219  4493 layer_factory.hpp:77] Creating layer conv3
I0109 20:40:06.763298  4493 net.cpp:94] Creating Layer conv3
I0109 20:40:06.763317  4493 net.cpp:435] conv3 <- drop1
I0109 20:40:06.763331  4493 net.cpp:409] conv3 -> conv3
I0109 20:40:06.764611  4493 net.cpp:144] Setting up conv3
I0109 20:40:06.764636  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.764654  4493 net.cpp:159] Memory required for data: 46490400
I0109 20:40:06.764739  4493 layer_factory.hpp:77] Creating layer bn3
I0109 20:40:06.764763  4493 net.cpp:94] Creating Layer bn3
I0109 20:40:06.764772  4493 net.cpp:435] bn3 <- conv3
I0109 20:40:06.764854  4493 net.cpp:409] bn3 -> scale3
I0109 20:40:06.765631  4493 net.cpp:144] Setting up bn3
I0109 20:40:06.765652  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.765661  4493 net.cpp:159] Memory required for data: 49767200
I0109 20:40:06.765712  4493 layer_factory.hpp:77] Creating layer relu3
I0109 20:40:06.765756  4493 net.cpp:94] Creating Layer relu3
I0109 20:40:06.765784  4493 net.cpp:435] relu3 <- scale3
I0109 20:40:06.765808  4493 net.cpp:409] relu3 -> relu3
I0109 20:40:06.765892  4493 net.cpp:144] Setting up relu3
I0109 20:40:06.765913  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.765920  4493 net.cpp:159] Memory required for data: 53044000
I0109 20:40:06.765926  4493 layer_factory.hpp:77] Creating layer conv4
I0109 20:40:06.766001  4493 net.cpp:94] Creating Layer conv4
I0109 20:40:06.766021  4493 net.cpp:435] conv4 <- relu3
I0109 20:40:06.766036  4493 net.cpp:409] conv4 -> conv4
I0109 20:40:06.766564  4493 net.cpp:144] Setting up conv4
I0109 20:40:06.766592  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.766599  4493 net.cpp:159] Memory required for data: 56320800
I0109 20:40:06.766610  4493 layer_factory.hpp:77] Creating layer bn4
I0109 20:40:06.766691  4493 net.cpp:94] Creating Layer bn4
I0109 20:40:06.766706  4493 net.cpp:435] bn4 <- conv4
I0109 20:40:06.766741  4493 net.cpp:409] bn4 -> scale4
I0109 20:40:06.767797  4493 net.cpp:144] Setting up bn4
I0109 20:40:06.767817  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.767827  4493 net.cpp:159] Memory required for data: 59597600
I0109 20:40:06.767866  4493 layer_factory.hpp:77] Creating layer relu4
I0109 20:40:06.767918  4493 net.cpp:94] Creating Layer relu4
I0109 20:40:06.767931  4493 net.cpp:435] relu4 <- scale4
I0109 20:40:06.767943  4493 net.cpp:409] relu4 -> relu4
I0109 20:40:06.768044  4493 net.cpp:144] Setting up relu4
I0109 20:40:06.768069  4493 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:06.768077  4493 net.cpp:159] Memory required for data: 62874400
I0109 20:40:06.768086  4493 layer_factory.hpp:77] Creating layer pool2
I0109 20:40:06.768163  4493 net.cpp:94] Creating Layer pool2
I0109 20:40:06.768177  4493 net.cpp:435] pool2 <- relu4
I0109 20:40:06.768215  4493 net.cpp:409] pool2 -> pool2
I0109 20:40:06.768307  4493 net.cpp:144] Setting up pool2
I0109 20:40:06.768327  4493 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:40:06.768335  4493 net.cpp:159] Memory required for data: 63693600
I0109 20:40:06.768342  4493 layer_factory.hpp:77] Creating layer drop2
I0109 20:40:06.768409  4493 net.cpp:94] Creating Layer drop2
I0109 20:40:06.768422  4493 net.cpp:435] drop2 <- pool2
I0109 20:40:06.768432  4493 net.cpp:409] drop2 -> drop2
I0109 20:40:06.768543  4493 net.cpp:144] Setting up drop2
I0109 20:40:06.768565  4493 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:40:06.768573  4493 net.cpp:159] Memory required for data: 64512800
I0109 20:40:06.768580  4493 layer_factory.hpp:77] Creating layer fc1
I0109 20:40:06.768656  4493 net.cpp:94] Creating Layer fc1
I0109 20:40:06.768676  4493 net.cpp:435] fc1 <- drop2
I0109 20:40:06.768688  4493 net.cpp:409] fc1 -> fc1
I0109 20:40:06.790035  4493 net.cpp:144] Setting up fc1
I0109 20:40:06.790066  4493 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:06.790074  4493 net.cpp:159] Memory required for data: 64615200
I0109 20:40:06.790088  4493 layer_factory.hpp:77] Creating layer bn5
I0109 20:40:06.790107  4493 net.cpp:94] Creating Layer bn5
I0109 20:40:06.790133  4493 net.cpp:435] bn5 <- fc1
I0109 20:40:06.790163  4493 net.cpp:409] bn5 -> scale5
I0109 20:40:06.790804  4493 net.cpp:144] Setting up bn5
I0109 20:40:06.790827  4493 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:06.790849  4493 net.cpp:159] Memory required for data: 64717600
I0109 20:40:06.790879  4493 layer_factory.hpp:77] Creating layer relu5
I0109 20:40:06.790894  4493 net.cpp:94] Creating Layer relu5
I0109 20:40:06.790900  4493 net.cpp:435] relu5 <- scale5
I0109 20:40:06.790910  4493 net.cpp:409] relu5 -> relu5
I0109 20:40:06.790956  4493 net.cpp:144] Setting up relu5
I0109 20:40:06.790969  4493 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:06.790976  4493 net.cpp:159] Memory required for data: 64820000
I0109 20:40:06.790982  4493 layer_factory.hpp:77] Creating layer drop3
I0109 20:40:06.790992  4493 net.cpp:94] Creating Layer drop3
I0109 20:40:06.791002  4493 net.cpp:435] drop3 <- relu5
I0109 20:40:06.791015  4493 net.cpp:409] drop3 -> drop3
I0109 20:40:06.791072  4493 net.cpp:144] Setting up drop3
I0109 20:40:06.791092  4493 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:06.791100  4493 net.cpp:159] Memory required for data: 64922400
I0109 20:40:06.791106  4493 layer_factory.hpp:77] Creating layer fc2
I0109 20:40:06.791118  4493 net.cpp:94] Creating Layer fc2
I0109 20:40:06.791127  4493 net.cpp:435] fc2 <- drop3
I0109 20:40:06.791141  4493 net.cpp:409] fc2 -> fc2
I0109 20:40:06.791335  4493 net.cpp:144] Setting up fc2
I0109 20:40:06.791355  4493 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:06.791363  4493 net.cpp:159] Memory required for data: 64924400
I0109 20:40:06.791375  4493 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:40:06.791390  4493 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:40:06.791424  4493 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:40:06.791446  4493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:40:06.791461  4493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:40:06.791476  4493 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:40:06.791560  4493 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:40:06.791576  4493 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:06.791581  4493 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:06.791585  4493 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:06.791589  4493 net.cpp:159] Memory required for data: 64930400
I0109 20:40:06.791594  4493 layer_factory.hpp:77] Creating layer loss
I0109 20:40:06.791606  4493 net.cpp:94] Creating Layer loss
I0109 20:40:06.791621  4493 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:40:06.791631  4493 net.cpp:435] loss <- label_data_1_split_0
I0109 20:40:06.791641  4493 net.cpp:409] loss -> loss
I0109 20:40:06.791666  4493 layer_factory.hpp:77] Creating layer loss
I0109 20:40:06.791776  4493 net.cpp:144] Setting up loss
I0109 20:40:06.791792  4493 net.cpp:151] Top shape: (1)
I0109 20:40:06.791797  4493 net.cpp:154]     with loss weight 1
I0109 20:40:06.791847  4493 net.cpp:159] Memory required for data: 64930404
I0109 20:40:06.791855  4493 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:40:06.791867  4493 net.cpp:94] Creating Layer accuracy-top1
I0109 20:40:06.791882  4493 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:40:06.791893  4493 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:40:06.791905  4493 net.cpp:409] accuracy-top1 -> top-1
I0109 20:40:06.791929  4493 net.cpp:144] Setting up accuracy-top1
I0109 20:40:06.791939  4493 net.cpp:151] Top shape: (1)
I0109 20:40:06.791954  4493 net.cpp:159] Memory required for data: 64930408
I0109 20:40:06.791961  4493 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:40:06.791972  4493 net.cpp:94] Creating Layer accuracy-top5
I0109 20:40:06.791981  4493 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:40:06.791990  4493 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:40:06.791999  4493 net.cpp:409] accuracy-top5 -> top-5
I0109 20:40:06.792021  4493 net.cpp:144] Setting up accuracy-top5
I0109 20:40:06.792030  4493 net.cpp:151] Top shape: (1)
I0109 20:40:06.792037  4493 net.cpp:159] Memory required for data: 64930412
I0109 20:40:06.792047  4493 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:40:06.792060  4493 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:40:06.792093  4493 net.cpp:220] loss needs backward computation.
I0109 20:40:06.792104  4493 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:40:06.792110  4493 net.cpp:220] fc2 needs backward computation.
I0109 20:40:06.792119  4493 net.cpp:220] drop3 needs backward computation.
I0109 20:40:06.792129  4493 net.cpp:220] relu5 needs backward computation.
I0109 20:40:06.792137  4493 net.cpp:220] bn5 needs backward computation.
I0109 20:40:06.792145  4493 net.cpp:220] fc1 needs backward computation.
I0109 20:40:06.792150  4493 net.cpp:220] drop2 needs backward computation.
I0109 20:40:06.792160  4493 net.cpp:220] pool2 needs backward computation.
I0109 20:40:06.792166  4493 net.cpp:220] relu4 needs backward computation.
I0109 20:40:06.792177  4493 net.cpp:220] bn4 needs backward computation.
I0109 20:40:06.792183  4493 net.cpp:220] conv4 needs backward computation.
I0109 20:40:06.792193  4493 net.cpp:220] relu3 needs backward computation.
I0109 20:40:06.792201  4493 net.cpp:220] bn3 needs backward computation.
I0109 20:40:06.792207  4493 net.cpp:220] conv3 needs backward computation.
I0109 20:40:06.792215  4493 net.cpp:220] drop1 needs backward computation.
I0109 20:40:06.792222  4493 net.cpp:220] pool1 needs backward computation.
I0109 20:40:06.792230  4493 net.cpp:220] relu2 needs backward computation.
I0109 20:40:06.792240  4493 net.cpp:220] bn2 needs backward computation.
I0109 20:40:06.792255  4493 net.cpp:220] conv2 needs backward computation.
I0109 20:40:06.792264  4493 net.cpp:220] relu1 needs backward computation.
I0109 20:40:06.792270  4493 net.cpp:220] bn1 needs backward computation.
I0109 20:40:06.792277  4493 net.cpp:220] conv1 needs backward computation.
I0109 20:40:06.792287  4493 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:40:06.792296  4493 net.cpp:222] data does not need backward computation.
I0109 20:40:06.792311  4493 net.cpp:264] This network produces output loss
I0109 20:40:06.792320  4493 net.cpp:264] This network produces output top-1
I0109 20:40:06.792326  4493 net.cpp:264] This network produces output top-5
I0109 20:40:06.792366  4493 net.cpp:284] Network initialization done.
I0109 20:40:06.796349  4493 model_transformer.cpp:80] layer: data
I0109 20:40:06.796386  4493 model_transformer.cpp:80] layer: conv1
I0109 20:40:06.796438  4493 model_transformer.cpp:80] layer: bn1
I0109 20:40:06.796504  4493 model_transformer.cpp:80] layer: relu1
I0109 20:40:06.796532  4493 model_transformer.cpp:80] layer: conv2
I0109 20:40:06.796638  4493 model_transformer.cpp:80] layer: bn2
I0109 20:40:06.796682  4493 model_transformer.cpp:80] layer: relu2
I0109 20:40:06.796703  4493 model_transformer.cpp:80] layer: pool1
I0109 20:40:06.796715  4493 model_transformer.cpp:80] layer: drop1
I0109 20:40:06.796726  4493 model_transformer.cpp:80] layer: conv3
I0109 20:40:06.796932  4493 model_transformer.cpp:80] layer: bn3
I0109 20:40:06.796970  4493 model_transformer.cpp:80] layer: relu3
I0109 20:40:06.796989  4493 model_transformer.cpp:80] layer: conv4
I0109 20:40:06.797327  4493 model_transformer.cpp:80] layer: bn4
I0109 20:40:06.797363  4493 model_transformer.cpp:80] layer: relu4
I0109 20:40:06.797382  4493 model_transformer.cpp:80] layer: pool2
I0109 20:40:06.797394  4493 model_transformer.cpp:80] layer: drop2
I0109 20:40:06.797407  4493 model_transformer.cpp:80] layer: fc1
I0109 20:40:06.859717  4493 model_transformer.cpp:80] layer: bn5
I0109 20:40:06.859824  4493 model_transformer.cpp:80] layer: relu5
I0109 20:40:06.859848  4493 model_transformer.cpp:80] layer: drop3
I0109 20:40:06.859863  4493 model_transformer.cpp:80] layer: fc2
I0109 20:40:06.860003  4493 model_transformer.cpp:80] layer: loss
Output transformed caffemodel: transformed.caffemodel
I0109 20:40:07.745821  4508 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0109 20:40:07.746358  4508 net.cpp:52] Initializing net from parameters: 
name: "miniVggNet on Cifar10 m3 NO-inPlace"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "/home/ML/cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "bn4"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "bn5"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "bn5"
  top: "scale5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0109 20:40:07.746527  4508 layer_factory.hpp:77] Creating layer data
I0109 20:40:07.747459  4508 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:40:07.747936  4508 net.cpp:94] Creating Layer data
I0109 20:40:07.747958  4508 net.cpp:409] data -> data
I0109 20:40:07.747990  4508 net.cpp:409] data -> label
I0109 20:40:07.749040  4515 db_lmdb.cpp:35] Opened lmdb /home/ML/cifar10/input/lmdb/valid_lmdb
I0109 20:40:07.749086  4515 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0109 20:40:07.749162  4508 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0109 20:40:07.749187  4508 data_layer.cpp:83] output data size: 50,3,32,32
I0109 20:40:07.755828  4508 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0109 20:40:07.755892  4508 net.cpp:144] Setting up data
I0109 20:40:07.755918  4508 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0109 20:40:07.755928  4508 net.cpp:151] Top shape: 50 (50)
I0109 20:40:07.755934  4508 net.cpp:159] Memory required for data: 614600
I0109 20:40:07.755952  4508 layer_factory.hpp:77] Creating layer label_data_1_split
I0109 20:40:07.755969  4508 net.cpp:94] Creating Layer label_data_1_split
I0109 20:40:07.755982  4508 net.cpp:435] label_data_1_split <- label
I0109 20:40:07.756009  4508 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0109 20:40:07.756034  4508 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0109 20:40:07.756055  4508 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0109 20:40:07.756072  4508 net.cpp:144] Setting up label_data_1_split
I0109 20:40:07.756088  4508 net.cpp:151] Top shape: 50 (50)
I0109 20:40:07.756095  4508 net.cpp:151] Top shape: 50 (50)
I0109 20:40:07.756103  4508 net.cpp:151] Top shape: 50 (50)
I0109 20:40:07.756114  4508 net.cpp:159] Memory required for data: 615200
I0109 20:40:07.756121  4508 layer_factory.hpp:77] Creating layer conv1
I0109 20:40:07.756137  4508 net.cpp:94] Creating Layer conv1
I0109 20:40:07.756153  4508 net.cpp:435] conv1 <- data
I0109 20:40:07.756165  4508 net.cpp:409] conv1 -> conv1
I0109 20:40:07.756275  4508 net.cpp:144] Setting up conv1
I0109 20:40:07.756300  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.756309  4508 net.cpp:159] Memory required for data: 7168800
I0109 20:40:07.756337  4508 layer_factory.hpp:77] Creating layer bn1
I0109 20:40:07.756359  4508 net.cpp:94] Creating Layer bn1
I0109 20:40:07.756367  4508 net.cpp:435] bn1 <- conv1
I0109 20:40:07.756378  4508 net.cpp:409] bn1 -> bn1
I0109 20:40:07.756455  4508 net.cpp:144] Setting up bn1
I0109 20:40:07.756494  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.756501  4508 net.cpp:159] Memory required for data: 13722400
I0109 20:40:07.756523  4508 layer_factory.hpp:77] Creating layer scale1
I0109 20:40:07.756575  4508 net.cpp:94] Creating Layer scale1
I0109 20:40:07.756616  4508 net.cpp:435] scale1 <- bn1
I0109 20:40:07.756664  4508 net.cpp:409] scale1 -> scale1
I0109 20:40:07.756706  4508 layer_factory.hpp:77] Creating layer scale1
I0109 20:40:07.756765  4508 net.cpp:144] Setting up scale1
I0109 20:40:07.756783  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.756789  4508 net.cpp:159] Memory required for data: 20276000
I0109 20:40:07.756803  4508 layer_factory.hpp:77] Creating layer relu1
I0109 20:40:07.756829  4508 net.cpp:94] Creating Layer relu1
I0109 20:40:07.756853  4508 net.cpp:435] relu1 <- scale1
I0109 20:40:07.756875  4508 net.cpp:409] relu1 -> relu1
I0109 20:40:07.756916  4508 net.cpp:144] Setting up relu1
I0109 20:40:07.756947  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.756971  4508 net.cpp:159] Memory required for data: 26829600
I0109 20:40:07.756989  4508 layer_factory.hpp:77] Creating layer conv2
I0109 20:40:07.757015  4508 net.cpp:94] Creating Layer conv2
I0109 20:40:07.757040  4508 net.cpp:435] conv2 <- relu1
I0109 20:40:07.757071  4508 net.cpp:409] conv2 -> conv2
I0109 20:40:07.757274  4508 net.cpp:144] Setting up conv2
I0109 20:40:07.757302  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.757308  4508 net.cpp:159] Memory required for data: 33383200
I0109 20:40:07.757319  4508 layer_factory.hpp:77] Creating layer bn2
I0109 20:40:07.757360  4508 net.cpp:94] Creating Layer bn2
I0109 20:40:07.757387  4508 net.cpp:435] bn2 <- conv2
I0109 20:40:07.757427  4508 net.cpp:409] bn2 -> bn2
I0109 20:40:07.757521  4508 net.cpp:144] Setting up bn2
I0109 20:40:07.757550  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.757560  4508 net.cpp:159] Memory required for data: 39936800
I0109 20:40:07.757583  4508 layer_factory.hpp:77] Creating layer scale2
I0109 20:40:07.757616  4508 net.cpp:94] Creating Layer scale2
I0109 20:40:07.757643  4508 net.cpp:435] scale2 <- bn2
I0109 20:40:07.757673  4508 net.cpp:409] scale2 -> scale2
I0109 20:40:07.757709  4508 layer_factory.hpp:77] Creating layer scale2
I0109 20:40:07.757747  4508 net.cpp:144] Setting up scale2
I0109 20:40:07.757766  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.757774  4508 net.cpp:159] Memory required for data: 46490400
I0109 20:40:07.757791  4508 layer_factory.hpp:77] Creating layer relu2
I0109 20:40:07.757804  4508 net.cpp:94] Creating Layer relu2
I0109 20:40:07.757810  4508 net.cpp:435] relu2 <- scale2
I0109 20:40:07.757827  4508 net.cpp:409] relu2 -> relu2
I0109 20:40:07.757840  4508 net.cpp:144] Setting up relu2
I0109 20:40:07.757850  4508 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0109 20:40:07.757856  4508 net.cpp:159] Memory required for data: 53044000
I0109 20:40:07.757863  4508 layer_factory.hpp:77] Creating layer pool1
I0109 20:40:07.757874  4508 net.cpp:94] Creating Layer pool1
I0109 20:40:07.757880  4508 net.cpp:435] pool1 <- relu2
I0109 20:40:07.757892  4508 net.cpp:409] pool1 -> pool1
I0109 20:40:07.757916  4508 net.cpp:144] Setting up pool1
I0109 20:40:07.757933  4508 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:40:07.757939  4508 net.cpp:159] Memory required for data: 54682400
I0109 20:40:07.757946  4508 layer_factory.hpp:77] Creating layer drop1
I0109 20:40:07.757962  4508 net.cpp:94] Creating Layer drop1
I0109 20:40:07.757969  4508 net.cpp:435] drop1 <- pool1
I0109 20:40:07.757978  4508 net.cpp:409] drop1 -> drop1
I0109 20:40:07.757992  4508 net.cpp:144] Setting up drop1
I0109 20:40:07.758006  4508 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0109 20:40:07.758013  4508 net.cpp:159] Memory required for data: 56320800
I0109 20:40:07.758020  4508 layer_factory.hpp:77] Creating layer conv3
I0109 20:40:07.758036  4508 net.cpp:94] Creating Layer conv3
I0109 20:40:07.758047  4508 net.cpp:435] conv3 <- drop1
I0109 20:40:07.758070  4508 net.cpp:409] conv3 -> conv3
I0109 20:40:07.758352  4508 net.cpp:144] Setting up conv3
I0109 20:40:07.758371  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.758379  4508 net.cpp:159] Memory required for data: 59597600
I0109 20:40:07.758389  4508 layer_factory.hpp:77] Creating layer bn3
I0109 20:40:07.758409  4508 net.cpp:94] Creating Layer bn3
I0109 20:40:07.758417  4508 net.cpp:435] bn3 <- conv3
I0109 20:40:07.758430  4508 net.cpp:409] bn3 -> bn3
I0109 20:40:07.758484  4508 net.cpp:144] Setting up bn3
I0109 20:40:07.758500  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.758507  4508 net.cpp:159] Memory required for data: 62874400
I0109 20:40:07.758522  4508 layer_factory.hpp:77] Creating layer scale3
I0109 20:40:07.758538  4508 net.cpp:94] Creating Layer scale3
I0109 20:40:07.758545  4508 net.cpp:435] scale3 <- bn3
I0109 20:40:07.758554  4508 net.cpp:409] scale3 -> scale3
I0109 20:40:07.758575  4508 layer_factory.hpp:77] Creating layer scale3
I0109 20:40:07.758612  4508 net.cpp:144] Setting up scale3
I0109 20:40:07.758630  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.758637  4508 net.cpp:159] Memory required for data: 66151200
I0109 20:40:07.758647  4508 layer_factory.hpp:77] Creating layer relu3
I0109 20:40:07.758657  4508 net.cpp:94] Creating Layer relu3
I0109 20:40:07.758664  4508 net.cpp:435] relu3 <- scale3
I0109 20:40:07.758672  4508 net.cpp:409] relu3 -> relu3
I0109 20:40:07.758687  4508 net.cpp:144] Setting up relu3
I0109 20:40:07.758695  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.758700  4508 net.cpp:159] Memory required for data: 69428000
I0109 20:40:07.758708  4508 layer_factory.hpp:77] Creating layer conv4
I0109 20:40:07.758726  4508 net.cpp:94] Creating Layer conv4
I0109 20:40:07.758735  4508 net.cpp:435] conv4 <- relu3
I0109 20:40:07.758746  4508 net.cpp:409] conv4 -> conv4
I0109 20:40:07.759165  4508 net.cpp:144] Setting up conv4
I0109 20:40:07.759187  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.759196  4508 net.cpp:159] Memory required for data: 72704800
I0109 20:40:07.759205  4508 layer_factory.hpp:77] Creating layer bn4
I0109 20:40:07.759217  4508 net.cpp:94] Creating Layer bn4
I0109 20:40:07.759225  4508 net.cpp:435] bn4 <- conv4
I0109 20:40:07.759238  4508 net.cpp:409] bn4 -> bn4
I0109 20:40:07.759294  4508 net.cpp:144] Setting up bn4
I0109 20:40:07.759310  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.759318  4508 net.cpp:159] Memory required for data: 75981600
I0109 20:40:07.759342  4508 layer_factory.hpp:77] Creating layer scale4
I0109 20:40:07.759358  4508 net.cpp:94] Creating Layer scale4
I0109 20:40:07.759367  4508 net.cpp:435] scale4 <- bn4
I0109 20:40:07.759377  4508 net.cpp:409] scale4 -> scale4
I0109 20:40:07.759397  4508 layer_factory.hpp:77] Creating layer scale4
I0109 20:40:07.759426  4508 net.cpp:144] Setting up scale4
I0109 20:40:07.759443  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.759451  4508 net.cpp:159] Memory required for data: 79258400
I0109 20:40:07.759461  4508 layer_factory.hpp:77] Creating layer relu4
I0109 20:40:07.759474  4508 net.cpp:94] Creating Layer relu4
I0109 20:40:07.759481  4508 net.cpp:435] relu4 <- scale4
I0109 20:40:07.759491  4508 net.cpp:409] relu4 -> relu4
I0109 20:40:07.759505  4508 net.cpp:144] Setting up relu4
I0109 20:40:07.759513  4508 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0109 20:40:07.759519  4508 net.cpp:159] Memory required for data: 82535200
I0109 20:40:07.759526  4508 layer_factory.hpp:77] Creating layer pool2
I0109 20:40:07.759536  4508 net.cpp:94] Creating Layer pool2
I0109 20:40:07.759542  4508 net.cpp:435] pool2 <- relu4
I0109 20:40:07.759553  4508 net.cpp:409] pool2 -> pool2
I0109 20:40:07.759568  4508 net.cpp:144] Setting up pool2
I0109 20:40:07.759577  4508 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:40:07.759583  4508 net.cpp:159] Memory required for data: 83354400
I0109 20:40:07.759588  4508 layer_factory.hpp:77] Creating layer drop2
I0109 20:40:07.759610  4508 net.cpp:94] Creating Layer drop2
I0109 20:40:07.759618  4508 net.cpp:435] drop2 <- pool2
I0109 20:40:07.759624  4508 net.cpp:409] drop2 -> drop2
I0109 20:40:07.759635  4508 net.cpp:144] Setting up drop2
I0109 20:40:07.759646  4508 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0109 20:40:07.759652  4508 net.cpp:159] Memory required for data: 84173600
I0109 20:40:07.759657  4508 layer_factory.hpp:77] Creating layer fc1
I0109 20:40:07.759672  4508 net.cpp:94] Creating Layer fc1
I0109 20:40:07.759682  4508 net.cpp:435] fc1 <- drop2
I0109 20:40:07.759696  4508 net.cpp:409] fc1 -> fc1
I0109 20:40:07.778686  4508 net.cpp:144] Setting up fc1
I0109 20:40:07.778712  4508 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:07.778719  4508 net.cpp:159] Memory required for data: 84276000
I0109 20:40:07.778730  4508 layer_factory.hpp:77] Creating layer bn5
I0109 20:40:07.778746  4508 net.cpp:94] Creating Layer bn5
I0109 20:40:07.778760  4508 net.cpp:435] bn5 <- fc1
I0109 20:40:07.778779  4508 net.cpp:409] bn5 -> bn5
I0109 20:40:07.778851  4508 net.cpp:144] Setting up bn5
I0109 20:40:07.778867  4508 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:07.778874  4508 net.cpp:159] Memory required for data: 84378400
I0109 20:40:07.778889  4508 layer_factory.hpp:77] Creating layer scale5
I0109 20:40:07.778905  4508 net.cpp:94] Creating Layer scale5
I0109 20:40:07.778918  4508 net.cpp:435] scale5 <- bn5
I0109 20:40:07.778928  4508 net.cpp:409] scale5 -> scale5
I0109 20:40:07.778954  4508 layer_factory.hpp:77] Creating layer scale5
I0109 20:40:07.778985  4508 net.cpp:144] Setting up scale5
I0109 20:40:07.779002  4508 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:07.779008  4508 net.cpp:159] Memory required for data: 84480800
I0109 20:40:07.779019  4508 layer_factory.hpp:77] Creating layer relu5
I0109 20:40:07.779028  4508 net.cpp:94] Creating Layer relu5
I0109 20:40:07.779040  4508 net.cpp:435] relu5 <- scale5
I0109 20:40:07.779055  4508 net.cpp:409] relu5 -> relu5
I0109 20:40:07.779072  4508 net.cpp:144] Setting up relu5
I0109 20:40:07.779085  4508 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:07.779093  4508 net.cpp:159] Memory required for data: 84583200
I0109 20:40:07.779098  4508 layer_factory.hpp:77] Creating layer drop3
I0109 20:40:07.779120  4508 net.cpp:94] Creating Layer drop3
I0109 20:40:07.779129  4508 net.cpp:435] drop3 <- relu5
I0109 20:40:07.779139  4508 net.cpp:409] drop3 -> drop3
I0109 20:40:07.779151  4508 net.cpp:144] Setting up drop3
I0109 20:40:07.779160  4508 net.cpp:151] Top shape: 50 512 (25600)
I0109 20:40:07.779168  4508 net.cpp:159] Memory required for data: 84685600
I0109 20:40:07.779175  4508 layer_factory.hpp:77] Creating layer fc2
I0109 20:40:07.779188  4508 net.cpp:94] Creating Layer fc2
I0109 20:40:07.779198  4508 net.cpp:435] fc2 <- drop3
I0109 20:40:07.779211  4508 net.cpp:409] fc2 -> fc2
I0109 20:40:07.779285  4508 net.cpp:144] Setting up fc2
I0109 20:40:07.779301  4508 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:07.779309  4508 net.cpp:159] Memory required for data: 84687600
I0109 20:40:07.779320  4508 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0109 20:40:07.779330  4508 net.cpp:94] Creating Layer fc2_fc2_0_split
I0109 20:40:07.779341  4508 net.cpp:435] fc2_fc2_0_split <- fc2
I0109 20:40:07.779357  4508 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0109 20:40:07.779377  4508 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0109 20:40:07.779398  4508 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0109 20:40:07.779417  4508 net.cpp:144] Setting up fc2_fc2_0_split
I0109 20:40:07.779428  4508 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:07.779438  4508 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:07.779443  4508 net.cpp:151] Top shape: 50 10 (500)
I0109 20:40:07.779449  4508 net.cpp:159] Memory required for data: 84693600
I0109 20:40:07.779456  4508 layer_factory.hpp:77] Creating layer loss
I0109 20:40:07.779466  4508 net.cpp:94] Creating Layer loss
I0109 20:40:07.779474  4508 net.cpp:435] loss <- fc2_fc2_0_split_0
I0109 20:40:07.779482  4508 net.cpp:435] loss <- label_data_1_split_0
I0109 20:40:07.779515  4508 net.cpp:409] loss -> loss
I0109 20:40:07.779531  4508 layer_factory.hpp:77] Creating layer loss
I0109 20:40:07.779562  4508 net.cpp:144] Setting up loss
I0109 20:40:07.779579  4508 net.cpp:151] Top shape: (1)
I0109 20:40:07.779588  4508 net.cpp:154]     with loss weight 1
I0109 20:40:07.779636  4508 net.cpp:159] Memory required for data: 84693604
I0109 20:40:07.779645  4508 layer_factory.hpp:77] Creating layer accuracy-top1
I0109 20:40:07.779660  4508 net.cpp:94] Creating Layer accuracy-top1
I0109 20:40:07.779670  4508 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0109 20:40:07.779678  4508 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0109 20:40:07.779688  4508 net.cpp:409] accuracy-top1 -> top-1
I0109 20:40:07.779702  4508 net.cpp:144] Setting up accuracy-top1
I0109 20:40:07.779712  4508 net.cpp:151] Top shape: (1)
I0109 20:40:07.779718  4508 net.cpp:159] Memory required for data: 84693608
I0109 20:40:07.779726  4508 layer_factory.hpp:77] Creating layer accuracy-top5
I0109 20:40:07.779741  4508 net.cpp:94] Creating Layer accuracy-top5
I0109 20:40:07.779748  4508 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0109 20:40:07.779758  4508 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0109 20:40:07.779772  4508 net.cpp:409] accuracy-top5 -> top-5
I0109 20:40:07.779783  4508 net.cpp:144] Setting up accuracy-top5
I0109 20:40:07.779793  4508 net.cpp:151] Top shape: (1)
I0109 20:40:07.779798  4508 net.cpp:159] Memory required for data: 84693612
I0109 20:40:07.779805  4508 net.cpp:222] accuracy-top5 does not need backward computation.
I0109 20:40:07.779821  4508 net.cpp:222] accuracy-top1 does not need backward computation.
I0109 20:40:07.779831  4508 net.cpp:220] loss needs backward computation.
I0109 20:40:07.779839  4508 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0109 20:40:07.779846  4508 net.cpp:220] fc2 needs backward computation.
I0109 20:40:07.779852  4508 net.cpp:220] drop3 needs backward computation.
I0109 20:40:07.779860  4508 net.cpp:220] relu5 needs backward computation.
I0109 20:40:07.779865  4508 net.cpp:220] scale5 needs backward computation.
I0109 20:40:07.779872  4508 net.cpp:220] bn5 needs backward computation.
I0109 20:40:07.779880  4508 net.cpp:220] fc1 needs backward computation.
I0109 20:40:07.779886  4508 net.cpp:220] drop2 needs backward computation.
I0109 20:40:07.779892  4508 net.cpp:220] pool2 needs backward computation.
I0109 20:40:07.779901  4508 net.cpp:220] relu4 needs backward computation.
I0109 20:40:07.779906  4508 net.cpp:220] scale4 needs backward computation.
I0109 20:40:07.779913  4508 net.cpp:220] bn4 needs backward computation.
I0109 20:40:07.779919  4508 net.cpp:220] conv4 needs backward computation.
I0109 20:40:07.779925  4508 net.cpp:220] relu3 needs backward computation.
I0109 20:40:07.779933  4508 net.cpp:220] scale3 needs backward computation.
I0109 20:40:07.779940  4508 net.cpp:220] bn3 needs backward computation.
I0109 20:40:07.779947  4508 net.cpp:220] conv3 needs backward computation.
I0109 20:40:07.779953  4508 net.cpp:220] drop1 needs backward computation.
I0109 20:40:07.779961  4508 net.cpp:220] pool1 needs backward computation.
I0109 20:40:07.779969  4508 net.cpp:220] relu2 needs backward computation.
I0109 20:40:07.779975  4508 net.cpp:220] scale2 needs backward computation.
I0109 20:40:07.779980  4508 net.cpp:220] bn2 needs backward computation.
I0109 20:40:07.779986  4508 net.cpp:220] conv2 needs backward computation.
I0109 20:40:07.779995  4508 net.cpp:220] relu1 needs backward computation.
I0109 20:40:07.780001  4508 net.cpp:220] scale1 needs backward computation.
I0109 20:40:07.780011  4508 net.cpp:220] bn1 needs backward computation.
I0109 20:40:07.780019  4508 net.cpp:220] conv1 needs backward computation.
I0109 20:40:07.780028  4508 net.cpp:222] label_data_1_split does not need backward computation.
I0109 20:40:07.780035  4508 net.cpp:222] data does not need backward computation.
I0109 20:40:07.780041  4508 net.cpp:264] This network produces output loss
I0109 20:40:07.780061  4508 net.cpp:264] This network produces output top-1
I0109 20:40:07.780069  4508 net.cpp:264] This network produces output top-5
I0109 20:40:07.780112  4508 net.cpp:284] Network initialization done.
I0109 20:40:07.780225  4508 net_counter.cpp:58] Convolution layer conv1 ops: 1802240
I0109 20:40:07.780239  4508 net_counter.cpp:62] Convolution layer conv1 params: 896
I0109 20:40:07.780248  4508 net_counter.cpp:62] BatchNorm layer bn1 params: 129
I0109 20:40:07.780254  4508 net_counter.cpp:58] Convolution layer conv2 ops: 18907136
I0109 20:40:07.780259  4508 net_counter.cpp:62] Convolution layer conv2 params: 9248
I0109 20:40:07.780266  4508 net_counter.cpp:62] BatchNorm layer bn2 params: 129
I0109 20:40:07.780280  4508 net_counter.cpp:58] Convolution layer conv3 ops: 9453568
I0109 20:40:07.780287  4508 net_counter.cpp:62] Convolution layer conv3 params: 18496
I0109 20:40:07.780292  4508 net_counter.cpp:62] BatchNorm layer bn3 params: 257
I0109 20:40:07.780300  4508 net_counter.cpp:58] Convolution layer conv4 ops: 18890752
I0109 20:40:07.780313  4508 net_counter.cpp:62] Convolution layer conv4 params: 36928
I0109 20:40:07.780319  4508 net_counter.cpp:62] BatchNorm layer bn4 params: 257
I0109 20:40:07.780325  4508 net_counter.cpp:62] BatchNorm layer bn5 params: 2049
I0109 20:40:07.780338  4508 net_counter.cpp:68] Total operations: 49053696
I0109 20:40:07.780345  4508 net_counter.cpp:69] Total params: 68389
