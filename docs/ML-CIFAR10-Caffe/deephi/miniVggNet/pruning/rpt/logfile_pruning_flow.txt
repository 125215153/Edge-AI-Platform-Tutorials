#!/bin/sh

PRUNE_ROOT=$HOME/ML/DNNDK/tools
WORK_DIR=cifar10/deephi/miniVggNet/pruning

#take the caffemodel with a soft link to save HD space
ln -s $HOME/ML/cifar10/caffe/models/miniVggNet/m3/snapshot_3_miniVggNet__iter_40000.caffemodel ${WORK_DIR}/float.caffemodel

# leave commented the next lines, here added only for "documentation" 

#copy the solver and edit it by reducing the amount of iterations and changing the pathnames
##cp $HOME/ML/cifar10/caffe/models/miniVggNet/m3/solver_3_miniVggNet.prototxt ./solver.prototxt

#copy the description model and edit it by adding top-1 and top-5 accuracy layers at the bottom and changing the pathnames
##cp $HOME/ML/cifar10/caffe/models/miniVggNet/m3/train_val_3_miniVggNet.prototxt ./train_val.prototxt

# analysis: you do it only once
${PRUNE_ROOT}/deephi_compress ana -config ${WORK_DIR}/config0.prototxt      2>&1 | tee ${WORK_DIR}/rpt/logfile_ana_miniVggNet.txt
I0122 16:22:12.726075 44208 deephi_compress.cpp:203] Starting analysis of cifar10/deephi/miniVggNet/pruning/float.caffemodel
I0122 16:22:12.726235 44208 sens_analyser.cpp:145] Analysis completed 0%
I0122 16:22:18.001878 44208 sens_analyser.cpp:212] Analysing layer [conv1] done
I0122 16:22:18.001894 44208 sens_analyser.cpp:213] Analysis completed 25%
I0122 16:22:22.069654 44208 sens_analyser.cpp:212] Analysing layer [conv2] done
I0122 16:22:22.069674 44208 sens_analyser.cpp:213] Analysis completed 50%
I0122 16:22:26.156232 44208 sens_analyser.cpp:212] Analysing layer [conv3] done
I0122 16:22:26.156256 44208 sens_analyser.cpp:213] Analysis completed 75%
I0122 16:22:30.232147 44208 sens_analyser.cpp:212] Analysing layer [conv4] done
I0122 16:22:30.232172 44208 sens_analyser.cpp:213] Analysis completed 100%
I0122 16:22:30.232836 44208 deephi_compress.cpp:205] Analysis done.
Now you can compress the model with the following command:
deephi_compress compress -config cifar10/deephi/miniVggNet/pruning/config0.prototxt

# compression: zero run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config0.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress0_miniVggNet.txt
I0122 16:22:30.464087 45359 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:22:30.480775 45359 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:22:31.862936 45359 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:22:31.863281 45359 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:22:31.863745 45359 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:22:31.863979 45359 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:22:31.864115 45359 layer_factory.hpp:77] Creating layer data
I0122 16:22:31.864164 45359 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:31.864564 45359 net.cpp:94] Creating Layer data
I0122 16:22:31.864574 45359 net.cpp:409] data -> data
I0122 16:22:31.864584 45359 net.cpp:409] data -> label
I0122 16:22:31.865618 45486 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:22:31.865648 45486 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:22:31.865766 45359 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:22:31.865877 45359 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:22:31.869055 45359 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:31.869102 45359 net.cpp:144] Setting up data
I0122 16:22:31.869110 45359 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:22:31.869115 45359 net.cpp:151] Top shape: 50 (50)
I0122 16:22:31.869118 45359 net.cpp:159] Memory required for data: 614600
I0122 16:22:31.869124 45359 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:22:31.869134 45359 net.cpp:94] Creating Layer label_data_1_split
I0122 16:22:31.869140 45359 net.cpp:435] label_data_1_split <- label
I0122 16:22:31.869148 45359 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:22:31.869163 45359 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:22:31.869171 45359 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:22:31.869276 45359 net.cpp:144] Setting up label_data_1_split
I0122 16:22:31.869283 45359 net.cpp:151] Top shape: 50 (50)
I0122 16:22:31.869300 45359 net.cpp:151] Top shape: 50 (50)
I0122 16:22:31.869305 45359 net.cpp:151] Top shape: 50 (50)
I0122 16:22:31.869308 45359 net.cpp:159] Memory required for data: 615200
I0122 16:22:31.869313 45359 layer_factory.hpp:77] Creating layer conv1
I0122 16:22:31.869325 45359 net.cpp:94] Creating Layer conv1
I0122 16:22:31.869330 45359 net.cpp:435] conv1 <- data
I0122 16:22:31.869338 45359 net.cpp:409] conv1 -> conv1
I0122 16:22:31.870457 45359 net.cpp:144] Setting up conv1
I0122 16:22:31.870471 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.870474 45359 net.cpp:159] Memory required for data: 7168800
I0122 16:22:31.870486 45359 layer_factory.hpp:77] Creating layer bn1
I0122 16:22:31.870496 45359 net.cpp:94] Creating Layer bn1
I0122 16:22:31.870501 45359 net.cpp:435] bn1 <- conv1
I0122 16:22:31.870507 45359 net.cpp:409] bn1 -> scale1
I0122 16:22:31.871227 45359 net.cpp:144] Setting up bn1
I0122 16:22:31.871235 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.871239 45359 net.cpp:159] Memory required for data: 13722400
I0122 16:22:31.871253 45359 layer_factory.hpp:77] Creating layer relu1
I0122 16:22:31.871261 45359 net.cpp:94] Creating Layer relu1
I0122 16:22:31.871265 45359 net.cpp:435] relu1 <- scale1
I0122 16:22:31.871271 45359 net.cpp:409] relu1 -> relu1
I0122 16:22:31.871292 45359 net.cpp:144] Setting up relu1
I0122 16:22:31.871299 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.871302 45359 net.cpp:159] Memory required for data: 20276000
I0122 16:22:31.871306 45359 layer_factory.hpp:77] Creating layer conv2
I0122 16:22:31.871316 45359 net.cpp:94] Creating Layer conv2
I0122 16:22:31.871321 45359 net.cpp:435] conv2 <- relu1
I0122 16:22:31.871327 45359 net.cpp:409] conv2 -> conv2
I0122 16:22:31.872467 45359 net.cpp:144] Setting up conv2
I0122 16:22:31.872476 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.872479 45359 net.cpp:159] Memory required for data: 26829600
I0122 16:22:31.872486 45359 layer_factory.hpp:77] Creating layer bn2
I0122 16:22:31.872494 45359 net.cpp:94] Creating Layer bn2
I0122 16:22:31.872498 45359 net.cpp:435] bn2 <- conv2
I0122 16:22:31.872503 45359 net.cpp:409] bn2 -> scale2
I0122 16:22:31.873173 45359 net.cpp:144] Setting up bn2
I0122 16:22:31.873180 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.873183 45359 net.cpp:159] Memory required for data: 33383200
I0122 16:22:31.873191 45359 layer_factory.hpp:77] Creating layer relu2
I0122 16:22:31.873195 45359 net.cpp:94] Creating Layer relu2
I0122 16:22:31.873199 45359 net.cpp:435] relu2 <- scale2
I0122 16:22:31.873204 45359 net.cpp:409] relu2 -> relu2
I0122 16:22:31.873222 45359 net.cpp:144] Setting up relu2
I0122 16:22:31.873227 45359 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:31.873230 45359 net.cpp:159] Memory required for data: 39936800
I0122 16:22:31.873234 45359 layer_factory.hpp:77] Creating layer pool1
I0122 16:22:31.873239 45359 net.cpp:94] Creating Layer pool1
I0122 16:22:31.873241 45359 net.cpp:435] pool1 <- relu2
I0122 16:22:31.873245 45359 net.cpp:409] pool1 -> pool1
I0122 16:22:31.873311 45359 net.cpp:144] Setting up pool1
I0122 16:22:31.873317 45359 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:22:31.873318 45359 net.cpp:159] Memory required for data: 41575200
I0122 16:22:31.873322 45359 layer_factory.hpp:77] Creating layer drop1
I0122 16:22:31.873327 45359 net.cpp:94] Creating Layer drop1
I0122 16:22:31.873328 45359 net.cpp:435] drop1 <- pool1
I0122 16:22:31.873332 45359 net.cpp:409] drop1 -> drop1
I0122 16:22:31.873358 45359 net.cpp:144] Setting up drop1
I0122 16:22:31.873381 45359 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:22:31.873383 45359 net.cpp:159] Memory required for data: 43213600
I0122 16:22:31.873386 45359 layer_factory.hpp:77] Creating layer conv3
I0122 16:22:31.873394 45359 net.cpp:94] Creating Layer conv3
I0122 16:22:31.873396 45359 net.cpp:435] conv3 <- drop1
I0122 16:22:31.873401 45359 net.cpp:409] conv3 -> conv3
I0122 16:22:31.874442 45359 net.cpp:144] Setting up conv3
I0122 16:22:31.874454 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.874469 45359 net.cpp:159] Memory required for data: 46490400
I0122 16:22:31.874475 45359 layer_factory.hpp:77] Creating layer bn3
I0122 16:22:31.874482 45359 net.cpp:94] Creating Layer bn3
I0122 16:22:31.874488 45359 net.cpp:435] bn3 <- conv3
I0122 16:22:31.874495 45359 net.cpp:409] bn3 -> scale3
I0122 16:22:31.875120 45359 net.cpp:144] Setting up bn3
I0122 16:22:31.875128 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.875133 45359 net.cpp:159] Memory required for data: 49767200
I0122 16:22:31.875144 45359 layer_factory.hpp:77] Creating layer relu3
I0122 16:22:31.875147 45359 net.cpp:94] Creating Layer relu3
I0122 16:22:31.875151 45359 net.cpp:435] relu3 <- scale3
I0122 16:22:31.875156 45359 net.cpp:409] relu3 -> relu3
I0122 16:22:31.875174 45359 net.cpp:144] Setting up relu3
I0122 16:22:31.875180 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.875182 45359 net.cpp:159] Memory required for data: 53044000
I0122 16:22:31.875185 45359 layer_factory.hpp:77] Creating layer conv4
I0122 16:22:31.875193 45359 net.cpp:94] Creating Layer conv4
I0122 16:22:31.875201 45359 net.cpp:435] conv4 <- relu3
I0122 16:22:31.875205 45359 net.cpp:409] conv4 -> conv4
I0122 16:22:31.875634 45359 net.cpp:144] Setting up conv4
I0122 16:22:31.875641 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.875644 45359 net.cpp:159] Memory required for data: 56320800
I0122 16:22:31.875649 45359 layer_factory.hpp:77] Creating layer bn4
I0122 16:22:31.875656 45359 net.cpp:94] Creating Layer bn4
I0122 16:22:31.875660 45359 net.cpp:435] bn4 <- conv4
I0122 16:22:31.875666 45359 net.cpp:409] bn4 -> scale4
I0122 16:22:31.876320 45359 net.cpp:144] Setting up bn4
I0122 16:22:31.876330 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.876332 45359 net.cpp:159] Memory required for data: 59597600
I0122 16:22:31.876341 45359 layer_factory.hpp:77] Creating layer relu4
I0122 16:22:31.876348 45359 net.cpp:94] Creating Layer relu4
I0122 16:22:31.876351 45359 net.cpp:435] relu4 <- scale4
I0122 16:22:31.876355 45359 net.cpp:409] relu4 -> relu4
I0122 16:22:31.876374 45359 net.cpp:144] Setting up relu4
I0122 16:22:31.876379 45359 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:31.876382 45359 net.cpp:159] Memory required for data: 62874400
I0122 16:22:31.876385 45359 layer_factory.hpp:77] Creating layer pool2
I0122 16:22:31.876391 45359 net.cpp:94] Creating Layer pool2
I0122 16:22:31.876394 45359 net.cpp:435] pool2 <- relu4
I0122 16:22:31.876399 45359 net.cpp:409] pool2 -> pool2
I0122 16:22:31.876488 45359 net.cpp:144] Setting up pool2
I0122 16:22:31.876494 45359 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:22:31.876497 45359 net.cpp:159] Memory required for data: 63693600
I0122 16:22:31.876500 45359 layer_factory.hpp:77] Creating layer drop2
I0122 16:22:31.876505 45359 net.cpp:94] Creating Layer drop2
I0122 16:22:31.876509 45359 net.cpp:435] drop2 <- pool2
I0122 16:22:31.876513 45359 net.cpp:409] drop2 -> drop2
I0122 16:22:31.876540 45359 net.cpp:144] Setting up drop2
I0122 16:22:31.876545 45359 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:22:31.876549 45359 net.cpp:159] Memory required for data: 64512800
I0122 16:22:31.876550 45359 layer_factory.hpp:77] Creating layer fc1
I0122 16:22:31.876557 45359 net.cpp:94] Creating Layer fc1
I0122 16:22:31.876560 45359 net.cpp:435] fc1 <- drop2
I0122 16:22:31.876565 45359 net.cpp:409] fc1 -> fc1
I0122 16:22:31.890420 45359 net.cpp:144] Setting up fc1
I0122 16:22:31.890439 45359 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:31.890440 45359 net.cpp:159] Memory required for data: 64615200
I0122 16:22:31.890447 45359 layer_factory.hpp:77] Creating layer bn5
I0122 16:22:31.890455 45359 net.cpp:94] Creating Layer bn5
I0122 16:22:31.890460 45359 net.cpp:435] bn5 <- fc1
I0122 16:22:31.890465 45359 net.cpp:409] bn5 -> scale5
I0122 16:22:31.891047 45359 net.cpp:144] Setting up bn5
I0122 16:22:31.891057 45359 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:31.891060 45359 net.cpp:159] Memory required for data: 64717600
I0122 16:22:31.891088 45359 layer_factory.hpp:77] Creating layer relu5
I0122 16:22:31.891095 45359 net.cpp:94] Creating Layer relu5
I0122 16:22:31.891098 45359 net.cpp:435] relu5 <- scale5
I0122 16:22:31.891104 45359 net.cpp:409] relu5 -> relu5
I0122 16:22:31.891124 45359 net.cpp:144] Setting up relu5
I0122 16:22:31.891129 45359 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:31.891131 45359 net.cpp:159] Memory required for data: 64820000
I0122 16:22:31.891135 45359 layer_factory.hpp:77] Creating layer drop3
I0122 16:22:31.891140 45359 net.cpp:94] Creating Layer drop3
I0122 16:22:31.891144 45359 net.cpp:435] drop3 <- relu5
I0122 16:22:31.891147 45359 net.cpp:409] drop3 -> drop3
I0122 16:22:31.891173 45359 net.cpp:144] Setting up drop3
I0122 16:22:31.891178 45359 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:31.891180 45359 net.cpp:159] Memory required for data: 64922400
I0122 16:22:31.891183 45359 layer_factory.hpp:77] Creating layer fc2
I0122 16:22:31.891191 45359 net.cpp:94] Creating Layer fc2
I0122 16:22:31.891193 45359 net.cpp:435] fc2 <- drop3
I0122 16:22:31.891198 45359 net.cpp:409] fc2 -> fc2
I0122 16:22:31.891328 45359 net.cpp:144] Setting up fc2
I0122 16:22:31.891335 45359 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:31.891337 45359 net.cpp:159] Memory required for data: 64924400
I0122 16:22:31.891342 45359 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:22:31.891346 45359 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:22:31.891352 45359 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:22:31.891357 45359 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:22:31.891363 45359 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:22:31.891371 45359 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:22:31.891407 45359 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:22:31.891412 45359 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:31.891415 45359 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:31.891418 45359 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:31.891422 45359 net.cpp:159] Memory required for data: 64930400
I0122 16:22:31.891423 45359 layer_factory.hpp:77] Creating layer loss
I0122 16:22:31.891429 45359 net.cpp:94] Creating Layer loss
I0122 16:22:31.891434 45359 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:22:31.891439 45359 net.cpp:435] loss <- label_data_1_split_0
I0122 16:22:31.891444 45359 net.cpp:409] loss -> loss
I0122 16:22:31.891451 45359 layer_factory.hpp:77] Creating layer loss
I0122 16:22:31.891516 45359 net.cpp:144] Setting up loss
I0122 16:22:31.891522 45359 net.cpp:151] Top shape: (1)
I0122 16:22:31.891525 45359 net.cpp:154]     with loss weight 1
I0122 16:22:31.891536 45359 net.cpp:159] Memory required for data: 64930404
I0122 16:22:31.891538 45359 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:22:31.891546 45359 net.cpp:94] Creating Layer accuracy-top1
I0122 16:22:31.891547 45359 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:22:31.891551 45359 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:22:31.891556 45359 net.cpp:409] accuracy-top1 -> top-1
I0122 16:22:31.891564 45359 net.cpp:144] Setting up accuracy-top1
I0122 16:22:31.891569 45359 net.cpp:151] Top shape: (1)
I0122 16:22:31.891572 45359 net.cpp:159] Memory required for data: 64930408
I0122 16:22:31.891574 45359 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:22:31.891579 45359 net.cpp:94] Creating Layer accuracy-top5
I0122 16:22:31.891582 45359 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:22:31.891587 45359 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:22:31.891590 45359 net.cpp:409] accuracy-top5 -> top-5
I0122 16:22:31.891597 45359 net.cpp:144] Setting up accuracy-top5
I0122 16:22:31.891600 45359 net.cpp:151] Top shape: (1)
I0122 16:22:31.891603 45359 net.cpp:159] Memory required for data: 64930412
I0122 16:22:31.891607 45359 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:22:31.891610 45359 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:22:31.891620 45359 net.cpp:220] loss needs backward computation.
I0122 16:22:31.891624 45359 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:22:31.891628 45359 net.cpp:220] fc2 needs backward computation.
I0122 16:22:31.891631 45359 net.cpp:220] drop3 needs backward computation.
I0122 16:22:31.891633 45359 net.cpp:220] relu5 needs backward computation.
I0122 16:22:31.891636 45359 net.cpp:220] bn5 needs backward computation.
I0122 16:22:31.891639 45359 net.cpp:220] fc1 needs backward computation.
I0122 16:22:31.891644 45359 net.cpp:220] drop2 needs backward computation.
I0122 16:22:31.891645 45359 net.cpp:220] pool2 needs backward computation.
I0122 16:22:31.891649 45359 net.cpp:220] relu4 needs backward computation.
I0122 16:22:31.891652 45359 net.cpp:220] bn4 needs backward computation.
I0122 16:22:31.891656 45359 net.cpp:220] conv4 needs backward computation.
I0122 16:22:31.891659 45359 net.cpp:220] relu3 needs backward computation.
I0122 16:22:31.891662 45359 net.cpp:220] bn3 needs backward computation.
I0122 16:22:31.891664 45359 net.cpp:220] conv3 needs backward computation.
I0122 16:22:31.891669 45359 net.cpp:220] drop1 needs backward computation.
I0122 16:22:31.891671 45359 net.cpp:220] pool1 needs backward computation.
I0122 16:22:31.891674 45359 net.cpp:220] relu2 needs backward computation.
I0122 16:22:31.891677 45359 net.cpp:220] bn2 needs backward computation.
I0122 16:22:31.891680 45359 net.cpp:220] conv2 needs backward computation.
I0122 16:22:31.891683 45359 net.cpp:220] relu1 needs backward computation.
I0122 16:22:31.891700 45359 net.cpp:220] bn1 needs backward computation.
I0122 16:22:31.891703 45359 net.cpp:220] conv1 needs backward computation.
I0122 16:22:31.891707 45359 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:22:31.891711 45359 net.cpp:222] data does not need backward computation.
I0122 16:22:31.891713 45359 net.cpp:264] This network produces output loss
I0122 16:22:31.891716 45359 net.cpp:264] This network produces output top-1
I0122 16:22:31.891721 45359 net.cpp:264] This network produces output top-5
I0122 16:22:31.891741 45359 net.cpp:284] Network initialization done.
W0122 16:22:31.892041 45359 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:31.892338 45359 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:31.892515 45359 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:31.892719 45359 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:31.894465 45359 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I0122 16:22:31.894521 45359 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:22:31.900882 45359 caffe_interface.cpp:125] Batch 0, loss = 0.299914
I0122 16:22:31.900902 45359 caffe_interface.cpp:125] Batch 0, top-1 = 0.84
I0122 16:22:31.900905 45359 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:22:31.902446 45359 caffe_interface.cpp:125] Batch 1, loss = 0.325377
I0122 16:22:31.902456 45359 caffe_interface.cpp:125] Batch 1, top-1 = 0.9
I0122 16:22:31.902460 45359 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0122 16:22:31.903687 45359 caffe_interface.cpp:125] Batch 2, loss = 0.230276
I0122 16:22:31.903695 45359 caffe_interface.cpp:125] Batch 2, top-1 = 0.92
I0122 16:22:31.903699 45359 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:22:31.904938 45359 caffe_interface.cpp:125] Batch 3, loss = 0.653513
I0122 16:22:31.904947 45359 caffe_interface.cpp:125] Batch 3, top-1 = 0.82
I0122 16:22:31.904949 45359 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0122 16:22:31.906172 45359 caffe_interface.cpp:125] Batch 4, loss = 0.456649
I0122 16:22:31.906191 45359 caffe_interface.cpp:125] Batch 4, top-1 = 0.86
I0122 16:22:31.906196 45359 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0122 16:22:31.907436 45359 caffe_interface.cpp:125] Batch 5, loss = 0.491117
I0122 16:22:31.907445 45359 caffe_interface.cpp:125] Batch 5, top-1 = 0.86
I0122 16:22:31.907447 45359 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0122 16:22:31.908674 45359 caffe_interface.cpp:125] Batch 6, loss = 0.457225
I0122 16:22:31.908681 45359 caffe_interface.cpp:125] Batch 6, top-1 = 0.8
I0122 16:22:31.908685 45359 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0122 16:22:31.909926 45359 caffe_interface.cpp:125] Batch 7, loss = 0.15889
I0122 16:22:31.909934 45359 caffe_interface.cpp:125] Batch 7, top-1 = 0.94
I0122 16:22:31.909937 45359 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0122 16:22:31.911164 45359 caffe_interface.cpp:125] Batch 8, loss = 0.411878
I0122 16:22:31.911172 45359 caffe_interface.cpp:125] Batch 8, top-1 = 0.92
I0122 16:22:31.911175 45359 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:22:31.912391 45359 caffe_interface.cpp:125] Batch 9, loss = 0.471177
I0122 16:22:31.912400 45359 caffe_interface.cpp:125] Batch 9, top-1 = 0.88
I0122 16:22:31.912402 45359 caffe_interface.cpp:125] Batch 9, top-5 = 0.96
I0122 16:22:31.913630 45359 caffe_interface.cpp:125] Batch 10, loss = 0.273154
I0122 16:22:31.913638 45359 caffe_interface.cpp:125] Batch 10, top-1 = 0.94
I0122 16:22:31.913642 45359 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:22:31.914865 45359 caffe_interface.cpp:125] Batch 11, loss = 0.506451
I0122 16:22:31.914880 45359 caffe_interface.cpp:125] Batch 11, top-1 = 0.86
I0122 16:22:31.914882 45359 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:22:31.916116 45359 caffe_interface.cpp:125] Batch 12, loss = 0.643945
I0122 16:22:31.916124 45359 caffe_interface.cpp:125] Batch 12, top-1 = 0.82
I0122 16:22:31.916128 45359 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:22:31.917347 45359 caffe_interface.cpp:125] Batch 13, loss = 0.578153
I0122 16:22:31.917361 45359 caffe_interface.cpp:125] Batch 13, top-1 = 0.8
I0122 16:22:31.917363 45359 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0122 16:22:31.918599 45359 caffe_interface.cpp:125] Batch 14, loss = 0.30383
I0122 16:22:31.918607 45359 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I0122 16:22:31.918612 45359 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0122 16:22:31.919852 45359 caffe_interface.cpp:125] Batch 15, loss = 0.315082
I0122 16:22:31.919860 45359 caffe_interface.cpp:125] Batch 15, top-1 = 0.86
I0122 16:22:31.919863 45359 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:22:31.921105 45359 caffe_interface.cpp:125] Batch 16, loss = 0.34325
I0122 16:22:31.921113 45359 caffe_interface.cpp:125] Batch 16, top-1 = 0.88
I0122 16:22:31.921116 45359 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0122 16:22:31.922345 45359 caffe_interface.cpp:125] Batch 17, loss = 0.240493
I0122 16:22:31.922354 45359 caffe_interface.cpp:125] Batch 17, top-1 = 0.9
I0122 16:22:31.922358 45359 caffe_interface.cpp:125] Batch 17, top-5 = 0.98
I0122 16:22:31.923578 45359 caffe_interface.cpp:125] Batch 18, loss = 0.508478
I0122 16:22:31.923585 45359 caffe_interface.cpp:125] Batch 18, top-1 = 0.84
I0122 16:22:31.923588 45359 caffe_interface.cpp:125] Batch 18, top-5 = 0.98
I0122 16:22:31.924811 45359 caffe_interface.cpp:125] Batch 19, loss = 0.373763
I0122 16:22:31.924819 45359 caffe_interface.cpp:125] Batch 19, top-1 = 0.88
I0122 16:22:31.924823 45359 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:22:31.926966 45359 caffe_interface.cpp:125] Batch 20, loss = 0.349708
I0122 16:22:31.926977 45359 caffe_interface.cpp:125] Batch 20, top-1 = 0.88
I0122 16:22:31.926980 45359 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0122 16:22:31.928304 45359 caffe_interface.cpp:125] Batch 21, loss = 0.586535
I0122 16:22:31.928311 45359 caffe_interface.cpp:125] Batch 21, top-1 = 0.78
I0122 16:22:31.928314 45359 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0122 16:22:31.929724 45359 caffe_interface.cpp:125] Batch 22, loss = 0.355108
I0122 16:22:31.929744 45359 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0122 16:22:31.929747 45359 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0122 16:22:31.930950 45359 caffe_interface.cpp:125] Batch 23, loss = 0.293919
I0122 16:22:31.930958 45359 caffe_interface.cpp:125] Batch 23, top-1 = 0.88
I0122 16:22:31.930959 45359 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:22:31.932185 45359 caffe_interface.cpp:125] Batch 24, loss = 0.541616
I0122 16:22:31.932193 45359 caffe_interface.cpp:125] Batch 24, top-1 = 0.84
I0122 16:22:31.932194 45359 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0122 16:22:31.933398 45359 caffe_interface.cpp:125] Batch 25, loss = 0.412324
I0122 16:22:31.933405 45359 caffe_interface.cpp:125] Batch 25, top-1 = 0.9
I0122 16:22:31.933408 45359 caffe_interface.cpp:125] Batch 25, top-5 = 1
I0122 16:22:31.934628 45359 caffe_interface.cpp:125] Batch 26, loss = 0.554614
I0122 16:22:31.934636 45359 caffe_interface.cpp:125] Batch 26, top-1 = 0.86
I0122 16:22:31.934640 45359 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0122 16:22:31.936094 45359 caffe_interface.cpp:125] Batch 27, loss = 0.372282
I0122 16:22:31.936101 45359 caffe_interface.cpp:125] Batch 27, top-1 = 0.88
I0122 16:22:31.936105 45359 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:22:31.937294 45359 caffe_interface.cpp:125] Batch 28, loss = 0.405212
I0122 16:22:31.937300 45359 caffe_interface.cpp:125] Batch 28, top-1 = 0.88
I0122 16:22:31.937302 45359 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:22:31.938496 45359 caffe_interface.cpp:125] Batch 29, loss = 0.202549
I0122 16:22:31.938504 45359 caffe_interface.cpp:125] Batch 29, top-1 = 0.94
I0122 16:22:31.938506 45359 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:22:31.939718 45359 caffe_interface.cpp:125] Batch 30, loss = 0.2138
I0122 16:22:31.939725 45359 caffe_interface.cpp:125] Batch 30, top-1 = 0.96
I0122 16:22:31.939729 45359 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:22:31.940944 45359 caffe_interface.cpp:125] Batch 31, loss = 0.514342
I0122 16:22:31.940951 45359 caffe_interface.cpp:125] Batch 31, top-1 = 0.86
I0122 16:22:31.940954 45359 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0122 16:22:31.942162 45359 caffe_interface.cpp:125] Batch 32, loss = 0.466537
I0122 16:22:31.942168 45359 caffe_interface.cpp:125] Batch 32, top-1 = 0.84
I0122 16:22:31.942171 45359 caffe_interface.cpp:125] Batch 32, top-5 = 0.98
I0122 16:22:31.943374 45359 caffe_interface.cpp:125] Batch 33, loss = 0.665031
I0122 16:22:31.943382 45359 caffe_interface.cpp:125] Batch 33, top-1 = 0.84
I0122 16:22:31.943384 45359 caffe_interface.cpp:125] Batch 33, top-5 = 0.98
I0122 16:22:31.944594 45359 caffe_interface.cpp:125] Batch 34, loss = 0.423757
I0122 16:22:31.944602 45359 caffe_interface.cpp:125] Batch 34, top-1 = 0.88
I0122 16:22:31.944605 45359 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0122 16:22:31.945812 45359 caffe_interface.cpp:125] Batch 35, loss = 0.439143
I0122 16:22:31.945819 45359 caffe_interface.cpp:125] Batch 35, top-1 = 0.88
I0122 16:22:31.945822 45359 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0122 16:22:31.947024 45359 caffe_interface.cpp:125] Batch 36, loss = 0.354894
I0122 16:22:31.947032 45359 caffe_interface.cpp:125] Batch 36, top-1 = 0.84
I0122 16:22:31.947034 45359 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:22:31.948235 45359 caffe_interface.cpp:125] Batch 37, loss = 0.721756
I0122 16:22:31.948241 45359 caffe_interface.cpp:125] Batch 37, top-1 = 0.84
I0122 16:22:31.948245 45359 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0122 16:22:31.949455 45359 caffe_interface.cpp:125] Batch 38, loss = 0.640244
I0122 16:22:31.949461 45359 caffe_interface.cpp:125] Batch 38, top-1 = 0.78
I0122 16:22:31.949465 45359 caffe_interface.cpp:125] Batch 38, top-5 = 0.98
I0122 16:22:31.950664 45359 caffe_interface.cpp:125] Batch 39, loss = 0.337686
I0122 16:22:31.950670 45359 caffe_interface.cpp:125] Batch 39, top-1 = 0.92
I0122 16:22:31.950673 45359 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:22:31.951869 45359 caffe_interface.cpp:125] Batch 40, loss = 0.199797
I0122 16:22:31.951889 45359 caffe_interface.cpp:125] Batch 40, top-1 = 0.94
I0122 16:22:31.951892 45359 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:22:31.953092 45359 caffe_interface.cpp:125] Batch 41, loss = 0.713151
I0122 16:22:31.953100 45359 caffe_interface.cpp:125] Batch 41, top-1 = 0.82
I0122 16:22:31.953102 45359 caffe_interface.cpp:125] Batch 41, top-5 = 0.98
I0122 16:22:31.954298 45359 caffe_interface.cpp:125] Batch 42, loss = 0.593652
I0122 16:22:31.954305 45359 caffe_interface.cpp:125] Batch 42, top-1 = 0.82
I0122 16:22:31.954309 45359 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0122 16:22:31.955513 45359 caffe_interface.cpp:125] Batch 43, loss = 0.439991
I0122 16:22:31.955519 45359 caffe_interface.cpp:125] Batch 43, top-1 = 0.84
I0122 16:22:31.955521 45359 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0122 16:22:31.956713 45359 caffe_interface.cpp:125] Batch 44, loss = 0.331684
I0122 16:22:31.956720 45359 caffe_interface.cpp:125] Batch 44, top-1 = 0.86
I0122 16:22:31.956722 45359 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0122 16:22:31.957936 45359 caffe_interface.cpp:125] Batch 45, loss = 0.271265
I0122 16:22:31.957943 45359 caffe_interface.cpp:125] Batch 45, top-1 = 0.92
I0122 16:22:31.957945 45359 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0122 16:22:31.959149 45359 caffe_interface.cpp:125] Batch 46, loss = 0.217147
I0122 16:22:31.959156 45359 caffe_interface.cpp:125] Batch 46, top-1 = 0.94
I0122 16:22:31.959161 45359 caffe_interface.cpp:125] Batch 46, top-5 = 0.98
I0122 16:22:31.960400 45359 caffe_interface.cpp:125] Batch 47, loss = 0.256532
I0122 16:22:31.960412 45359 caffe_interface.cpp:125] Batch 47, top-1 = 0.9
I0122 16:22:31.960415 45359 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0122 16:22:31.962663 45359 caffe_interface.cpp:125] Batch 48, loss = 0.31536
I0122 16:22:31.962671 45359 caffe_interface.cpp:125] Batch 48, top-1 = 0.86
I0122 16:22:31.962674 45359 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:22:31.964042 45359 caffe_interface.cpp:125] Batch 49, loss = 0.514128
I0122 16:22:31.964051 45359 caffe_interface.cpp:125] Batch 49, top-1 = 0.84
I0122 16:22:31.964053 45359 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0122 16:22:31.965481 45359 caffe_interface.cpp:125] Batch 50, loss = 0.367913
I0122 16:22:31.965489 45359 caffe_interface.cpp:125] Batch 50, top-1 = 0.82
I0122 16:22:31.965493 45359 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:22:31.966748 45359 caffe_interface.cpp:125] Batch 51, loss = 0.24127
I0122 16:22:31.966758 45359 caffe_interface.cpp:125] Batch 51, top-1 = 0.92
I0122 16:22:31.966760 45359 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0122 16:22:31.968015 45359 caffe_interface.cpp:125] Batch 52, loss = 0.365338
I0122 16:22:31.968024 45359 caffe_interface.cpp:125] Batch 52, top-1 = 0.84
I0122 16:22:31.968026 45359 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0122 16:22:31.969492 45359 caffe_interface.cpp:125] Batch 53, loss = 0.474087
I0122 16:22:31.969501 45359 caffe_interface.cpp:125] Batch 53, top-1 = 0.84
I0122 16:22:31.969504 45359 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0122 16:22:31.970726 45359 caffe_interface.cpp:125] Batch 54, loss = 0.483599
I0122 16:22:31.970734 45359 caffe_interface.cpp:125] Batch 54, top-1 = 0.8
I0122 16:22:31.970737 45359 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0122 16:22:31.971946 45359 caffe_interface.cpp:125] Batch 55, loss = 0.229277
I0122 16:22:31.971953 45359 caffe_interface.cpp:125] Batch 55, top-1 = 0.9
I0122 16:22:31.971957 45359 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:22:31.973170 45359 caffe_interface.cpp:125] Batch 56, loss = 0.392883
I0122 16:22:31.973177 45359 caffe_interface.cpp:125] Batch 56, top-1 = 0.88
I0122 16:22:31.973181 45359 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0122 16:22:31.974390 45359 caffe_interface.cpp:125] Batch 57, loss = 0.227314
I0122 16:22:31.974397 45359 caffe_interface.cpp:125] Batch 57, top-1 = 0.88
I0122 16:22:31.974400 45359 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:22:31.975610 45359 caffe_interface.cpp:125] Batch 58, loss = 0.464669
I0122 16:22:31.975628 45359 caffe_interface.cpp:125] Batch 58, top-1 = 0.88
I0122 16:22:31.975632 45359 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0122 16:22:31.976848 45359 caffe_interface.cpp:125] Batch 59, loss = 0.273001
I0122 16:22:31.976856 45359 caffe_interface.cpp:125] Batch 59, top-1 = 0.9
I0122 16:22:31.976861 45359 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:22:31.978085 45359 caffe_interface.cpp:125] Batch 60, loss = 0.50395
I0122 16:22:31.978093 45359 caffe_interface.cpp:125] Batch 60, top-1 = 0.84
I0122 16:22:31.978097 45359 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0122 16:22:31.979307 45359 caffe_interface.cpp:125] Batch 61, loss = 0.763385
I0122 16:22:31.979316 45359 caffe_interface.cpp:125] Batch 61, top-1 = 0.72
I0122 16:22:31.979319 45359 caffe_interface.cpp:125] Batch 61, top-5 = 0.98
I0122 16:22:31.980522 45359 caffe_interface.cpp:125] Batch 62, loss = 0.436759
I0122 16:22:31.980530 45359 caffe_interface.cpp:125] Batch 62, top-1 = 0.84
I0122 16:22:31.980533 45359 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:22:31.981750 45359 caffe_interface.cpp:125] Batch 63, loss = 0.430745
I0122 16:22:31.981757 45359 caffe_interface.cpp:125] Batch 63, top-1 = 0.84
I0122 16:22:31.981761 45359 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0122 16:22:31.983002 45359 caffe_interface.cpp:125] Batch 64, loss = 0.335452
I0122 16:22:31.983011 45359 caffe_interface.cpp:125] Batch 64, top-1 = 0.9
I0122 16:22:31.983014 45359 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:22:31.984212 45359 caffe_interface.cpp:125] Batch 65, loss = 0.324953
I0122 16:22:31.984220 45359 caffe_interface.cpp:125] Batch 65, top-1 = 0.9
I0122 16:22:31.984225 45359 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:22:31.985443 45359 caffe_interface.cpp:125] Batch 66, loss = 0.494629
I0122 16:22:31.985457 45359 caffe_interface.cpp:125] Batch 66, top-1 = 0.84
I0122 16:22:31.985461 45359 caffe_interface.cpp:125] Batch 66, top-5 = 1
I0122 16:22:31.986681 45359 caffe_interface.cpp:125] Batch 67, loss = 0.612565
I0122 16:22:31.986690 45359 caffe_interface.cpp:125] Batch 67, top-1 = 0.9
I0122 16:22:31.986692 45359 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0122 16:22:31.987900 45359 caffe_interface.cpp:125] Batch 68, loss = 0.262681
I0122 16:22:31.987906 45359 caffe_interface.cpp:125] Batch 68, top-1 = 0.94
I0122 16:22:31.987910 45359 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:22:31.989127 45359 caffe_interface.cpp:125] Batch 69, loss = 0.331036
I0122 16:22:31.989135 45359 caffe_interface.cpp:125] Batch 69, top-1 = 0.88
I0122 16:22:31.989140 45359 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0122 16:22:31.990365 45359 caffe_interface.cpp:125] Batch 70, loss = 0.303858
I0122 16:22:31.990373 45359 caffe_interface.cpp:125] Batch 70, top-1 = 0.9
I0122 16:22:31.990377 45359 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0122 16:22:31.991580 45359 caffe_interface.cpp:125] Batch 71, loss = 0.425864
I0122 16:22:31.991587 45359 caffe_interface.cpp:125] Batch 71, top-1 = 0.84
I0122 16:22:31.991591 45359 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:22:31.992806 45359 caffe_interface.cpp:125] Batch 72, loss = 0.620933
I0122 16:22:31.992817 45359 caffe_interface.cpp:125] Batch 72, top-1 = 0.76
I0122 16:22:31.992821 45359 caffe_interface.cpp:125] Batch 72, top-5 = 0.98
I0122 16:22:31.994035 45359 caffe_interface.cpp:125] Batch 73, loss = 0.380312
I0122 16:22:31.994043 45359 caffe_interface.cpp:125] Batch 73, top-1 = 0.84
I0122 16:22:31.994045 45359 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0122 16:22:31.995276 45359 caffe_interface.cpp:125] Batch 74, loss = 0.65269
I0122 16:22:31.995287 45359 caffe_interface.cpp:125] Batch 74, top-1 = 0.82
I0122 16:22:31.995291 45359 caffe_interface.cpp:125] Batch 74, top-5 = 0.96
I0122 16:22:31.997463 45359 caffe_interface.cpp:125] Batch 75, loss = 0.39477
I0122 16:22:31.997470 45359 caffe_interface.cpp:125] Batch 75, top-1 = 0.9
I0122 16:22:31.997473 45359 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0122 16:22:31.998960 45359 caffe_interface.cpp:125] Batch 76, loss = 0.507052
I0122 16:22:31.998968 45359 caffe_interface.cpp:125] Batch 76, top-1 = 0.84
I0122 16:22:31.998971 45359 caffe_interface.cpp:125] Batch 76, top-5 = 0.94
I0122 16:22:32.000165 45359 caffe_interface.cpp:125] Batch 77, loss = 0.284156
I0122 16:22:32.000172 45359 caffe_interface.cpp:125] Batch 77, top-1 = 0.92
I0122 16:22:32.000175 45359 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0122 16:22:32.001364 45359 caffe_interface.cpp:125] Batch 78, loss = 0.295495
I0122 16:22:32.001370 45359 caffe_interface.cpp:125] Batch 78, top-1 = 0.9
I0122 16:22:32.001374 45359 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0122 16:22:32.002791 45359 caffe_interface.cpp:125] Batch 79, loss = 0.359082
I0122 16:22:32.002799 45359 caffe_interface.cpp:125] Batch 79, top-1 = 0.92
I0122 16:22:32.002801 45359 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0122 16:22:32.003988 45359 caffe_interface.cpp:125] Batch 80, loss = 0.164618
I0122 16:22:32.003994 45359 caffe_interface.cpp:125] Batch 80, top-1 = 0.94
I0122 16:22:32.003998 45359 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:22:32.005178 45359 caffe_interface.cpp:125] Batch 81, loss = 0.466065
I0122 16:22:32.005185 45359 caffe_interface.cpp:125] Batch 81, top-1 = 0.82
I0122 16:22:32.005187 45359 caffe_interface.cpp:125] Batch 81, top-5 = 1
I0122 16:22:32.006371 45359 caffe_interface.cpp:125] Batch 82, loss = 0.295356
I0122 16:22:32.006377 45359 caffe_interface.cpp:125] Batch 82, top-1 = 0.92
I0122 16:22:32.006381 45359 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0122 16:22:32.007573 45359 caffe_interface.cpp:125] Batch 83, loss = 0.492395
I0122 16:22:32.007580 45359 caffe_interface.cpp:125] Batch 83, top-1 = 0.8
I0122 16:22:32.007583 45359 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:22:32.008769 45359 caffe_interface.cpp:125] Batch 84, loss = 0.665956
I0122 16:22:32.008775 45359 caffe_interface.cpp:125] Batch 84, top-1 = 0.8
I0122 16:22:32.008777 45359 caffe_interface.cpp:125] Batch 84, top-5 = 0.98
I0122 16:22:32.009966 45359 caffe_interface.cpp:125] Batch 85, loss = 0.446226
I0122 16:22:32.009974 45359 caffe_interface.cpp:125] Batch 85, top-1 = 0.9
I0122 16:22:32.009976 45359 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:22:32.011165 45359 caffe_interface.cpp:125] Batch 86, loss = 0.420212
I0122 16:22:32.011173 45359 caffe_interface.cpp:125] Batch 86, top-1 = 0.84
I0122 16:22:32.011176 45359 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:22:32.012362 45359 caffe_interface.cpp:125] Batch 87, loss = 0.355803
I0122 16:22:32.012368 45359 caffe_interface.cpp:125] Batch 87, top-1 = 0.88
I0122 16:22:32.012370 45359 caffe_interface.cpp:125] Batch 87, top-5 = 0.98
I0122 16:22:32.013572 45359 caffe_interface.cpp:125] Batch 88, loss = 0.552921
I0122 16:22:32.013579 45359 caffe_interface.cpp:125] Batch 88, top-1 = 0.82
I0122 16:22:32.013581 45359 caffe_interface.cpp:125] Batch 88, top-5 = 1
I0122 16:22:32.014771 45359 caffe_interface.cpp:125] Batch 89, loss = 0.280698
I0122 16:22:32.014778 45359 caffe_interface.cpp:125] Batch 89, top-1 = 0.92
I0122 16:22:32.014781 45359 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0122 16:22:32.015977 45359 caffe_interface.cpp:125] Batch 90, loss = 0.438724
I0122 16:22:32.015985 45359 caffe_interface.cpp:125] Batch 90, top-1 = 0.82
I0122 16:22:32.015988 45359 caffe_interface.cpp:125] Batch 90, top-5 = 0.98
I0122 16:22:32.017177 45359 caffe_interface.cpp:125] Batch 91, loss = 0.466414
I0122 16:22:32.017184 45359 caffe_interface.cpp:125] Batch 91, top-1 = 0.84
I0122 16:22:32.017186 45359 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0122 16:22:32.018383 45359 caffe_interface.cpp:125] Batch 92, loss = 0.635777
I0122 16:22:32.018391 45359 caffe_interface.cpp:125] Batch 92, top-1 = 0.84
I0122 16:22:32.018395 45359 caffe_interface.cpp:125] Batch 92, top-5 = 0.94
I0122 16:22:32.019606 45359 caffe_interface.cpp:125] Batch 93, loss = 0.503575
I0122 16:22:32.019614 45359 caffe_interface.cpp:125] Batch 93, top-1 = 0.86
I0122 16:22:32.019618 45359 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0122 16:22:32.020849 45359 caffe_interface.cpp:125] Batch 94, loss = 0.728195
I0122 16:22:32.020857 45359 caffe_interface.cpp:125] Batch 94, top-1 = 0.78
I0122 16:22:32.020861 45359 caffe_interface.cpp:125] Batch 94, top-5 = 0.98
I0122 16:22:32.022070 45359 caffe_interface.cpp:125] Batch 95, loss = 0.39248
I0122 16:22:32.022079 45359 caffe_interface.cpp:125] Batch 95, top-1 = 0.84
I0122 16:22:32.022083 45359 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0122 16:22:32.023289 45359 caffe_interface.cpp:125] Batch 96, loss = 0.637651
I0122 16:22:32.023298 45359 caffe_interface.cpp:125] Batch 96, top-1 = 0.8
I0122 16:22:32.023300 45359 caffe_interface.cpp:125] Batch 96, top-5 = 0.96
I0122 16:22:32.024519 45359 caffe_interface.cpp:125] Batch 97, loss = 0.32242
I0122 16:22:32.024528 45359 caffe_interface.cpp:125] Batch 97, top-1 = 0.84
I0122 16:22:32.024530 45359 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:22:32.025728 45359 caffe_interface.cpp:125] Batch 98, loss = 0.423502
I0122 16:22:32.025735 45359 caffe_interface.cpp:125] Batch 98, top-1 = 0.9
I0122 16:22:32.025739 45359 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:22:32.026958 45359 caffe_interface.cpp:125] Batch 99, loss = 0.527237
I0122 16:22:32.026967 45359 caffe_interface.cpp:125] Batch 99, top-1 = 0.8
I0122 16:22:32.026969 45359 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:22:32.028182 45359 caffe_interface.cpp:125] Batch 100, loss = 0.282152
I0122 16:22:32.028189 45359 caffe_interface.cpp:125] Batch 100, top-1 = 0.9
I0122 16:22:32.028193 45359 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0122 16:22:32.029403 45359 caffe_interface.cpp:125] Batch 101, loss = 0.281503
I0122 16:22:32.029412 45359 caffe_interface.cpp:125] Batch 101, top-1 = 0.88
I0122 16:22:32.029415 45359 caffe_interface.cpp:125] Batch 101, top-5 = 0.98
I0122 16:22:32.031594 45359 caffe_interface.cpp:125] Batch 102, loss = 0.711937
I0122 16:22:32.031602 45359 caffe_interface.cpp:125] Batch 102, top-1 = 0.78
I0122 16:22:32.031605 45359 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0122 16:22:32.032946 45359 caffe_interface.cpp:125] Batch 103, loss = 0.412743
I0122 16:22:32.032953 45359 caffe_interface.cpp:125] Batch 103, top-1 = 0.88
I0122 16:22:32.032958 45359 caffe_interface.cpp:125] Batch 103, top-5 = 0.98
I0122 16:22:32.034179 45359 caffe_interface.cpp:125] Batch 104, loss = 0.29446
I0122 16:22:32.034188 45359 caffe_interface.cpp:125] Batch 104, top-1 = 0.92
I0122 16:22:32.034190 45359 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0122 16:22:32.035583 45359 caffe_interface.cpp:125] Batch 105, loss = 0.227125
I0122 16:22:32.035591 45359 caffe_interface.cpp:125] Batch 105, top-1 = 0.92
I0122 16:22:32.035595 45359 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:22:32.036809 45359 caffe_interface.cpp:125] Batch 106, loss = 0.388519
I0122 16:22:32.036818 45359 caffe_interface.cpp:125] Batch 106, top-1 = 0.84
I0122 16:22:32.036821 45359 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:22:32.038028 45359 caffe_interface.cpp:125] Batch 107, loss = 0.577786
I0122 16:22:32.038036 45359 caffe_interface.cpp:125] Batch 107, top-1 = 0.84
I0122 16:22:32.038039 45359 caffe_interface.cpp:125] Batch 107, top-5 = 0.98
I0122 16:22:32.039261 45359 caffe_interface.cpp:125] Batch 108, loss = 0.327633
I0122 16:22:32.039269 45359 caffe_interface.cpp:125] Batch 108, top-1 = 0.9
I0122 16:22:32.039273 45359 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0122 16:22:32.040488 45359 caffe_interface.cpp:125] Batch 109, loss = 0.235483
I0122 16:22:32.040495 45359 caffe_interface.cpp:125] Batch 109, top-1 = 0.9
I0122 16:22:32.040498 45359 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:22:32.041699 45359 caffe_interface.cpp:125] Batch 110, loss = 0.466008
I0122 16:22:32.041707 45359 caffe_interface.cpp:125] Batch 110, top-1 = 0.82
I0122 16:22:32.041712 45359 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0122 16:22:32.042912 45359 caffe_interface.cpp:125] Batch 111, loss = 0.669046
I0122 16:22:32.042920 45359 caffe_interface.cpp:125] Batch 111, top-1 = 0.78
I0122 16:22:32.042934 45359 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0122 16:22:32.044147 45359 caffe_interface.cpp:125] Batch 112, loss = 0.234589
I0122 16:22:32.044153 45359 caffe_interface.cpp:125] Batch 112, top-1 = 0.96
I0122 16:22:32.044157 45359 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0122 16:22:32.045364 45359 caffe_interface.cpp:125] Batch 113, loss = 0.466355
I0122 16:22:32.045372 45359 caffe_interface.cpp:125] Batch 113, top-1 = 0.86
I0122 16:22:32.045375 45359 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:22:32.046597 45359 caffe_interface.cpp:125] Batch 114, loss = 0.615609
I0122 16:22:32.046605 45359 caffe_interface.cpp:125] Batch 114, top-1 = 0.76
I0122 16:22:32.046608 45359 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0122 16:22:32.047817 45359 caffe_interface.cpp:125] Batch 115, loss = 0.136299
I0122 16:22:32.047827 45359 caffe_interface.cpp:125] Batch 115, top-1 = 0.94
I0122 16:22:32.047832 45359 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:22:32.049048 45359 caffe_interface.cpp:125] Batch 116, loss = 0.452598
I0122 16:22:32.049055 45359 caffe_interface.cpp:125] Batch 116, top-1 = 0.84
I0122 16:22:32.049059 45359 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0122 16:22:32.050262 45359 caffe_interface.cpp:125] Batch 117, loss = 0.625671
I0122 16:22:32.050271 45359 caffe_interface.cpp:125] Batch 117, top-1 = 0.8
I0122 16:22:32.050274 45359 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0122 16:22:32.051486 45359 caffe_interface.cpp:125] Batch 118, loss = 0.461799
I0122 16:22:32.051494 45359 caffe_interface.cpp:125] Batch 118, top-1 = 0.82
I0122 16:22:32.051497 45359 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0122 16:22:32.052696 45359 caffe_interface.cpp:125] Batch 119, loss = 0.580944
I0122 16:22:32.052704 45359 caffe_interface.cpp:125] Batch 119, top-1 = 0.84
I0122 16:22:32.052708 45359 caffe_interface.cpp:125] Batch 119, top-5 = 0.98
I0122 16:22:32.053928 45359 caffe_interface.cpp:125] Batch 120, loss = 0.239605
I0122 16:22:32.053936 45359 caffe_interface.cpp:125] Batch 120, top-1 = 0.9
I0122 16:22:32.053939 45359 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0122 16:22:32.055157 45359 caffe_interface.cpp:125] Batch 121, loss = 0.744847
I0122 16:22:32.055166 45359 caffe_interface.cpp:125] Batch 121, top-1 = 0.8
I0122 16:22:32.055169 45359 caffe_interface.cpp:125] Batch 121, top-5 = 0.96
I0122 16:22:32.056375 45359 caffe_interface.cpp:125] Batch 122, loss = 0.500105
I0122 16:22:32.056382 45359 caffe_interface.cpp:125] Batch 122, top-1 = 0.86
I0122 16:22:32.056386 45359 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:22:32.057585 45359 caffe_interface.cpp:125] Batch 123, loss = 0.381619
I0122 16:22:32.057592 45359 caffe_interface.cpp:125] Batch 123, top-1 = 0.9
I0122 16:22:32.057595 45359 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0122 16:22:32.058797 45359 caffe_interface.cpp:125] Batch 124, loss = 0.29271
I0122 16:22:32.058805 45359 caffe_interface.cpp:125] Batch 124, top-1 = 0.88
I0122 16:22:32.058809 45359 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0122 16:22:32.060026 45359 caffe_interface.cpp:125] Batch 125, loss = 0.324836
I0122 16:22:32.060034 45359 caffe_interface.cpp:125] Batch 125, top-1 = 0.88
I0122 16:22:32.060037 45359 caffe_interface.cpp:125] Batch 125, top-5 = 1
I0122 16:22:32.061236 45359 caffe_interface.cpp:125] Batch 126, loss = 0.392574
I0122 16:22:32.061244 45359 caffe_interface.cpp:125] Batch 126, top-1 = 0.88
I0122 16:22:32.061247 45359 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:22:32.062469 45359 caffe_interface.cpp:125] Batch 127, loss = 0.467654
I0122 16:22:32.062476 45359 caffe_interface.cpp:125] Batch 127, top-1 = 0.92
I0122 16:22:32.062479 45359 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:22:32.063699 45359 caffe_interface.cpp:125] Batch 128, loss = 0.319243
I0122 16:22:32.063706 45359 caffe_interface.cpp:125] Batch 128, top-1 = 0.88
I0122 16:22:32.063709 45359 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0122 16:22:32.065975 45359 caffe_interface.cpp:125] Batch 129, loss = 0.572552
I0122 16:22:32.065992 45359 caffe_interface.cpp:125] Batch 129, top-1 = 0.78
I0122 16:22:32.065994 45359 caffe_interface.cpp:125] Batch 129, top-5 = 0.98
I0122 16:22:32.067298 45359 caffe_interface.cpp:125] Batch 130, loss = 0.461359
I0122 16:22:32.067306 45359 caffe_interface.cpp:125] Batch 130, top-1 = 0.84
I0122 16:22:32.067309 45359 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:22:32.068765 45359 caffe_interface.cpp:125] Batch 131, loss = 0.528482
I0122 16:22:32.068771 45359 caffe_interface.cpp:125] Batch 131, top-1 = 0.74
I0122 16:22:32.068774 45359 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0122 16:22:32.069975 45359 caffe_interface.cpp:125] Batch 132, loss = 0.492567
I0122 16:22:32.069983 45359 caffe_interface.cpp:125] Batch 132, top-1 = 0.88
I0122 16:22:32.069985 45359 caffe_interface.cpp:125] Batch 132, top-5 = 0.98
I0122 16:22:32.071177 45359 caffe_interface.cpp:125] Batch 133, loss = 0.584027
I0122 16:22:32.071183 45359 caffe_interface.cpp:125] Batch 133, top-1 = 0.8
I0122 16:22:32.071187 45359 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0122 16:22:32.072367 45359 caffe_interface.cpp:125] Batch 134, loss = 0.543005
I0122 16:22:32.072373 45359 caffe_interface.cpp:125] Batch 134, top-1 = 0.86
I0122 16:22:32.072377 45359 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:22:32.073565 45359 caffe_interface.cpp:125] Batch 135, loss = 0.660671
I0122 16:22:32.073572 45359 caffe_interface.cpp:125] Batch 135, top-1 = 0.78
I0122 16:22:32.073575 45359 caffe_interface.cpp:125] Batch 135, top-5 = 0.96
I0122 16:22:32.074784 45359 caffe_interface.cpp:125] Batch 136, loss = 0.380425
I0122 16:22:32.074792 45359 caffe_interface.cpp:125] Batch 136, top-1 = 0.88
I0122 16:22:32.074796 45359 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0122 16:22:32.076005 45359 caffe_interface.cpp:125] Batch 137, loss = 0.28188
I0122 16:22:32.076014 45359 caffe_interface.cpp:125] Batch 137, top-1 = 0.9
I0122 16:22:32.076016 45359 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:22:32.077227 45359 caffe_interface.cpp:125] Batch 138, loss = 0.623155
I0122 16:22:32.077235 45359 caffe_interface.cpp:125] Batch 138, top-1 = 0.8
I0122 16:22:32.077239 45359 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0122 16:22:32.078446 45359 caffe_interface.cpp:125] Batch 139, loss = 0.571167
I0122 16:22:32.078454 45359 caffe_interface.cpp:125] Batch 139, top-1 = 0.84
I0122 16:22:32.078459 45359 caffe_interface.cpp:125] Batch 139, top-5 = 0.96
I0122 16:22:32.079658 45359 caffe_interface.cpp:125] Batch 140, loss = 0.332098
I0122 16:22:32.079665 45359 caffe_interface.cpp:125] Batch 140, top-1 = 0.86
I0122 16:22:32.079669 45359 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0122 16:22:32.080888 45359 caffe_interface.cpp:125] Batch 141, loss = 0.57475
I0122 16:22:32.080895 45359 caffe_interface.cpp:125] Batch 141, top-1 = 0.82
I0122 16:22:32.080899 45359 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:22:32.082103 45359 caffe_interface.cpp:125] Batch 142, loss = 0.198774
I0122 16:22:32.082111 45359 caffe_interface.cpp:125] Batch 142, top-1 = 0.92
I0122 16:22:32.082115 45359 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:22:32.083338 45359 caffe_interface.cpp:125] Batch 143, loss = 0.348855
I0122 16:22:32.083344 45359 caffe_interface.cpp:125] Batch 143, top-1 = 0.86
I0122 16:22:32.083348 45359 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0122 16:22:32.084561 45359 caffe_interface.cpp:125] Batch 144, loss = 0.621154
I0122 16:22:32.084569 45359 caffe_interface.cpp:125] Batch 144, top-1 = 0.86
I0122 16:22:32.084571 45359 caffe_interface.cpp:125] Batch 144, top-5 = 0.96
I0122 16:22:32.085767 45359 caffe_interface.cpp:125] Batch 145, loss = 0.405799
I0122 16:22:32.085774 45359 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0122 16:22:32.085777 45359 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0122 16:22:32.086984 45359 caffe_interface.cpp:125] Batch 146, loss = 0.378101
I0122 16:22:32.086992 45359 caffe_interface.cpp:125] Batch 146, top-1 = 0.86
I0122 16:22:32.086995 45359 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0122 16:22:32.088217 45359 caffe_interface.cpp:125] Batch 147, loss = 0.587243
I0122 16:22:32.088224 45359 caffe_interface.cpp:125] Batch 147, top-1 = 0.82
I0122 16:22:32.088227 45359 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0122 16:22:32.089438 45359 caffe_interface.cpp:125] Batch 148, loss = 0.237604
I0122 16:22:32.089445 45359 caffe_interface.cpp:125] Batch 148, top-1 = 0.92
I0122 16:22:32.089449 45359 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:22:32.090668 45359 caffe_interface.cpp:125] Batch 149, loss = 0.263484
I0122 16:22:32.090677 45359 caffe_interface.cpp:125] Batch 149, top-1 = 0.9
I0122 16:22:32.090680 45359 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:22:32.091892 45359 caffe_interface.cpp:125] Batch 150, loss = 0.112758
I0122 16:22:32.091898 45359 caffe_interface.cpp:125] Batch 150, top-1 = 0.98
I0122 16:22:32.091902 45359 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0122 16:22:32.093101 45359 caffe_interface.cpp:125] Batch 151, loss = 0.429675
I0122 16:22:32.093108 45359 caffe_interface.cpp:125] Batch 151, top-1 = 0.8
I0122 16:22:32.093112 45359 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:22:32.094327 45359 caffe_interface.cpp:125] Batch 152, loss = 0.362916
I0122 16:22:32.094336 45359 caffe_interface.cpp:125] Batch 152, top-1 = 0.84
I0122 16:22:32.094338 45359 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0122 16:22:32.095544 45359 caffe_interface.cpp:125] Batch 153, loss = 0.247697
I0122 16:22:32.095552 45359 caffe_interface.cpp:125] Batch 153, top-1 = 0.96
I0122 16:22:32.095556 45359 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:22:32.096768 45359 caffe_interface.cpp:125] Batch 154, loss = 0.559503
I0122 16:22:32.096776 45359 caffe_interface.cpp:125] Batch 154, top-1 = 0.8
I0122 16:22:32.096779 45359 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:22:32.097992 45359 caffe_interface.cpp:125] Batch 155, loss = 0.324966
I0122 16:22:32.098001 45359 caffe_interface.cpp:125] Batch 155, top-1 = 0.92
I0122 16:22:32.098006 45359 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:22:32.100098 45359 caffe_interface.cpp:125] Batch 156, loss = 0.402033
I0122 16:22:32.100106 45359 caffe_interface.cpp:125] Batch 156, top-1 = 0.84
I0122 16:22:32.100108 45359 caffe_interface.cpp:125] Batch 156, top-5 = 1
I0122 16:22:32.101397 45359 caffe_interface.cpp:125] Batch 157, loss = 0.182676
I0122 16:22:32.101403 45359 caffe_interface.cpp:125] Batch 157, top-1 = 0.96
I0122 16:22:32.101406 45359 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:22:32.102777 45359 caffe_interface.cpp:125] Batch 158, loss = 0.488489
I0122 16:22:32.102784 45359 caffe_interface.cpp:125] Batch 158, top-1 = 0.84
I0122 16:22:32.102788 45359 caffe_interface.cpp:125] Batch 158, top-5 = 1
I0122 16:22:32.104015 45359 caffe_interface.cpp:125] Batch 159, loss = 0.572767
I0122 16:22:32.104022 45359 caffe_interface.cpp:125] Batch 159, top-1 = 0.78
I0122 16:22:32.104024 45359 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0122 16:22:32.105212 45359 caffe_interface.cpp:125] Batch 160, loss = 0.255649
I0122 16:22:32.105219 45359 caffe_interface.cpp:125] Batch 160, top-1 = 0.94
I0122 16:22:32.105221 45359 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0122 16:22:32.106433 45359 caffe_interface.cpp:125] Batch 161, loss = 0.205379
I0122 16:22:32.106441 45359 caffe_interface.cpp:125] Batch 161, top-1 = 0.9
I0122 16:22:32.106443 45359 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0122 16:22:32.107632 45359 caffe_interface.cpp:125] Batch 162, loss = 0.566328
I0122 16:22:32.107640 45359 caffe_interface.cpp:125] Batch 162, top-1 = 0.84
I0122 16:22:32.107643 45359 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:22:32.108819 45359 caffe_interface.cpp:125] Batch 163, loss = 0.32976
I0122 16:22:32.108825 45359 caffe_interface.cpp:125] Batch 163, top-1 = 0.92
I0122 16:22:32.108827 45359 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:22:32.110028 45359 caffe_interface.cpp:125] Batch 164, loss = 0.705044
I0122 16:22:32.110034 45359 caffe_interface.cpp:125] Batch 164, top-1 = 0.8
I0122 16:22:32.110036 45359 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:22:32.111237 45359 caffe_interface.cpp:125] Batch 165, loss = 0.363054
I0122 16:22:32.111243 45359 caffe_interface.cpp:125] Batch 165, top-1 = 0.9
I0122 16:22:32.111246 45359 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:22:32.112447 45359 caffe_interface.cpp:125] Batch 166, loss = 0.253631
I0122 16:22:32.112453 45359 caffe_interface.cpp:125] Batch 166, top-1 = 0.92
I0122 16:22:32.112457 45359 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:22:32.113651 45359 caffe_interface.cpp:125] Batch 167, loss = 0.533834
I0122 16:22:32.113657 45359 caffe_interface.cpp:125] Batch 167, top-1 = 0.82
I0122 16:22:32.113659 45359 caffe_interface.cpp:125] Batch 167, top-5 = 1
I0122 16:22:32.114858 45359 caffe_interface.cpp:125] Batch 168, loss = 0.257038
I0122 16:22:32.114866 45359 caffe_interface.cpp:125] Batch 168, top-1 = 0.88
I0122 16:22:32.114868 45359 caffe_interface.cpp:125] Batch 168, top-5 = 0.98
I0122 16:22:32.116081 45359 caffe_interface.cpp:125] Batch 169, loss = 0.437926
I0122 16:22:32.116088 45359 caffe_interface.cpp:125] Batch 169, top-1 = 0.9
I0122 16:22:32.116092 45359 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0122 16:22:32.117276 45359 caffe_interface.cpp:125] Batch 170, loss = 0.241952
I0122 16:22:32.117283 45359 caffe_interface.cpp:125] Batch 170, top-1 = 0.92
I0122 16:22:32.117287 45359 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0122 16:22:32.118468 45359 caffe_interface.cpp:125] Batch 171, loss = 0.297071
I0122 16:22:32.118475 45359 caffe_interface.cpp:125] Batch 171, top-1 = 0.92
I0122 16:22:32.118477 45359 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0122 16:22:32.119758 45359 caffe_interface.cpp:125] Batch 172, loss = 0.651788
I0122 16:22:32.119765 45359 caffe_interface.cpp:125] Batch 172, top-1 = 0.82
I0122 16:22:32.119767 45359 caffe_interface.cpp:125] Batch 172, top-5 = 0.94
I0122 16:22:32.120944 45359 caffe_interface.cpp:125] Batch 173, loss = 0.1892
I0122 16:22:32.120951 45359 caffe_interface.cpp:125] Batch 173, top-1 = 0.94
I0122 16:22:32.120954 45359 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0122 16:22:32.122138 45359 caffe_interface.cpp:125] Batch 174, loss = 0.361792
I0122 16:22:32.122144 45359 caffe_interface.cpp:125] Batch 174, top-1 = 0.82
I0122 16:22:32.122148 45359 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:22:32.123337 45359 caffe_interface.cpp:125] Batch 175, loss = 0.540502
I0122 16:22:32.123345 45359 caffe_interface.cpp:125] Batch 175, top-1 = 0.8
I0122 16:22:32.123347 45359 caffe_interface.cpp:125] Batch 175, top-5 = 0.98
I0122 16:22:32.124544 45359 caffe_interface.cpp:125] Batch 176, loss = 0.887522
I0122 16:22:32.124552 45359 caffe_interface.cpp:125] Batch 176, top-1 = 0.76
I0122 16:22:32.124554 45359 caffe_interface.cpp:125] Batch 176, top-5 = 0.96
I0122 16:22:32.125743 45359 caffe_interface.cpp:125] Batch 177, loss = 0.434801
I0122 16:22:32.125757 45359 caffe_interface.cpp:125] Batch 177, top-1 = 0.84
I0122 16:22:32.125761 45359 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0122 16:22:32.126940 45359 caffe_interface.cpp:125] Batch 178, loss = 0.533065
I0122 16:22:32.126946 45359 caffe_interface.cpp:125] Batch 178, top-1 = 0.84
I0122 16:22:32.126950 45359 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0122 16:22:32.128140 45359 caffe_interface.cpp:125] Batch 179, loss = 0.287511
I0122 16:22:32.128147 45359 caffe_interface.cpp:125] Batch 179, top-1 = 0.88
I0122 16:22:32.128149 45359 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:22:32.128154 45359 caffe_interface.cpp:130] Loss: 0.422096
I0122 16:22:32.128157 45359 caffe_interface.cpp:142] loss = 0.422096 (* 1 = 0.422096 loss)
I0122 16:22:32.128162 45359 caffe_interface.cpp:142] top-1 = 0.862666
I0122 16:22:32.128165 45359 caffe_interface.cpp:142] top-5 = 0.991778
I0122 16:22:32.282176 45359 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0/sparse.caffemodel
I0122 16:22:32.282212 45359 pruning_runner.cpp:320] summary of REGULAR compression with rate 0:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.862666428    | 0              |
+-------------------------------------------------------------------+
| Weights        | 68389          | 68389          | 0%             |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 49053696       | 0%             |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config0.prototxt
# fine-tuning: zero run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config0.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune0_miniVggNet.txt
I0122 16:22:32.520529 45517 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0122 16:22:32.700454 45517 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:22:32.700963 45517 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24971247616, dev_info[0]: total=25620447232 free=24971247616
I0122 16:22:32.700973 45517 caffe_interface.cpp:493] Using GPUs 0
I0122 16:22:32.701225 45517 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:22:33.296068 45517 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt"
type: "SGD"
I0122 16:22:33.296185 45517 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0122 16:22:33.296435 45517 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:22:33.296450 45517 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:22:33.296452 45517 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:22:33.296615 45517 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:22:33.296685 45517 layer_factory.hpp:77] Creating layer data
I0122 16:22:33.296774 45517 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:33.297201 45517 net.cpp:94] Creating Layer data
I0122 16:22:33.297211 45517 net.cpp:409] data -> data
I0122 16:22:33.297236 45517 net.cpp:409] data -> label
I0122 16:22:33.298705 45556 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:22:33.298753 45556 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:22:33.298840 45517 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:22:33.298921 45517 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:22:33.306414 45517 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:33.306481 45517 net.cpp:144] Setting up data
I0122 16:22:33.306489 45517 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:22:33.306493 45517 net.cpp:151] Top shape: 128 (128)
I0122 16:22:33.306496 45517 net.cpp:159] Memory required for data: 1573376
I0122 16:22:33.306501 45517 layer_factory.hpp:77] Creating layer conv1
I0122 16:22:33.306515 45517 net.cpp:94] Creating Layer conv1
I0122 16:22:33.306521 45517 net.cpp:435] conv1 <- data
I0122 16:22:33.306537 45517 net.cpp:409] conv1 -> conv1
I0122 16:22:33.307565 45517 net.cpp:144] Setting up conv1
I0122 16:22:33.307576 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.307579 45517 net.cpp:159] Memory required for data: 18350592
I0122 16:22:33.307595 45517 layer_factory.hpp:77] Creating layer bn1
I0122 16:22:33.307605 45517 net.cpp:94] Creating Layer bn1
I0122 16:22:33.307608 45517 net.cpp:435] bn1 <- conv1
I0122 16:22:33.307613 45517 net.cpp:409] bn1 -> scale1
I0122 16:22:33.308192 45517 net.cpp:144] Setting up bn1
I0122 16:22:33.308199 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.308202 45517 net.cpp:159] Memory required for data: 35127808
I0122 16:22:33.308212 45517 layer_factory.hpp:77] Creating layer relu1
I0122 16:22:33.308220 45517 net.cpp:94] Creating Layer relu1
I0122 16:22:33.308223 45517 net.cpp:435] relu1 <- scale1
I0122 16:22:33.308228 45517 net.cpp:409] relu1 -> relu1
I0122 16:22:33.308248 45517 net.cpp:144] Setting up relu1
I0122 16:22:33.308254 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.308257 45517 net.cpp:159] Memory required for data: 51905024
I0122 16:22:33.308259 45517 layer_factory.hpp:77] Creating layer conv2
I0122 16:22:33.308267 45517 net.cpp:94] Creating Layer conv2
I0122 16:22:33.308272 45517 net.cpp:435] conv2 <- relu1
I0122 16:22:33.308277 45517 net.cpp:409] conv2 -> conv2
I0122 16:22:33.309762 45517 net.cpp:144] Setting up conv2
I0122 16:22:33.309772 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.309777 45517 net.cpp:159] Memory required for data: 68682240
I0122 16:22:33.309783 45517 layer_factory.hpp:77] Creating layer bn2
I0122 16:22:33.309792 45517 net.cpp:94] Creating Layer bn2
I0122 16:22:33.309795 45517 net.cpp:435] bn2 <- conv2
I0122 16:22:33.309803 45517 net.cpp:409] bn2 -> scale2
I0122 16:22:33.310590 45517 net.cpp:144] Setting up bn2
I0122 16:22:33.310597 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.310600 45517 net.cpp:159] Memory required for data: 85459456
I0122 16:22:33.310609 45517 layer_factory.hpp:77] Creating layer relu2
I0122 16:22:33.310614 45517 net.cpp:94] Creating Layer relu2
I0122 16:22:33.310617 45517 net.cpp:435] relu2 <- scale2
I0122 16:22:33.310621 45517 net.cpp:409] relu2 -> relu2
I0122 16:22:33.310659 45517 net.cpp:144] Setting up relu2
I0122 16:22:33.310665 45517 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:22:33.310668 45517 net.cpp:159] Memory required for data: 102236672
I0122 16:22:33.310672 45517 layer_factory.hpp:77] Creating layer pool1
I0122 16:22:33.310678 45517 net.cpp:94] Creating Layer pool1
I0122 16:22:33.310680 45517 net.cpp:435] pool1 <- relu2
I0122 16:22:33.310684 45517 net.cpp:409] pool1 -> pool1
I0122 16:22:33.310722 45517 net.cpp:144] Setting up pool1
I0122 16:22:33.310729 45517 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:22:33.310731 45517 net.cpp:159] Memory required for data: 106430976
I0122 16:22:33.310734 45517 layer_factory.hpp:77] Creating layer drop1
I0122 16:22:33.310739 45517 net.cpp:94] Creating Layer drop1
I0122 16:22:33.310748 45517 net.cpp:435] drop1 <- pool1
I0122 16:22:33.310766 45517 net.cpp:409] drop1 -> drop1
I0122 16:22:33.310927 45517 net.cpp:144] Setting up drop1
I0122 16:22:33.310933 45517 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:22:33.310936 45517 net.cpp:159] Memory required for data: 110625280
I0122 16:22:33.310940 45517 layer_factory.hpp:77] Creating layer conv3
I0122 16:22:33.310946 45517 net.cpp:94] Creating Layer conv3
I0122 16:22:33.310950 45517 net.cpp:435] conv3 <- drop1
I0122 16:22:33.310955 45517 net.cpp:409] conv3 -> conv3
I0122 16:22:33.311975 45517 net.cpp:144] Setting up conv3
I0122 16:22:33.311987 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.311990 45517 net.cpp:159] Memory required for data: 119013888
I0122 16:22:33.311997 45517 layer_factory.hpp:77] Creating layer bn3
I0122 16:22:33.312005 45517 net.cpp:94] Creating Layer bn3
I0122 16:22:33.312021 45517 net.cpp:435] bn3 <- conv3
I0122 16:22:33.312026 45517 net.cpp:409] bn3 -> scale3
I0122 16:22:33.312649 45517 net.cpp:144] Setting up bn3
I0122 16:22:33.312656 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.312659 45517 net.cpp:159] Memory required for data: 127402496
I0122 16:22:33.312670 45517 layer_factory.hpp:77] Creating layer relu3
I0122 16:22:33.312678 45517 net.cpp:94] Creating Layer relu3
I0122 16:22:33.312681 45517 net.cpp:435] relu3 <- scale3
I0122 16:22:33.312687 45517 net.cpp:409] relu3 -> relu3
I0122 16:22:33.312705 45517 net.cpp:144] Setting up relu3
I0122 16:22:33.312711 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.312714 45517 net.cpp:159] Memory required for data: 135791104
I0122 16:22:33.312717 45517 layer_factory.hpp:77] Creating layer conv4
I0122 16:22:33.312724 45517 net.cpp:94] Creating Layer conv4
I0122 16:22:33.312731 45517 net.cpp:435] conv4 <- relu3
I0122 16:22:33.312736 45517 net.cpp:409] conv4 -> conv4
I0122 16:22:33.313149 45517 net.cpp:144] Setting up conv4
I0122 16:22:33.313156 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.313159 45517 net.cpp:159] Memory required for data: 144179712
I0122 16:22:33.313164 45517 layer_factory.hpp:77] Creating layer bn4
I0122 16:22:33.313170 45517 net.cpp:94] Creating Layer bn4
I0122 16:22:33.313174 45517 net.cpp:435] bn4 <- conv4
I0122 16:22:33.313179 45517 net.cpp:409] bn4 -> scale4
I0122 16:22:33.313822 45517 net.cpp:144] Setting up bn4
I0122 16:22:33.313827 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.313830 45517 net.cpp:159] Memory required for data: 152568320
I0122 16:22:33.313839 45517 layer_factory.hpp:77] Creating layer relu4
I0122 16:22:33.313844 45517 net.cpp:94] Creating Layer relu4
I0122 16:22:33.313848 45517 net.cpp:435] relu4 <- scale4
I0122 16:22:33.313851 45517 net.cpp:409] relu4 -> relu4
I0122 16:22:33.313884 45517 net.cpp:144] Setting up relu4
I0122 16:22:33.313890 45517 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:22:33.313892 45517 net.cpp:159] Memory required for data: 160956928
I0122 16:22:33.313895 45517 layer_factory.hpp:77] Creating layer pool2
I0122 16:22:33.313901 45517 net.cpp:94] Creating Layer pool2
I0122 16:22:33.313910 45517 net.cpp:435] pool2 <- relu4
I0122 16:22:33.313915 45517 net.cpp:409] pool2 -> pool2
I0122 16:22:33.313944 45517 net.cpp:144] Setting up pool2
I0122 16:22:33.313951 45517 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:22:33.313954 45517 net.cpp:159] Memory required for data: 163054080
I0122 16:22:33.313956 45517 layer_factory.hpp:77] Creating layer drop2
I0122 16:22:33.313963 45517 net.cpp:94] Creating Layer drop2
I0122 16:22:33.313966 45517 net.cpp:435] drop2 <- pool2
I0122 16:22:33.313972 45517 net.cpp:409] drop2 -> drop2
I0122 16:22:33.313997 45517 net.cpp:144] Setting up drop2
I0122 16:22:33.314002 45517 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:22:33.314007 45517 net.cpp:159] Memory required for data: 165151232
I0122 16:22:33.314008 45517 layer_factory.hpp:77] Creating layer fc1
I0122 16:22:33.314015 45517 net.cpp:94] Creating Layer fc1
I0122 16:22:33.314018 45517 net.cpp:435] fc1 <- drop2
I0122 16:22:33.314023 45517 net.cpp:409] fc1 -> fc1
I0122 16:22:33.328212 45517 net.cpp:144] Setting up fc1
I0122 16:22:33.328229 45517 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:22:33.328243 45517 net.cpp:159] Memory required for data: 165413376
I0122 16:22:33.328253 45517 layer_factory.hpp:77] Creating layer bn5
I0122 16:22:33.328260 45517 net.cpp:94] Creating Layer bn5
I0122 16:22:33.328264 45517 net.cpp:435] bn5 <- fc1
I0122 16:22:33.328271 45517 net.cpp:409] bn5 -> scale5
I0122 16:22:33.328819 45517 net.cpp:144] Setting up bn5
I0122 16:22:33.328824 45517 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:22:33.328828 45517 net.cpp:159] Memory required for data: 165675520
I0122 16:22:33.328840 45517 layer_factory.hpp:77] Creating layer relu5
I0122 16:22:33.328848 45517 net.cpp:94] Creating Layer relu5
I0122 16:22:33.328851 45517 net.cpp:435] relu5 <- scale5
I0122 16:22:33.328856 45517 net.cpp:409] relu5 -> relu5
I0122 16:22:33.328874 45517 net.cpp:144] Setting up relu5
I0122 16:22:33.328881 45517 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:22:33.328883 45517 net.cpp:159] Memory required for data: 165937664
I0122 16:22:33.328886 45517 layer_factory.hpp:77] Creating layer drop3
I0122 16:22:33.328891 45517 net.cpp:94] Creating Layer drop3
I0122 16:22:33.328896 45517 net.cpp:435] drop3 <- relu5
I0122 16:22:33.328900 45517 net.cpp:409] drop3 -> drop3
I0122 16:22:33.328928 45517 net.cpp:144] Setting up drop3
I0122 16:22:33.328934 45517 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:22:33.328938 45517 net.cpp:159] Memory required for data: 166199808
I0122 16:22:33.328939 45517 layer_factory.hpp:77] Creating layer fc2
I0122 16:22:33.328945 45517 net.cpp:94] Creating Layer fc2
I0122 16:22:33.328948 45517 net.cpp:435] fc2 <- drop3
I0122 16:22:33.328953 45517 net.cpp:409] fc2 -> fc2
I0122 16:22:33.329108 45517 net.cpp:144] Setting up fc2
I0122 16:22:33.329114 45517 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:22:33.329116 45517 net.cpp:159] Memory required for data: 166204928
I0122 16:22:33.329123 45517 layer_factory.hpp:77] Creating layer loss
I0122 16:22:33.329128 45517 net.cpp:94] Creating Layer loss
I0122 16:22:33.329130 45517 net.cpp:435] loss <- fc2
I0122 16:22:33.329134 45517 net.cpp:435] loss <- label
I0122 16:22:33.329139 45517 net.cpp:409] loss -> loss
I0122 16:22:33.329147 45517 layer_factory.hpp:77] Creating layer loss
I0122 16:22:33.329917 45517 net.cpp:144] Setting up loss
I0122 16:22:33.329927 45517 net.cpp:151] Top shape: (1)
I0122 16:22:33.329931 45517 net.cpp:154]     with loss weight 1
I0122 16:22:33.329941 45517 net.cpp:159] Memory required for data: 166204932
I0122 16:22:33.329944 45517 net.cpp:220] loss needs backward computation.
I0122 16:22:33.329957 45517 net.cpp:220] fc2 needs backward computation.
I0122 16:22:33.329962 45517 net.cpp:220] drop3 needs backward computation.
I0122 16:22:33.329964 45517 net.cpp:220] relu5 needs backward computation.
I0122 16:22:33.329967 45517 net.cpp:220] bn5 needs backward computation.
I0122 16:22:33.329970 45517 net.cpp:220] fc1 needs backward computation.
I0122 16:22:33.329974 45517 net.cpp:220] drop2 needs backward computation.
I0122 16:22:33.329977 45517 net.cpp:220] pool2 needs backward computation.
I0122 16:22:33.329980 45517 net.cpp:220] relu4 needs backward computation.
I0122 16:22:33.329983 45517 net.cpp:220] bn4 needs backward computation.
I0122 16:22:33.329988 45517 net.cpp:220] conv4 needs backward computation.
I0122 16:22:33.329991 45517 net.cpp:220] relu3 needs backward computation.
I0122 16:22:33.329994 45517 net.cpp:220] bn3 needs backward computation.
I0122 16:22:33.329998 45517 net.cpp:220] conv3 needs backward computation.
I0122 16:22:33.330001 45517 net.cpp:220] drop1 needs backward computation.
I0122 16:22:33.330004 45517 net.cpp:220] pool1 needs backward computation.
I0122 16:22:33.330008 45517 net.cpp:220] relu2 needs backward computation.
I0122 16:22:33.330010 45517 net.cpp:220] bn2 needs backward computation.
I0122 16:22:33.330014 45517 net.cpp:220] conv2 needs backward computation.
I0122 16:22:33.330018 45517 net.cpp:220] relu1 needs backward computation.
I0122 16:22:33.330020 45517 net.cpp:220] bn1 needs backward computation.
I0122 16:22:33.330034 45517 net.cpp:220] conv1 needs backward computation.
I0122 16:22:33.330039 45517 net.cpp:222] data does not need backward computation.
I0122 16:22:33.330042 45517 net.cpp:264] This network produces output loss
I0122 16:22:33.330061 45517 net.cpp:284] Network initialization done.
I0122 16:22:33.330370 45517 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0/net_finetune.prototxt
I0122 16:22:33.330406 45517 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:22:33.330605 45517 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:22:33.330704 45517 layer_factory.hpp:77] Creating layer data
I0122 16:22:33.330744 45517 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:33.332033 45517 net.cpp:94] Creating Layer data
I0122 16:22:33.332059 45517 net.cpp:409] data -> data
I0122 16:22:33.332077 45517 net.cpp:409] data -> label
I0122 16:22:33.332763 45586 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:22:33.332806 45586 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:22:33.332937 45517 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:22:33.333106 45517 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:22:33.338877 45517 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:22:33.338958 45517 net.cpp:144] Setting up data
I0122 16:22:33.338973 45517 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:22:33.338984 45517 net.cpp:151] Top shape: 50 (50)
I0122 16:22:33.338989 45517 net.cpp:159] Memory required for data: 614600
I0122 16:22:33.338999 45517 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:22:33.339015 45517 net.cpp:94] Creating Layer label_data_1_split
I0122 16:22:33.339022 45517 net.cpp:435] label_data_1_split <- label
I0122 16:22:33.339033 45517 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:22:33.339051 45517 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:22:33.339063 45517 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:22:33.339241 45517 net.cpp:144] Setting up label_data_1_split
I0122 16:22:33.339251 45517 net.cpp:151] Top shape: 50 (50)
I0122 16:22:33.339259 45517 net.cpp:151] Top shape: 50 (50)
I0122 16:22:33.339265 45517 net.cpp:151] Top shape: 50 (50)
I0122 16:22:33.339272 45517 net.cpp:159] Memory required for data: 615200
I0122 16:22:33.339277 45517 layer_factory.hpp:77] Creating layer conv1
I0122 16:22:33.339294 45517 net.cpp:94] Creating Layer conv1
I0122 16:22:33.339303 45517 net.cpp:435] conv1 <- data
I0122 16:22:33.339311 45517 net.cpp:409] conv1 -> conv1
I0122 16:22:33.339798 45517 net.cpp:144] Setting up conv1
I0122 16:22:33.339812 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.339817 45517 net.cpp:159] Memory required for data: 7168800
I0122 16:22:33.339833 45517 layer_factory.hpp:77] Creating layer bn1
I0122 16:22:33.339846 45517 net.cpp:94] Creating Layer bn1
I0122 16:22:33.339855 45517 net.cpp:435] bn1 <- conv1
I0122 16:22:33.339864 45517 net.cpp:409] bn1 -> scale1
I0122 16:22:33.341538 45517 net.cpp:144] Setting up bn1
I0122 16:22:33.341552 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.341558 45517 net.cpp:159] Memory required for data: 13722400
I0122 16:22:33.341583 45517 layer_factory.hpp:77] Creating layer relu1
I0122 16:22:33.341594 45517 net.cpp:94] Creating Layer relu1
I0122 16:22:33.341600 45517 net.cpp:435] relu1 <- scale1
I0122 16:22:33.341611 45517 net.cpp:409] relu1 -> relu1
I0122 16:22:33.341694 45517 net.cpp:144] Setting up relu1
I0122 16:22:33.341704 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.341709 45517 net.cpp:159] Memory required for data: 20276000
I0122 16:22:33.341717 45517 layer_factory.hpp:77] Creating layer conv2
I0122 16:22:33.341730 45517 net.cpp:94] Creating Layer conv2
I0122 16:22:33.341737 45517 net.cpp:435] conv2 <- relu1
I0122 16:22:33.341747 45517 net.cpp:409] conv2 -> conv2
I0122 16:22:33.342387 45517 net.cpp:144] Setting up conv2
I0122 16:22:33.342411 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.342414 45517 net.cpp:159] Memory required for data: 26829600
I0122 16:22:33.342424 45517 layer_factory.hpp:77] Creating layer bn2
I0122 16:22:33.342437 45517 net.cpp:94] Creating Layer bn2
I0122 16:22:33.342442 45517 net.cpp:435] bn2 <- conv2
I0122 16:22:33.342449 45517 net.cpp:409] bn2 -> scale2
I0122 16:22:33.343504 45517 net.cpp:144] Setting up bn2
I0122 16:22:33.343514 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.343518 45517 net.cpp:159] Memory required for data: 33383200
I0122 16:22:33.343530 45517 layer_factory.hpp:77] Creating layer relu2
I0122 16:22:33.343540 45517 net.cpp:94] Creating Layer relu2
I0122 16:22:33.343546 45517 net.cpp:435] relu2 <- scale2
I0122 16:22:33.343552 45517 net.cpp:409] relu2 -> relu2
I0122 16:22:33.343581 45517 net.cpp:144] Setting up relu2
I0122 16:22:33.343590 45517 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:22:33.343595 45517 net.cpp:159] Memory required for data: 39936800
I0122 16:22:33.343597 45517 layer_factory.hpp:77] Creating layer pool1
I0122 16:22:33.343606 45517 net.cpp:94] Creating Layer pool1
I0122 16:22:33.343616 45517 net.cpp:435] pool1 <- relu2
I0122 16:22:33.343623 45517 net.cpp:409] pool1 -> pool1
I0122 16:22:33.343706 45517 net.cpp:144] Setting up pool1
I0122 16:22:33.343731 45517 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:22:33.343735 45517 net.cpp:159] Memory required for data: 41575200
I0122 16:22:33.343740 45517 layer_factory.hpp:77] Creating layer drop1
I0122 16:22:33.343749 45517 net.cpp:94] Creating Layer drop1
I0122 16:22:33.343752 45517 net.cpp:435] drop1 <- pool1
I0122 16:22:33.343760 45517 net.cpp:409] drop1 -> drop1
I0122 16:22:33.343809 45517 net.cpp:144] Setting up drop1
I0122 16:22:33.343816 45517 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:22:33.343821 45517 net.cpp:159] Memory required for data: 43213600
I0122 16:22:33.343833 45517 layer_factory.hpp:77] Creating layer conv3
I0122 16:22:33.343845 45517 net.cpp:94] Creating Layer conv3
I0122 16:22:33.343850 45517 net.cpp:435] conv3 <- drop1
I0122 16:22:33.343858 45517 net.cpp:409] conv3 -> conv3
I0122 16:22:33.344360 45517 net.cpp:144] Setting up conv3
I0122 16:22:33.344369 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.344373 45517 net.cpp:159] Memory required for data: 46490400
I0122 16:22:33.344382 45517 layer_factory.hpp:77] Creating layer bn3
I0122 16:22:33.344391 45517 net.cpp:94] Creating Layer bn3
I0122 16:22:33.344398 45517 net.cpp:435] bn3 <- conv3
I0122 16:22:33.344408 45517 net.cpp:409] bn3 -> scale3
I0122 16:22:33.345530 45517 net.cpp:144] Setting up bn3
I0122 16:22:33.345541 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.345546 45517 net.cpp:159] Memory required for data: 49767200
I0122 16:22:33.345563 45517 layer_factory.hpp:77] Creating layer relu3
I0122 16:22:33.345571 45517 net.cpp:94] Creating Layer relu3
I0122 16:22:33.345577 45517 net.cpp:435] relu3 <- scale3
I0122 16:22:33.345583 45517 net.cpp:409] relu3 -> relu3
I0122 16:22:33.345623 45517 net.cpp:144] Setting up relu3
I0122 16:22:33.345631 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.345643 45517 net.cpp:159] Memory required for data: 53044000
I0122 16:22:33.345647 45517 layer_factory.hpp:77] Creating layer conv4
I0122 16:22:33.345659 45517 net.cpp:94] Creating Layer conv4
I0122 16:22:33.345665 45517 net.cpp:435] conv4 <- relu3
I0122 16:22:33.345674 45517 net.cpp:409] conv4 -> conv4
I0122 16:22:33.346362 45517 net.cpp:144] Setting up conv4
I0122 16:22:33.346372 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.346377 45517 net.cpp:159] Memory required for data: 56320800
I0122 16:22:33.346385 45517 layer_factory.hpp:77] Creating layer bn4
I0122 16:22:33.346396 45517 net.cpp:94] Creating Layer bn4
I0122 16:22:33.346401 45517 net.cpp:435] bn4 <- conv4
I0122 16:22:33.346410 45517 net.cpp:409] bn4 -> scale4
I0122 16:22:33.347537 45517 net.cpp:144] Setting up bn4
I0122 16:22:33.347548 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.347551 45517 net.cpp:159] Memory required for data: 59597600
I0122 16:22:33.347563 45517 layer_factory.hpp:77] Creating layer relu4
I0122 16:22:33.347571 45517 net.cpp:94] Creating Layer relu4
I0122 16:22:33.347578 45517 net.cpp:435] relu4 <- scale4
I0122 16:22:33.347584 45517 net.cpp:409] relu4 -> relu4
I0122 16:22:33.347613 45517 net.cpp:144] Setting up relu4
I0122 16:22:33.347621 45517 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:22:33.347626 45517 net.cpp:159] Memory required for data: 62874400
I0122 16:22:33.347630 45517 layer_factory.hpp:77] Creating layer pool2
I0122 16:22:33.347638 45517 net.cpp:94] Creating Layer pool2
I0122 16:22:33.347643 45517 net.cpp:435] pool2 <- relu4
I0122 16:22:33.347651 45517 net.cpp:409] pool2 -> pool2
I0122 16:22:33.347698 45517 net.cpp:144] Setting up pool2
I0122 16:22:33.347707 45517 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:22:33.347710 45517 net.cpp:159] Memory required for data: 63693600
I0122 16:22:33.347714 45517 layer_factory.hpp:77] Creating layer drop2
I0122 16:22:33.347728 45517 net.cpp:94] Creating Layer drop2
I0122 16:22:33.347733 45517 net.cpp:435] drop2 <- pool2
I0122 16:22:33.347738 45517 net.cpp:409] drop2 -> drop2
I0122 16:22:33.347781 45517 net.cpp:144] Setting up drop2
I0122 16:22:33.347790 45517 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:22:33.347815 45517 net.cpp:159] Memory required for data: 64512800
I0122 16:22:33.347820 45517 layer_factory.hpp:77] Creating layer fc1
I0122 16:22:33.347829 45517 net.cpp:94] Creating Layer fc1
I0122 16:22:33.347836 45517 net.cpp:435] fc1 <- drop2
I0122 16:22:33.347843 45517 net.cpp:409] fc1 -> fc1
I0122 16:22:33.364512 45517 net.cpp:144] Setting up fc1
I0122 16:22:33.364533 45517 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:33.364536 45517 net.cpp:159] Memory required for data: 64615200
I0122 16:22:33.364543 45517 layer_factory.hpp:77] Creating layer bn5
I0122 16:22:33.364552 45517 net.cpp:94] Creating Layer bn5
I0122 16:22:33.364557 45517 net.cpp:435] bn5 <- fc1
I0122 16:22:33.364573 45517 net.cpp:409] bn5 -> scale5
I0122 16:22:33.365161 45517 net.cpp:144] Setting up bn5
I0122 16:22:33.365169 45517 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:33.365172 45517 net.cpp:159] Memory required for data: 64717600
I0122 16:22:33.365185 45517 layer_factory.hpp:77] Creating layer relu5
I0122 16:22:33.365191 45517 net.cpp:94] Creating Layer relu5
I0122 16:22:33.365195 45517 net.cpp:435] relu5 <- scale5
I0122 16:22:33.365200 45517 net.cpp:409] relu5 -> relu5
I0122 16:22:33.365218 45517 net.cpp:144] Setting up relu5
I0122 16:22:33.365223 45517 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:33.365227 45517 net.cpp:159] Memory required for data: 64820000
I0122 16:22:33.365231 45517 layer_factory.hpp:77] Creating layer drop3
I0122 16:22:33.365236 45517 net.cpp:94] Creating Layer drop3
I0122 16:22:33.365238 45517 net.cpp:435] drop3 <- relu5
I0122 16:22:33.365242 45517 net.cpp:409] drop3 -> drop3
I0122 16:22:33.365269 45517 net.cpp:144] Setting up drop3
I0122 16:22:33.365275 45517 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:22:33.365278 45517 net.cpp:159] Memory required for data: 64922400
I0122 16:22:33.365280 45517 layer_factory.hpp:77] Creating layer fc2
I0122 16:22:33.365286 45517 net.cpp:94] Creating Layer fc2
I0122 16:22:33.365289 45517 net.cpp:435] fc2 <- drop3
I0122 16:22:33.365294 45517 net.cpp:409] fc2 -> fc2
I0122 16:22:33.365434 45517 net.cpp:144] Setting up fc2
I0122 16:22:33.365440 45517 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:33.365442 45517 net.cpp:159] Memory required for data: 64924400
I0122 16:22:33.365447 45517 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:22:33.365453 45517 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:22:33.365458 45517 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:22:33.365463 45517 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:22:33.365470 45517 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:22:33.365475 45517 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:22:33.365515 45517 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:22:33.365520 45517 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:33.365523 45517 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:33.365526 45517 net.cpp:151] Top shape: 50 10 (500)
I0122 16:22:33.365530 45517 net.cpp:159] Memory required for data: 64930400
I0122 16:22:33.365532 45517 layer_factory.hpp:77] Creating layer loss
I0122 16:22:33.365537 45517 net.cpp:94] Creating Layer loss
I0122 16:22:33.365540 45517 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:22:33.365545 45517 net.cpp:435] loss <- label_data_1_split_0
I0122 16:22:33.365550 45517 net.cpp:409] loss -> loss
I0122 16:22:33.365556 45517 layer_factory.hpp:77] Creating layer loss
I0122 16:22:33.365628 45517 net.cpp:144] Setting up loss
I0122 16:22:33.365633 45517 net.cpp:151] Top shape: (1)
I0122 16:22:33.365635 45517 net.cpp:154]     with loss weight 1
I0122 16:22:33.365648 45517 net.cpp:159] Memory required for data: 64930404
I0122 16:22:33.365650 45517 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:22:33.365656 45517 net.cpp:94] Creating Layer accuracy-top1
I0122 16:22:33.365659 45517 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:22:33.365662 45517 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:22:33.365667 45517 net.cpp:409] accuracy-top1 -> top-1
I0122 16:22:33.365689 45517 net.cpp:144] Setting up accuracy-top1
I0122 16:22:33.365691 45517 net.cpp:151] Top shape: (1)
I0122 16:22:33.365694 45517 net.cpp:159] Memory required for data: 64930408
I0122 16:22:33.365696 45517 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:22:33.365702 45517 net.cpp:94] Creating Layer accuracy-top5
I0122 16:22:33.365705 45517 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:22:33.365710 45517 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:22:33.365715 45517 net.cpp:409] accuracy-top5 -> top-5
I0122 16:22:33.365723 45517 net.cpp:144] Setting up accuracy-top5
I0122 16:22:33.365726 45517 net.cpp:151] Top shape: (1)
I0122 16:22:33.365730 45517 net.cpp:159] Memory required for data: 64930412
I0122 16:22:33.365731 45517 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:22:33.365736 45517 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:22:33.365739 45517 net.cpp:220] loss needs backward computation.
I0122 16:22:33.365742 45517 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:22:33.365746 45517 net.cpp:220] fc2 needs backward computation.
I0122 16:22:33.365749 45517 net.cpp:220] drop3 needs backward computation.
I0122 16:22:33.365752 45517 net.cpp:220] relu5 needs backward computation.
I0122 16:22:33.365754 45517 net.cpp:220] bn5 needs backward computation.
I0122 16:22:33.365758 45517 net.cpp:220] fc1 needs backward computation.
I0122 16:22:33.365761 45517 net.cpp:220] drop2 needs backward computation.
I0122 16:22:33.365764 45517 net.cpp:220] pool2 needs backward computation.
I0122 16:22:33.365768 45517 net.cpp:220] relu4 needs backward computation.
I0122 16:22:33.365772 45517 net.cpp:220] bn4 needs backward computation.
I0122 16:22:33.365774 45517 net.cpp:220] conv4 needs backward computation.
I0122 16:22:33.365777 45517 net.cpp:220] relu3 needs backward computation.
I0122 16:22:33.365780 45517 net.cpp:220] bn3 needs backward computation.
I0122 16:22:33.365783 45517 net.cpp:220] conv3 needs backward computation.
I0122 16:22:33.365787 45517 net.cpp:220] drop1 needs backward computation.
I0122 16:22:33.365789 45517 net.cpp:220] pool1 needs backward computation.
I0122 16:22:33.365792 45517 net.cpp:220] relu2 needs backward computation.
I0122 16:22:33.365795 45517 net.cpp:220] bn2 needs backward computation.
I0122 16:22:33.365799 45517 net.cpp:220] conv2 needs backward computation.
I0122 16:22:33.365803 45517 net.cpp:220] relu1 needs backward computation.
I0122 16:22:33.365805 45517 net.cpp:220] bn1 needs backward computation.
I0122 16:22:33.365808 45517 net.cpp:220] conv1 needs backward computation.
I0122 16:22:33.365813 45517 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:22:33.365816 45517 net.cpp:222] data does not need backward computation.
I0122 16:22:33.365818 45517 net.cpp:264] This network produces output loss
I0122 16:22:33.365821 45517 net.cpp:264] This network produces output top-1
I0122 16:22:33.365825 45517 net.cpp:264] This network produces output top-5
I0122 16:22:33.365847 45517 net.cpp:284] Network initialization done.
I0122 16:22:33.365967 45517 solver.cpp:63] Solver scaffolding done.
I0122 16:22:33.367110 45517 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0/sparse.caffemodel
W0122 16:22:33.395884 45517 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.396226 45517 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.396425 45517 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.396656 45517 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.398564 45517 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.417405 45517 net.cpp:860] Force copying param 4 weights from layer 'bn1'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.417603 45517 net.cpp:860] Force copying param 4 weights from layer 'bn2'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.417788 45517 net.cpp:860] Force copying param 4 weights from layer 'bn3'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.418010 45517 net.cpp:860] Force copying param 4 weights from layer 'bn4'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
W0122 16:22:33.419656 45517 net.cpp:860] Force copying param 4 weights from layer 'bn5'; shape mismatch.  Source param shape is 1 (1); target param shape is 1 1 1 1 (1).
I0122 16:22:33.419769 45517 caffe_interface.cpp:527] Starting Optimization
I0122 16:22:33.419775 45517 solver.cpp:335] Solving 
I0122 16:22:33.419778 45517 solver.cpp:336] Learning Rate Policy: poly
I0122 16:22:33.420883 45517 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:22:33.657761 45517 solver.cpp:517]     Test net output #0: loss = 0.422096 (* 1 = 0.422096 loss)
I0122 16:22:33.657783 45517 solver.cpp:517]     Test net output #1: top-1 = 0.862666
I0122 16:22:33.657788 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0122 16:22:33.674765 45517 solver.cpp:266] Iteration 0 (0 iter/s, 0.254962s/100 iter), loss = 0.137372
I0122 16:22:33.674799 45517 solver.cpp:285]     Train net output #0: loss = 0.137372 (* 1 = 0.137372 loss)
I0122 16:22:33.674811 45517 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:22:34.548415 45517 solver.cpp:266] Iteration 100 (114.472 iter/s, 0.873575s/100 iter), loss = 0.276892
I0122 16:22:34.548454 45517 solver.cpp:285]     Train net output #0: loss = 0.276892 (* 1 = 0.276892 loss)
I0122 16:22:34.548460 45517 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:22:35.412030 45517 solver.cpp:266] Iteration 200 (115.802 iter/s, 0.863546s/100 iter), loss = 0.482227
I0122 16:22:35.412058 45517 solver.cpp:285]     Train net output #0: loss = 0.482227 (* 1 = 0.482227 loss)
I0122 16:22:35.412065 45517 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:22:36.275924 45517 solver.cpp:266] Iteration 300 (115.764 iter/s, 0.863826s/100 iter), loss = 0.30458
I0122 16:22:36.275952 45517 solver.cpp:285]     Train net output #0: loss = 0.30458 (* 1 = 0.30458 loss)
I0122 16:22:36.275959 45517 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:22:37.145617 45517 solver.cpp:266] Iteration 400 (114.992 iter/s, 0.869622s/100 iter), loss = 0.364794
I0122 16:22:37.145644 45517 solver.cpp:285]     Train net output #0: loss = 0.364794 (* 1 = 0.364794 loss)
I0122 16:22:37.145650 45517 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:22:38.012542 45517 solver.cpp:266] Iteration 500 (115.359 iter/s, 0.866857s/100 iter), loss = 0.331902
I0122 16:22:38.012573 45517 solver.cpp:285]     Train net output #0: loss = 0.331902 (* 1 = 0.331902 loss)
I0122 16:22:38.012578 45517 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:22:38.875365 45517 solver.cpp:266] Iteration 600 (115.908 iter/s, 0.862752s/100 iter), loss = 0.298577
I0122 16:22:38.875394 45517 solver.cpp:285]     Train net output #0: loss = 0.298577 (* 1 = 0.298577 loss)
I0122 16:22:38.875401 45517 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:22:39.742223 45517 solver.cpp:266] Iteration 700 (115.369 iter/s, 0.866787s/100 iter), loss = 0.509219
I0122 16:22:39.742250 45517 solver.cpp:285]     Train net output #0: loss = 0.509219 (* 1 = 0.509219 loss)
I0122 16:22:39.742256 45517 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:22:40.610102 45517 solver.cpp:266] Iteration 800 (115.233 iter/s, 0.86781s/100 iter), loss = 0.474098
I0122 16:22:40.610131 45517 solver.cpp:285]     Train net output #0: loss = 0.474098 (* 1 = 0.474098 loss)
I0122 16:22:40.610136 45517 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:22:41.472148 45517 solver.cpp:266] Iteration 900 (116.013 iter/s, 0.861976s/100 iter), loss = 0.313383
I0122 16:22:41.472187 45517 solver.cpp:285]     Train net output #0: loss = 0.313383 (* 1 = 0.313383 loss)
I0122 16:22:41.472193 45517 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:22:42.326165 45517 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:22:42.545943 45517 solver.cpp:517]     Test net output #0: loss = 0.827814 (* 1 = 0.827814 loss)
I0122 16:22:42.545964 45517 solver.cpp:517]     Test net output #1: top-1 = 0.782
I0122 16:22:42.545967 45517 solver.cpp:517]     Test net output #2: top-5 = 0.981778
I0122 16:22:42.554055 45517 solver.cpp:266] Iteration 1000 (92.4365 iter/s, 1.08182s/100 iter), loss = 0.256638
I0122 16:22:42.554083 45517 solver.cpp:285]     Train net output #0: loss = 0.256638 (* 1 = 0.256638 loss)
I0122 16:22:42.554090 45517 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:22:43.419651 45517 solver.cpp:266] Iteration 1100 (115.535 iter/s, 0.865535s/100 iter), loss = 0.37589
I0122 16:22:43.419675 45517 solver.cpp:285]     Train net output #0: loss = 0.37589 (* 1 = 0.37589 loss)
I0122 16:22:43.419682 45517 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:22:44.286375 45517 solver.cpp:266] Iteration 1200 (115.386 iter/s, 0.866657s/100 iter), loss = 0.314256
I0122 16:22:44.286401 45517 solver.cpp:285]     Train net output #0: loss = 0.314256 (* 1 = 0.314256 loss)
I0122 16:22:44.286407 45517 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:22:45.151878 45517 solver.cpp:266] Iteration 1300 (115.549 iter/s, 0.865436s/100 iter), loss = 0.30015
I0122 16:22:45.151906 45517 solver.cpp:285]     Train net output #0: loss = 0.30015 (* 1 = 0.30015 loss)
I0122 16:22:45.151978 45517 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:22:46.016423 45517 solver.cpp:266] Iteration 1400 (115.686 iter/s, 0.864406s/100 iter), loss = 0.303192
I0122 16:22:46.016449 45517 solver.cpp:285]     Train net output #0: loss = 0.303192 (* 1 = 0.303192 loss)
I0122 16:22:46.016454 45517 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:22:46.876452 45517 solver.cpp:266] Iteration 1500 (116.284 iter/s, 0.859963s/100 iter), loss = 0.26834
I0122 16:22:46.876480 45517 solver.cpp:285]     Train net output #0: loss = 0.26834 (* 1 = 0.26834 loss)
I0122 16:22:46.876487 45517 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:22:47.737098 45517 solver.cpp:266] Iteration 1600 (116.201 iter/s, 0.860577s/100 iter), loss = 0.276662
I0122 16:22:47.737126 45517 solver.cpp:285]     Train net output #0: loss = 0.276662 (* 1 = 0.276662 loss)
I0122 16:22:47.737131 45517 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:22:48.598497 45517 solver.cpp:266] Iteration 1700 (116.1 iter/s, 0.86133s/100 iter), loss = 0.230204
I0122 16:22:48.598525 45517 solver.cpp:285]     Train net output #0: loss = 0.230204 (* 1 = 0.230204 loss)
I0122 16:22:48.598529 45517 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:22:49.463555 45517 solver.cpp:266] Iteration 1800 (115.608 iter/s, 0.86499s/100 iter), loss = 0.309921
I0122 16:22:49.463582 45517 solver.cpp:285]     Train net output #0: loss = 0.309921 (* 1 = 0.309921 loss)
I0122 16:22:49.463588 45517 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:22:50.329944 45517 solver.cpp:266] Iteration 1900 (115.431 iter/s, 0.86632s/100 iter), loss = 0.296328
I0122 16:22:50.329972 45517 solver.cpp:285]     Train net output #0: loss = 0.296328 (* 1 = 0.296328 loss)
I0122 16:22:50.329977 45517 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:22:51.188685 45517 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:22:51.417873 45517 solver.cpp:517]     Test net output #0: loss = 0.561653 (* 1 = 0.561653 loss)
I0122 16:22:51.417889 45517 solver.cpp:517]     Test net output #1: top-1 = 0.820889
I0122 16:22:51.417894 45517 solver.cpp:517]     Test net output #2: top-5 = 0.988222
I0122 16:22:51.426192 45517 solver.cpp:266] Iteration 2000 (91.2263 iter/s, 1.09617s/100 iter), loss = 0.304056
I0122 16:22:51.426226 45517 solver.cpp:285]     Train net output #0: loss = 0.304056 (* 1 = 0.304056 loss)
I0122 16:22:51.426234 45517 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:22:52.286955 45517 solver.cpp:266] Iteration 2100 (116.186 iter/s, 0.860687s/100 iter), loss = 0.27481
I0122 16:22:52.286983 45517 solver.cpp:285]     Train net output #0: loss = 0.27481 (* 1 = 0.27481 loss)
I0122 16:22:52.286988 45517 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:22:53.148279 45517 solver.cpp:266] Iteration 2200 (116.11 iter/s, 0.861255s/100 iter), loss = 0.304108
I0122 16:22:53.148319 45517 solver.cpp:285]     Train net output #0: loss = 0.304108 (* 1 = 0.304108 loss)
I0122 16:22:53.148325 45517 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:22:54.014551 45517 solver.cpp:266] Iteration 2300 (115.448 iter/s, 0.866192s/100 iter), loss = 0.403935
I0122 16:22:54.014591 45517 solver.cpp:285]     Train net output #0: loss = 0.403935 (* 1 = 0.403935 loss)
I0122 16:22:54.014600 45517 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:22:54.879370 45517 solver.cpp:266] Iteration 2400 (115.642 iter/s, 0.864737s/100 iter), loss = 0.28839
I0122 16:22:54.879397 45517 solver.cpp:285]     Train net output #0: loss = 0.28839 (* 1 = 0.28839 loss)
I0122 16:22:54.879403 45517 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:22:55.742226 45517 solver.cpp:266] Iteration 2500 (115.903 iter/s, 0.862788s/100 iter), loss = 0.299767
I0122 16:22:55.742265 45517 solver.cpp:285]     Train net output #0: loss = 0.299767 (* 1 = 0.299767 loss)
I0122 16:22:55.742271 45517 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:22:56.605636 45517 solver.cpp:266] Iteration 2600 (115.831 iter/s, 0.86333s/100 iter), loss = 0.152021
I0122 16:22:56.605674 45517 solver.cpp:285]     Train net output #0: loss = 0.152021 (* 1 = 0.152021 loss)
I0122 16:22:56.605680 45517 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:22:57.470911 45517 solver.cpp:266] Iteration 2700 (115.581 iter/s, 0.865197s/100 iter), loss = 0.363545
I0122 16:22:57.470939 45517 solver.cpp:285]     Train net output #0: loss = 0.363545 (* 1 = 0.363545 loss)
I0122 16:22:57.470947 45517 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:22:58.336706 45517 solver.cpp:266] Iteration 2800 (115.51 iter/s, 0.865725s/100 iter), loss = 0.278007
I0122 16:22:58.336735 45517 solver.cpp:285]     Train net output #0: loss = 0.278007 (* 1 = 0.278007 loss)
I0122 16:22:58.336740 45517 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:22:59.202046 45517 solver.cpp:266] Iteration 2900 (115.571 iter/s, 0.865271s/100 iter), loss = 0.26318
I0122 16:22:59.202073 45517 solver.cpp:285]     Train net output #0: loss = 0.26318 (* 1 = 0.26318 loss)
I0122 16:22:59.202080 45517 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:23:00.056854 45517 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:23:00.277195 45517 solver.cpp:517]     Test net output #0: loss = 0.542607 (* 1 = 0.542607 loss)
I0122 16:23:00.277211 45517 solver.cpp:517]     Test net output #1: top-1 = 0.836111
I0122 16:23:00.277215 45517 solver.cpp:517]     Test net output #2: top-5 = 0.988222
I0122 16:23:00.285329 45517 solver.cpp:266] Iteration 3000 (92.3182 iter/s, 1.08321s/100 iter), loss = 0.220364
I0122 16:23:00.285357 45517 solver.cpp:285]     Train net output #0: loss = 0.220364 (* 1 = 0.220364 loss)
I0122 16:23:00.285365 45517 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:23:01.148711 45517 solver.cpp:266] Iteration 3100 (115.832 iter/s, 0.863323s/100 iter), loss = 0.20832
I0122 16:23:01.148749 45517 solver.cpp:285]     Train net output #0: loss = 0.20832 (* 1 = 0.20832 loss)
I0122 16:23:01.148757 45517 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:23:02.015858 45517 solver.cpp:266] Iteration 3200 (115.331 iter/s, 0.867067s/100 iter), loss = 0.288679
I0122 16:23:02.015884 45517 solver.cpp:285]     Train net output #0: loss = 0.288679 (* 1 = 0.288679 loss)
I0122 16:23:02.015889 45517 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:23:02.880838 45517 solver.cpp:266] Iteration 3300 (115.619 iter/s, 0.864913s/100 iter), loss = 0.116451
I0122 16:23:02.881033 45517 solver.cpp:285]     Train net output #0: loss = 0.116451 (* 1 = 0.116451 loss)
I0122 16:23:02.881042 45517 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:23:03.745604 45517 solver.cpp:266] Iteration 3400 (115.669 iter/s, 0.864533s/100 iter), loss = 0.348961
I0122 16:23:03.745632 45517 solver.cpp:285]     Train net output #0: loss = 0.348961 (* 1 = 0.348961 loss)
I0122 16:23:03.745638 45517 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:23:04.609256 45517 solver.cpp:266] Iteration 3500 (115.797 iter/s, 0.863582s/100 iter), loss = 0.263571
I0122 16:23:04.609282 45517 solver.cpp:285]     Train net output #0: loss = 0.263571 (* 1 = 0.263571 loss)
I0122 16:23:04.609288 45517 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:23:05.473060 45517 solver.cpp:266] Iteration 3600 (115.776 iter/s, 0.863737s/100 iter), loss = 0.236735
I0122 16:23:05.473088 45517 solver.cpp:285]     Train net output #0: loss = 0.236735 (* 1 = 0.236735 loss)
I0122 16:23:05.473093 45517 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:23:06.346469 45517 solver.cpp:266] Iteration 3700 (114.503 iter/s, 0.873341s/100 iter), loss = 0.310034
I0122 16:23:06.346496 45517 solver.cpp:285]     Train net output #0: loss = 0.310035 (* 1 = 0.310035 loss)
I0122 16:23:06.346503 45517 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:23:07.212421 45517 solver.cpp:266] Iteration 3800 (115.489 iter/s, 0.865883s/100 iter), loss = 0.267076
I0122 16:23:07.212460 45517 solver.cpp:285]     Train net output #0: loss = 0.267076 (* 1 = 0.267076 loss)
I0122 16:23:07.212466 45517 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:23:08.078588 45517 solver.cpp:266] Iteration 3900 (115.462 iter/s, 0.866085s/100 iter), loss = 0.153965
I0122 16:23:08.078626 45517 solver.cpp:285]     Train net output #0: loss = 0.153965 (* 1 = 0.153965 loss)
I0122 16:23:08.078632 45517 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:23:08.936161 45517 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:23:09.168154 45517 solver.cpp:517]     Test net output #0: loss = 0.504701 (* 1 = 0.504701 loss)
I0122 16:23:09.168171 45517 solver.cpp:517]     Test net output #1: top-1 = 0.835778
I0122 16:23:09.168176 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0122 16:23:09.176618 45517 solver.cpp:266] Iteration 4000 (91.0792 iter/s, 1.09795s/100 iter), loss = 0.193304
I0122 16:23:09.176636 45517 solver.cpp:285]     Train net output #0: loss = 0.193304 (* 1 = 0.193304 loss)
I0122 16:23:09.176643 45517 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:23:10.043716 45517 solver.cpp:266] Iteration 4100 (115.335 iter/s, 0.867037s/100 iter), loss = 0.247614
I0122 16:23:10.043756 45517 solver.cpp:285]     Train net output #0: loss = 0.247614 (* 1 = 0.247614 loss)
I0122 16:23:10.043762 45517 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:23:10.910624 45517 solver.cpp:266] Iteration 4200 (115.363 iter/s, 0.866827s/100 iter), loss = 0.255381
I0122 16:23:10.910663 45517 solver.cpp:285]     Train net output #0: loss = 0.255381 (* 1 = 0.255381 loss)
I0122 16:23:10.910670 45517 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:23:11.790737 45517 solver.cpp:266] Iteration 4300 (113.632 iter/s, 0.880032s/100 iter), loss = 0.264422
I0122 16:23:11.790766 45517 solver.cpp:285]     Train net output #0: loss = 0.264422 (* 1 = 0.264422 loss)
I0122 16:23:11.790788 45517 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:23:12.657332 45517 solver.cpp:266] Iteration 4400 (115.403 iter/s, 0.866525s/100 iter), loss = 0.300175
I0122 16:23:12.657372 45517 solver.cpp:285]     Train net output #0: loss = 0.300175 (* 1 = 0.300175 loss)
I0122 16:23:12.657378 45517 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:23:13.524451 45517 solver.cpp:266] Iteration 4500 (115.335 iter/s, 0.867038s/100 iter), loss = 0.217712
I0122 16:23:13.524482 45517 solver.cpp:285]     Train net output #0: loss = 0.217712 (* 1 = 0.217712 loss)
I0122 16:23:13.524487 45517 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:23:14.393981 45517 solver.cpp:266] Iteration 4600 (115.014 iter/s, 0.869458s/100 iter), loss = 0.248791
I0122 16:23:14.394032 45517 solver.cpp:285]     Train net output #0: loss = 0.248791 (* 1 = 0.248791 loss)
I0122 16:23:14.394038 45517 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:23:15.260843 45517 solver.cpp:266] Iteration 4700 (115.371 iter/s, 0.866769s/100 iter), loss = 0.323297
I0122 16:23:15.260885 45517 solver.cpp:285]     Train net output #0: loss = 0.323297 (* 1 = 0.323297 loss)
I0122 16:23:15.260891 45517 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:23:16.130081 45517 solver.cpp:266] Iteration 4800 (115.054 iter/s, 0.869155s/100 iter), loss = 0.255786
I0122 16:23:16.130110 45517 solver.cpp:285]     Train net output #0: loss = 0.255786 (* 1 = 0.255786 loss)
I0122 16:23:16.130115 45517 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:23:16.994412 45517 solver.cpp:266] Iteration 4900 (115.706 iter/s, 0.864261s/100 iter), loss = 0.362551
I0122 16:23:16.994441 45517 solver.cpp:285]     Train net output #0: loss = 0.362551 (* 1 = 0.362551 loss)
I0122 16:23:16.994446 45517 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:23:17.851228 45517 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:23:18.071849 45517 solver.cpp:517]     Test net output #0: loss = 0.498761 (* 1 = 0.498761 loss)
I0122 16:23:18.071864 45517 solver.cpp:517]     Test net output #1: top-1 = 0.839
I0122 16:23:18.071868 45517 solver.cpp:517]     Test net output #2: top-5 = 0.988889
I0122 16:23:18.079972 45517 solver.cpp:266] Iteration 5000 (92.1246 iter/s, 1.08549s/100 iter), loss = 0.290276
I0122 16:23:18.079990 45517 solver.cpp:285]     Train net output #0: loss = 0.290276 (* 1 = 0.290276 loss)
I0122 16:23:18.079996 45517 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:23:18.947054 45517 solver.cpp:266] Iteration 5100 (115.337 iter/s, 0.867022s/100 iter), loss = 0.189481
I0122 16:23:18.947091 45517 solver.cpp:285]     Train net output #0: loss = 0.189481 (* 1 = 0.189481 loss)
I0122 16:23:18.947098 45517 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:23:19.831089 45517 solver.cpp:266] Iteration 5200 (113.128 iter/s, 0.883953s/100 iter), loss = 0.353314
I0122 16:23:19.831118 45517 solver.cpp:285]     Train net output #0: loss = 0.353314 (* 1 = 0.353314 loss)
I0122 16:23:19.831125 45517 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:23:20.705768 45517 solver.cpp:266] Iteration 5300 (114.337 iter/s, 0.874609s/100 iter), loss = 0.323861
I0122 16:23:20.705796 45517 solver.cpp:285]     Train net output #0: loss = 0.323861 (* 1 = 0.323861 loss)
I0122 16:23:20.705801 45517 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:23:21.578660 45517 solver.cpp:266] Iteration 5400 (114.571 iter/s, 0.872821s/100 iter), loss = 0.325598
I0122 16:23:21.578688 45517 solver.cpp:285]     Train net output #0: loss = 0.325599 (* 1 = 0.325599 loss)
I0122 16:23:21.578694 45517 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:23:22.458392 45517 solver.cpp:266] Iteration 5500 (113.68 iter/s, 0.879661s/100 iter), loss = 0.265844
I0122 16:23:22.458420 45517 solver.cpp:285]     Train net output #0: loss = 0.265844 (* 1 = 0.265844 loss)
I0122 16:23:22.458431 45517 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:23:23.339169 45517 solver.cpp:266] Iteration 5600 (113.545 iter/s, 0.880705s/100 iter), loss = 0.267842
I0122 16:23:23.339207 45517 solver.cpp:285]     Train net output #0: loss = 0.267842 (* 1 = 0.267842 loss)
I0122 16:23:23.339213 45517 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:23:24.207437 45517 solver.cpp:266] Iteration 5700 (115.183 iter/s, 0.868187s/100 iter), loss = 0.199409
I0122 16:23:24.207465 45517 solver.cpp:285]     Train net output #0: loss = 0.199409 (* 1 = 0.199409 loss)
I0122 16:23:24.207470 45517 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:23:25.074059 45517 solver.cpp:266] Iteration 5800 (115.4 iter/s, 0.866553s/100 iter), loss = 0.20273
I0122 16:23:25.074097 45517 solver.cpp:285]     Train net output #0: loss = 0.20273 (* 1 = 0.20273 loss)
I0122 16:23:25.074139 45517 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:23:25.940237 45517 solver.cpp:266] Iteration 5900 (115.46 iter/s, 0.866097s/100 iter), loss = 0.317121
I0122 16:23:25.940264 45517 solver.cpp:285]     Train net output #0: loss = 0.317121 (* 1 = 0.317121 loss)
I0122 16:23:25.940271 45517 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:23:26.799173 45517 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:23:27.023953 45517 solver.cpp:517]     Test net output #0: loss = 0.506269 (* 1 = 0.506269 loss)
I0122 16:23:27.023970 45517 solver.cpp:517]     Test net output #1: top-1 = 0.835666
I0122 16:23:27.023975 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0122 16:23:27.032089 45517 solver.cpp:266] Iteration 6000 (91.5937 iter/s, 1.09178s/100 iter), loss = 0.220062
I0122 16:23:27.032107 45517 solver.cpp:285]     Train net output #0: loss = 0.220062 (* 1 = 0.220062 loss)
I0122 16:23:27.032114 45517 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:23:27.900274 45517 solver.cpp:266] Iteration 6100 (115.191 iter/s, 0.868123s/100 iter), loss = 0.250183
I0122 16:23:27.900301 45517 solver.cpp:285]     Train net output #0: loss = 0.250183 (* 1 = 0.250183 loss)
I0122 16:23:27.900307 45517 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:23:28.766245 45517 solver.cpp:266] Iteration 6200 (115.487 iter/s, 0.865902s/100 iter), loss = 0.236523
I0122 16:23:28.766286 45517 solver.cpp:285]     Train net output #0: loss = 0.236523 (* 1 = 0.236523 loss)
I0122 16:23:28.766293 45517 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:23:29.631460 45517 solver.cpp:266] Iteration 6300 (115.589 iter/s, 0.865132s/100 iter), loss = 0.158516
I0122 16:23:29.631489 45517 solver.cpp:285]     Train net output #0: loss = 0.158516 (* 1 = 0.158516 loss)
I0122 16:23:29.631494 45517 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:23:30.497572 45517 solver.cpp:266] Iteration 6400 (115.468 iter/s, 0.866041s/100 iter), loss = 0.186363
I0122 16:23:30.497598 45517 solver.cpp:285]     Train net output #0: loss = 0.186363 (* 1 = 0.186363 loss)
I0122 16:23:30.497603 45517 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:23:31.362527 45517 solver.cpp:266] Iteration 6500 (115.622 iter/s, 0.864886s/100 iter), loss = 0.285085
I0122 16:23:31.362555 45517 solver.cpp:285]     Train net output #0: loss = 0.285085 (* 1 = 0.285085 loss)
I0122 16:23:31.362560 45517 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:23:32.227401 45517 solver.cpp:266] Iteration 6600 (115.633 iter/s, 0.864804s/100 iter), loss = 0.215493
I0122 16:23:32.227429 45517 solver.cpp:285]     Train net output #0: loss = 0.215493 (* 1 = 0.215493 loss)
I0122 16:23:32.227435 45517 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:23:33.091925 45517 solver.cpp:266] Iteration 6700 (115.68 iter/s, 0.864455s/100 iter), loss = 0.214491
I0122 16:23:33.092074 45517 solver.cpp:285]     Train net output #0: loss = 0.214492 (* 1 = 0.214492 loss)
I0122 16:23:33.092084 45517 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:23:33.960288 45517 solver.cpp:266] Iteration 6800 (115.184 iter/s, 0.868173s/100 iter), loss = 0.261958
I0122 16:23:33.960327 45517 solver.cpp:285]     Train net output #0: loss = 0.261958 (* 1 = 0.261958 loss)
I0122 16:23:33.960333 45517 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:23:34.827677 45517 solver.cpp:266] Iteration 6900 (115.299 iter/s, 0.867307s/100 iter), loss = 0.248961
I0122 16:23:34.827703 45517 solver.cpp:285]     Train net output #0: loss = 0.248961 (* 1 = 0.248961 loss)
I0122 16:23:34.827709 45517 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:23:35.720350 45517 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:23:35.939054 45517 solver.cpp:517]     Test net output #0: loss = 0.477318 (* 1 = 0.477318 loss)
I0122 16:23:35.939069 45517 solver.cpp:517]     Test net output #1: top-1 = 0.842222
I0122 16:23:35.939074 45517 solver.cpp:517]     Test net output #2: top-5 = 0.988334
I0122 16:23:35.947170 45517 solver.cpp:266] Iteration 7000 (89.3321 iter/s, 1.11942s/100 iter), loss = 0.254138
I0122 16:23:35.947188 45517 solver.cpp:285]     Train net output #0: loss = 0.254138 (* 1 = 0.254138 loss)
I0122 16:23:35.947206 45517 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:23:36.809134 45517 solver.cpp:266] Iteration 7100 (116.022 iter/s, 0.861903s/100 iter), loss = 0.258077
I0122 16:23:36.809172 45517 solver.cpp:285]     Train net output #0: loss = 0.258077 (* 1 = 0.258077 loss)
I0122 16:23:36.809180 45517 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:23:37.670074 45517 solver.cpp:266] Iteration 7200 (116.163 iter/s, 0.86086s/100 iter), loss = 0.210578
I0122 16:23:37.670102 45517 solver.cpp:285]     Train net output #0: loss = 0.210578 (* 1 = 0.210578 loss)
I0122 16:23:37.670109 45517 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:23:38.538091 45517 solver.cpp:266] Iteration 7300 (115.215 iter/s, 0.867945s/100 iter), loss = 0.344879
I0122 16:23:38.538120 45517 solver.cpp:285]     Train net output #0: loss = 0.344879 (* 1 = 0.344879 loss)
I0122 16:23:38.538125 45517 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:23:39.400051 45517 solver.cpp:266] Iteration 7400 (116.024 iter/s, 0.861889s/100 iter), loss = 0.164244
I0122 16:23:39.400080 45517 solver.cpp:285]     Train net output #0: loss = 0.164244 (* 1 = 0.164244 loss)
I0122 16:23:39.400085 45517 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:23:40.265449 45517 solver.cpp:266] Iteration 7500 (115.563 iter/s, 0.865326s/100 iter), loss = 0.230703
I0122 16:23:40.265489 45517 solver.cpp:285]     Train net output #0: loss = 0.230703 (* 1 = 0.230703 loss)
I0122 16:23:40.265496 45517 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:23:41.127454 45517 solver.cpp:266] Iteration 7600 (116.02 iter/s, 0.861923s/100 iter), loss = 0.19326
I0122 16:23:41.127494 45517 solver.cpp:285]     Train net output #0: loss = 0.19326 (* 1 = 0.19326 loss)
I0122 16:23:41.127501 45517 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:23:41.989004 45517 solver.cpp:266] Iteration 7700 (116.081 iter/s, 0.861467s/100 iter), loss = 0.192126
I0122 16:23:41.989044 45517 solver.cpp:285]     Train net output #0: loss = 0.192126 (* 1 = 0.192126 loss)
I0122 16:23:41.989051 45517 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:23:42.854269 45517 solver.cpp:266] Iteration 7800 (115.583 iter/s, 0.865181s/100 iter), loss = 0.243475
I0122 16:23:42.854297 45517 solver.cpp:285]     Train net output #0: loss = 0.243475 (* 1 = 0.243475 loss)
I0122 16:23:42.854302 45517 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:23:43.727846 45517 solver.cpp:266] Iteration 7900 (114.481 iter/s, 0.873506s/100 iter), loss = 0.205373
I0122 16:23:43.727885 45517 solver.cpp:285]     Train net output #0: loss = 0.205373 (* 1 = 0.205373 loss)
I0122 16:23:43.727892 45517 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:23:44.582209 45517 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:23:44.801087 45517 solver.cpp:517]     Test net output #0: loss = 0.507382 (* 1 = 0.507382 loss)
I0122 16:23:44.801102 45517 solver.cpp:517]     Test net output #1: top-1 = 0.845
I0122 16:23:44.801115 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991445
I0122 16:23:44.809178 45517 solver.cpp:266] Iteration 8000 (92.4859 iter/s, 1.08125s/100 iter), loss = 0.208703
I0122 16:23:44.809206 45517 solver.cpp:285]     Train net output #0: loss = 0.208703 (* 1 = 0.208703 loss)
I0122 16:23:44.809213 45517 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:23:45.703416 45517 solver.cpp:266] Iteration 8100 (111.835 iter/s, 0.894176s/100 iter), loss = 0.225327
I0122 16:23:45.703445 45517 solver.cpp:285]     Train net output #0: loss = 0.225327 (* 1 = 0.225327 loss)
I0122 16:23:45.703451 45517 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:23:46.565013 45517 solver.cpp:266] Iteration 8200 (116.073 iter/s, 0.861525s/100 iter), loss = 0.120068
I0122 16:23:46.565054 45517 solver.cpp:285]     Train net output #0: loss = 0.120068 (* 1 = 0.120068 loss)
I0122 16:23:46.565060 45517 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:23:47.427376 45517 solver.cpp:266] Iteration 8300 (115.972 iter/s, 0.86228s/100 iter), loss = 0.19701
I0122 16:23:47.427403 45517 solver.cpp:285]     Train net output #0: loss = 0.197011 (* 1 = 0.197011 loss)
I0122 16:23:47.427410 45517 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:23:48.294030 45517 solver.cpp:266] Iteration 8400 (115.396 iter/s, 0.866584s/100 iter), loss = 0.16013
I0122 16:23:48.294059 45517 solver.cpp:285]     Train net output #0: loss = 0.16013 (* 1 = 0.16013 loss)
I0122 16:23:48.294064 45517 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:23:49.157346 45517 solver.cpp:266] Iteration 8500 (115.842 iter/s, 0.863244s/100 iter), loss = 0.237808
I0122 16:23:49.157384 45517 solver.cpp:285]     Train net output #0: loss = 0.237808 (* 1 = 0.237808 loss)
I0122 16:23:49.157390 45517 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:23:50.023010 45517 solver.cpp:266] Iteration 8600 (115.529 iter/s, 0.865582s/100 iter), loss = 0.238057
I0122 16:23:50.023051 45517 solver.cpp:285]     Train net output #0: loss = 0.238057 (* 1 = 0.238057 loss)
I0122 16:23:50.023058 45517 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:23:50.887012 45517 solver.cpp:266] Iteration 8700 (115.752 iter/s, 0.863918s/100 iter), loss = 0.194287
I0122 16:23:50.887053 45517 solver.cpp:285]     Train net output #0: loss = 0.194287 (* 1 = 0.194287 loss)
I0122 16:23:50.887059 45517 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:23:51.750602 45517 solver.cpp:266] Iteration 8800 (115.807 iter/s, 0.863507s/100 iter), loss = 0.238649
I0122 16:23:51.750632 45517 solver.cpp:285]     Train net output #0: loss = 0.238649 (* 1 = 0.238649 loss)
I0122 16:23:51.750636 45517 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:23:52.614078 45517 solver.cpp:266] Iteration 8900 (115.821 iter/s, 0.863404s/100 iter), loss = 0.204637
I0122 16:23:52.614117 45517 solver.cpp:285]     Train net output #0: loss = 0.204637 (* 1 = 0.204637 loss)
I0122 16:23:52.614123 45517 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:23:53.472270 45517 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:23:53.692407 45517 solver.cpp:517]     Test net output #0: loss = 0.465098 (* 1 = 0.465098 loss)
I0122 16:23:53.692425 45517 solver.cpp:517]     Test net output #1: top-1 = 0.849111
I0122 16:23:53.692428 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991222
I0122 16:23:53.700510 45517 solver.cpp:266] Iteration 9000 (92.0517 iter/s, 1.08635s/100 iter), loss = 0.145027
I0122 16:23:53.700527 45517 solver.cpp:285]     Train net output #0: loss = 0.145027 (* 1 = 0.145027 loss)
I0122 16:23:53.700533 45517 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:23:54.569844 45517 solver.cpp:266] Iteration 9100 (115.039 iter/s, 0.869272s/100 iter), loss = 0.110316
I0122 16:23:54.569883 45517 solver.cpp:285]     Train net output #0: loss = 0.110316 (* 1 = 0.110316 loss)
I0122 16:23:54.569916 45517 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:23:55.433728 45517 solver.cpp:266] Iteration 9200 (115.766 iter/s, 0.863812s/100 iter), loss = 0.201813
I0122 16:23:55.433768 45517 solver.cpp:285]     Train net output #0: loss = 0.201813 (* 1 = 0.201813 loss)
I0122 16:23:55.433774 45517 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:23:56.298223 45517 solver.cpp:266] Iteration 9300 (115.686 iter/s, 0.864412s/100 iter), loss = 0.17105
I0122 16:23:56.298251 45517 solver.cpp:285]     Train net output #0: loss = 0.17105 (* 1 = 0.17105 loss)
I0122 16:23:56.298256 45517 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:23:57.162973 45517 solver.cpp:266] Iteration 9400 (115.65 iter/s, 0.86468s/100 iter), loss = 0.117638
I0122 16:23:57.163010 45517 solver.cpp:285]     Train net output #0: loss = 0.117638 (* 1 = 0.117638 loss)
I0122 16:23:57.163017 45517 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:23:58.035930 45517 solver.cpp:266] Iteration 9500 (114.564 iter/s, 0.872878s/100 iter), loss = 0.166336
I0122 16:23:58.035957 45517 solver.cpp:285]     Train net output #0: loss = 0.166336 (* 1 = 0.166336 loss)
I0122 16:23:58.035964 45517 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:23:58.900568 45517 solver.cpp:266] Iteration 9600 (115.665 iter/s, 0.864567s/100 iter), loss = 0.221949
I0122 16:23:58.900595 45517 solver.cpp:285]     Train net output #0: loss = 0.221949 (* 1 = 0.221949 loss)
I0122 16:23:58.900600 45517 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:23:59.770794 45517 solver.cpp:266] Iteration 9700 (114.922 iter/s, 0.870155s/100 iter), loss = 0.151686
I0122 16:23:59.770822 45517 solver.cpp:285]     Train net output #0: loss = 0.151686 (* 1 = 0.151686 loss)
I0122 16:23:59.770828 45517 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:24:00.637449 45517 solver.cpp:266] Iteration 9800 (115.396 iter/s, 0.866584s/100 iter), loss = 0.165693
I0122 16:24:00.637490 45517 solver.cpp:285]     Train net output #0: loss = 0.165693 (* 1 = 0.165693 loss)
I0122 16:24:00.637496 45517 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:24:01.499805 45517 solver.cpp:266] Iteration 9900 (115.973 iter/s, 0.862273s/100 iter), loss = 0.253357
I0122 16:24:01.499832 45517 solver.cpp:285]     Train net output #0: loss = 0.253357 (* 1 = 0.253357 loss)
I0122 16:24:01.499840 45517 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:24:02.379459 45517 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:24:02.603022 45517 solver.cpp:517]     Test net output #0: loss = 0.474728 (* 1 = 0.474728 loss)
I0122 16:24:02.603044 45517 solver.cpp:517]     Test net output #1: top-1 = 0.846333
I0122 16:24:02.603049 45517 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:24:02.611542 45517 solver.cpp:266] Iteration 10000 (89.9556 iter/s, 1.11166s/100 iter), loss = 0.254189
I0122 16:24:02.611560 45517 solver.cpp:285]     Train net output #0: loss = 0.254189 (* 1 = 0.254189 loss)
I0122 16:24:02.611567 45517 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:24:03.512585 45517 solver.cpp:266] Iteration 10100 (110.99 iter/s, 0.900979s/100 iter), loss = 0.172314
I0122 16:24:03.512744 45517 solver.cpp:285]     Train net output #0: loss = 0.172314 (* 1 = 0.172314 loss)
I0122 16:24:03.512751 45517 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:24:04.383059 45517 solver.cpp:266] Iteration 10200 (114.906 iter/s, 0.870275s/100 iter), loss = 0.18443
I0122 16:24:04.383098 45517 solver.cpp:285]     Train net output #0: loss = 0.18443 (* 1 = 0.18443 loss)
I0122 16:24:04.383105 45517 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:24:05.277006 45517 solver.cpp:266] Iteration 10300 (111.874 iter/s, 0.893863s/100 iter), loss = 0.157341
I0122 16:24:05.277046 45517 solver.cpp:285]     Train net output #0: loss = 0.157341 (* 1 = 0.157341 loss)
I0122 16:24:05.277058 45517 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:24:06.159319 45517 solver.cpp:266] Iteration 10400 (113.349 iter/s, 0.882229s/100 iter), loss = 0.27437
I0122 16:24:06.159348 45517 solver.cpp:285]     Train net output #0: loss = 0.27437 (* 1 = 0.27437 loss)
I0122 16:24:06.159353 45517 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:24:07.023006 45517 solver.cpp:266] Iteration 10500 (115.792 iter/s, 0.863615s/100 iter), loss = 0.130437
I0122 16:24:07.023047 45517 solver.cpp:285]     Train net output #0: loss = 0.130437 (* 1 = 0.130437 loss)
I0122 16:24:07.023053 45517 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:24:07.892809 45517 solver.cpp:266] Iteration 10600 (114.98 iter/s, 0.869719s/100 iter), loss = 0.232839
I0122 16:24:07.892838 45517 solver.cpp:285]     Train net output #0: loss = 0.232839 (* 1 = 0.232839 loss)
I0122 16:24:07.892843 45517 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:24:08.759260 45517 solver.cpp:266] Iteration 10700 (115.423 iter/s, 0.866378s/100 iter), loss = 0.14465
I0122 16:24:08.759299 45517 solver.cpp:285]     Train net output #0: loss = 0.14465 (* 1 = 0.14465 loss)
I0122 16:24:08.759306 45517 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:24:09.621484 45517 solver.cpp:266] Iteration 10800 (115.99 iter/s, 0.862141s/100 iter), loss = 0.193738
I0122 16:24:09.621523 45517 solver.cpp:285]     Train net output #0: loss = 0.193738 (* 1 = 0.193738 loss)
I0122 16:24:09.621529 45517 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:24:10.485585 45517 solver.cpp:266] Iteration 10900 (115.738 iter/s, 0.86402s/100 iter), loss = 0.205847
I0122 16:24:10.485615 45517 solver.cpp:285]     Train net output #0: loss = 0.205847 (* 1 = 0.205847 loss)
I0122 16:24:10.485620 45517 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:24:11.350600 45517 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:24:11.569653 45517 solver.cpp:517]     Test net output #0: loss = 0.509931 (* 1 = 0.509931 loss)
I0122 16:24:11.569669 45517 solver.cpp:517]     Test net output #1: top-1 = 0.838555
I0122 16:24:11.569672 45517 solver.cpp:517]     Test net output #2: top-5 = 0.990112
I0122 16:24:11.577749 45517 solver.cpp:266] Iteration 11000 (91.5679 iter/s, 1.09209s/100 iter), loss = 0.173878
I0122 16:24:11.577778 45517 solver.cpp:285]     Train net output #0: loss = 0.173878 (* 1 = 0.173878 loss)
I0122 16:24:11.577785 45517 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:24:12.440138 45517 solver.cpp:266] Iteration 11100 (115.965 iter/s, 0.862327s/100 iter), loss = 0.126047
I0122 16:24:12.440177 45517 solver.cpp:285]     Train net output #0: loss = 0.126047 (* 1 = 0.126047 loss)
I0122 16:24:12.440184 45517 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:24:13.315799 45517 solver.cpp:266] Iteration 11200 (114.21 iter/s, 0.875577s/100 iter), loss = 0.155685
I0122 16:24:13.315827 45517 solver.cpp:285]     Train net output #0: loss = 0.155685 (* 1 = 0.155685 loss)
I0122 16:24:13.315832 45517 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:24:14.192240 45517 solver.cpp:266] Iteration 11300 (114.107 iter/s, 0.87637s/100 iter), loss = 0.154962
I0122 16:24:14.192268 45517 solver.cpp:285]     Train net output #0: loss = 0.154962 (* 1 = 0.154962 loss)
I0122 16:24:14.192275 45517 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:24:15.074601 45517 solver.cpp:266] Iteration 11400 (113.342 iter/s, 0.882289s/100 iter), loss = 0.180759
I0122 16:24:15.074643 45517 solver.cpp:285]     Train net output #0: loss = 0.180759 (* 1 = 0.180759 loss)
I0122 16:24:15.074651 45517 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:24:15.950836 45517 solver.cpp:266] Iteration 11500 (114.136 iter/s, 0.87615s/100 iter), loss = 0.17506
I0122 16:24:15.950865 45517 solver.cpp:285]     Train net output #0: loss = 0.17506 (* 1 = 0.17506 loss)
I0122 16:24:15.950872 45517 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:24:16.820356 45517 solver.cpp:266] Iteration 11600 (115.016 iter/s, 0.869447s/100 iter), loss = 0.118816
I0122 16:24:16.820396 45517 solver.cpp:285]     Train net output #0: loss = 0.118815 (* 1 = 0.118815 loss)
I0122 16:24:16.820403 45517 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:24:17.694032 45517 solver.cpp:266] Iteration 11700 (114.47 iter/s, 0.873592s/100 iter), loss = 0.210145
I0122 16:24:17.694062 45517 solver.cpp:285]     Train net output #0: loss = 0.210145 (* 1 = 0.210145 loss)
I0122 16:24:17.694068 45517 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:24:18.560585 45517 solver.cpp:266] Iteration 11800 (115.41 iter/s, 0.866479s/100 iter), loss = 0.158618
I0122 16:24:18.560626 45517 solver.cpp:285]     Train net output #0: loss = 0.158618 (* 1 = 0.158618 loss)
I0122 16:24:18.560631 45517 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:24:19.430356 45517 solver.cpp:266] Iteration 11900 (114.984 iter/s, 0.869688s/100 iter), loss = 0.196092
I0122 16:24:19.430397 45517 solver.cpp:285]     Train net output #0: loss = 0.196092 (* 1 = 0.196092 loss)
I0122 16:24:19.430404 45517 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:24:20.288918 45517 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:24:20.510947 45517 solver.cpp:517]     Test net output #0: loss = 0.461779 (* 1 = 0.461779 loss)
I0122 16:24:20.510970 45517 solver.cpp:517]     Test net output #1: top-1 = 0.855666
I0122 16:24:20.510974 45517 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:24:20.519052 45517 solver.cpp:266] Iteration 12000 (91.8607 iter/s, 1.0886s/100 iter), loss = 0.161172
I0122 16:24:20.519068 45517 solver.cpp:285]     Train net output #0: loss = 0.161172 (* 1 = 0.161172 loss)
I0122 16:24:20.519074 45517 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:24:21.433398 45517 solver.cpp:266] Iteration 12100 (109.376 iter/s, 0.914281s/100 iter), loss = 0.139672
I0122 16:24:21.433434 45517 solver.cpp:285]     Train net output #0: loss = 0.139672 (* 1 = 0.139672 loss)
I0122 16:24:21.433440 45517 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:24:22.303493 45517 solver.cpp:266] Iteration 12200 (114.941 iter/s, 0.870014s/100 iter), loss = 0.267559
I0122 16:24:22.303524 45517 solver.cpp:285]     Train net output #0: loss = 0.267559 (* 1 = 0.267559 loss)
I0122 16:24:22.303537 45517 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:24:23.185886 45517 solver.cpp:266] Iteration 12300 (113.338 iter/s, 0.882318s/100 iter), loss = 0.188292
I0122 16:24:23.185928 45517 solver.cpp:285]     Train net output #0: loss = 0.188292 (* 1 = 0.188292 loss)
I0122 16:24:23.185935 45517 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:24:24.056692 45517 solver.cpp:266] Iteration 12400 (114.847 iter/s, 0.870721s/100 iter), loss = 0.16559
I0122 16:24:24.056722 45517 solver.cpp:285]     Train net output #0: loss = 0.16559 (* 1 = 0.16559 loss)
I0122 16:24:24.056728 45517 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:24:24.923573 45517 solver.cpp:266] Iteration 12500 (115.366 iter/s, 0.866808s/100 iter), loss = 0.162582
I0122 16:24:24.923614 45517 solver.cpp:285]     Train net output #0: loss = 0.162582 (* 1 = 0.162582 loss)
I0122 16:24:24.923621 45517 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:24:25.789229 45517 solver.cpp:266] Iteration 12600 (115.531 iter/s, 0.86557s/100 iter), loss = 0.195119
I0122 16:24:25.789259 45517 solver.cpp:285]     Train net output #0: loss = 0.195119 (* 1 = 0.195119 loss)
I0122 16:24:25.789295 45517 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:24:26.652581 45517 solver.cpp:266] Iteration 12700 (115.837 iter/s, 0.863279s/100 iter), loss = 0.23881
I0122 16:24:26.652611 45517 solver.cpp:285]     Train net output #0: loss = 0.23881 (* 1 = 0.23881 loss)
I0122 16:24:26.652617 45517 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:24:27.518230 45517 solver.cpp:266] Iteration 12800 (115.53 iter/s, 0.865576s/100 iter), loss = 0.134229
I0122 16:24:27.518259 45517 solver.cpp:285]     Train net output #0: loss = 0.134229 (* 1 = 0.134229 loss)
I0122 16:24:27.518265 45517 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:24:28.381585 45517 solver.cpp:266] Iteration 12900 (115.837 iter/s, 0.863283s/100 iter), loss = 0.184217
I0122 16:24:28.381613 45517 solver.cpp:285]     Train net output #0: loss = 0.184217 (* 1 = 0.184217 loss)
I0122 16:24:28.381618 45517 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:24:29.238063 45517 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:24:29.458863 45517 solver.cpp:517]     Test net output #0: loss = 0.444643 (* 1 = 0.444643 loss)
I0122 16:24:29.458878 45517 solver.cpp:517]     Test net output #1: top-1 = 0.858334
I0122 16:24:29.458884 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991445
I0122 16:24:29.466951 45517 solver.cpp:266] Iteration 13000 (92.1413 iter/s, 1.08529s/100 iter), loss = 0.135965
I0122 16:24:29.466969 45517 solver.cpp:285]     Train net output #0: loss = 0.135965 (* 1 = 0.135965 loss)
I0122 16:24:29.466975 45517 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:24:30.329535 45517 solver.cpp:266] Iteration 13100 (115.939 iter/s, 0.862523s/100 iter), loss = 0.196511
I0122 16:24:30.329572 45517 solver.cpp:285]     Train net output #0: loss = 0.196511 (* 1 = 0.196511 loss)
I0122 16:24:30.329581 45517 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:24:31.196696 45517 solver.cpp:266] Iteration 13200 (115.33 iter/s, 0.867079s/100 iter), loss = 0.2342
I0122 16:24:31.196733 45517 solver.cpp:285]     Train net output #0: loss = 0.2342 (* 1 = 0.2342 loss)
I0122 16:24:31.196739 45517 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:24:32.061723 45517 solver.cpp:266] Iteration 13300 (115.614 iter/s, 0.864948s/100 iter), loss = 0.191841
I0122 16:24:32.061763 45517 solver.cpp:285]     Train net output #0: loss = 0.191841 (* 1 = 0.191841 loss)
I0122 16:24:32.061769 45517 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:24:32.926427 45517 solver.cpp:266] Iteration 13400 (115.658 iter/s, 0.864621s/100 iter), loss = 0.16098
I0122 16:24:32.926466 45517 solver.cpp:285]     Train net output #0: loss = 0.16098 (* 1 = 0.16098 loss)
I0122 16:24:32.926473 45517 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:24:33.791311 45517 solver.cpp:266] Iteration 13500 (115.634 iter/s, 0.864801s/100 iter), loss = 0.118013
I0122 16:24:33.791461 45517 solver.cpp:285]     Train net output #0: loss = 0.118013 (* 1 = 0.118013 loss)
I0122 16:24:33.791469 45517 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:24:34.655652 45517 solver.cpp:266] Iteration 13600 (115.721 iter/s, 0.864149s/100 iter), loss = 0.125074
I0122 16:24:34.655691 45517 solver.cpp:285]     Train net output #0: loss = 0.125074 (* 1 = 0.125074 loss)
I0122 16:24:34.655697 45517 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:24:35.519626 45517 solver.cpp:266] Iteration 13700 (115.755 iter/s, 0.863892s/100 iter), loss = 0.136709
I0122 16:24:35.519654 45517 solver.cpp:285]     Train net output #0: loss = 0.136709 (* 1 = 0.136709 loss)
I0122 16:24:35.519659 45517 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:24:36.382812 45517 solver.cpp:266] Iteration 13800 (115.859 iter/s, 0.863115s/100 iter), loss = 0.169637
I0122 16:24:36.382841 45517 solver.cpp:285]     Train net output #0: loss = 0.169637 (* 1 = 0.169637 loss)
I0122 16:24:36.382848 45517 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:24:37.247884 45517 solver.cpp:266] Iteration 13900 (115.607 iter/s, 0.864997s/100 iter), loss = 0.178444
I0122 16:24:37.247921 45517 solver.cpp:285]     Train net output #0: loss = 0.178444 (* 1 = 0.178444 loss)
I0122 16:24:37.247928 45517 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:24:38.102241 45517 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:24:38.322293 45517 solver.cpp:517]     Test net output #0: loss = 0.464436 (* 1 = 0.464436 loss)
I0122 16:24:38.322309 45517 solver.cpp:517]     Test net output #1: top-1 = 0.856777
I0122 16:24:38.322314 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0122 16:24:38.330504 45517 solver.cpp:266] Iteration 14000 (92.3759 iter/s, 1.08253s/100 iter), loss = 0.18353
I0122 16:24:38.330523 45517 solver.cpp:285]     Train net output #0: loss = 0.18353 (* 1 = 0.18353 loss)
I0122 16:24:38.330528 45517 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:24:39.194213 45517 solver.cpp:266] Iteration 14100 (115.788 iter/s, 0.863648s/100 iter), loss = 0.105242
I0122 16:24:39.194241 45517 solver.cpp:285]     Train net output #0: loss = 0.105242 (* 1 = 0.105242 loss)
I0122 16:24:39.194247 45517 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:24:40.059535 45517 solver.cpp:266] Iteration 14200 (115.573 iter/s, 0.865251s/100 iter), loss = 0.172097
I0122 16:24:40.059576 45517 solver.cpp:285]     Train net output #0: loss = 0.172097 (* 1 = 0.172097 loss)
I0122 16:24:40.059581 45517 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:24:40.924002 45517 solver.cpp:266] Iteration 14300 (115.689 iter/s, 0.864384s/100 iter), loss = 0.217074
I0122 16:24:40.924041 45517 solver.cpp:285]     Train net output #0: loss = 0.217074 (* 1 = 0.217074 loss)
I0122 16:24:40.924048 45517 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:24:41.787070 45517 solver.cpp:266] Iteration 14400 (115.877 iter/s, 0.862986s/100 iter), loss = 0.145226
I0122 16:24:41.787098 45517 solver.cpp:285]     Train net output #0: loss = 0.145226 (* 1 = 0.145226 loss)
I0122 16:24:41.787103 45517 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:24:42.652331 45517 solver.cpp:266] Iteration 14500 (115.582 iter/s, 0.865188s/100 iter), loss = 0.106063
I0122 16:24:42.652360 45517 solver.cpp:285]     Train net output #0: loss = 0.106063 (* 1 = 0.106063 loss)
I0122 16:24:42.652366 45517 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:24:43.518223 45517 solver.cpp:266] Iteration 14600 (115.498 iter/s, 0.865818s/100 iter), loss = 0.138898
I0122 16:24:43.518252 45517 solver.cpp:285]     Train net output #0: loss = 0.138898 (* 1 = 0.138898 loss)
I0122 16:24:43.518257 45517 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:24:44.380847 45517 solver.cpp:266] Iteration 14700 (115.935 iter/s, 0.862551s/100 iter), loss = 0.102674
I0122 16:24:44.380875 45517 solver.cpp:285]     Train net output #0: loss = 0.102674 (* 1 = 0.102674 loss)
I0122 16:24:44.380882 45517 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:24:45.245301 45517 solver.cpp:266] Iteration 14800 (115.69 iter/s, 0.864381s/100 iter), loss = 0.221146
I0122 16:24:45.245328 45517 solver.cpp:285]     Train net output #0: loss = 0.221146 (* 1 = 0.221146 loss)
I0122 16:24:45.245334 45517 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:24:46.110236 45517 solver.cpp:266] Iteration 14900 (115.625 iter/s, 0.864863s/100 iter), loss = 0.116642
I0122 16:24:46.110263 45517 solver.cpp:285]     Train net output #0: loss = 0.116642 (* 1 = 0.116642 loss)
I0122 16:24:46.110270 45517 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:24:46.965266 45517 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:24:47.186223 45517 solver.cpp:517]     Test net output #0: loss = 0.459709 (* 1 = 0.459709 loss)
I0122 16:24:47.186239 45517 solver.cpp:517]     Test net output #1: top-1 = 0.857111
I0122 16:24:47.186244 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0122 16:24:47.194340 45517 solver.cpp:266] Iteration 15000 (92.2486 iter/s, 1.08403s/100 iter), loss = 0.164886
I0122 16:24:47.194360 45517 solver.cpp:285]     Train net output #0: loss = 0.164886 (* 1 = 0.164886 loss)
I0122 16:24:47.194365 45517 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:24:48.058004 45517 solver.cpp:266] Iteration 15100 (115.794 iter/s, 0.863601s/100 iter), loss = 0.130746
I0122 16:24:48.058032 45517 solver.cpp:285]     Train net output #0: loss = 0.130746 (* 1 = 0.130746 loss)
I0122 16:24:48.058038 45517 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:24:48.961928 45517 solver.cpp:266] Iteration 15200 (110.638 iter/s, 0.90385s/100 iter), loss = 0.280447
I0122 16:24:48.961969 45517 solver.cpp:285]     Train net output #0: loss = 0.280447 (* 1 = 0.280447 loss)
I0122 16:24:48.961975 45517 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:24:49.885857 45517 solver.cpp:266] Iteration 15300 (108.244 iter/s, 0.923842s/100 iter), loss = 0.161301
I0122 16:24:49.885896 45517 solver.cpp:285]     Train net output #0: loss = 0.161301 (* 1 = 0.161301 loss)
I0122 16:24:49.885902 45517 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:24:50.761729 45517 solver.cpp:266] Iteration 15400 (114.183 iter/s, 0.875789s/100 iter), loss = 0.1282
I0122 16:24:50.761759 45517 solver.cpp:285]     Train net output #0: loss = 0.1282 (* 1 = 0.1282 loss)
I0122 16:24:50.761765 45517 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:24:51.629678 45517 solver.cpp:266] Iteration 15500 (115.224 iter/s, 0.867876s/100 iter), loss = 0.12876
I0122 16:24:51.629707 45517 solver.cpp:285]     Train net output #0: loss = 0.12876 (* 1 = 0.12876 loss)
I0122 16:24:51.629714 45517 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:24:52.497360 45517 solver.cpp:266] Iteration 15600 (115.259 iter/s, 0.867608s/100 iter), loss = 0.137151
I0122 16:24:52.497401 45517 solver.cpp:285]     Train net output #0: loss = 0.137151 (* 1 = 0.137151 loss)
I0122 16:24:52.497407 45517 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:24:53.361109 45517 solver.cpp:266] Iteration 15700 (115.786 iter/s, 0.863664s/100 iter), loss = 0.133905
I0122 16:24:53.361136 45517 solver.cpp:285]     Train net output #0: loss = 0.133905 (* 1 = 0.133905 loss)
I0122 16:24:53.361141 45517 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:24:54.228672 45517 solver.cpp:266] Iteration 15800 (115.275 iter/s, 0.86749s/100 iter), loss = 0.0911221
I0122 16:24:54.228700 45517 solver.cpp:285]     Train net output #0: loss = 0.0911221 (* 1 = 0.0911221 loss)
I0122 16:24:54.228705 45517 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:24:55.096400 45517 solver.cpp:266] Iteration 15900 (115.253 iter/s, 0.867654s/100 iter), loss = 0.157951
I0122 16:24:55.096428 45517 solver.cpp:285]     Train net output #0: loss = 0.157951 (* 1 = 0.157951 loss)
I0122 16:24:55.096436 45517 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:24:55.953483 45517 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:24:56.173324 45517 solver.cpp:517]     Test net output #0: loss = 0.430487 (* 1 = 0.430487 loss)
I0122 16:24:56.173357 45517 solver.cpp:517]     Test net output #1: top-1 = 0.862667
I0122 16:24:56.173367 45517 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0122 16:24:56.181454 45517 solver.cpp:266] Iteration 16000 (92.1679 iter/s, 1.08498s/100 iter), loss = 0.160622
I0122 16:24:56.181473 45517 solver.cpp:285]     Train net output #0: loss = 0.160622 (* 1 = 0.160622 loss)
I0122 16:24:56.181478 45517 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:24:57.051048 45517 solver.cpp:266] Iteration 16100 (115.005 iter/s, 0.869529s/100 iter), loss = 0.177982
I0122 16:24:57.051075 45517 solver.cpp:285]     Train net output #0: loss = 0.177982 (* 1 = 0.177982 loss)
I0122 16:24:57.051081 45517 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:24:57.918267 45517 solver.cpp:266] Iteration 16200 (115.32 iter/s, 0.867149s/100 iter), loss = 0.0985254
I0122 16:24:57.918298 45517 solver.cpp:285]     Train net output #0: loss = 0.0985253 (* 1 = 0.0985253 loss)
I0122 16:24:57.918303 45517 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:24:58.785069 45517 solver.cpp:266] Iteration 16300 (115.376 iter/s, 0.866728s/100 iter), loss = 0.151371
I0122 16:24:58.785109 45517 solver.cpp:285]     Train net output #0: loss = 0.151371 (* 1 = 0.151371 loss)
I0122 16:24:58.785116 45517 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:24:59.651952 45517 solver.cpp:266] Iteration 16400 (115.367 iter/s, 0.866799s/100 iter), loss = 0.159248
I0122 16:24:59.651979 45517 solver.cpp:285]     Train net output #0: loss = 0.159248 (* 1 = 0.159248 loss)
I0122 16:24:59.651984 45517 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:25:00.525523 45517 solver.cpp:266] Iteration 16500 (114.482 iter/s, 0.873501s/100 iter), loss = 0.109443
I0122 16:25:00.525563 45517 solver.cpp:285]     Train net output #0: loss = 0.109443 (* 1 = 0.109443 loss)
I0122 16:25:00.525569 45517 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:25:01.390190 45517 solver.cpp:266] Iteration 16600 (115.663 iter/s, 0.864584s/100 iter), loss = 0.128937
I0122 16:25:01.390219 45517 solver.cpp:285]     Train net output #0: loss = 0.128937 (* 1 = 0.128937 loss)
I0122 16:25:01.390225 45517 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:25:02.260728 45517 solver.cpp:266] Iteration 16700 (114.881 iter/s, 0.870463s/100 iter), loss = 0.183348
I0122 16:25:02.260756 45517 solver.cpp:285]     Train net output #0: loss = 0.183348 (* 1 = 0.183348 loss)
I0122 16:25:02.260762 45517 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:25:03.127135 45517 solver.cpp:266] Iteration 16800 (115.429 iter/s, 0.866335s/100 iter), loss = 0.104462
I0122 16:25:03.127164 45517 solver.cpp:285]     Train net output #0: loss = 0.104462 (* 1 = 0.104462 loss)
I0122 16:25:03.127171 45517 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:25:03.993294 45517 solver.cpp:266] Iteration 16900 (115.462 iter/s, 0.866086s/100 iter), loss = 0.211674
I0122 16:25:03.993434 45517 solver.cpp:285]     Train net output #0: loss = 0.211674 (* 1 = 0.211674 loss)
I0122 16:25:03.993443 45517 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:25:04.854789 45517 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:25:05.075289 45517 solver.cpp:517]     Test net output #0: loss = 0.425041 (* 1 = 0.425041 loss)
I0122 16:25:05.075304 45517 solver.cpp:517]     Test net output #1: top-1 = 0.864
I0122 16:25:05.075309 45517 solver.cpp:517]     Test net output #2: top-5 = 0.993667
I0122 16:25:05.083391 45517 solver.cpp:266] Iteration 17000 (91.7508 iter/s, 1.08991s/100 iter), loss = 0.211295
I0122 16:25:05.083420 45517 solver.cpp:285]     Train net output #0: loss = 0.211295 (* 1 = 0.211295 loss)
I0122 16:25:05.083426 45517 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:25:05.951191 45517 solver.cpp:266] Iteration 17100 (115.242 iter/s, 0.867736s/100 iter), loss = 0.161323
I0122 16:25:05.951220 45517 solver.cpp:285]     Train net output #0: loss = 0.161323 (* 1 = 0.161323 loss)
I0122 16:25:05.951226 45517 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:25:06.818791 45517 solver.cpp:266] Iteration 17200 (115.27 iter/s, 0.867528s/100 iter), loss = 0.0938245
I0122 16:25:06.818819 45517 solver.cpp:285]     Train net output #0: loss = 0.0938245 (* 1 = 0.0938245 loss)
I0122 16:25:06.818825 45517 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:25:07.684046 45517 solver.cpp:266] Iteration 17300 (115.583 iter/s, 0.865182s/100 iter), loss = 0.161303
I0122 16:25:07.684074 45517 solver.cpp:285]     Train net output #0: loss = 0.161303 (* 1 = 0.161303 loss)
I0122 16:25:07.684080 45517 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:25:08.552304 45517 solver.cpp:266] Iteration 17400 (115.183 iter/s, 0.868184s/100 iter), loss = 0.140173
I0122 16:25:08.552343 45517 solver.cpp:285]     Train net output #0: loss = 0.140173 (* 1 = 0.140173 loss)
I0122 16:25:08.552350 45517 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:25:09.421722 45517 solver.cpp:266] Iteration 17500 (115.03 iter/s, 0.869335s/100 iter), loss = 0.107701
I0122 16:25:09.421751 45517 solver.cpp:285]     Train net output #0: loss = 0.107701 (* 1 = 0.107701 loss)
I0122 16:25:09.421757 45517 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:25:10.286275 45517 solver.cpp:266] Iteration 17600 (115.676 iter/s, 0.86448s/100 iter), loss = 0.062394
I0122 16:25:10.286304 45517 solver.cpp:285]     Train net output #0: loss = 0.062394 (* 1 = 0.062394 loss)
I0122 16:25:10.286310 45517 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:25:11.190865 45517 solver.cpp:266] Iteration 17700 (110.556 iter/s, 0.904515s/100 iter), loss = 0.152518
I0122 16:25:11.190894 45517 solver.cpp:285]     Train net output #0: loss = 0.152518 (* 1 = 0.152518 loss)
I0122 16:25:11.190901 45517 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:25:12.085425 45517 solver.cpp:266] Iteration 17800 (111.796 iter/s, 0.894486s/100 iter), loss = 0.156237
I0122 16:25:12.085454 45517 solver.cpp:285]     Train net output #0: loss = 0.156237 (* 1 = 0.156237 loss)
I0122 16:25:12.085459 45517 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:25:12.960628 45517 solver.cpp:266] Iteration 17900 (114.269 iter/s, 0.87513s/100 iter), loss = 0.118915
I0122 16:25:12.960657 45517 solver.cpp:285]     Train net output #0: loss = 0.118915 (* 1 = 0.118915 loss)
I0122 16:25:12.960664 45517 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:25:13.820570 45517 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:25:14.045657 45517 solver.cpp:517]     Test net output #0: loss = 0.430168 (* 1 = 0.430168 loss)
I0122 16:25:14.045691 45517 solver.cpp:517]     Test net output #1: top-1 = 0.864222
I0122 16:25:14.045696 45517 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:25:14.054102 45517 solver.cpp:266] Iteration 18000 (91.4583 iter/s, 1.09339s/100 iter), loss = 0.10415
I0122 16:25:14.054129 45517 solver.cpp:285]     Train net output #0: loss = 0.10415 (* 1 = 0.10415 loss)
I0122 16:25:14.054137 45517 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:25:14.921659 45517 solver.cpp:266] Iteration 18100 (115.274 iter/s, 0.867495s/100 iter), loss = 0.124588
I0122 16:25:14.921687 45517 solver.cpp:285]     Train net output #0: loss = 0.124588 (* 1 = 0.124588 loss)
I0122 16:25:14.921694 45517 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:25:15.787142 45517 solver.cpp:266] Iteration 18200 (115.552 iter/s, 0.865411s/100 iter), loss = 0.0978456
I0122 16:25:15.787170 45517 solver.cpp:285]     Train net output #0: loss = 0.0978456 (* 1 = 0.0978456 loss)
I0122 16:25:15.787176 45517 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:25:16.661993 45517 solver.cpp:266] Iteration 18300 (114.315 iter/s, 0.874778s/100 iter), loss = 0.1399
I0122 16:25:16.662020 45517 solver.cpp:285]     Train net output #0: loss = 0.1399 (* 1 = 0.1399 loss)
I0122 16:25:16.662027 45517 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:25:17.526185 45517 solver.cpp:266] Iteration 18400 (115.725 iter/s, 0.864121s/100 iter), loss = 0.105536
I0122 16:25:17.526212 45517 solver.cpp:285]     Train net output #0: loss = 0.105536 (* 1 = 0.105536 loss)
I0122 16:25:17.526218 45517 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:25:18.392920 45517 solver.cpp:266] Iteration 18500 (115.385 iter/s, 0.866662s/100 iter), loss = 0.10198
I0122 16:25:18.392947 45517 solver.cpp:285]     Train net output #0: loss = 0.10198 (* 1 = 0.10198 loss)
I0122 16:25:18.392953 45517 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:25:19.262459 45517 solver.cpp:266] Iteration 18600 (115.013 iter/s, 0.869467s/100 iter), loss = 0.0986458
I0122 16:25:19.262500 45517 solver.cpp:285]     Train net output #0: loss = 0.0986459 (* 1 = 0.0986459 loss)
I0122 16:25:19.262506 45517 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:25:20.139621 45517 solver.cpp:266] Iteration 18700 (114.015 iter/s, 0.877074s/100 iter), loss = 0.106006
I0122 16:25:20.139649 45517 solver.cpp:285]     Train net output #0: loss = 0.106006 (* 1 = 0.106006 loss)
I0122 16:25:20.139657 45517 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:25:21.029546 45517 solver.cpp:266] Iteration 18800 (112.378 iter/s, 0.889851s/100 iter), loss = 0.0751613
I0122 16:25:21.029585 45517 solver.cpp:285]     Train net output #0: loss = 0.0751614 (* 1 = 0.0751614 loss)
I0122 16:25:21.029592 45517 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:25:21.916260 45517 solver.cpp:266] Iteration 18900 (112.787 iter/s, 0.886629s/100 iter), loss = 0.0837442
I0122 16:25:21.916288 45517 solver.cpp:285]     Train net output #0: loss = 0.0837442 (* 1 = 0.0837442 loss)
I0122 16:25:21.916294 45517 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:25:22.799901 45517 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:25:23.026551 45517 solver.cpp:517]     Test net output #0: loss = 0.422382 (* 1 = 0.422382 loss)
I0122 16:25:23.026566 45517 solver.cpp:517]     Test net output #1: top-1 = 0.866222
I0122 16:25:23.026571 45517 solver.cpp:517]     Test net output #2: top-5 = 0.993111
I0122 16:25:23.034976 45517 solver.cpp:266] Iteration 19000 (89.3944 iter/s, 1.11864s/100 iter), loss = 0.153649
I0122 16:25:23.034994 45517 solver.cpp:285]     Train net output #0: loss = 0.153649 (* 1 = 0.153649 loss)
I0122 16:25:23.034999 45517 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:25:23.924475 45517 solver.cpp:266] Iteration 19100 (112.431 iter/s, 0.889435s/100 iter), loss = 0.0763902
I0122 16:25:23.924504 45517 solver.cpp:285]     Train net output #0: loss = 0.0763902 (* 1 = 0.0763902 loss)
I0122 16:25:23.924510 45517 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:25:24.809191 45517 solver.cpp:266] Iteration 19200 (113.04 iter/s, 0.884643s/100 iter), loss = 0.143802
I0122 16:25:24.809219 45517 solver.cpp:285]     Train net output #0: loss = 0.143802 (* 1 = 0.143802 loss)
I0122 16:25:24.809226 45517 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:25:25.711758 45517 solver.cpp:266] Iteration 19300 (110.804 iter/s, 0.902492s/100 iter), loss = 0.0878341
I0122 16:25:25.711814 45517 solver.cpp:285]     Train net output #0: loss = 0.0878341 (* 1 = 0.0878341 loss)
I0122 16:25:25.711820 45517 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:25:26.601140 45517 solver.cpp:266] Iteration 19400 (112.45 iter/s, 0.889281s/100 iter), loss = 0.0734475
I0122 16:25:26.601171 45517 solver.cpp:285]     Train net output #0: loss = 0.0734475 (* 1 = 0.0734475 loss)
I0122 16:25:26.601176 45517 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:25:27.491192 45517 solver.cpp:266] Iteration 19500 (112.362 iter/s, 0.889977s/100 iter), loss = 0.0768548
I0122 16:25:27.491221 45517 solver.cpp:285]     Train net output #0: loss = 0.0768548 (* 1 = 0.0768548 loss)
I0122 16:25:27.491245 45517 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:25:28.373134 45517 solver.cpp:266] Iteration 19600 (113.396 iter/s, 0.881868s/100 iter), loss = 0.110095
I0122 16:25:28.373162 45517 solver.cpp:285]     Train net output #0: loss = 0.110095 (* 1 = 0.110095 loss)
I0122 16:25:28.373168 45517 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:25:29.258415 45517 solver.cpp:266] Iteration 19700 (112.968 iter/s, 0.885206s/100 iter), loss = 0.0949747
I0122 16:25:29.258455 45517 solver.cpp:285]     Train net output #0: loss = 0.0949747 (* 1 = 0.0949747 loss)
I0122 16:25:29.258461 45517 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:25:30.142711 45517 solver.cpp:266] Iteration 19800 (113.095 iter/s, 0.884211s/100 iter), loss = 0.0849639
I0122 16:25:30.142742 45517 solver.cpp:285]     Train net output #0: loss = 0.0849639 (* 1 = 0.0849639 loss)
I0122 16:25:30.142748 45517 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:25:31.028897 45517 solver.cpp:266] Iteration 19900 (112.853 iter/s, 0.886108s/100 iter), loss = 0.131902
I0122 16:25:31.028925 45517 solver.cpp:285]     Train net output #0: loss = 0.131902 (* 1 = 0.131902 loss)
I0122 16:25:31.028931 45517 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:25:31.910689 45517 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/_iter_20000.caffemodel
I0122 16:25:31.982439 45517 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0/snapshots/_iter_20000.solverstate
I0122 16:25:31.994808 45517 solver.cpp:378] Iteration 20000, loss = 0.0216124
I0122 16:25:31.994832 45517 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:25:32.219501 45517 solver.cpp:517]     Test net output #0: loss = 0.418945 (* 1 = 0.418945 loss)
I0122 16:25:32.219517 45517 solver.cpp:517]     Test net output #1: top-1 = 0.868111
I0122 16:25:32.219521 45517 solver.cpp:517]     Test net output #2: top-5 = 0.993222
I0122 16:25:32.219527 45517 solver.cpp:386] Optimization Done (112.617 iter/s).
I0122 16:25:32.219532 45517 caffe_interface.cpp:530] Optimization Done.


# compression: first run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config1.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress1_miniVggNet.txt
I0122 16:25:32.758579 45797 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:25:32.807334 45797 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:25:34.217092 45797 pruning_runner.cpp:264] Compression complete 1.17777%
I0122 16:25:34.859011 45797 pruning_runner.cpp:264] Compression complete 2.32813%
I0122 16:25:35.491183 45797 pruning_runner.cpp:264] Compression complete 4.55032%
I0122 16:25:36.122323 45797 pruning_runner.cpp:264] Compression complete 8.70455%
I0122 16:25:36.751960 45797 pruning_runner.cpp:264] Compression complete 16.0151%
I0122 16:25:37.374542 45797 pruning_runner.cpp:264] Compression complete 27.6086%
I0122 16:25:37.996568 45797 pruning_runner.cpp:264] Compression complete 60.4042%
I0122 16:25:38.623749 45797 pruning_runner.cpp:264] Compression complete 75.315%
I0122 16:25:39.256398 45797 pruning_runner.cpp:264] Compression complete 96.9144%
I0122 16:25:39.895278 45797 pruning_runner.cpp:264] Compression complete 99.2103%
I0122 16:25:40.542317 45797 pruning_runner.cpp:264] Compression complete 99.6052%
I0122 16:25:41.184013 45797 pruning_runner.cpp:264] Compression complete 99.9505%
I0122 16:25:41.826220 45797 pruning_runner.cpp:264] Compression complete 99.9876%
I0122 16:25:42.466776 45797 pruning_runner.cpp:264] Compression complete 99.9938%
I0122 16:25:43.101270 45797 pruning_runner.cpp:264] Compression complete 99.9985%
I0122 16:25:43.741681 45797 pruning_runner.cpp:264] Compression complete 99.9998%
I0122 16:25:44.384842 45797 pruning_runner.cpp:264] Compression complete 99.9999%
I0122 16:25:45.009599 45797 pruning_runner.cpp:264] Compression complete 100%
I0122 16:25:45.643887 45797 pruning_runner.cpp:264] Compression complete 100%
I0122 16:25:46.287605 45797 pruning_runner.cpp:264] Compression complete 100%
I0122 16:25:46.916663 45797 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:25:46.917073 45797 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:25:46.917655 45797 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:25:46.917987 45797 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:25:46.918174 45797 layer_factory.hpp:77] Creating layer data
I0122 16:25:46.918238 45797 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:46.918764 45797 net.cpp:94] Creating Layer data
I0122 16:25:46.918776 45797 net.cpp:409] data -> data
I0122 16:25:46.918789 45797 net.cpp:409] data -> label
I0122 16:25:46.919700 47724 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:25:46.919734 47724 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:25:46.919823 45797 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:25:46.919955 45797 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:25:46.923185 45797 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:46.923230 45797 net.cpp:144] Setting up data
I0122 16:25:46.923238 45797 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:25:46.923245 45797 net.cpp:151] Top shape: 50 (50)
I0122 16:25:46.923249 45797 net.cpp:159] Memory required for data: 614600
I0122 16:25:46.923254 45797 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:25:46.923262 45797 net.cpp:94] Creating Layer label_data_1_split
I0122 16:25:46.923266 45797 net.cpp:435] label_data_1_split <- label
I0122 16:25:46.923274 45797 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:25:46.923286 45797 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:25:46.923293 45797 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:25:46.923372 45797 net.cpp:144] Setting up label_data_1_split
I0122 16:25:46.923380 45797 net.cpp:151] Top shape: 50 (50)
I0122 16:25:46.923385 45797 net.cpp:151] Top shape: 50 (50)
I0122 16:25:46.923388 45797 net.cpp:151] Top shape: 50 (50)
I0122 16:25:46.923391 45797 net.cpp:159] Memory required for data: 615200
I0122 16:25:46.923395 45797 layer_factory.hpp:77] Creating layer conv1
I0122 16:25:46.923406 45797 net.cpp:94] Creating Layer conv1
I0122 16:25:46.923413 45797 net.cpp:435] conv1 <- data
I0122 16:25:46.923418 45797 net.cpp:409] conv1 -> conv1
I0122 16:25:46.924492 45797 net.cpp:144] Setting up conv1
I0122 16:25:46.924504 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.924509 45797 net.cpp:159] Memory required for data: 7168800
I0122 16:25:46.924520 45797 layer_factory.hpp:77] Creating layer bn1
I0122 16:25:46.924527 45797 net.cpp:94] Creating Layer bn1
I0122 16:25:46.924532 45797 net.cpp:435] bn1 <- conv1
I0122 16:25:46.924540 45797 net.cpp:409] bn1 -> scale1
I0122 16:25:46.925226 45797 net.cpp:144] Setting up bn1
I0122 16:25:46.925235 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.925240 45797 net.cpp:159] Memory required for data: 13722400
I0122 16:25:46.925251 45797 layer_factory.hpp:77] Creating layer relu1
I0122 16:25:46.925261 45797 net.cpp:94] Creating Layer relu1
I0122 16:25:46.925264 45797 net.cpp:435] relu1 <- scale1
I0122 16:25:46.925271 45797 net.cpp:409] relu1 -> relu1
I0122 16:25:46.925290 45797 net.cpp:144] Setting up relu1
I0122 16:25:46.925297 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.925299 45797 net.cpp:159] Memory required for data: 20276000
I0122 16:25:46.925303 45797 layer_factory.hpp:77] Creating layer conv2
I0122 16:25:46.925312 45797 net.cpp:94] Creating Layer conv2
I0122 16:25:46.925318 45797 net.cpp:435] conv2 <- relu1
I0122 16:25:46.925323 45797 net.cpp:409] conv2 -> conv2
I0122 16:25:46.926512 45797 net.cpp:144] Setting up conv2
I0122 16:25:46.926525 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.926529 45797 net.cpp:159] Memory required for data: 26829600
I0122 16:25:46.926539 45797 layer_factory.hpp:77] Creating layer bn2
I0122 16:25:46.926548 45797 net.cpp:94] Creating Layer bn2
I0122 16:25:46.926553 45797 net.cpp:435] bn2 <- conv2
I0122 16:25:46.926559 45797 net.cpp:409] bn2 -> scale2
I0122 16:25:46.927399 45797 net.cpp:144] Setting up bn2
I0122 16:25:46.927408 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.927412 45797 net.cpp:159] Memory required for data: 33383200
I0122 16:25:46.927422 45797 layer_factory.hpp:77] Creating layer relu2
I0122 16:25:46.927443 45797 net.cpp:94] Creating Layer relu2
I0122 16:25:46.927448 45797 net.cpp:435] relu2 <- scale2
I0122 16:25:46.927453 45797 net.cpp:409] relu2 -> relu2
I0122 16:25:46.927475 45797 net.cpp:144] Setting up relu2
I0122 16:25:46.927482 45797 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:46.927486 45797 net.cpp:159] Memory required for data: 39936800
I0122 16:25:46.927489 45797 layer_factory.hpp:77] Creating layer pool1
I0122 16:25:46.927496 45797 net.cpp:94] Creating Layer pool1
I0122 16:25:46.927506 45797 net.cpp:435] pool1 <- relu2
I0122 16:25:46.927512 45797 net.cpp:409] pool1 -> pool1
I0122 16:25:46.927544 45797 net.cpp:144] Setting up pool1
I0122 16:25:46.927548 45797 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:25:46.927552 45797 net.cpp:159] Memory required for data: 41575200
I0122 16:25:46.927556 45797 layer_factory.hpp:77] Creating layer drop1
I0122 16:25:46.927561 45797 net.cpp:94] Creating Layer drop1
I0122 16:25:46.927567 45797 net.cpp:435] drop1 <- pool1
I0122 16:25:46.927572 45797 net.cpp:409] drop1 -> drop1
I0122 16:25:46.927601 45797 net.cpp:144] Setting up drop1
I0122 16:25:46.927609 45797 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:25:46.927614 45797 net.cpp:159] Memory required for data: 43213600
I0122 16:25:46.927618 45797 layer_factory.hpp:77] Creating layer conv3
I0122 16:25:46.927626 45797 net.cpp:94] Creating Layer conv3
I0122 16:25:46.927629 45797 net.cpp:435] conv3 <- drop1
I0122 16:25:46.927635 45797 net.cpp:409] conv3 -> conv3
I0122 16:25:46.928789 45797 net.cpp:144] Setting up conv3
I0122 16:25:46.928803 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.928807 45797 net.cpp:159] Memory required for data: 46490400
I0122 16:25:46.928815 45797 layer_factory.hpp:77] Creating layer bn3
I0122 16:25:46.928823 45797 net.cpp:94] Creating Layer bn3
I0122 16:25:46.928831 45797 net.cpp:435] bn3 <- conv3
I0122 16:25:46.928838 45797 net.cpp:409] bn3 -> scale3
I0122 16:25:46.929594 45797 net.cpp:144] Setting up bn3
I0122 16:25:46.929600 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.929605 45797 net.cpp:159] Memory required for data: 49767200
I0122 16:25:46.929617 45797 layer_factory.hpp:77] Creating layer relu3
I0122 16:25:46.929625 45797 net.cpp:94] Creating Layer relu3
I0122 16:25:46.929630 45797 net.cpp:435] relu3 <- scale3
I0122 16:25:46.929636 45797 net.cpp:409] relu3 -> relu3
I0122 16:25:46.929656 45797 net.cpp:144] Setting up relu3
I0122 16:25:46.929661 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.929666 45797 net.cpp:159] Memory required for data: 53044000
I0122 16:25:46.929671 45797 layer_factory.hpp:77] Creating layer conv4
I0122 16:25:46.929679 45797 net.cpp:94] Creating Layer conv4
I0122 16:25:46.929684 45797 net.cpp:435] conv4 <- relu3
I0122 16:25:46.929690 45797 net.cpp:409] conv4 -> conv4
I0122 16:25:46.930192 45797 net.cpp:144] Setting up conv4
I0122 16:25:46.930200 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.930203 45797 net.cpp:159] Memory required for data: 56320800
I0122 16:25:46.930208 45797 layer_factory.hpp:77] Creating layer bn4
I0122 16:25:46.930215 45797 net.cpp:94] Creating Layer bn4
I0122 16:25:46.930219 45797 net.cpp:435] bn4 <- conv4
I0122 16:25:46.930227 45797 net.cpp:409] bn4 -> scale4
I0122 16:25:46.930864 45797 net.cpp:144] Setting up bn4
I0122 16:25:46.930872 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.930876 45797 net.cpp:159] Memory required for data: 59597600
I0122 16:25:46.930883 45797 layer_factory.hpp:77] Creating layer relu4
I0122 16:25:46.930887 45797 net.cpp:94] Creating Layer relu4
I0122 16:25:46.930891 45797 net.cpp:435] relu4 <- scale4
I0122 16:25:46.930896 45797 net.cpp:409] relu4 -> relu4
I0122 16:25:46.930912 45797 net.cpp:144] Setting up relu4
I0122 16:25:46.930918 45797 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:46.930922 45797 net.cpp:159] Memory required for data: 62874400
I0122 16:25:46.930924 45797 layer_factory.hpp:77] Creating layer pool2
I0122 16:25:46.930930 45797 net.cpp:94] Creating Layer pool2
I0122 16:25:46.930948 45797 net.cpp:435] pool2 <- relu4
I0122 16:25:46.930953 45797 net.cpp:409] pool2 -> pool2
I0122 16:25:46.931040 45797 net.cpp:144] Setting up pool2
I0122 16:25:46.931046 45797 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:25:46.931048 45797 net.cpp:159] Memory required for data: 63693600
I0122 16:25:46.931051 45797 layer_factory.hpp:77] Creating layer drop2
I0122 16:25:46.931056 45797 net.cpp:94] Creating Layer drop2
I0122 16:25:46.931061 45797 net.cpp:435] drop2 <- pool2
I0122 16:25:46.931066 45797 net.cpp:409] drop2 -> drop2
I0122 16:25:46.931092 45797 net.cpp:144] Setting up drop2
I0122 16:25:46.931099 45797 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:25:46.931102 45797 net.cpp:159] Memory required for data: 64512800
I0122 16:25:46.931104 45797 layer_factory.hpp:77] Creating layer fc1
I0122 16:25:46.931111 45797 net.cpp:94] Creating Layer fc1
I0122 16:25:46.931115 45797 net.cpp:435] fc1 <- drop2
I0122 16:25:46.931120 45797 net.cpp:409] fc1 -> fc1
I0122 16:25:46.945384 45797 net.cpp:144] Setting up fc1
I0122 16:25:46.945402 45797 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:46.945405 45797 net.cpp:159] Memory required for data: 64615200
I0122 16:25:46.945411 45797 layer_factory.hpp:77] Creating layer bn5
I0122 16:25:46.945420 45797 net.cpp:94] Creating Layer bn5
I0122 16:25:46.945425 45797 net.cpp:435] bn5 <- fc1
I0122 16:25:46.945430 45797 net.cpp:409] bn5 -> scale5
I0122 16:25:46.945991 45797 net.cpp:144] Setting up bn5
I0122 16:25:46.945997 45797 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:46.946002 45797 net.cpp:159] Memory required for data: 64717600
I0122 16:25:46.946013 45797 layer_factory.hpp:77] Creating layer relu5
I0122 16:25:46.946020 45797 net.cpp:94] Creating Layer relu5
I0122 16:25:46.946023 45797 net.cpp:435] relu5 <- scale5
I0122 16:25:46.946028 45797 net.cpp:409] relu5 -> relu5
I0122 16:25:46.946048 45797 net.cpp:144] Setting up relu5
I0122 16:25:46.946053 45797 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:46.946055 45797 net.cpp:159] Memory required for data: 64820000
I0122 16:25:46.946058 45797 layer_factory.hpp:77] Creating layer drop3
I0122 16:25:46.946063 45797 net.cpp:94] Creating Layer drop3
I0122 16:25:46.946066 45797 net.cpp:435] drop3 <- relu5
I0122 16:25:46.946071 45797 net.cpp:409] drop3 -> drop3
I0122 16:25:46.946096 45797 net.cpp:144] Setting up drop3
I0122 16:25:46.946101 45797 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:46.946105 45797 net.cpp:159] Memory required for data: 64922400
I0122 16:25:46.946106 45797 layer_factory.hpp:77] Creating layer fc2
I0122 16:25:46.946113 45797 net.cpp:94] Creating Layer fc2
I0122 16:25:46.946116 45797 net.cpp:435] fc2 <- drop3
I0122 16:25:46.946121 45797 net.cpp:409] fc2 -> fc2
I0122 16:25:46.946249 45797 net.cpp:144] Setting up fc2
I0122 16:25:46.946254 45797 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:46.946257 45797 net.cpp:159] Memory required for data: 64924400
I0122 16:25:46.946261 45797 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:25:46.946267 45797 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:25:46.946272 45797 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:25:46.946277 45797 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:25:46.946285 45797 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:25:46.946291 45797 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:25:46.946327 45797 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:25:46.946333 45797 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:46.946336 45797 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:46.946339 45797 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:46.946342 45797 net.cpp:159] Memory required for data: 64930400
I0122 16:25:46.946346 45797 layer_factory.hpp:77] Creating layer loss
I0122 16:25:46.946350 45797 net.cpp:94] Creating Layer loss
I0122 16:25:46.946357 45797 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:25:46.946360 45797 net.cpp:435] loss <- label_data_1_split_0
I0122 16:25:46.946365 45797 net.cpp:409] loss -> loss
I0122 16:25:46.946386 45797 layer_factory.hpp:77] Creating layer loss
I0122 16:25:46.946451 45797 net.cpp:144] Setting up loss
I0122 16:25:46.946457 45797 net.cpp:151] Top shape: (1)
I0122 16:25:46.946460 45797 net.cpp:154]     with loss weight 1
I0122 16:25:46.946470 45797 net.cpp:159] Memory required for data: 64930404
I0122 16:25:46.946472 45797 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:25:46.946478 45797 net.cpp:94] Creating Layer accuracy-top1
I0122 16:25:46.946481 45797 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:25:46.946485 45797 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:25:46.946491 45797 net.cpp:409] accuracy-top1 -> top-1
I0122 16:25:46.946498 45797 net.cpp:144] Setting up accuracy-top1
I0122 16:25:46.946503 45797 net.cpp:151] Top shape: (1)
I0122 16:25:46.946506 45797 net.cpp:159] Memory required for data: 64930408
I0122 16:25:46.946508 45797 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:25:46.946514 45797 net.cpp:94] Creating Layer accuracy-top5
I0122 16:25:46.946516 45797 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:25:46.946521 45797 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:25:46.946524 45797 net.cpp:409] accuracy-top5 -> top-5
I0122 16:25:46.946532 45797 net.cpp:144] Setting up accuracy-top5
I0122 16:25:46.946535 45797 net.cpp:151] Top shape: (1)
I0122 16:25:46.946538 45797 net.cpp:159] Memory required for data: 64930412
I0122 16:25:46.946542 45797 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:25:46.946545 45797 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:25:46.946549 45797 net.cpp:220] loss needs backward computation.
I0122 16:25:46.946552 45797 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:25:46.946555 45797 net.cpp:220] fc2 needs backward computation.
I0122 16:25:46.946559 45797 net.cpp:220] drop3 needs backward computation.
I0122 16:25:46.946563 45797 net.cpp:220] relu5 needs backward computation.
I0122 16:25:46.946565 45797 net.cpp:220] bn5 needs backward computation.
I0122 16:25:46.946568 45797 net.cpp:220] fc1 needs backward computation.
I0122 16:25:46.946573 45797 net.cpp:220] drop2 needs backward computation.
I0122 16:25:46.946575 45797 net.cpp:220] pool2 needs backward computation.
I0122 16:25:46.946578 45797 net.cpp:220] relu4 needs backward computation.
I0122 16:25:46.946583 45797 net.cpp:220] bn4 needs backward computation.
I0122 16:25:46.946586 45797 net.cpp:220] conv4 needs backward computation.
I0122 16:25:46.946589 45797 net.cpp:220] relu3 needs backward computation.
I0122 16:25:46.946593 45797 net.cpp:220] bn3 needs backward computation.
I0122 16:25:46.946595 45797 net.cpp:220] conv3 needs backward computation.
I0122 16:25:46.946599 45797 net.cpp:220] drop1 needs backward computation.
I0122 16:25:46.946601 45797 net.cpp:220] pool1 needs backward computation.
I0122 16:25:46.946604 45797 net.cpp:220] relu2 needs backward computation.
I0122 16:25:46.946607 45797 net.cpp:220] bn2 needs backward computation.
I0122 16:25:46.946611 45797 net.cpp:220] conv2 needs backward computation.
I0122 16:25:46.946614 45797 net.cpp:220] relu1 needs backward computation.
I0122 16:25:46.946617 45797 net.cpp:220] bn1 needs backward computation.
I0122 16:25:46.946621 45797 net.cpp:220] conv1 needs backward computation.
I0122 16:25:46.946625 45797 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:25:46.946630 45797 net.cpp:222] data does not need backward computation.
I0122 16:25:46.946631 45797 net.cpp:264] This network produces output loss
I0122 16:25:46.946635 45797 net.cpp:264] This network produces output top-1
I0122 16:25:46.946640 45797 net.cpp:264] This network produces output top-5
I0122 16:25:46.946661 45797 net.cpp:284] Network initialization done.
I0122 16:25:46.949615 45797 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:25:46.956295 45797 caffe_interface.cpp:125] Batch 0, loss = 0.313754
I0122 16:25:46.956315 45797 caffe_interface.cpp:125] Batch 0, top-1 = 0.86
I0122 16:25:46.956320 45797 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:25:46.957648 45797 caffe_interface.cpp:125] Batch 1, loss = 0.587936
I0122 16:25:46.957657 45797 caffe_interface.cpp:125] Batch 1, top-1 = 0.8
I0122 16:25:46.957661 45797 caffe_interface.cpp:125] Batch 1, top-5 = 0.98
I0122 16:25:46.958946 45797 caffe_interface.cpp:125] Batch 2, loss = 0.392139
I0122 16:25:46.958954 45797 caffe_interface.cpp:125] Batch 2, top-1 = 0.9
I0122 16:25:46.958958 45797 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:25:46.960252 45797 caffe_interface.cpp:125] Batch 3, loss = 0.777406
I0122 16:25:46.960259 45797 caffe_interface.cpp:125] Batch 3, top-1 = 0.76
I0122 16:25:46.960263 45797 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0122 16:25:46.961540 45797 caffe_interface.cpp:125] Batch 4, loss = 0.421464
I0122 16:25:46.961549 45797 caffe_interface.cpp:125] Batch 4, top-1 = 0.86
I0122 16:25:46.961552 45797 caffe_interface.cpp:125] Batch 4, top-5 = 0.96
I0122 16:25:46.962832 45797 caffe_interface.cpp:125] Batch 5, loss = 0.415211
I0122 16:25:46.962841 45797 caffe_interface.cpp:125] Batch 5, top-1 = 0.86
I0122 16:25:46.962844 45797 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0122 16:25:46.964130 45797 caffe_interface.cpp:125] Batch 6, loss = 0.579565
I0122 16:25:46.964138 45797 caffe_interface.cpp:125] Batch 6, top-1 = 0.8
I0122 16:25:46.964141 45797 caffe_interface.cpp:125] Batch 6, top-5 = 1
I0122 16:25:46.965440 45797 caffe_interface.cpp:125] Batch 7, loss = 0.211779
I0122 16:25:46.965448 45797 caffe_interface.cpp:125] Batch 7, top-1 = 0.92
I0122 16:25:46.965451 45797 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0122 16:25:46.966725 45797 caffe_interface.cpp:125] Batch 8, loss = 0.448774
I0122 16:25:46.966734 45797 caffe_interface.cpp:125] Batch 8, top-1 = 0.88
I0122 16:25:46.966737 45797 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:25:46.968009 45797 caffe_interface.cpp:125] Batch 9, loss = 0.576042
I0122 16:25:46.968016 45797 caffe_interface.cpp:125] Batch 9, top-1 = 0.84
I0122 16:25:46.968020 45797 caffe_interface.cpp:125] Batch 9, top-5 = 0.98
I0122 16:25:46.969308 45797 caffe_interface.cpp:125] Batch 10, loss = 0.429093
I0122 16:25:46.969316 45797 caffe_interface.cpp:125] Batch 10, top-1 = 0.88
I0122 16:25:46.969318 45797 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:25:46.970613 45797 caffe_interface.cpp:125] Batch 11, loss = 0.554713
I0122 16:25:46.970623 45797 caffe_interface.cpp:125] Batch 11, top-1 = 0.86
I0122 16:25:46.970626 45797 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:25:46.971904 45797 caffe_interface.cpp:125] Batch 12, loss = 0.655029
I0122 16:25:46.971913 45797 caffe_interface.cpp:125] Batch 12, top-1 = 0.8
I0122 16:25:46.971917 45797 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:25:46.973217 45797 caffe_interface.cpp:125] Batch 13, loss = 0.65815
I0122 16:25:46.973225 45797 caffe_interface.cpp:125] Batch 13, top-1 = 0.84
I0122 16:25:46.973229 45797 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0122 16:25:46.974510 45797 caffe_interface.cpp:125] Batch 14, loss = 0.354363
I0122 16:25:46.974519 45797 caffe_interface.cpp:125] Batch 14, top-1 = 0.94
I0122 16:25:46.974521 45797 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0122 16:25:46.975787 45797 caffe_interface.cpp:125] Batch 15, loss = 0.311378
I0122 16:25:46.975795 45797 caffe_interface.cpp:125] Batch 15, top-1 = 0.86
I0122 16:25:46.975798 45797 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:25:46.977072 45797 caffe_interface.cpp:125] Batch 16, loss = 0.381973
I0122 16:25:46.977079 45797 caffe_interface.cpp:125] Batch 16, top-1 = 0.9
I0122 16:25:46.977083 45797 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0122 16:25:46.978361 45797 caffe_interface.cpp:125] Batch 17, loss = 0.350592
I0122 16:25:46.978369 45797 caffe_interface.cpp:125] Batch 17, top-1 = 0.82
I0122 16:25:46.978372 45797 caffe_interface.cpp:125] Batch 17, top-5 = 0.98
I0122 16:25:46.979660 45797 caffe_interface.cpp:125] Batch 18, loss = 0.586732
I0122 16:25:46.979668 45797 caffe_interface.cpp:125] Batch 18, top-1 = 0.82
I0122 16:25:46.979671 45797 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0122 16:25:46.981856 45797 caffe_interface.cpp:125] Batch 19, loss = 0.423492
I0122 16:25:46.981868 45797 caffe_interface.cpp:125] Batch 19, top-1 = 0.84
I0122 16:25:46.981871 45797 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:25:46.983273 45797 caffe_interface.cpp:125] Batch 20, loss = 0.360648
I0122 16:25:46.983281 45797 caffe_interface.cpp:125] Batch 20, top-1 = 0.9
I0122 16:25:46.983283 45797 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0122 16:25:46.984753 45797 caffe_interface.cpp:125] Batch 21, loss = 0.690883
I0122 16:25:46.984760 45797 caffe_interface.cpp:125] Batch 21, top-1 = 0.82
I0122 16:25:46.984763 45797 caffe_interface.cpp:125] Batch 21, top-5 = 0.96
I0122 16:25:46.986035 45797 caffe_interface.cpp:125] Batch 22, loss = 0.421758
I0122 16:25:46.986043 45797 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0122 16:25:46.986045 45797 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0122 16:25:46.987421 45797 caffe_interface.cpp:125] Batch 23, loss = 0.292322
I0122 16:25:46.987427 45797 caffe_interface.cpp:125] Batch 23, top-1 = 0.88
I0122 16:25:46.987430 45797 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:25:46.988685 45797 caffe_interface.cpp:125] Batch 24, loss = 0.397498
I0122 16:25:46.988693 45797 caffe_interface.cpp:125] Batch 24, top-1 = 0.84
I0122 16:25:46.988695 45797 caffe_interface.cpp:125] Batch 24, top-5 = 1
I0122 16:25:46.989945 45797 caffe_interface.cpp:125] Batch 25, loss = 0.365664
I0122 16:25:46.989953 45797 caffe_interface.cpp:125] Batch 25, top-1 = 0.9
I0122 16:25:46.989956 45797 caffe_interface.cpp:125] Batch 25, top-5 = 0.96
I0122 16:25:46.991206 45797 caffe_interface.cpp:125] Batch 26, loss = 0.373659
I0122 16:25:46.991212 45797 caffe_interface.cpp:125] Batch 26, top-1 = 0.86
I0122 16:25:46.991215 45797 caffe_interface.cpp:125] Batch 26, top-5 = 1
I0122 16:25:46.992466 45797 caffe_interface.cpp:125] Batch 27, loss = 0.35941
I0122 16:25:46.992472 45797 caffe_interface.cpp:125] Batch 27, top-1 = 0.9
I0122 16:25:46.992475 45797 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:25:46.993739 45797 caffe_interface.cpp:125] Batch 28, loss = 0.571065
I0122 16:25:46.993746 45797 caffe_interface.cpp:125] Batch 28, top-1 = 0.8
I0122 16:25:46.993749 45797 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:25:46.995018 45797 caffe_interface.cpp:125] Batch 29, loss = 0.26214
I0122 16:25:46.995024 45797 caffe_interface.cpp:125] Batch 29, top-1 = 0.92
I0122 16:25:46.995028 45797 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:25:46.996299 45797 caffe_interface.cpp:125] Batch 30, loss = 0.303751
I0122 16:25:46.996305 45797 caffe_interface.cpp:125] Batch 30, top-1 = 0.92
I0122 16:25:46.996309 45797 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:25:46.997575 45797 caffe_interface.cpp:125] Batch 31, loss = 0.477578
I0122 16:25:46.997581 45797 caffe_interface.cpp:125] Batch 31, top-1 = 0.86
I0122 16:25:46.997584 45797 caffe_interface.cpp:125] Batch 31, top-5 = 0.98
I0122 16:25:46.998857 45797 caffe_interface.cpp:125] Batch 32, loss = 0.501736
I0122 16:25:46.998865 45797 caffe_interface.cpp:125] Batch 32, top-1 = 0.84
I0122 16:25:46.998868 45797 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:25:47.000142 45797 caffe_interface.cpp:125] Batch 33, loss = 0.875903
I0122 16:25:47.000150 45797 caffe_interface.cpp:125] Batch 33, top-1 = 0.76
I0122 16:25:47.000154 45797 caffe_interface.cpp:125] Batch 33, top-5 = 1
I0122 16:25:47.001413 45797 caffe_interface.cpp:125] Batch 34, loss = 0.451763
I0122 16:25:47.001421 45797 caffe_interface.cpp:125] Batch 34, top-1 = 0.86
I0122 16:25:47.001425 45797 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0122 16:25:47.002708 45797 caffe_interface.cpp:125] Batch 35, loss = 0.608332
I0122 16:25:47.002717 45797 caffe_interface.cpp:125] Batch 35, top-1 = 0.84
I0122 16:25:47.002719 45797 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0122 16:25:47.003995 45797 caffe_interface.cpp:125] Batch 36, loss = 0.362321
I0122 16:25:47.004004 45797 caffe_interface.cpp:125] Batch 36, top-1 = 0.86
I0122 16:25:47.004006 45797 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:25:47.005309 45797 caffe_interface.cpp:125] Batch 37, loss = 0.549192
I0122 16:25:47.005317 45797 caffe_interface.cpp:125] Batch 37, top-1 = 0.84
I0122 16:25:47.005321 45797 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0122 16:25:47.006592 45797 caffe_interface.cpp:125] Batch 38, loss = 0.759624
I0122 16:25:47.006598 45797 caffe_interface.cpp:125] Batch 38, top-1 = 0.82
I0122 16:25:47.006603 45797 caffe_interface.cpp:125] Batch 38, top-5 = 0.94
I0122 16:25:47.007869 45797 caffe_interface.cpp:125] Batch 39, loss = 0.271415
I0122 16:25:47.007875 45797 caffe_interface.cpp:125] Batch 39, top-1 = 0.9
I0122 16:25:47.007879 45797 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:25:47.009147 45797 caffe_interface.cpp:125] Batch 40, loss = 0.259506
I0122 16:25:47.009155 45797 caffe_interface.cpp:125] Batch 40, top-1 = 0.92
I0122 16:25:47.009160 45797 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:25:47.010429 45797 caffe_interface.cpp:125] Batch 41, loss = 0.950388
I0122 16:25:47.010437 45797 caffe_interface.cpp:125] Batch 41, top-1 = 0.76
I0122 16:25:47.010442 45797 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0122 16:25:47.011713 45797 caffe_interface.cpp:125] Batch 42, loss = 0.795366
I0122 16:25:47.011719 45797 caffe_interface.cpp:125] Batch 42, top-1 = 0.76
I0122 16:25:47.011723 45797 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0122 16:25:47.013005 45797 caffe_interface.cpp:125] Batch 43, loss = 0.438692
I0122 16:25:47.013012 45797 caffe_interface.cpp:125] Batch 43, top-1 = 0.82
I0122 16:25:47.013016 45797 caffe_interface.cpp:125] Batch 43, top-5 = 1
I0122 16:25:47.015180 45797 caffe_interface.cpp:125] Batch 44, loss = 0.466585
I0122 16:25:47.015187 45797 caffe_interface.cpp:125] Batch 44, top-1 = 0.86
I0122 16:25:47.015190 45797 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0122 16:25:47.016587 45797 caffe_interface.cpp:125] Batch 45, loss = 0.419213
I0122 16:25:47.016593 45797 caffe_interface.cpp:125] Batch 45, top-1 = 0.84
I0122 16:25:47.016597 45797 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0122 16:25:47.018075 45797 caffe_interface.cpp:125] Batch 46, loss = 0.242522
I0122 16:25:47.018083 45797 caffe_interface.cpp:125] Batch 46, top-1 = 0.9
I0122 16:25:47.018087 45797 caffe_interface.cpp:125] Batch 46, top-5 = 1
I0122 16:25:47.019371 45797 caffe_interface.cpp:125] Batch 47, loss = 0.31664
I0122 16:25:47.019378 45797 caffe_interface.cpp:125] Batch 47, top-1 = 0.88
I0122 16:25:47.019382 45797 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0122 16:25:47.020674 45797 caffe_interface.cpp:125] Batch 48, loss = 0.432896
I0122 16:25:47.020689 45797 caffe_interface.cpp:125] Batch 48, top-1 = 0.8
I0122 16:25:47.020694 45797 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:25:47.022032 45797 caffe_interface.cpp:125] Batch 49, loss = 0.519704
I0122 16:25:47.022040 45797 caffe_interface.cpp:125] Batch 49, top-1 = 0.86
I0122 16:25:47.022044 45797 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0122 16:25:47.023319 45797 caffe_interface.cpp:125] Batch 50, loss = 0.414017
I0122 16:25:47.023326 45797 caffe_interface.cpp:125] Batch 50, top-1 = 0.86
I0122 16:25:47.023330 45797 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:25:47.024608 45797 caffe_interface.cpp:125] Batch 51, loss = 0.254622
I0122 16:25:47.024616 45797 caffe_interface.cpp:125] Batch 51, top-1 = 0.86
I0122 16:25:47.024621 45797 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0122 16:25:47.025882 45797 caffe_interface.cpp:125] Batch 52, loss = 0.533816
I0122 16:25:47.025890 45797 caffe_interface.cpp:125] Batch 52, top-1 = 0.82
I0122 16:25:47.025893 45797 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0122 16:25:47.027173 45797 caffe_interface.cpp:125] Batch 53, loss = 0.555105
I0122 16:25:47.027180 45797 caffe_interface.cpp:125] Batch 53, top-1 = 0.78
I0122 16:25:47.027184 45797 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0122 16:25:47.028448 45797 caffe_interface.cpp:125] Batch 54, loss = 0.517237
I0122 16:25:47.028455 45797 caffe_interface.cpp:125] Batch 54, top-1 = 0.84
I0122 16:25:47.028468 45797 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0122 16:25:47.029752 45797 caffe_interface.cpp:125] Batch 55, loss = 0.219571
I0122 16:25:47.029758 45797 caffe_interface.cpp:125] Batch 55, top-1 = 0.94
I0122 16:25:47.029762 45797 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:25:47.031036 45797 caffe_interface.cpp:125] Batch 56, loss = 0.517309
I0122 16:25:47.031044 45797 caffe_interface.cpp:125] Batch 56, top-1 = 0.84
I0122 16:25:47.031049 45797 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0122 16:25:47.032322 45797 caffe_interface.cpp:125] Batch 57, loss = 0.132031
I0122 16:25:47.032330 45797 caffe_interface.cpp:125] Batch 57, top-1 = 0.96
I0122 16:25:47.032335 45797 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:25:47.033601 45797 caffe_interface.cpp:125] Batch 58, loss = 0.530458
I0122 16:25:47.033607 45797 caffe_interface.cpp:125] Batch 58, top-1 = 0.86
I0122 16:25:47.033610 45797 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0122 16:25:47.034898 45797 caffe_interface.cpp:125] Batch 59, loss = 0.353707
I0122 16:25:47.034905 45797 caffe_interface.cpp:125] Batch 59, top-1 = 0.82
I0122 16:25:47.034909 45797 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:25:47.036183 45797 caffe_interface.cpp:125] Batch 60, loss = 0.435075
I0122 16:25:47.036191 45797 caffe_interface.cpp:125] Batch 60, top-1 = 0.88
I0122 16:25:47.036195 45797 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0122 16:25:47.037474 45797 caffe_interface.cpp:125] Batch 61, loss = 0.956268
I0122 16:25:47.037482 45797 caffe_interface.cpp:125] Batch 61, top-1 = 0.74
I0122 16:25:47.037485 45797 caffe_interface.cpp:125] Batch 61, top-5 = 0.94
I0122 16:25:47.038779 45797 caffe_interface.cpp:125] Batch 62, loss = 0.602046
I0122 16:25:47.038795 45797 caffe_interface.cpp:125] Batch 62, top-1 = 0.8
I0122 16:25:47.038800 45797 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:25:47.040078 45797 caffe_interface.cpp:125] Batch 63, loss = 0.248844
I0122 16:25:47.040086 45797 caffe_interface.cpp:125] Batch 63, top-1 = 0.9
I0122 16:25:47.040089 45797 caffe_interface.cpp:125] Batch 63, top-5 = 1
I0122 16:25:47.041348 45797 caffe_interface.cpp:125] Batch 64, loss = 0.40914
I0122 16:25:47.041357 45797 caffe_interface.cpp:125] Batch 64, top-1 = 0.82
I0122 16:25:47.041360 45797 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:25:47.042657 45797 caffe_interface.cpp:125] Batch 65, loss = 0.396647
I0122 16:25:47.042668 45797 caffe_interface.cpp:125] Batch 65, top-1 = 0.84
I0122 16:25:47.042672 45797 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:25:47.043962 45797 caffe_interface.cpp:125] Batch 66, loss = 0.310206
I0122 16:25:47.043969 45797 caffe_interface.cpp:125] Batch 66, top-1 = 0.88
I0122 16:25:47.043973 45797 caffe_interface.cpp:125] Batch 66, top-5 = 1
I0122 16:25:47.045240 45797 caffe_interface.cpp:125] Batch 67, loss = 0.633637
I0122 16:25:47.045248 45797 caffe_interface.cpp:125] Batch 67, top-1 = 0.86
I0122 16:25:47.045251 45797 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0122 16:25:47.046536 45797 caffe_interface.cpp:125] Batch 68, loss = 0.278748
I0122 16:25:47.046545 45797 caffe_interface.cpp:125] Batch 68, top-1 = 0.94
I0122 16:25:47.046548 45797 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:25:47.048683 45797 caffe_interface.cpp:125] Batch 69, loss = 0.329926
I0122 16:25:47.048689 45797 caffe_interface.cpp:125] Batch 69, top-1 = 0.86
I0122 16:25:47.048693 45797 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0122 16:25:47.049989 45797 caffe_interface.cpp:125] Batch 70, loss = 0.346356
I0122 16:25:47.049996 45797 caffe_interface.cpp:125] Batch 70, top-1 = 0.86
I0122 16:25:47.049998 45797 caffe_interface.cpp:125] Batch 70, top-5 = 0.98
I0122 16:25:47.051324 45797 caffe_interface.cpp:125] Batch 71, loss = 0.479645
I0122 16:25:47.051331 45797 caffe_interface.cpp:125] Batch 71, top-1 = 0.84
I0122 16:25:47.051333 45797 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:25:47.052521 45797 caffe_interface.cpp:125] Batch 72, loss = 0.535139
I0122 16:25:47.052528 45797 caffe_interface.cpp:125] Batch 72, top-1 = 0.78
I0122 16:25:47.052541 45797 caffe_interface.cpp:125] Batch 72, top-5 = 0.98
I0122 16:25:47.053730 45797 caffe_interface.cpp:125] Batch 73, loss = 0.612623
I0122 16:25:47.053736 45797 caffe_interface.cpp:125] Batch 73, top-1 = 0.8
I0122 16:25:47.053740 45797 caffe_interface.cpp:125] Batch 73, top-5 = 0.96
I0122 16:25:47.055181 45797 caffe_interface.cpp:125] Batch 74, loss = 0.861687
I0122 16:25:47.055188 45797 caffe_interface.cpp:125] Batch 74, top-1 = 0.82
I0122 16:25:47.055192 45797 caffe_interface.cpp:125] Batch 74, top-5 = 0.94
I0122 16:25:47.056380 45797 caffe_interface.cpp:125] Batch 75, loss = 0.492238
I0122 16:25:47.056387 45797 caffe_interface.cpp:125] Batch 75, top-1 = 0.9
I0122 16:25:47.056390 45797 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0122 16:25:47.057600 45797 caffe_interface.cpp:125] Batch 76, loss = 0.562829
I0122 16:25:47.057607 45797 caffe_interface.cpp:125] Batch 76, top-1 = 0.86
I0122 16:25:47.057611 45797 caffe_interface.cpp:125] Batch 76, top-5 = 0.96
I0122 16:25:47.058816 45797 caffe_interface.cpp:125] Batch 77, loss = 0.351327
I0122 16:25:47.058823 45797 caffe_interface.cpp:125] Batch 77, top-1 = 0.86
I0122 16:25:47.058827 45797 caffe_interface.cpp:125] Batch 77, top-5 = 0.96
I0122 16:25:47.060032 45797 caffe_interface.cpp:125] Batch 78, loss = 0.434425
I0122 16:25:47.060039 45797 caffe_interface.cpp:125] Batch 78, top-1 = 0.86
I0122 16:25:47.060042 45797 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0122 16:25:47.061262 45797 caffe_interface.cpp:125] Batch 79, loss = 0.502003
I0122 16:25:47.061280 45797 caffe_interface.cpp:125] Batch 79, top-1 = 0.88
I0122 16:25:47.061282 45797 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0122 16:25:47.062497 45797 caffe_interface.cpp:125] Batch 80, loss = 0.192822
I0122 16:25:47.062505 45797 caffe_interface.cpp:125] Batch 80, top-1 = 0.92
I0122 16:25:47.062508 45797 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:25:47.063727 45797 caffe_interface.cpp:125] Batch 81, loss = 0.531217
I0122 16:25:47.063735 45797 caffe_interface.cpp:125] Batch 81, top-1 = 0.82
I0122 16:25:47.063738 45797 caffe_interface.cpp:125] Batch 81, top-5 = 1
I0122 16:25:47.064936 45797 caffe_interface.cpp:125] Batch 82, loss = 0.385169
I0122 16:25:47.064944 45797 caffe_interface.cpp:125] Batch 82, top-1 = 0.86
I0122 16:25:47.064947 45797 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0122 16:25:47.066155 45797 caffe_interface.cpp:125] Batch 83, loss = 0.513403
I0122 16:25:47.066162 45797 caffe_interface.cpp:125] Batch 83, top-1 = 0.88
I0122 16:25:47.066166 45797 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:25:47.067375 45797 caffe_interface.cpp:125] Batch 84, loss = 0.628607
I0122 16:25:47.067382 45797 caffe_interface.cpp:125] Batch 84, top-1 = 0.8
I0122 16:25:47.067386 45797 caffe_interface.cpp:125] Batch 84, top-5 = 0.98
I0122 16:25:47.068593 45797 caffe_interface.cpp:125] Batch 85, loss = 0.471664
I0122 16:25:47.068599 45797 caffe_interface.cpp:125] Batch 85, top-1 = 0.82
I0122 16:25:47.068603 45797 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:25:47.069823 45797 caffe_interface.cpp:125] Batch 86, loss = 0.459144
I0122 16:25:47.069830 45797 caffe_interface.cpp:125] Batch 86, top-1 = 0.86
I0122 16:25:47.069834 45797 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:25:47.071048 45797 caffe_interface.cpp:125] Batch 87, loss = 0.354043
I0122 16:25:47.071054 45797 caffe_interface.cpp:125] Batch 87, top-1 = 0.86
I0122 16:25:47.071058 45797 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0122 16:25:47.072276 45797 caffe_interface.cpp:125] Batch 88, loss = 0.657153
I0122 16:25:47.072283 45797 caffe_interface.cpp:125] Batch 88, top-1 = 0.8
I0122 16:25:47.072288 45797 caffe_interface.cpp:125] Batch 88, top-5 = 0.96
I0122 16:25:47.073483 45797 caffe_interface.cpp:125] Batch 89, loss = 0.23398
I0122 16:25:47.073491 45797 caffe_interface.cpp:125] Batch 89, top-1 = 0.94
I0122 16:25:47.073494 45797 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0122 16:25:47.074719 45797 caffe_interface.cpp:125] Batch 90, loss = 0.468467
I0122 16:25:47.074728 45797 caffe_interface.cpp:125] Batch 90, top-1 = 0.86
I0122 16:25:47.074740 45797 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0122 16:25:47.075958 45797 caffe_interface.cpp:125] Batch 91, loss = 0.466391
I0122 16:25:47.075966 45797 caffe_interface.cpp:125] Batch 91, top-1 = 0.82
I0122 16:25:47.075970 45797 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0122 16:25:47.077183 45797 caffe_interface.cpp:125] Batch 92, loss = 0.592972
I0122 16:25:47.077191 45797 caffe_interface.cpp:125] Batch 92, top-1 = 0.84
I0122 16:25:47.077195 45797 caffe_interface.cpp:125] Batch 92, top-5 = 0.98
I0122 16:25:47.078426 45797 caffe_interface.cpp:125] Batch 93, loss = 0.491524
I0122 16:25:47.078434 45797 caffe_interface.cpp:125] Batch 93, top-1 = 0.86
I0122 16:25:47.078438 45797 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0122 16:25:47.079641 45797 caffe_interface.cpp:125] Batch 94, loss = 0.672357
I0122 16:25:47.079648 45797 caffe_interface.cpp:125] Batch 94, top-1 = 0.76
I0122 16:25:47.079653 45797 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0122 16:25:47.080857 45797 caffe_interface.cpp:125] Batch 95, loss = 0.453226
I0122 16:25:47.080863 45797 caffe_interface.cpp:125] Batch 95, top-1 = 0.78
I0122 16:25:47.080866 45797 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0122 16:25:47.082072 45797 caffe_interface.cpp:125] Batch 96, loss = 0.53567
I0122 16:25:47.082079 45797 caffe_interface.cpp:125] Batch 96, top-1 = 0.82
I0122 16:25:47.082082 45797 caffe_interface.cpp:125] Batch 96, top-5 = 0.98
I0122 16:25:47.084327 45797 caffe_interface.cpp:125] Batch 97, loss = 0.283376
I0122 16:25:47.084336 45797 caffe_interface.cpp:125] Batch 97, top-1 = 0.92
I0122 16:25:47.084340 45797 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:25:47.085753 45797 caffe_interface.cpp:125] Batch 98, loss = 0.547521
I0122 16:25:47.085767 45797 caffe_interface.cpp:125] Batch 98, top-1 = 0.86
I0122 16:25:47.085770 45797 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:25:47.086989 45797 caffe_interface.cpp:125] Batch 99, loss = 0.457498
I0122 16:25:47.086997 45797 caffe_interface.cpp:125] Batch 99, top-1 = 0.82
I0122 16:25:47.087000 45797 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:25:47.088299 45797 caffe_interface.cpp:125] Batch 100, loss = 0.363578
I0122 16:25:47.088307 45797 caffe_interface.cpp:125] Batch 100, top-1 = 0.94
I0122 16:25:47.088310 45797 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0122 16:25:47.089516 45797 caffe_interface.cpp:125] Batch 101, loss = 0.388705
I0122 16:25:47.089524 45797 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0122 16:25:47.089529 45797 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:25:47.090742 45797 caffe_interface.cpp:125] Batch 102, loss = 0.649826
I0122 16:25:47.090749 45797 caffe_interface.cpp:125] Batch 102, top-1 = 0.8
I0122 16:25:47.090754 45797 caffe_interface.cpp:125] Batch 102, top-5 = 0.96
I0122 16:25:47.091969 45797 caffe_interface.cpp:125] Batch 103, loss = 0.377497
I0122 16:25:47.091976 45797 caffe_interface.cpp:125] Batch 103, top-1 = 0.88
I0122 16:25:47.091979 45797 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0122 16:25:47.093190 45797 caffe_interface.cpp:125] Batch 104, loss = 0.338015
I0122 16:25:47.093199 45797 caffe_interface.cpp:125] Batch 104, top-1 = 0.92
I0122 16:25:47.093201 45797 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0122 16:25:47.094408 45797 caffe_interface.cpp:125] Batch 105, loss = 0.39638
I0122 16:25:47.094415 45797 caffe_interface.cpp:125] Batch 105, top-1 = 0.88
I0122 16:25:47.094419 45797 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:25:47.095644 45797 caffe_interface.cpp:125] Batch 106, loss = 0.262809
I0122 16:25:47.095651 45797 caffe_interface.cpp:125] Batch 106, top-1 = 0.86
I0122 16:25:47.095654 45797 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:25:47.096873 45797 caffe_interface.cpp:125] Batch 107, loss = 0.693437
I0122 16:25:47.096879 45797 caffe_interface.cpp:125] Batch 107, top-1 = 0.8
I0122 16:25:47.096882 45797 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0122 16:25:47.098093 45797 caffe_interface.cpp:125] Batch 108, loss = 0.415177
I0122 16:25:47.098110 45797 caffe_interface.cpp:125] Batch 108, top-1 = 0.82
I0122 16:25:47.098114 45797 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0122 16:25:47.099313 45797 caffe_interface.cpp:125] Batch 109, loss = 0.357024
I0122 16:25:47.099319 45797 caffe_interface.cpp:125] Batch 109, top-1 = 0.86
I0122 16:25:47.099324 45797 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:25:47.100528 45797 caffe_interface.cpp:125] Batch 110, loss = 0.490035
I0122 16:25:47.100535 45797 caffe_interface.cpp:125] Batch 110, top-1 = 0.82
I0122 16:25:47.100539 45797 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0122 16:25:47.101743 45797 caffe_interface.cpp:125] Batch 111, loss = 0.669489
I0122 16:25:47.101750 45797 caffe_interface.cpp:125] Batch 111, top-1 = 0.8
I0122 16:25:47.101753 45797 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0122 16:25:47.102962 45797 caffe_interface.cpp:125] Batch 112, loss = 0.357297
I0122 16:25:47.102970 45797 caffe_interface.cpp:125] Batch 112, top-1 = 0.94
I0122 16:25:47.102973 45797 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0122 16:25:47.104187 45797 caffe_interface.cpp:125] Batch 113, loss = 0.469892
I0122 16:25:47.104194 45797 caffe_interface.cpp:125] Batch 113, top-1 = 0.82
I0122 16:25:47.104199 45797 caffe_interface.cpp:125] Batch 113, top-5 = 0.98
I0122 16:25:47.105417 45797 caffe_interface.cpp:125] Batch 114, loss = 0.719508
I0122 16:25:47.105423 45797 caffe_interface.cpp:125] Batch 114, top-1 = 0.8
I0122 16:25:47.105427 45797 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0122 16:25:47.106649 45797 caffe_interface.cpp:125] Batch 115, loss = 0.118574
I0122 16:25:47.106657 45797 caffe_interface.cpp:125] Batch 115, top-1 = 0.98
I0122 16:25:47.106662 45797 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:25:47.107879 45797 caffe_interface.cpp:125] Batch 116, loss = 0.582491
I0122 16:25:47.107887 45797 caffe_interface.cpp:125] Batch 116, top-1 = 0.82
I0122 16:25:47.107890 45797 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0122 16:25:47.109115 45797 caffe_interface.cpp:125] Batch 117, loss = 0.572454
I0122 16:25:47.109123 45797 caffe_interface.cpp:125] Batch 117, top-1 = 0.82
I0122 16:25:47.109127 45797 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0122 16:25:47.110332 45797 caffe_interface.cpp:125] Batch 118, loss = 0.397965
I0122 16:25:47.110342 45797 caffe_interface.cpp:125] Batch 118, top-1 = 0.88
I0122 16:25:47.110345 45797 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0122 16:25:47.111560 45797 caffe_interface.cpp:125] Batch 119, loss = 0.70771
I0122 16:25:47.111567 45797 caffe_interface.cpp:125] Batch 119, top-1 = 0.8
I0122 16:25:47.111570 45797 caffe_interface.cpp:125] Batch 119, top-5 = 0.96
I0122 16:25:47.112789 45797 caffe_interface.cpp:125] Batch 120, loss = 0.235446
I0122 16:25:47.112797 45797 caffe_interface.cpp:125] Batch 120, top-1 = 0.92
I0122 16:25:47.112800 45797 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0122 16:25:47.114017 45797 caffe_interface.cpp:125] Batch 121, loss = 0.838219
I0122 16:25:47.114023 45797 caffe_interface.cpp:125] Batch 121, top-1 = 0.8
I0122 16:25:47.114028 45797 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0122 16:25:47.115237 45797 caffe_interface.cpp:125] Batch 122, loss = 0.361537
I0122 16:25:47.115245 45797 caffe_interface.cpp:125] Batch 122, top-1 = 0.86
I0122 16:25:47.115247 45797 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:25:47.116451 45797 caffe_interface.cpp:125] Batch 123, loss = 0.473364
I0122 16:25:47.116458 45797 caffe_interface.cpp:125] Batch 123, top-1 = 0.9
I0122 16:25:47.116461 45797 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0122 16:25:47.118655 45797 caffe_interface.cpp:125] Batch 124, loss = 0.471443
I0122 16:25:47.118664 45797 caffe_interface.cpp:125] Batch 124, top-1 = 0.86
I0122 16:25:47.118665 45797 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0122 16:25:47.120151 45797 caffe_interface.cpp:125] Batch 125, loss = 0.498954
I0122 16:25:47.120157 45797 caffe_interface.cpp:125] Batch 125, top-1 = 0.84
I0122 16:25:47.120160 45797 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0122 16:25:47.121470 45797 caffe_interface.cpp:125] Batch 126, loss = 0.472457
I0122 16:25:47.121477 45797 caffe_interface.cpp:125] Batch 126, top-1 = 0.82
I0122 16:25:47.121479 45797 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:25:47.122668 45797 caffe_interface.cpp:125] Batch 127, loss = 0.498795
I0122 16:25:47.122675 45797 caffe_interface.cpp:125] Batch 127, top-1 = 0.88
I0122 16:25:47.122678 45797 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:25:47.123868 45797 caffe_interface.cpp:125] Batch 128, loss = 0.34379
I0122 16:25:47.123875 45797 caffe_interface.cpp:125] Batch 128, top-1 = 0.88
I0122 16:25:47.123878 45797 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0122 16:25:47.125052 45797 caffe_interface.cpp:125] Batch 129, loss = 0.651483
I0122 16:25:47.125058 45797 caffe_interface.cpp:125] Batch 129, top-1 = 0.84
I0122 16:25:47.125061 45797 caffe_interface.cpp:125] Batch 129, top-5 = 0.98
I0122 16:25:47.126251 45797 caffe_interface.cpp:125] Batch 130, loss = 0.465789
I0122 16:25:47.126257 45797 caffe_interface.cpp:125] Batch 130, top-1 = 0.82
I0122 16:25:47.126260 45797 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:25:47.127462 45797 caffe_interface.cpp:125] Batch 131, loss = 0.539002
I0122 16:25:47.127470 45797 caffe_interface.cpp:125] Batch 131, top-1 = 0.82
I0122 16:25:47.127472 45797 caffe_interface.cpp:125] Batch 131, top-5 = 1
I0122 16:25:47.128660 45797 caffe_interface.cpp:125] Batch 132, loss = 0.519748
I0122 16:25:47.128667 45797 caffe_interface.cpp:125] Batch 132, top-1 = 0.86
I0122 16:25:47.128669 45797 caffe_interface.cpp:125] Batch 132, top-5 = 0.98
I0122 16:25:47.129873 45797 caffe_interface.cpp:125] Batch 133, loss = 0.697805
I0122 16:25:47.129880 45797 caffe_interface.cpp:125] Batch 133, top-1 = 0.76
I0122 16:25:47.129884 45797 caffe_interface.cpp:125] Batch 133, top-5 = 0.96
I0122 16:25:47.131103 45797 caffe_interface.cpp:125] Batch 134, loss = 0.561512
I0122 16:25:47.131110 45797 caffe_interface.cpp:125] Batch 134, top-1 = 0.86
I0122 16:25:47.131114 45797 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:25:47.132349 45797 caffe_interface.cpp:125] Batch 135, loss = 0.700783
I0122 16:25:47.132356 45797 caffe_interface.cpp:125] Batch 135, top-1 = 0.8
I0122 16:25:47.132359 45797 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0122 16:25:47.133571 45797 caffe_interface.cpp:125] Batch 136, loss = 0.368044
I0122 16:25:47.133579 45797 caffe_interface.cpp:125] Batch 136, top-1 = 0.9
I0122 16:25:47.133582 45797 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0122 16:25:47.134804 45797 caffe_interface.cpp:125] Batch 137, loss = 0.419791
I0122 16:25:47.134812 45797 caffe_interface.cpp:125] Batch 137, top-1 = 0.88
I0122 16:25:47.134816 45797 caffe_interface.cpp:125] Batch 137, top-5 = 0.98
I0122 16:25:47.136044 45797 caffe_interface.cpp:125] Batch 138, loss = 0.708136
I0122 16:25:47.136051 45797 caffe_interface.cpp:125] Batch 138, top-1 = 0.78
I0122 16:25:47.136055 45797 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0122 16:25:47.137264 45797 caffe_interface.cpp:125] Batch 139, loss = 0.497668
I0122 16:25:47.137271 45797 caffe_interface.cpp:125] Batch 139, top-1 = 0.86
I0122 16:25:47.137274 45797 caffe_interface.cpp:125] Batch 139, top-5 = 0.96
I0122 16:25:47.138485 45797 caffe_interface.cpp:125] Batch 140, loss = 0.400863
I0122 16:25:47.138494 45797 caffe_interface.cpp:125] Batch 140, top-1 = 0.86
I0122 16:25:47.138497 45797 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0122 16:25:47.139693 45797 caffe_interface.cpp:125] Batch 141, loss = 0.505996
I0122 16:25:47.139699 45797 caffe_interface.cpp:125] Batch 141, top-1 = 0.84
I0122 16:25:47.139703 45797 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:25:47.140933 45797 caffe_interface.cpp:125] Batch 142, loss = 0.127161
I0122 16:25:47.140940 45797 caffe_interface.cpp:125] Batch 142, top-1 = 0.94
I0122 16:25:47.140944 45797 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:25:47.142166 45797 caffe_interface.cpp:125] Batch 143, loss = 0.538058
I0122 16:25:47.142174 45797 caffe_interface.cpp:125] Batch 143, top-1 = 0.82
I0122 16:25:47.142186 45797 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0122 16:25:47.143389 45797 caffe_interface.cpp:125] Batch 144, loss = 0.618419
I0122 16:25:47.143398 45797 caffe_interface.cpp:125] Batch 144, top-1 = 0.88
I0122 16:25:47.143400 45797 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0122 16:25:47.144608 45797 caffe_interface.cpp:125] Batch 145, loss = 0.514735
I0122 16:25:47.144614 45797 caffe_interface.cpp:125] Batch 145, top-1 = 0.86
I0122 16:25:47.144618 45797 caffe_interface.cpp:125] Batch 145, top-5 = 0.96
I0122 16:25:47.145823 45797 caffe_interface.cpp:125] Batch 146, loss = 0.595573
I0122 16:25:47.145829 45797 caffe_interface.cpp:125] Batch 146, top-1 = 0.82
I0122 16:25:47.145833 45797 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0122 16:25:47.147043 45797 caffe_interface.cpp:125] Batch 147, loss = 0.639344
I0122 16:25:47.147050 45797 caffe_interface.cpp:125] Batch 147, top-1 = 0.76
I0122 16:25:47.147054 45797 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0122 16:25:47.148260 45797 caffe_interface.cpp:125] Batch 148, loss = 0.237914
I0122 16:25:47.148268 45797 caffe_interface.cpp:125] Batch 148, top-1 = 0.9
I0122 16:25:47.148272 45797 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:25:47.149482 45797 caffe_interface.cpp:125] Batch 149, loss = 0.316601
I0122 16:25:47.149488 45797 caffe_interface.cpp:125] Batch 149, top-1 = 0.84
I0122 16:25:47.149493 45797 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:25:47.151607 45797 caffe_interface.cpp:125] Batch 150, loss = 0.131889
I0122 16:25:47.151615 45797 caffe_interface.cpp:125] Batch 150, top-1 = 0.94
I0122 16:25:47.151618 45797 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0122 16:25:47.152966 45797 caffe_interface.cpp:125] Batch 151, loss = 0.574119
I0122 16:25:47.152977 45797 caffe_interface.cpp:125] Batch 151, top-1 = 0.76
I0122 16:25:47.152981 45797 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:25:47.154402 45797 caffe_interface.cpp:125] Batch 152, loss = 0.228971
I0122 16:25:47.154409 45797 caffe_interface.cpp:125] Batch 152, top-1 = 0.92
I0122 16:25:47.154413 45797 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0122 16:25:47.155628 45797 caffe_interface.cpp:125] Batch 153, loss = 0.230815
I0122 16:25:47.155635 45797 caffe_interface.cpp:125] Batch 153, top-1 = 0.92
I0122 16:25:47.155639 45797 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:25:47.156846 45797 caffe_interface.cpp:125] Batch 154, loss = 0.526443
I0122 16:25:47.156852 45797 caffe_interface.cpp:125] Batch 154, top-1 = 0.86
I0122 16:25:47.156857 45797 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:25:47.158059 45797 caffe_interface.cpp:125] Batch 155, loss = 0.350835
I0122 16:25:47.158066 45797 caffe_interface.cpp:125] Batch 155, top-1 = 0.84
I0122 16:25:47.158069 45797 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:25:47.159296 45797 caffe_interface.cpp:125] Batch 156, loss = 0.371769
I0122 16:25:47.159303 45797 caffe_interface.cpp:125] Batch 156, top-1 = 0.84
I0122 16:25:47.159307 45797 caffe_interface.cpp:125] Batch 156, top-5 = 1
I0122 16:25:47.160507 45797 caffe_interface.cpp:125] Batch 157, loss = 0.210178
I0122 16:25:47.160514 45797 caffe_interface.cpp:125] Batch 157, top-1 = 0.96
I0122 16:25:47.160518 45797 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:25:47.161729 45797 caffe_interface.cpp:125] Batch 158, loss = 0.454543
I0122 16:25:47.161736 45797 caffe_interface.cpp:125] Batch 158, top-1 = 0.88
I0122 16:25:47.161741 45797 caffe_interface.cpp:125] Batch 158, top-5 = 0.98
I0122 16:25:47.162940 45797 caffe_interface.cpp:125] Batch 159, loss = 0.513888
I0122 16:25:47.162948 45797 caffe_interface.cpp:125] Batch 159, top-1 = 0.76
I0122 16:25:47.162951 45797 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0122 16:25:47.164162 45797 caffe_interface.cpp:125] Batch 160, loss = 0.378064
I0122 16:25:47.164180 45797 caffe_interface.cpp:125] Batch 160, top-1 = 0.86
I0122 16:25:47.164183 45797 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0122 16:25:47.165396 45797 caffe_interface.cpp:125] Batch 161, loss = 0.375068
I0122 16:25:47.165412 45797 caffe_interface.cpp:125] Batch 161, top-1 = 0.84
I0122 16:25:47.165416 45797 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0122 16:25:47.166626 45797 caffe_interface.cpp:125] Batch 162, loss = 0.512172
I0122 16:25:47.166633 45797 caffe_interface.cpp:125] Batch 162, top-1 = 0.88
I0122 16:25:47.166637 45797 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:25:47.167841 45797 caffe_interface.cpp:125] Batch 163, loss = 0.35601
I0122 16:25:47.167848 45797 caffe_interface.cpp:125] Batch 163, top-1 = 0.94
I0122 16:25:47.167852 45797 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:25:47.169061 45797 caffe_interface.cpp:125] Batch 164, loss = 0.710992
I0122 16:25:47.169068 45797 caffe_interface.cpp:125] Batch 164, top-1 = 0.8
I0122 16:25:47.169072 45797 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:25:47.170277 45797 caffe_interface.cpp:125] Batch 165, loss = 0.420855
I0122 16:25:47.170285 45797 caffe_interface.cpp:125] Batch 165, top-1 = 0.9
I0122 16:25:47.170289 45797 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:25:47.171504 45797 caffe_interface.cpp:125] Batch 166, loss = 0.302865
I0122 16:25:47.171511 45797 caffe_interface.cpp:125] Batch 166, top-1 = 0.92
I0122 16:25:47.171514 45797 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:25:47.172724 45797 caffe_interface.cpp:125] Batch 167, loss = 0.474291
I0122 16:25:47.172730 45797 caffe_interface.cpp:125] Batch 167, top-1 = 0.82
I0122 16:25:47.172734 45797 caffe_interface.cpp:125] Batch 167, top-5 = 1
I0122 16:25:47.173943 45797 caffe_interface.cpp:125] Batch 168, loss = 0.180667
I0122 16:25:47.173951 45797 caffe_interface.cpp:125] Batch 168, top-1 = 0.94
I0122 16:25:47.173954 45797 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:25:47.175168 45797 caffe_interface.cpp:125] Batch 169, loss = 0.562541
I0122 16:25:47.175176 45797 caffe_interface.cpp:125] Batch 169, top-1 = 0.86
I0122 16:25:47.175180 45797 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0122 16:25:47.176395 45797 caffe_interface.cpp:125] Batch 170, loss = 0.283447
I0122 16:25:47.176403 45797 caffe_interface.cpp:125] Batch 170, top-1 = 0.88
I0122 16:25:47.176405 45797 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0122 16:25:47.177613 45797 caffe_interface.cpp:125] Batch 171, loss = 0.502419
I0122 16:25:47.177620 45797 caffe_interface.cpp:125] Batch 171, top-1 = 0.86
I0122 16:25:47.177623 45797 caffe_interface.cpp:125] Batch 171, top-5 = 0.96
I0122 16:25:47.178836 45797 caffe_interface.cpp:125] Batch 172, loss = 0.80725
I0122 16:25:47.178844 45797 caffe_interface.cpp:125] Batch 172, top-1 = 0.78
I0122 16:25:47.178848 45797 caffe_interface.cpp:125] Batch 172, top-5 = 0.96
I0122 16:25:47.180064 45797 caffe_interface.cpp:125] Batch 173, loss = 0.225349
I0122 16:25:47.180071 45797 caffe_interface.cpp:125] Batch 173, top-1 = 0.94
I0122 16:25:47.180076 45797 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0122 16:25:47.181289 45797 caffe_interface.cpp:125] Batch 174, loss = 0.402104
I0122 16:25:47.181298 45797 caffe_interface.cpp:125] Batch 174, top-1 = 0.86
I0122 16:25:47.181301 45797 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:25:47.182510 45797 caffe_interface.cpp:125] Batch 175, loss = 0.823674
I0122 16:25:47.182518 45797 caffe_interface.cpp:125] Batch 175, top-1 = 0.74
I0122 16:25:47.182521 45797 caffe_interface.cpp:125] Batch 175, top-5 = 0.96
I0122 16:25:47.183743 45797 caffe_interface.cpp:125] Batch 176, loss = 0.899715
I0122 16:25:47.183751 45797 caffe_interface.cpp:125] Batch 176, top-1 = 0.78
I0122 16:25:47.183754 45797 caffe_interface.cpp:125] Batch 176, top-5 = 0.98
I0122 16:25:47.185870 45797 caffe_interface.cpp:125] Batch 177, loss = 0.537497
I0122 16:25:47.185878 45797 caffe_interface.cpp:125] Batch 177, top-1 = 0.84
I0122 16:25:47.185881 45797 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0122 16:25:47.187216 45797 caffe_interface.cpp:125] Batch 178, loss = 0.695268
I0122 16:25:47.187223 45797 caffe_interface.cpp:125] Batch 178, top-1 = 0.78
I0122 16:25:47.187227 45797 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0122 16:25:47.188657 45797 caffe_interface.cpp:125] Batch 179, loss = 0.248565
I0122 16:25:47.188664 45797 caffe_interface.cpp:125] Batch 179, top-1 = 0.88
I0122 16:25:47.188674 45797 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:25:47.188678 45797 caffe_interface.cpp:130] Loss: 0.467607
I0122 16:25:47.188684 45797 caffe_interface.cpp:142] loss = 0.467607 (* 1 = 0.467607 loss)
I0122 16:25:47.188689 45797 caffe_interface.cpp:142] top-1 = 0.852222
I0122 16:25:47.188694 45797 caffe_interface.cpp:142] top-5 = 0.990778
I0122 16:25:47.334213 45797 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/sparse.caffemodel
I0122 16:25:47.334240 45797 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.1:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.852222264    | -0.0104441643  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 66047          | -3.42452526%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 46063616       | -6.09552288%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config1.prototxt
# fine-tuning: first run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config1.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune1_miniVggNet.txt
I0122 16:25:47.567296 47756 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0122 16:25:47.744050 47756 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:25:47.744592 47756 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24870518784, dev_info[0]: total=25620447232 free=24870518784
I0122 16:25:47.744603 47756 caffe_interface.cpp:493] Using GPUs 0
I0122 16:25:47.744866 47756 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:25:48.326330 47756 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt"
type: "SGD"
I0122 16:25:48.326432 47756 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0122 16:25:48.326678 47756 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:25:48.326692 47756 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:25:48.326695 47756 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:25:48.326858 47756 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:25:48.326925 47756 layer_factory.hpp:77] Creating layer data
I0122 16:25:48.327015 47756 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:48.327636 47756 net.cpp:94] Creating Layer data
I0122 16:25:48.327646 47756 net.cpp:409] data -> data
I0122 16:25:48.327670 47756 net.cpp:409] data -> label
I0122 16:25:48.329018 47795 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:25:48.329069 47795 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:25:48.329151 47756 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:25:48.329226 47756 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:25:48.336788 47756 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:48.336833 47756 net.cpp:144] Setting up data
I0122 16:25:48.336840 47756 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:25:48.336844 47756 net.cpp:151] Top shape: 128 (128)
I0122 16:25:48.336848 47756 net.cpp:159] Memory required for data: 1573376
I0122 16:25:48.336851 47756 layer_factory.hpp:77] Creating layer conv1
I0122 16:25:48.336864 47756 net.cpp:94] Creating Layer conv1
I0122 16:25:48.336868 47756 net.cpp:435] conv1 <- data
I0122 16:25:48.336896 47756 net.cpp:409] conv1 -> conv1
I0122 16:25:48.337916 47756 net.cpp:144] Setting up conv1
I0122 16:25:48.337929 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.337931 47756 net.cpp:159] Memory required for data: 18350592
I0122 16:25:48.337947 47756 layer_factory.hpp:77] Creating layer bn1
I0122 16:25:48.337956 47756 net.cpp:94] Creating Layer bn1
I0122 16:25:48.337960 47756 net.cpp:435] bn1 <- conv1
I0122 16:25:48.337965 47756 net.cpp:409] bn1 -> scale1
I0122 16:25:48.338557 47756 net.cpp:144] Setting up bn1
I0122 16:25:48.338563 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.338567 47756 net.cpp:159] Memory required for data: 35127808
I0122 16:25:48.338577 47756 layer_factory.hpp:77] Creating layer relu1
I0122 16:25:48.338584 47756 net.cpp:94] Creating Layer relu1
I0122 16:25:48.338587 47756 net.cpp:435] relu1 <- scale1
I0122 16:25:48.338591 47756 net.cpp:409] relu1 -> relu1
I0122 16:25:48.338613 47756 net.cpp:144] Setting up relu1
I0122 16:25:48.338618 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.338623 47756 net.cpp:159] Memory required for data: 51905024
I0122 16:25:48.338624 47756 layer_factory.hpp:77] Creating layer conv2
I0122 16:25:48.338632 47756 net.cpp:94] Creating Layer conv2
I0122 16:25:48.338637 47756 net.cpp:435] conv2 <- relu1
I0122 16:25:48.338641 47756 net.cpp:409] conv2 -> conv2
I0122 16:25:48.340191 47756 net.cpp:144] Setting up conv2
I0122 16:25:48.340201 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.340204 47756 net.cpp:159] Memory required for data: 68682240
I0122 16:25:48.340212 47756 layer_factory.hpp:77] Creating layer bn2
I0122 16:25:48.340219 47756 net.cpp:94] Creating Layer bn2
I0122 16:25:48.340222 47756 net.cpp:435] bn2 <- conv2
I0122 16:25:48.340227 47756 net.cpp:409] bn2 -> scale2
I0122 16:25:48.340950 47756 net.cpp:144] Setting up bn2
I0122 16:25:48.340956 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.340960 47756 net.cpp:159] Memory required for data: 85459456
I0122 16:25:48.340968 47756 layer_factory.hpp:77] Creating layer relu2
I0122 16:25:48.340973 47756 net.cpp:94] Creating Layer relu2
I0122 16:25:48.340976 47756 net.cpp:435] relu2 <- scale2
I0122 16:25:48.340981 47756 net.cpp:409] relu2 -> relu2
I0122 16:25:48.341011 47756 net.cpp:144] Setting up relu2
I0122 16:25:48.341017 47756 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:25:48.341019 47756 net.cpp:159] Memory required for data: 102236672
I0122 16:25:48.341022 47756 layer_factory.hpp:77] Creating layer pool1
I0122 16:25:48.341027 47756 net.cpp:94] Creating Layer pool1
I0122 16:25:48.341030 47756 net.cpp:435] pool1 <- relu2
I0122 16:25:48.341034 47756 net.cpp:409] pool1 -> pool1
I0122 16:25:48.341073 47756 net.cpp:144] Setting up pool1
I0122 16:25:48.341078 47756 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:25:48.341081 47756 net.cpp:159] Memory required for data: 106430976
I0122 16:25:48.341084 47756 layer_factory.hpp:77] Creating layer drop1
I0122 16:25:48.341089 47756 net.cpp:94] Creating Layer drop1
I0122 16:25:48.341092 47756 net.cpp:435] drop1 <- pool1
I0122 16:25:48.341109 47756 net.cpp:409] drop1 -> drop1
I0122 16:25:48.341142 47756 net.cpp:144] Setting up drop1
I0122 16:25:48.341147 47756 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:25:48.341151 47756 net.cpp:159] Memory required for data: 110625280
I0122 16:25:48.341153 47756 layer_factory.hpp:77] Creating layer conv3
I0122 16:25:48.341161 47756 net.cpp:94] Creating Layer conv3
I0122 16:25:48.341171 47756 net.cpp:435] conv3 <- drop1
I0122 16:25:48.341176 47756 net.cpp:409] conv3 -> conv3
I0122 16:25:48.342288 47756 net.cpp:144] Setting up conv3
I0122 16:25:48.342305 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.342309 47756 net.cpp:159] Memory required for data: 119013888
I0122 16:25:48.342315 47756 layer_factory.hpp:77] Creating layer bn3
I0122 16:25:48.342322 47756 net.cpp:94] Creating Layer bn3
I0122 16:25:48.342325 47756 net.cpp:435] bn3 <- conv3
I0122 16:25:48.342330 47756 net.cpp:409] bn3 -> scale3
I0122 16:25:48.342959 47756 net.cpp:144] Setting up bn3
I0122 16:25:48.342967 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.342970 47756 net.cpp:159] Memory required for data: 127402496
I0122 16:25:48.342983 47756 layer_factory.hpp:77] Creating layer relu3
I0122 16:25:48.342988 47756 net.cpp:94] Creating Layer relu3
I0122 16:25:48.342990 47756 net.cpp:435] relu3 <- scale3
I0122 16:25:48.342995 47756 net.cpp:409] relu3 -> relu3
I0122 16:25:48.343013 47756 net.cpp:144] Setting up relu3
I0122 16:25:48.343019 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.343021 47756 net.cpp:159] Memory required for data: 135791104
I0122 16:25:48.343024 47756 layer_factory.hpp:77] Creating layer conv4
I0122 16:25:48.343031 47756 net.cpp:94] Creating Layer conv4
I0122 16:25:48.343034 47756 net.cpp:435] conv4 <- relu3
I0122 16:25:48.343039 47756 net.cpp:409] conv4 -> conv4
I0122 16:25:48.343451 47756 net.cpp:144] Setting up conv4
I0122 16:25:48.343464 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.343468 47756 net.cpp:159] Memory required for data: 144179712
I0122 16:25:48.343474 47756 layer_factory.hpp:77] Creating layer bn4
I0122 16:25:48.343480 47756 net.cpp:94] Creating Layer bn4
I0122 16:25:48.343483 47756 net.cpp:435] bn4 <- conv4
I0122 16:25:48.343488 47756 net.cpp:409] bn4 -> scale4
I0122 16:25:48.344096 47756 net.cpp:144] Setting up bn4
I0122 16:25:48.344103 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.344107 47756 net.cpp:159] Memory required for data: 152568320
I0122 16:25:48.344116 47756 layer_factory.hpp:77] Creating layer relu4
I0122 16:25:48.344120 47756 net.cpp:94] Creating Layer relu4
I0122 16:25:48.344125 47756 net.cpp:435] relu4 <- scale4
I0122 16:25:48.344128 47756 net.cpp:409] relu4 -> relu4
I0122 16:25:48.344146 47756 net.cpp:144] Setting up relu4
I0122 16:25:48.344151 47756 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:25:48.344153 47756 net.cpp:159] Memory required for data: 160956928
I0122 16:25:48.344156 47756 layer_factory.hpp:77] Creating layer pool2
I0122 16:25:48.344161 47756 net.cpp:94] Creating Layer pool2
I0122 16:25:48.344164 47756 net.cpp:435] pool2 <- relu4
I0122 16:25:48.344168 47756 net.cpp:409] pool2 -> pool2
I0122 16:25:48.344195 47756 net.cpp:144] Setting up pool2
I0122 16:25:48.344202 47756 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:25:48.344207 47756 net.cpp:159] Memory required for data: 163054080
I0122 16:25:48.344208 47756 layer_factory.hpp:77] Creating layer drop2
I0122 16:25:48.344213 47756 net.cpp:94] Creating Layer drop2
I0122 16:25:48.344218 47756 net.cpp:435] drop2 <- pool2
I0122 16:25:48.344221 47756 net.cpp:409] drop2 -> drop2
I0122 16:25:48.344251 47756 net.cpp:144] Setting up drop2
I0122 16:25:48.344256 47756 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:25:48.344260 47756 net.cpp:159] Memory required for data: 165151232
I0122 16:25:48.344262 47756 layer_factory.hpp:77] Creating layer fc1
I0122 16:25:48.344269 47756 net.cpp:94] Creating Layer fc1
I0122 16:25:48.344271 47756 net.cpp:435] fc1 <- drop2
I0122 16:25:48.344276 47756 net.cpp:409] fc1 -> fc1
I0122 16:25:48.358328 47756 net.cpp:144] Setting up fc1
I0122 16:25:48.358345 47756 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:25:48.358348 47756 net.cpp:159] Memory required for data: 165413376
I0122 16:25:48.358355 47756 layer_factory.hpp:77] Creating layer bn5
I0122 16:25:48.358364 47756 net.cpp:94] Creating Layer bn5
I0122 16:25:48.358367 47756 net.cpp:435] bn5 <- fc1
I0122 16:25:48.358373 47756 net.cpp:409] bn5 -> scale5
I0122 16:25:48.358944 47756 net.cpp:144] Setting up bn5
I0122 16:25:48.358950 47756 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:25:48.358954 47756 net.cpp:159] Memory required for data: 165675520
I0122 16:25:48.358965 47756 layer_factory.hpp:77] Creating layer relu5
I0122 16:25:48.358973 47756 net.cpp:94] Creating Layer relu5
I0122 16:25:48.358976 47756 net.cpp:435] relu5 <- scale5
I0122 16:25:48.358983 47756 net.cpp:409] relu5 -> relu5
I0122 16:25:48.359000 47756 net.cpp:144] Setting up relu5
I0122 16:25:48.359006 47756 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:25:48.359009 47756 net.cpp:159] Memory required for data: 165937664
I0122 16:25:48.359011 47756 layer_factory.hpp:77] Creating layer drop3
I0122 16:25:48.359016 47756 net.cpp:94] Creating Layer drop3
I0122 16:25:48.359022 47756 net.cpp:435] drop3 <- relu5
I0122 16:25:48.359026 47756 net.cpp:409] drop3 -> drop3
I0122 16:25:48.359055 47756 net.cpp:144] Setting up drop3
I0122 16:25:48.359058 47756 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:25:48.359062 47756 net.cpp:159] Memory required for data: 166199808
I0122 16:25:48.359064 47756 layer_factory.hpp:77] Creating layer fc2
I0122 16:25:48.359071 47756 net.cpp:94] Creating Layer fc2
I0122 16:25:48.359074 47756 net.cpp:435] fc2 <- drop3
I0122 16:25:48.359081 47756 net.cpp:409] fc2 -> fc2
I0122 16:25:48.359223 47756 net.cpp:144] Setting up fc2
I0122 16:25:48.359228 47756 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:25:48.359231 47756 net.cpp:159] Memory required for data: 166204928
I0122 16:25:48.359236 47756 layer_factory.hpp:77] Creating layer loss
I0122 16:25:48.359241 47756 net.cpp:94] Creating Layer loss
I0122 16:25:48.359244 47756 net.cpp:435] loss <- fc2
I0122 16:25:48.359248 47756 net.cpp:435] loss <- label
I0122 16:25:48.359253 47756 net.cpp:409] loss -> loss
I0122 16:25:48.359261 47756 layer_factory.hpp:77] Creating layer loss
I0122 16:25:48.360002 47756 net.cpp:144] Setting up loss
I0122 16:25:48.360011 47756 net.cpp:151] Top shape: (1)
I0122 16:25:48.360014 47756 net.cpp:154]     with loss weight 1
I0122 16:25:48.360026 47756 net.cpp:159] Memory required for data: 166204932
I0122 16:25:48.360029 47756 net.cpp:220] loss needs backward computation.
I0122 16:25:48.360044 47756 net.cpp:220] fc2 needs backward computation.
I0122 16:25:48.360047 47756 net.cpp:220] drop3 needs backward computation.
I0122 16:25:48.360050 47756 net.cpp:220] relu5 needs backward computation.
I0122 16:25:48.360054 47756 net.cpp:220] bn5 needs backward computation.
I0122 16:25:48.360056 47756 net.cpp:220] fc1 needs backward computation.
I0122 16:25:48.360060 47756 net.cpp:220] drop2 needs backward computation.
I0122 16:25:48.360064 47756 net.cpp:220] pool2 needs backward computation.
I0122 16:25:48.360065 47756 net.cpp:220] relu4 needs backward computation.
I0122 16:25:48.360069 47756 net.cpp:220] bn4 needs backward computation.
I0122 16:25:48.360072 47756 net.cpp:220] conv4 needs backward computation.
I0122 16:25:48.360076 47756 net.cpp:220] relu3 needs backward computation.
I0122 16:25:48.360080 47756 net.cpp:220] bn3 needs backward computation.
I0122 16:25:48.360081 47756 net.cpp:220] conv3 needs backward computation.
I0122 16:25:48.360085 47756 net.cpp:220] drop1 needs backward computation.
I0122 16:25:48.360090 47756 net.cpp:220] pool1 needs backward computation.
I0122 16:25:48.360092 47756 net.cpp:220] relu2 needs backward computation.
I0122 16:25:48.360095 47756 net.cpp:220] bn2 needs backward computation.
I0122 16:25:48.360100 47756 net.cpp:220] conv2 needs backward computation.
I0122 16:25:48.360101 47756 net.cpp:220] relu1 needs backward computation.
I0122 16:25:48.360119 47756 net.cpp:220] bn1 needs backward computation.
I0122 16:25:48.360122 47756 net.cpp:220] conv1 needs backward computation.
I0122 16:25:48.360126 47756 net.cpp:222] data does not need backward computation.
I0122 16:25:48.360131 47756 net.cpp:264] This network produces output loss
I0122 16:25:48.360148 47756 net.cpp:284] Network initialization done.
I0122 16:25:48.360469 47756 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/net_finetune.prototxt
I0122 16:25:48.360505 47756 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:25:48.360702 47756 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:25:48.360801 47756 layer_factory.hpp:77] Creating layer data
I0122 16:25:48.360841 47756 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:48.362001 47756 net.cpp:94] Creating Layer data
I0122 16:25:48.362027 47756 net.cpp:409] data -> data
I0122 16:25:48.362048 47756 net.cpp:409] data -> label
I0122 16:25:48.362819 47825 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:25:48.362852 47825 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:25:48.362978 47756 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:25:48.363152 47756 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:25:48.368912 47756 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:25:48.368989 47756 net.cpp:144] Setting up data
I0122 16:25:48.369007 47756 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:25:48.369016 47756 net.cpp:151] Top shape: 50 (50)
I0122 16:25:48.369021 47756 net.cpp:159] Memory required for data: 614600
I0122 16:25:48.369036 47756 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:25:48.369052 47756 net.cpp:94] Creating Layer label_data_1_split
I0122 16:25:48.369060 47756 net.cpp:435] label_data_1_split <- label
I0122 16:25:48.369073 47756 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:25:48.369091 47756 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:25:48.369103 47756 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:25:48.369267 47756 net.cpp:144] Setting up label_data_1_split
I0122 16:25:48.369278 47756 net.cpp:151] Top shape: 50 (50)
I0122 16:25:48.369285 47756 net.cpp:151] Top shape: 50 (50)
I0122 16:25:48.369292 47756 net.cpp:151] Top shape: 50 (50)
I0122 16:25:48.369298 47756 net.cpp:159] Memory required for data: 615200
I0122 16:25:48.369303 47756 layer_factory.hpp:77] Creating layer conv1
I0122 16:25:48.369324 47756 net.cpp:94] Creating Layer conv1
I0122 16:25:48.369331 47756 net.cpp:435] conv1 <- data
I0122 16:25:48.369343 47756 net.cpp:409] conv1 -> conv1
I0122 16:25:48.369863 47756 net.cpp:144] Setting up conv1
I0122 16:25:48.369876 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.369881 47756 net.cpp:159] Memory required for data: 7168800
I0122 16:25:48.369897 47756 layer_factory.hpp:77] Creating layer bn1
I0122 16:25:48.369920 47756 net.cpp:94] Creating Layer bn1
I0122 16:25:48.369927 47756 net.cpp:435] bn1 <- conv1
I0122 16:25:48.369943 47756 net.cpp:409] bn1 -> scale1
I0122 16:25:48.371685 47756 net.cpp:144] Setting up bn1
I0122 16:25:48.371698 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.371704 47756 net.cpp:159] Memory required for data: 13722400
I0122 16:25:48.371727 47756 layer_factory.hpp:77] Creating layer relu1
I0122 16:25:48.371740 47756 net.cpp:94] Creating Layer relu1
I0122 16:25:48.371748 47756 net.cpp:435] relu1 <- scale1
I0122 16:25:48.371760 47756 net.cpp:409] relu1 -> relu1
I0122 16:25:48.371829 47756 net.cpp:144] Setting up relu1
I0122 16:25:48.371839 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.371843 47756 net.cpp:159] Memory required for data: 20276000
I0122 16:25:48.371850 47756 layer_factory.hpp:77] Creating layer conv2
I0122 16:25:48.371866 47756 net.cpp:94] Creating Layer conv2
I0122 16:25:48.371873 47756 net.cpp:435] conv2 <- relu1
I0122 16:25:48.371886 47756 net.cpp:409] conv2 -> conv2
I0122 16:25:48.372534 47756 net.cpp:144] Setting up conv2
I0122 16:25:48.372548 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.372553 47756 net.cpp:159] Memory required for data: 26829600
I0122 16:25:48.372566 47756 layer_factory.hpp:77] Creating layer bn2
I0122 16:25:48.372599 47756 net.cpp:94] Creating Layer bn2
I0122 16:25:48.372607 47756 net.cpp:435] bn2 <- conv2
I0122 16:25:48.372619 47756 net.cpp:409] bn2 -> scale2
I0122 16:25:48.374027 47756 net.cpp:144] Setting up bn2
I0122 16:25:48.374043 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.374049 47756 net.cpp:159] Memory required for data: 33383200
I0122 16:25:48.374064 47756 layer_factory.hpp:77] Creating layer relu2
I0122 16:25:48.374078 47756 net.cpp:94] Creating Layer relu2
I0122 16:25:48.374086 47756 net.cpp:435] relu2 <- scale2
I0122 16:25:48.374095 47756 net.cpp:409] relu2 -> relu2
I0122 16:25:48.374150 47756 net.cpp:144] Setting up relu2
I0122 16:25:48.374158 47756 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:25:48.374166 47756 net.cpp:159] Memory required for data: 39936800
I0122 16:25:48.374173 47756 layer_factory.hpp:77] Creating layer pool1
I0122 16:25:48.374186 47756 net.cpp:94] Creating Layer pool1
I0122 16:25:48.374194 47756 net.cpp:435] pool1 <- relu2
I0122 16:25:48.374203 47756 net.cpp:409] pool1 -> pool1
I0122 16:25:48.374269 47756 net.cpp:144] Setting up pool1
I0122 16:25:48.374300 47756 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:25:48.374305 47756 net.cpp:159] Memory required for data: 41575200
I0122 16:25:48.374311 47756 layer_factory.hpp:77] Creating layer drop1
I0122 16:25:48.374320 47756 net.cpp:94] Creating Layer drop1
I0122 16:25:48.374327 47756 net.cpp:435] drop1 <- pool1
I0122 16:25:48.374336 47756 net.cpp:409] drop1 -> drop1
I0122 16:25:48.374393 47756 net.cpp:144] Setting up drop1
I0122 16:25:48.374403 47756 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:25:48.374408 47756 net.cpp:159] Memory required for data: 43213600
I0122 16:25:48.374413 47756 layer_factory.hpp:77] Creating layer conv3
I0122 16:25:48.374431 47756 net.cpp:94] Creating Layer conv3
I0122 16:25:48.374439 47756 net.cpp:435] conv3 <- drop1
I0122 16:25:48.374449 47756 net.cpp:409] conv3 -> conv3
I0122 16:25:48.375149 47756 net.cpp:144] Setting up conv3
I0122 16:25:48.375162 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.375167 47756 net.cpp:159] Memory required for data: 46490400
I0122 16:25:48.375177 47756 layer_factory.hpp:77] Creating layer bn3
I0122 16:25:48.375191 47756 net.cpp:94] Creating Layer bn3
I0122 16:25:48.375200 47756 net.cpp:435] bn3 <- conv3
I0122 16:25:48.375212 47756 net.cpp:409] bn3 -> scale3
I0122 16:25:48.376579 47756 net.cpp:144] Setting up bn3
I0122 16:25:48.376591 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.376598 47756 net.cpp:159] Memory required for data: 49767200
I0122 16:25:48.376619 47756 layer_factory.hpp:77] Creating layer relu3
I0122 16:25:48.376631 47756 net.cpp:94] Creating Layer relu3
I0122 16:25:48.376636 47756 net.cpp:435] relu3 <- scale3
I0122 16:25:48.376646 47756 net.cpp:409] relu3 -> relu3
I0122 16:25:48.376744 47756 net.cpp:144] Setting up relu3
I0122 16:25:48.376754 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.376760 47756 net.cpp:159] Memory required for data: 53044000
I0122 16:25:48.376765 47756 layer_factory.hpp:77] Creating layer conv4
I0122 16:25:48.376780 47756 net.cpp:94] Creating Layer conv4
I0122 16:25:48.376787 47756 net.cpp:435] conv4 <- relu3
I0122 16:25:48.376799 47756 net.cpp:409] conv4 -> conv4
I0122 16:25:48.377723 47756 net.cpp:144] Setting up conv4
I0122 16:25:48.377737 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.377743 47756 net.cpp:159] Memory required for data: 56320800
I0122 16:25:48.377751 47756 layer_factory.hpp:77] Creating layer bn4
I0122 16:25:48.377768 47756 net.cpp:94] Creating Layer bn4
I0122 16:25:48.377775 47756 net.cpp:435] bn4 <- conv4
I0122 16:25:48.377787 47756 net.cpp:409] bn4 -> scale4
I0122 16:25:48.379091 47756 net.cpp:144] Setting up bn4
I0122 16:25:48.379101 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.379104 47756 net.cpp:159] Memory required for data: 59597600
I0122 16:25:48.379114 47756 layer_factory.hpp:77] Creating layer relu4
I0122 16:25:48.379122 47756 net.cpp:94] Creating Layer relu4
I0122 16:25:48.379127 47756 net.cpp:435] relu4 <- scale4
I0122 16:25:48.379134 47756 net.cpp:409] relu4 -> relu4
I0122 16:25:48.379160 47756 net.cpp:144] Setting up relu4
I0122 16:25:48.379168 47756 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:25:48.379171 47756 net.cpp:159] Memory required for data: 62874400
I0122 16:25:48.379175 47756 layer_factory.hpp:77] Creating layer pool2
I0122 16:25:48.379184 47756 net.cpp:94] Creating Layer pool2
I0122 16:25:48.379189 47756 net.cpp:435] pool2 <- relu4
I0122 16:25:48.379195 47756 net.cpp:409] pool2 -> pool2
I0122 16:25:48.379272 47756 net.cpp:144] Setting up pool2
I0122 16:25:48.379281 47756 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:25:48.379283 47756 net.cpp:159] Memory required for data: 63693600
I0122 16:25:48.379287 47756 layer_factory.hpp:77] Creating layer drop2
I0122 16:25:48.379293 47756 net.cpp:94] Creating Layer drop2
I0122 16:25:48.379297 47756 net.cpp:435] drop2 <- pool2
I0122 16:25:48.379304 47756 net.cpp:409] drop2 -> drop2
I0122 16:25:48.379345 47756 net.cpp:144] Setting up drop2
I0122 16:25:48.379353 47756 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:25:48.379369 47756 net.cpp:159] Memory required for data: 64512800
I0122 16:25:48.379371 47756 layer_factory.hpp:77] Creating layer fc1
I0122 16:25:48.379381 47756 net.cpp:94] Creating Layer fc1
I0122 16:25:48.379384 47756 net.cpp:435] fc1 <- drop2
I0122 16:25:48.379392 47756 net.cpp:409] fc1 -> fc1
I0122 16:25:48.396109 47756 net.cpp:144] Setting up fc1
I0122 16:25:48.396127 47756 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:48.396131 47756 net.cpp:159] Memory required for data: 64615200
I0122 16:25:48.396136 47756 layer_factory.hpp:77] Creating layer bn5
I0122 16:25:48.396145 47756 net.cpp:94] Creating Layer bn5
I0122 16:25:48.396149 47756 net.cpp:435] bn5 <- fc1
I0122 16:25:48.396155 47756 net.cpp:409] bn5 -> scale5
I0122 16:25:48.396745 47756 net.cpp:144] Setting up bn5
I0122 16:25:48.396752 47756 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:48.396755 47756 net.cpp:159] Memory required for data: 64717600
I0122 16:25:48.396769 47756 layer_factory.hpp:77] Creating layer relu5
I0122 16:25:48.396775 47756 net.cpp:94] Creating Layer relu5
I0122 16:25:48.396778 47756 net.cpp:435] relu5 <- scale5
I0122 16:25:48.396785 47756 net.cpp:409] relu5 -> relu5
I0122 16:25:48.396805 47756 net.cpp:144] Setting up relu5
I0122 16:25:48.396809 47756 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:48.396811 47756 net.cpp:159] Memory required for data: 64820000
I0122 16:25:48.396814 47756 layer_factory.hpp:77] Creating layer drop3
I0122 16:25:48.396821 47756 net.cpp:94] Creating Layer drop3
I0122 16:25:48.396823 47756 net.cpp:435] drop3 <- relu5
I0122 16:25:48.396826 47756 net.cpp:409] drop3 -> drop3
I0122 16:25:48.396857 47756 net.cpp:144] Setting up drop3
I0122 16:25:48.396862 47756 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:25:48.396863 47756 net.cpp:159] Memory required for data: 64922400
I0122 16:25:48.396865 47756 layer_factory.hpp:77] Creating layer fc2
I0122 16:25:48.396873 47756 net.cpp:94] Creating Layer fc2
I0122 16:25:48.396876 47756 net.cpp:435] fc2 <- drop3
I0122 16:25:48.396880 47756 net.cpp:409] fc2 -> fc2
I0122 16:25:48.397022 47756 net.cpp:144] Setting up fc2
I0122 16:25:48.397028 47756 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:48.397030 47756 net.cpp:159] Memory required for data: 64924400
I0122 16:25:48.397034 47756 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:25:48.397039 47756 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:25:48.397043 47756 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:25:48.397049 47756 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:25:48.397055 47756 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:25:48.397063 47756 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:25:48.397102 47756 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:25:48.397107 47756 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:48.397110 47756 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:48.397112 47756 net.cpp:151] Top shape: 50 10 (500)
I0122 16:25:48.397115 47756 net.cpp:159] Memory required for data: 64930400
I0122 16:25:48.397119 47756 layer_factory.hpp:77] Creating layer loss
I0122 16:25:48.397123 47756 net.cpp:94] Creating Layer loss
I0122 16:25:48.397126 47756 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:25:48.397130 47756 net.cpp:435] loss <- label_data_1_split_0
I0122 16:25:48.397136 47756 net.cpp:409] loss -> loss
I0122 16:25:48.397143 47756 layer_factory.hpp:77] Creating layer loss
I0122 16:25:48.397215 47756 net.cpp:144] Setting up loss
I0122 16:25:48.397220 47756 net.cpp:151] Top shape: (1)
I0122 16:25:48.397223 47756 net.cpp:154]     with loss weight 1
I0122 16:25:48.397234 47756 net.cpp:159] Memory required for data: 64930404
I0122 16:25:48.397236 47756 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:25:48.397243 47756 net.cpp:94] Creating Layer accuracy-top1
I0122 16:25:48.397245 47756 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:25:48.397248 47756 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:25:48.397254 47756 net.cpp:409] accuracy-top1 -> top-1
I0122 16:25:48.397274 47756 net.cpp:144] Setting up accuracy-top1
I0122 16:25:48.397276 47756 net.cpp:151] Top shape: (1)
I0122 16:25:48.397279 47756 net.cpp:159] Memory required for data: 64930408
I0122 16:25:48.397281 47756 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:25:48.397287 47756 net.cpp:94] Creating Layer accuracy-top5
I0122 16:25:48.397290 47756 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:25:48.397294 47756 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:25:48.397298 47756 net.cpp:409] accuracy-top5 -> top-5
I0122 16:25:48.397305 47756 net.cpp:144] Setting up accuracy-top5
I0122 16:25:48.397310 47756 net.cpp:151] Top shape: (1)
I0122 16:25:48.397312 47756 net.cpp:159] Memory required for data: 64930412
I0122 16:25:48.397315 47756 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:25:48.397320 47756 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:25:48.397322 47756 net.cpp:220] loss needs backward computation.
I0122 16:25:48.397326 47756 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:25:48.397330 47756 net.cpp:220] fc2 needs backward computation.
I0122 16:25:48.397332 47756 net.cpp:220] drop3 needs backward computation.
I0122 16:25:48.397336 47756 net.cpp:220] relu5 needs backward computation.
I0122 16:25:48.397338 47756 net.cpp:220] bn5 needs backward computation.
I0122 16:25:48.397341 47756 net.cpp:220] fc1 needs backward computation.
I0122 16:25:48.397344 47756 net.cpp:220] drop2 needs backward computation.
I0122 16:25:48.397347 47756 net.cpp:220] pool2 needs backward computation.
I0122 16:25:48.397351 47756 net.cpp:220] relu4 needs backward computation.
I0122 16:25:48.397353 47756 net.cpp:220] bn4 needs backward computation.
I0122 16:25:48.397356 47756 net.cpp:220] conv4 needs backward computation.
I0122 16:25:48.397359 47756 net.cpp:220] relu3 needs backward computation.
I0122 16:25:48.397362 47756 net.cpp:220] bn3 needs backward computation.
I0122 16:25:48.397366 47756 net.cpp:220] conv3 needs backward computation.
I0122 16:25:48.397368 47756 net.cpp:220] drop1 needs backward computation.
I0122 16:25:48.397370 47756 net.cpp:220] pool1 needs backward computation.
I0122 16:25:48.397373 47756 net.cpp:220] relu2 needs backward computation.
I0122 16:25:48.397377 47756 net.cpp:220] bn2 needs backward computation.
I0122 16:25:48.397379 47756 net.cpp:220] conv2 needs backward computation.
I0122 16:25:48.397382 47756 net.cpp:220] relu1 needs backward computation.
I0122 16:25:48.397384 47756 net.cpp:220] bn1 needs backward computation.
I0122 16:25:48.397387 47756 net.cpp:220] conv1 needs backward computation.
I0122 16:25:48.397392 47756 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:25:48.397395 47756 net.cpp:222] data does not need backward computation.
I0122 16:25:48.397397 47756 net.cpp:264] This network produces output loss
I0122 16:25:48.397400 47756 net.cpp:264] This network produces output top-1
I0122 16:25:48.397404 47756 net.cpp:264] This network produces output top-5
I0122 16:25:48.397425 47756 net.cpp:284] Network initialization done.
I0122 16:25:48.397531 47756 solver.cpp:63] Solver scaffolding done.
I0122 16:25:48.398706 47756 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/sparse.caffemodel
I0122 16:25:48.457772 47756 caffe_interface.cpp:527] Starting Optimization
I0122 16:25:48.457801 47756 solver.cpp:335] Solving 
I0122 16:25:48.457804 47756 solver.cpp:336] Learning Rate Policy: poly
I0122 16:25:48.459050 47756 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:25:48.685359 47756 solver.cpp:517]     Test net output #0: loss = 0.467607 (* 1 = 0.467607 loss)
I0122 16:25:48.685379 47756 solver.cpp:517]     Test net output #1: top-1 = 0.852222
I0122 16:25:48.685382 47756 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:25:48.702440 47756 solver.cpp:266] Iteration 0 (0 iter/s, 0.244595s/100 iter), loss = 0.0977562
I0122 16:25:48.702474 47756 solver.cpp:285]     Train net output #0: loss = 0.0977562 (* 1 = 0.0977562 loss)
I0122 16:25:48.702499 47756 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:25:49.565800 47756 solver.cpp:266] Iteration 100 (115.837 iter/s, 0.863283s/100 iter), loss = 0.117433
I0122 16:25:49.565829 47756 solver.cpp:285]     Train net output #0: loss = 0.117433 (* 1 = 0.117433 loss)
I0122 16:25:49.565835 47756 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:25:50.427763 47756 solver.cpp:266] Iteration 200 (116.024 iter/s, 0.861891s/100 iter), loss = 0.217661
I0122 16:25:50.427790 47756 solver.cpp:285]     Train net output #0: loss = 0.217661 (* 1 = 0.217661 loss)
I0122 16:25:50.427795 47756 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:25:51.293475 47756 solver.cpp:266] Iteration 300 (115.521 iter/s, 0.865641s/100 iter), loss = 0.137532
I0122 16:25:51.293503 47756 solver.cpp:285]     Train net output #0: loss = 0.137532 (* 1 = 0.137532 loss)
I0122 16:25:51.293509 47756 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:25:52.163808 47756 solver.cpp:266] Iteration 400 (114.908 iter/s, 0.870259s/100 iter), loss = 0.136467
I0122 16:25:52.163837 47756 solver.cpp:285]     Train net output #0: loss = 0.136467 (* 1 = 0.136467 loss)
I0122 16:25:52.163842 47756 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:25:53.045143 47756 solver.cpp:266] Iteration 500 (113.474 iter/s, 0.88126s/100 iter), loss = 0.184178
I0122 16:25:53.045171 47756 solver.cpp:285]     Train net output #0: loss = 0.184178 (* 1 = 0.184178 loss)
I0122 16:25:53.045176 47756 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:25:53.979588 47756 solver.cpp:266] Iteration 600 (107.024 iter/s, 0.934369s/100 iter), loss = 0.0924863
I0122 16:25:53.979620 47756 solver.cpp:285]     Train net output #0: loss = 0.0924863 (* 1 = 0.0924863 loss)
I0122 16:25:53.979626 47756 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:25:54.897416 47756 solver.cpp:266] Iteration 700 (108.962 iter/s, 0.917751s/100 iter), loss = 0.257659
I0122 16:25:54.897447 47756 solver.cpp:285]     Train net output #0: loss = 0.257659 (* 1 = 0.257659 loss)
I0122 16:25:54.897452 47756 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:25:55.763490 47756 solver.cpp:266] Iteration 800 (115.474 iter/s, 0.865999s/100 iter), loss = 0.304038
I0122 16:25:55.763520 47756 solver.cpp:285]     Train net output #0: loss = 0.304038 (* 1 = 0.304038 loss)
I0122 16:25:55.763525 47756 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:25:56.627370 47756 solver.cpp:266] Iteration 900 (115.767 iter/s, 0.863805s/100 iter), loss = 0.124011
I0122 16:25:56.627399 47756 solver.cpp:285]     Train net output #0: loss = 0.124011 (* 1 = 0.124011 loss)
I0122 16:25:56.627405 47756 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:25:57.481896 47756 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:25:57.700455 47756 solver.cpp:517]     Test net output #0: loss = 0.659998 (* 1 = 0.659998 loss)
I0122 16:25:57.700469 47756 solver.cpp:517]     Test net output #1: top-1 = 0.825111
I0122 16:25:57.700474 47756 solver.cpp:517]     Test net output #2: top-5 = 0.986889
I0122 16:25:57.708528 47756 solver.cpp:266] Iteration 1000 (92.5003 iter/s, 1.08108s/100 iter), loss = 0.201224
I0122 16:25:57.708545 47756 solver.cpp:285]     Train net output #0: loss = 0.201224 (* 1 = 0.201224 loss)
I0122 16:25:57.708551 47756 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:25:58.588726 47756 solver.cpp:266] Iteration 1100 (113.619 iter/s, 0.880135s/100 iter), loss = 0.197268
I0122 16:25:58.588755 47756 solver.cpp:285]     Train net output #0: loss = 0.197268 (* 1 = 0.197268 loss)
I0122 16:25:58.588760 47756 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:25:59.450139 47756 solver.cpp:266] Iteration 1200 (116.098 iter/s, 0.861338s/100 iter), loss = 0.19363
I0122 16:25:59.450167 47756 solver.cpp:285]     Train net output #0: loss = 0.19363 (* 1 = 0.19363 loss)
I0122 16:25:59.450172 47756 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:26:00.366324 47756 solver.cpp:266] Iteration 1300 (109.157 iter/s, 0.916109s/100 iter), loss = 0.137106
I0122 16:26:00.366371 47756 solver.cpp:285]     Train net output #0: loss = 0.137106 (* 1 = 0.137106 loss)
I0122 16:26:00.366379 47756 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:26:01.227651 47756 solver.cpp:266] Iteration 1400 (116.112 iter/s, 0.861235s/100 iter), loss = 0.165819
I0122 16:26:01.227679 47756 solver.cpp:285]     Train net output #0: loss = 0.165819 (* 1 = 0.165819 loss)
I0122 16:26:01.227684 47756 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:26:02.111371 47756 solver.cpp:266] Iteration 1500 (113.167 iter/s, 0.883646s/100 iter), loss = 0.16542
I0122 16:26:02.111402 47756 solver.cpp:285]     Train net output #0: loss = 0.16542 (* 1 = 0.16542 loss)
I0122 16:26:02.111407 47756 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:26:03.053576 47756 solver.cpp:266] Iteration 1600 (106.143 iter/s, 0.942125s/100 iter), loss = 0.195389
I0122 16:26:03.053607 47756 solver.cpp:285]     Train net output #0: loss = 0.195389 (* 1 = 0.195389 loss)
I0122 16:26:03.053612 47756 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:26:03.916340 47756 solver.cpp:266] Iteration 1700 (115.917 iter/s, 0.862689s/100 iter), loss = 0.111023
I0122 16:26:03.916371 47756 solver.cpp:285]     Train net output #0: loss = 0.111023 (* 1 = 0.111023 loss)
I0122 16:26:03.916378 47756 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:26:04.878556 47756 solver.cpp:266] Iteration 1800 (103.935 iter/s, 0.962136s/100 iter), loss = 0.215613
I0122 16:26:04.878584 47756 solver.cpp:285]     Train net output #0: loss = 0.215613 (* 1 = 0.215613 loss)
I0122 16:26:04.878590 47756 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:26:05.902977 47756 solver.cpp:266] Iteration 1900 (97.6237 iter/s, 1.02434s/100 iter), loss = 0.18882
I0122 16:26:05.903007 47756 solver.cpp:285]     Train net output #0: loss = 0.18882 (* 1 = 0.18882 loss)
I0122 16:26:05.903054 47756 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:26:06.924409 47756 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:26:07.270247 47756 solver.cpp:517]     Test net output #0: loss = 0.569348 (* 1 = 0.569348 loss)
I0122 16:26:07.270263 47756 solver.cpp:517]     Test net output #1: top-1 = 0.830333
I0122 16:26:07.270268 47756 solver.cpp:517]     Test net output #2: top-5 = 0.988445
I0122 16:26:07.280124 47756 solver.cpp:266] Iteration 2000 (72.6212 iter/s, 1.37701s/100 iter), loss = 0.11952
I0122 16:26:07.280155 47756 solver.cpp:285]     Train net output #0: loss = 0.11952 (* 1 = 0.11952 loss)
I0122 16:26:07.280162 47756 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:26:08.157310 47756 solver.cpp:266] Iteration 2100 (114.011 iter/s, 0.877109s/100 iter), loss = 0.182078
I0122 16:26:08.157339 47756 solver.cpp:285]     Train net output #0: loss = 0.182078 (* 1 = 0.182078 loss)
I0122 16:26:08.157346 47756 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:26:09.093242 47756 solver.cpp:266] Iteration 2200 (106.854 iter/s, 0.935855s/100 iter), loss = 0.185789
I0122 16:26:09.093271 47756 solver.cpp:285]     Train net output #0: loss = 0.185789 (* 1 = 0.185789 loss)
I0122 16:26:09.093276 47756 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:26:09.984870 47756 solver.cpp:266] Iteration 2300 (112.164 iter/s, 0.891553s/100 iter), loss = 0.235444
I0122 16:26:09.984900 47756 solver.cpp:285]     Train net output #0: loss = 0.235444 (* 1 = 0.235444 loss)
I0122 16:26:09.984947 47756 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:26:10.997565 47756 solver.cpp:266] Iteration 2400 (98.7588 iter/s, 1.01257s/100 iter), loss = 0.154529
I0122 16:26:10.997596 47756 solver.cpp:285]     Train net output #0: loss = 0.154529 (* 1 = 0.154529 loss)
I0122 16:26:10.997642 47756 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:26:11.895334 47756 solver.cpp:266] Iteration 2500 (111.403 iter/s, 0.897645s/100 iter), loss = 0.173366
I0122 16:26:11.895364 47756 solver.cpp:285]     Train net output #0: loss = 0.173366 (* 1 = 0.173366 loss)
I0122 16:26:11.895370 47756 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:26:12.756553 47756 solver.cpp:266] Iteration 2600 (116.124 iter/s, 0.861145s/100 iter), loss = 0.109011
I0122 16:26:12.756602 47756 solver.cpp:285]     Train net output #0: loss = 0.109011 (* 1 = 0.109011 loss)
I0122 16:26:12.756608 47756 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:26:13.663888 47756 solver.cpp:266] Iteration 2700 (110.224 iter/s, 0.90724s/100 iter), loss = 0.234752
I0122 16:26:13.663919 47756 solver.cpp:285]     Train net output #0: loss = 0.234752 (* 1 = 0.234752 loss)
I0122 16:26:13.663925 47756 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:26:14.539724 47756 solver.cpp:266] Iteration 2800 (114.186 iter/s, 0.87576s/100 iter), loss = 0.179513
I0122 16:26:14.539754 47756 solver.cpp:285]     Train net output #0: loss = 0.179513 (* 1 = 0.179513 loss)
I0122 16:26:14.539759 47756 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:26:15.401326 47756 solver.cpp:266] Iteration 2900 (116.073 iter/s, 0.861528s/100 iter), loss = 0.198839
I0122 16:26:15.401355 47756 solver.cpp:285]     Train net output #0: loss = 0.198839 (* 1 = 0.198839 loss)
I0122 16:26:15.401361 47756 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:26:16.266129 47756 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:26:16.485010 47756 solver.cpp:517]     Test net output #0: loss = 0.791608 (* 1 = 0.791608 loss)
I0122 16:26:16.485026 47756 solver.cpp:517]     Test net output #1: top-1 = 0.800556
I0122 16:26:16.485031 47756 solver.cpp:517]     Test net output #2: top-5 = 0.975
I0122 16:26:16.493108 47756 solver.cpp:266] Iteration 3000 (91.6001 iter/s, 1.0917s/100 iter), loss = 0.188715
I0122 16:26:16.493125 47756 solver.cpp:285]     Train net output #0: loss = 0.188715 (* 1 = 0.188715 loss)
I0122 16:26:16.493131 47756 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:26:17.396809 47756 solver.cpp:266] Iteration 3100 (110.664 iter/s, 0.903638s/100 iter), loss = 0.135689
I0122 16:26:17.396839 47756 solver.cpp:285]     Train net output #0: loss = 0.135689 (* 1 = 0.135689 loss)
I0122 16:26:17.396845 47756 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:26:18.257479 47756 solver.cpp:266] Iteration 3200 (116.199 iter/s, 0.860594s/100 iter), loss = 0.239842
I0122 16:26:18.257628 47756 solver.cpp:285]     Train net output #0: loss = 0.239842 (* 1 = 0.239842 loss)
I0122 16:26:18.257637 47756 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:26:19.171144 47756 solver.cpp:266] Iteration 3300 (109.472 iter/s, 0.913472s/100 iter), loss = 0.0786874
I0122 16:26:19.171175 47756 solver.cpp:285]     Train net output #0: loss = 0.0786874 (* 1 = 0.0786874 loss)
I0122 16:26:19.171181 47756 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:26:20.064759 47756 solver.cpp:266] Iteration 3400 (111.915 iter/s, 0.893539s/100 iter), loss = 0.218703
I0122 16:26:20.064790 47756 solver.cpp:285]     Train net output #0: loss = 0.218703 (* 1 = 0.218703 loss)
I0122 16:26:20.064795 47756 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:26:20.925999 47756 solver.cpp:266] Iteration 3500 (116.122 iter/s, 0.861164s/100 iter), loss = 0.152845
I0122 16:26:20.926029 47756 solver.cpp:285]     Train net output #0: loss = 0.152845 (* 1 = 0.152845 loss)
I0122 16:26:20.926034 47756 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:26:21.788125 47756 solver.cpp:266] Iteration 3600 (116.002 iter/s, 0.862052s/100 iter), loss = 0.165246
I0122 16:26:21.788156 47756 solver.cpp:285]     Train net output #0: loss = 0.165246 (* 1 = 0.165246 loss)
I0122 16:26:21.788161 47756 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:26:22.649410 47756 solver.cpp:266] Iteration 3700 (116.115 iter/s, 0.861212s/100 iter), loss = 0.201082
I0122 16:26:22.649438 47756 solver.cpp:285]     Train net output #0: loss = 0.201081 (* 1 = 0.201081 loss)
I0122 16:26:22.649443 47756 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:26:23.511112 47756 solver.cpp:266] Iteration 3800 (116.059 iter/s, 0.861629s/100 iter), loss = 0.212155
I0122 16:26:23.511142 47756 solver.cpp:285]     Train net output #0: loss = 0.212155 (* 1 = 0.212155 loss)
I0122 16:26:23.511147 47756 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:26:24.373457 47756 solver.cpp:266] Iteration 3900 (115.973 iter/s, 0.862271s/100 iter), loss = 0.130903
I0122 16:26:24.373484 47756 solver.cpp:285]     Train net output #0: loss = 0.130903 (* 1 = 0.130903 loss)
I0122 16:26:24.373489 47756 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:26:25.227088 47756 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:26:25.464082 47756 solver.cpp:517]     Test net output #0: loss = 0.495623 (* 1 = 0.495623 loss)
I0122 16:26:25.464097 47756 solver.cpp:517]     Test net output #1: top-1 = 0.850777
I0122 16:26:25.464102 47756 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:26:25.472203 47756 solver.cpp:266] Iteration 4000 (91.0195 iter/s, 1.09867s/100 iter), loss = 0.172471
I0122 16:26:25.472223 47756 solver.cpp:285]     Train net output #0: loss = 0.172471 (* 1 = 0.172471 loss)
I0122 16:26:25.472229 47756 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:26:26.335093 47756 solver.cpp:266] Iteration 4100 (115.898 iter/s, 0.862826s/100 iter), loss = 0.143719
I0122 16:26:26.335124 47756 solver.cpp:285]     Train net output #0: loss = 0.143719 (* 1 = 0.143719 loss)
I0122 16:26:26.335130 47756 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:26:27.196254 47756 solver.cpp:266] Iteration 4200 (116.132 iter/s, 0.861086s/100 iter), loss = 0.266364
I0122 16:26:27.196282 47756 solver.cpp:285]     Train net output #0: loss = 0.266364 (* 1 = 0.266364 loss)
I0122 16:26:27.196287 47756 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:26:28.079068 47756 solver.cpp:266] Iteration 4300 (113.284 iter/s, 0.882739s/100 iter), loss = 0.170654
I0122 16:26:28.079098 47756 solver.cpp:285]     Train net output #0: loss = 0.170654 (* 1 = 0.170654 loss)
I0122 16:26:28.079104 47756 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:26:28.951241 47756 solver.cpp:266] Iteration 4400 (114.666 iter/s, 0.872097s/100 iter), loss = 0.245171
I0122 16:26:28.951272 47756 solver.cpp:285]     Train net output #0: loss = 0.245171 (* 1 = 0.245171 loss)
I0122 16:26:28.951318 47756 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:26:29.813910 47756 solver.cpp:266] Iteration 4500 (115.936 iter/s, 0.862545s/100 iter), loss = 0.135053
I0122 16:26:29.813980 47756 solver.cpp:285]     Train net output #0: loss = 0.135053 (* 1 = 0.135053 loss)
I0122 16:26:29.813987 47756 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:26:30.675834 47756 solver.cpp:266] Iteration 4600 (116.035 iter/s, 0.861811s/100 iter), loss = 0.119239
I0122 16:26:30.675863 47756 solver.cpp:285]     Train net output #0: loss = 0.119239 (* 1 = 0.119239 loss)
I0122 16:26:30.675868 47756 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:26:31.537184 47756 solver.cpp:266] Iteration 4700 (116.107 iter/s, 0.861275s/100 iter), loss = 0.243584
I0122 16:26:31.537211 47756 solver.cpp:285]     Train net output #0: loss = 0.243584 (* 1 = 0.243584 loss)
I0122 16:26:31.537216 47756 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:26:32.400002 47756 solver.cpp:266] Iteration 4800 (115.909 iter/s, 0.862744s/100 iter), loss = 0.140499
I0122 16:26:32.400032 47756 solver.cpp:285]     Train net output #0: loss = 0.140499 (* 1 = 0.140499 loss)
I0122 16:26:32.400038 47756 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:26:33.307361 47756 solver.cpp:266] Iteration 4900 (110.219 iter/s, 0.907281s/100 iter), loss = 0.304317
I0122 16:26:33.307392 47756 solver.cpp:285]     Train net output #0: loss = 0.304317 (* 1 = 0.304317 loss)
I0122 16:26:33.307438 47756 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:26:34.330376 47756 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:26:34.589486 47756 solver.cpp:517]     Test net output #0: loss = 0.503809 (* 1 = 0.503809 loss)
I0122 16:26:34.589504 47756 solver.cpp:517]     Test net output #1: top-1 = 0.842444
I0122 16:26:34.589509 47756 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:26:34.597582 47756 solver.cpp:266] Iteration 5000 (77.5144 iter/s, 1.29008s/100 iter), loss = 0.158405
I0122 16:26:34.597601 47756 solver.cpp:285]     Train net output #0: loss = 0.158405 (* 1 = 0.158405 loss)
I0122 16:26:34.597607 47756 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:26:35.571382 47756 solver.cpp:266] Iteration 5100 (102.698 iter/s, 0.973727s/100 iter), loss = 0.128397
I0122 16:26:35.571413 47756 solver.cpp:285]     Train net output #0: loss = 0.128397 (* 1 = 0.128397 loss)
I0122 16:26:35.571457 47756 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:26:36.597896 47756 solver.cpp:266] Iteration 5200 (97.4292 iter/s, 1.02639s/100 iter), loss = 0.216831
I0122 16:26:36.597932 47756 solver.cpp:285]     Train net output #0: loss = 0.216831 (* 1 = 0.216831 loss)
I0122 16:26:36.597971 47756 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:26:37.570133 47756 solver.cpp:266] Iteration 5300 (102.869 iter/s, 0.972112s/100 iter), loss = 0.202876
I0122 16:26:37.570166 47756 solver.cpp:285]     Train net output #0: loss = 0.202876 (* 1 = 0.202876 loss)
I0122 16:26:37.570211 47756 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:26:38.519567 47756 solver.cpp:266] Iteration 5400 (105.34 iter/s, 0.949307s/100 iter), loss = 0.238258
I0122 16:26:38.519599 47756 solver.cpp:285]     Train net output #0: loss = 0.238258 (* 1 = 0.238258 loss)
I0122 16:26:38.519644 47756 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:26:39.503000 47756 solver.cpp:266] Iteration 5500 (101.698 iter/s, 0.983305s/100 iter), loss = 0.146434
I0122 16:26:39.503031 47756 solver.cpp:285]     Train net output #0: loss = 0.146434 (* 1 = 0.146434 loss)
I0122 16:26:39.503077 47756 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:26:40.478933 47756 solver.cpp:266] Iteration 5600 (102.479 iter/s, 0.975806s/100 iter), loss = 0.161666
I0122 16:26:40.478965 47756 solver.cpp:285]     Train net output #0: loss = 0.161666 (* 1 = 0.161666 loss)
I0122 16:26:40.478969 47756 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:26:41.340394 47756 solver.cpp:266] Iteration 5700 (116.092 iter/s, 0.861383s/100 iter), loss = 0.108891
I0122 16:26:41.340425 47756 solver.cpp:285]     Train net output #0: loss = 0.108891 (* 1 = 0.108891 loss)
I0122 16:26:41.340451 47756 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:26:42.202008 47756 solver.cpp:266] Iteration 5800 (116.072 iter/s, 0.861537s/100 iter), loss = 0.113008
I0122 16:26:42.202036 47756 solver.cpp:285]     Train net output #0: loss = 0.113008 (* 1 = 0.113008 loss)
I0122 16:26:42.202041 47756 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:26:43.063766 47756 solver.cpp:266] Iteration 5900 (116.052 iter/s, 0.861686s/100 iter), loss = 0.203807
I0122 16:26:43.063794 47756 solver.cpp:285]     Train net output #0: loss = 0.203807 (* 1 = 0.203807 loss)
I0122 16:26:43.063799 47756 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:26:43.918113 47756 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:26:44.137761 47756 solver.cpp:517]     Test net output #0: loss = 0.533175 (* 1 = 0.533175 loss)
I0122 16:26:44.137785 47756 solver.cpp:517]     Test net output #1: top-1 = 0.841333
I0122 16:26:44.137789 47756 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0122 16:26:44.145900 47756 solver.cpp:266] Iteration 6000 (92.4167 iter/s, 1.08206s/100 iter), loss = 0.136079
I0122 16:26:44.145921 47756 solver.cpp:285]     Train net output #0: loss = 0.136079 (* 1 = 0.136079 loss)
I0122 16:26:44.145928 47756 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:26:45.007560 47756 solver.cpp:266] Iteration 6100 (116.064 iter/s, 0.861592s/100 iter), loss = 0.150514
I0122 16:26:45.007586 47756 solver.cpp:285]     Train net output #0: loss = 0.150514 (* 1 = 0.150514 loss)
I0122 16:26:45.007592 47756 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:26:45.869335 47756 solver.cpp:266] Iteration 6200 (116.049 iter/s, 0.861702s/100 iter), loss = 0.15766
I0122 16:26:45.869365 47756 solver.cpp:285]     Train net output #0: loss = 0.15766 (* 1 = 0.15766 loss)
I0122 16:26:45.869371 47756 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:26:46.730995 47756 solver.cpp:266] Iteration 6300 (116.065 iter/s, 0.861584s/100 iter), loss = 0.115137
I0122 16:26:46.731024 47756 solver.cpp:285]     Train net output #0: loss = 0.115137 (* 1 = 0.115137 loss)
I0122 16:26:46.731029 47756 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:26:47.592564 47756 solver.cpp:266] Iteration 6400 (116.077 iter/s, 0.861495s/100 iter), loss = 0.120817
I0122 16:26:47.592594 47756 solver.cpp:285]     Train net output #0: loss = 0.120817 (* 1 = 0.120817 loss)
I0122 16:26:47.592600 47756 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:26:48.454668 47756 solver.cpp:266] Iteration 6500 (116.006 iter/s, 0.862028s/100 iter), loss = 0.22115
I0122 16:26:48.454855 47756 solver.cpp:285]     Train net output #0: loss = 0.22115 (* 1 = 0.22115 loss)
I0122 16:26:48.454864 47756 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:26:49.316970 47756 solver.cpp:266] Iteration 6600 (116 iter/s, 0.862072s/100 iter), loss = 0.159214
I0122 16:26:49.316996 47756 solver.cpp:285]     Train net output #0: loss = 0.159214 (* 1 = 0.159214 loss)
I0122 16:26:49.317001 47756 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:26:50.178948 47756 solver.cpp:266] Iteration 6700 (116.022 iter/s, 0.861906s/100 iter), loss = 0.184178
I0122 16:26:50.178977 47756 solver.cpp:285]     Train net output #0: loss = 0.184178 (* 1 = 0.184178 loss)
I0122 16:26:50.178983 47756 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:26:51.041016 47756 solver.cpp:266] Iteration 6800 (116.01 iter/s, 0.861993s/100 iter), loss = 0.127957
I0122 16:26:51.041045 47756 solver.cpp:285]     Train net output #0: loss = 0.127957 (* 1 = 0.127957 loss)
I0122 16:26:51.041050 47756 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:26:51.903192 47756 solver.cpp:266] Iteration 6900 (115.996 iter/s, 0.8621s/100 iter), loss = 0.155273
I0122 16:26:51.903218 47756 solver.cpp:285]     Train net output #0: loss = 0.155273 (* 1 = 0.155273 loss)
I0122 16:26:51.903224 47756 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:26:52.757230 47756 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:26:52.976651 47756 solver.cpp:517]     Test net output #0: loss = 0.499976 (* 1 = 0.499976 loss)
I0122 16:26:52.976666 47756 solver.cpp:517]     Test net output #1: top-1 = 0.847222
I0122 16:26:52.976670 47756 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0122 16:26:52.984761 47756 solver.cpp:266] Iteration 7000 (92.465 iter/s, 1.08149s/100 iter), loss = 0.207928
I0122 16:26:52.984778 47756 solver.cpp:285]     Train net output #0: loss = 0.207928 (* 1 = 0.207928 loss)
I0122 16:26:52.984798 47756 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:26:53.846433 47756 solver.cpp:266] Iteration 7100 (116.062 iter/s, 0.861609s/100 iter), loss = 0.186891
I0122 16:26:53.846460 47756 solver.cpp:285]     Train net output #0: loss = 0.186891 (* 1 = 0.186891 loss)
I0122 16:26:53.846467 47756 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:26:54.708426 47756 solver.cpp:266] Iteration 7200 (116.02 iter/s, 0.86192s/100 iter), loss = 0.118627
I0122 16:26:54.708456 47756 solver.cpp:285]     Train net output #0: loss = 0.118627 (* 1 = 0.118627 loss)
I0122 16:26:54.708461 47756 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:26:55.570297 47756 solver.cpp:266] Iteration 7300 (116.037 iter/s, 0.861795s/100 iter), loss = 0.249344
I0122 16:26:55.570324 47756 solver.cpp:285]     Train net output #0: loss = 0.249344 (* 1 = 0.249344 loss)
I0122 16:26:55.570329 47756 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:26:56.544340 47756 solver.cpp:266] Iteration 7400 (102.673 iter/s, 0.973964s/100 iter), loss = 0.116834
I0122 16:26:56.544371 47756 solver.cpp:285]     Train net output #0: loss = 0.116834 (* 1 = 0.116834 loss)
I0122 16:26:56.544376 47756 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:26:57.406261 47756 solver.cpp:266] Iteration 7500 (116.03 iter/s, 0.861845s/100 iter), loss = 0.169249
I0122 16:26:57.406291 47756 solver.cpp:285]     Train net output #0: loss = 0.169249 (* 1 = 0.169249 loss)
I0122 16:26:57.406296 47756 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:26:58.268113 47756 solver.cpp:266] Iteration 7600 (116.039 iter/s, 0.861778s/100 iter), loss = 0.212299
I0122 16:26:58.268142 47756 solver.cpp:285]     Train net output #0: loss = 0.212299 (* 1 = 0.212299 loss)
I0122 16:26:58.268147 47756 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:26:59.129981 47756 solver.cpp:266] Iteration 7700 (116.037 iter/s, 0.861792s/100 iter), loss = 0.116924
I0122 16:26:59.130008 47756 solver.cpp:285]     Train net output #0: loss = 0.116923 (* 1 = 0.116923 loss)
I0122 16:26:59.130013 47756 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:26:59.991734 47756 solver.cpp:266] Iteration 7800 (116.052 iter/s, 0.861681s/100 iter), loss = 0.20213
I0122 16:26:59.991780 47756 solver.cpp:285]     Train net output #0: loss = 0.20213 (* 1 = 0.20213 loss)
I0122 16:26:59.991784 47756 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:27:00.854583 47756 solver.cpp:266] Iteration 7900 (115.907 iter/s, 0.86276s/100 iter), loss = 0.170766
I0122 16:27:00.854612 47756 solver.cpp:285]     Train net output #0: loss = 0.170766 (* 1 = 0.170766 loss)
I0122 16:27:00.854619 47756 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:27:01.708240 47756 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:27:01.927073 47756 solver.cpp:517]     Test net output #0: loss = 0.531845 (* 1 = 0.531845 loss)
I0122 16:27:01.927095 47756 solver.cpp:517]     Test net output #1: top-1 = 0.840889
I0122 16:27:01.927100 47756 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:27:01.935205 47756 solver.cpp:266] Iteration 8000 (92.5462 iter/s, 1.08054s/100 iter), loss = 0.185079
I0122 16:27:01.935223 47756 solver.cpp:285]     Train net output #0: loss = 0.185079 (* 1 = 0.185079 loss)
I0122 16:27:01.935230 47756 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:27:02.796984 47756 solver.cpp:266] Iteration 8100 (116.048 iter/s, 0.861715s/100 iter), loss = 0.149001
I0122 16:27:02.797011 47756 solver.cpp:285]     Train net output #0: loss = 0.149001 (* 1 = 0.149001 loss)
I0122 16:27:02.797017 47756 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:27:03.660964 47756 solver.cpp:266] Iteration 8200 (115.753 iter/s, 0.863908s/100 iter), loss = 0.0946664
I0122 16:27:03.660992 47756 solver.cpp:285]     Train net output #0: loss = 0.0946663 (* 1 = 0.0946663 loss)
I0122 16:27:03.660998 47756 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:27:04.534049 47756 solver.cpp:266] Iteration 8300 (114.546 iter/s, 0.87301s/100 iter), loss = 0.141082
I0122 16:27:04.534078 47756 solver.cpp:285]     Train net output #0: loss = 0.141082 (* 1 = 0.141082 loss)
I0122 16:27:04.534085 47756 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:27:05.415887 47756 solver.cpp:266] Iteration 8400 (113.409 iter/s, 0.881762s/100 iter), loss = 0.13751
I0122 16:27:05.415916 47756 solver.cpp:285]     Train net output #0: loss = 0.13751 (* 1 = 0.13751 loss)
I0122 16:27:05.415922 47756 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:27:06.278748 47756 solver.cpp:266] Iteration 8500 (115.904 iter/s, 0.862783s/100 iter), loss = 0.169537
I0122 16:27:06.278775 47756 solver.cpp:285]     Train net output #0: loss = 0.169537 (* 1 = 0.169537 loss)
I0122 16:27:06.278781 47756 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:27:07.140240 47756 solver.cpp:266] Iteration 8600 (116.087 iter/s, 0.861419s/100 iter), loss = 0.171199
I0122 16:27:07.140267 47756 solver.cpp:285]     Train net output #0: loss = 0.171199 (* 1 = 0.171199 loss)
I0122 16:27:07.140272 47756 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:27:08.002634 47756 solver.cpp:266] Iteration 8700 (115.966 iter/s, 0.86232s/100 iter), loss = 0.164058
I0122 16:27:08.002660 47756 solver.cpp:285]     Train net output #0: loss = 0.164058 (* 1 = 0.164058 loss)
I0122 16:27:08.002665 47756 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:27:08.864627 47756 solver.cpp:266] Iteration 8800 (116.02 iter/s, 0.861922s/100 iter), loss = 0.224523
I0122 16:27:08.864653 47756 solver.cpp:285]     Train net output #0: loss = 0.224523 (* 1 = 0.224523 loss)
I0122 16:27:08.864658 47756 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:27:09.728044 47756 solver.cpp:266] Iteration 8900 (115.829 iter/s, 0.863345s/100 iter), loss = 0.152805
I0122 16:27:09.728070 47756 solver.cpp:285]     Train net output #0: loss = 0.152805 (* 1 = 0.152805 loss)
I0122 16:27:09.728075 47756 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:27:10.583124 47756 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:27:10.805917 47756 solver.cpp:517]     Test net output #0: loss = 0.485218 (* 1 = 0.485218 loss)
I0122 16:27:10.805932 47756 solver.cpp:517]     Test net output #1: top-1 = 0.849111
I0122 16:27:10.805959 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992556
I0122 16:27:10.814055 47756 solver.cpp:266] Iteration 9000 (92.0866 iter/s, 1.08593s/100 iter), loss = 0.10066
I0122 16:27:10.814074 47756 solver.cpp:285]     Train net output #0: loss = 0.10066 (* 1 = 0.10066 loss)
I0122 16:27:10.814081 47756 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:27:11.680786 47756 solver.cpp:266] Iteration 9100 (115.385 iter/s, 0.866665s/100 iter), loss = 0.0772672
I0122 16:27:11.680814 47756 solver.cpp:285]     Train net output #0: loss = 0.0772672 (* 1 = 0.0772672 loss)
I0122 16:27:11.680820 47756 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:27:12.555647 47756 solver.cpp:266] Iteration 9200 (114.313 iter/s, 0.874788s/100 iter), loss = 0.129807
I0122 16:27:12.555676 47756 solver.cpp:285]     Train net output #0: loss = 0.129807 (* 1 = 0.129807 loss)
I0122 16:27:12.555682 47756 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:27:13.419860 47756 solver.cpp:266] Iteration 9300 (115.722 iter/s, 0.864138s/100 iter), loss = 0.112455
I0122 16:27:13.419888 47756 solver.cpp:285]     Train net output #0: loss = 0.112455 (* 1 = 0.112455 loss)
I0122 16:27:13.419894 47756 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:27:14.287178 47756 solver.cpp:266] Iteration 9400 (115.308 iter/s, 0.867244s/100 iter), loss = 0.113498
I0122 16:27:14.287205 47756 solver.cpp:285]     Train net output #0: loss = 0.113498 (* 1 = 0.113498 loss)
I0122 16:27:14.287210 47756 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:27:15.153187 47756 solver.cpp:266] Iteration 9500 (115.482 iter/s, 0.865936s/100 iter), loss = 0.139651
I0122 16:27:15.153215 47756 solver.cpp:285]     Train net output #0: loss = 0.139651 (* 1 = 0.139651 loss)
I0122 16:27:15.153221 47756 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:27:16.017033 47756 solver.cpp:266] Iteration 9600 (115.771 iter/s, 0.863772s/100 iter), loss = 0.131048
I0122 16:27:16.017062 47756 solver.cpp:285]     Train net output #0: loss = 0.131048 (* 1 = 0.131048 loss)
I0122 16:27:16.017067 47756 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:27:16.880834 47756 solver.cpp:266] Iteration 9700 (115.777 iter/s, 0.863727s/100 iter), loss = 0.209833
I0122 16:27:16.880863 47756 solver.cpp:285]     Train net output #0: loss = 0.209833 (* 1 = 0.209833 loss)
I0122 16:27:16.880869 47756 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:27:17.748270 47756 solver.cpp:266] Iteration 9800 (115.292 iter/s, 0.867361s/100 iter), loss = 0.169451
I0122 16:27:17.748296 47756 solver.cpp:285]     Train net output #0: loss = 0.169451 (* 1 = 0.169451 loss)
I0122 16:27:17.748301 47756 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:27:18.612604 47756 solver.cpp:266] Iteration 9900 (115.705 iter/s, 0.864263s/100 iter), loss = 0.177919
I0122 16:27:18.612699 47756 solver.cpp:285]     Train net output #0: loss = 0.177919 (* 1 = 0.177919 loss)
I0122 16:27:18.612705 47756 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:27:19.468605 47756 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:27:19.688657 47756 solver.cpp:517]     Test net output #0: loss = 0.483197 (* 1 = 0.483197 loss)
I0122 16:27:19.688673 47756 solver.cpp:517]     Test net output #1: top-1 = 0.852111
I0122 16:27:19.688678 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0122 16:27:19.696745 47756 solver.cpp:266] Iteration 10000 (92.2511 iter/s, 1.084s/100 iter), loss = 0.168869
I0122 16:27:19.696764 47756 solver.cpp:285]     Train net output #0: loss = 0.168869 (* 1 = 0.168869 loss)
I0122 16:27:19.696770 47756 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:27:20.562495 47756 solver.cpp:266] Iteration 10100 (115.516 iter/s, 0.865685s/100 iter), loss = 0.104145
I0122 16:27:20.562521 47756 solver.cpp:285]     Train net output #0: loss = 0.104145 (* 1 = 0.104145 loss)
I0122 16:27:20.562527 47756 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:27:21.424502 47756 solver.cpp:266] Iteration 10200 (116.018 iter/s, 0.861936s/100 iter), loss = 0.155565
I0122 16:27:21.424527 47756 solver.cpp:285]     Train net output #0: loss = 0.155565 (* 1 = 0.155565 loss)
I0122 16:27:21.424533 47756 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:27:22.286434 47756 solver.cpp:266] Iteration 10300 (116.028 iter/s, 0.861862s/100 iter), loss = 0.0957773
I0122 16:27:22.286461 47756 solver.cpp:285]     Train net output #0: loss = 0.0957773 (* 1 = 0.0957773 loss)
I0122 16:27:22.286466 47756 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:27:23.152652 47756 solver.cpp:266] Iteration 10400 (115.454 iter/s, 0.866145s/100 iter), loss = 0.204616
I0122 16:27:23.152678 47756 solver.cpp:285]     Train net output #0: loss = 0.204616 (* 1 = 0.204616 loss)
I0122 16:27:23.152683 47756 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:27:24.024858 47756 solver.cpp:266] Iteration 10500 (114.661 iter/s, 0.872134s/100 iter), loss = 0.106298
I0122 16:27:24.024885 47756 solver.cpp:285]     Train net output #0: loss = 0.106298 (* 1 = 0.106298 loss)
I0122 16:27:24.024891 47756 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:27:24.889425 47756 solver.cpp:266] Iteration 10600 (115.675 iter/s, 0.864492s/100 iter), loss = 0.136108
I0122 16:27:24.889453 47756 solver.cpp:285]     Train net output #0: loss = 0.136108 (* 1 = 0.136108 loss)
I0122 16:27:24.889459 47756 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:27:25.763871 47756 solver.cpp:266] Iteration 10700 (114.368 iter/s, 0.874373s/100 iter), loss = 0.143533
I0122 16:27:25.763901 47756 solver.cpp:285]     Train net output #0: loss = 0.143533 (* 1 = 0.143533 loss)
I0122 16:27:25.763906 47756 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:27:26.630897 47756 solver.cpp:266] Iteration 10800 (115.347 iter/s, 0.866951s/100 iter), loss = 0.101691
I0122 16:27:26.630925 47756 solver.cpp:285]     Train net output #0: loss = 0.101691 (* 1 = 0.101691 loss)
I0122 16:27:26.630930 47756 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:27:27.495340 47756 solver.cpp:266] Iteration 10900 (115.691 iter/s, 0.86437s/100 iter), loss = 0.124488
I0122 16:27:27.495368 47756 solver.cpp:285]     Train net output #0: loss = 0.124488 (* 1 = 0.124488 loss)
I0122 16:27:27.495373 47756 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:27:28.352165 47756 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:27:28.572837 47756 solver.cpp:517]     Test net output #0: loss = 0.540641 (* 1 = 0.540641 loss)
I0122 16:27:28.572852 47756 solver.cpp:517]     Test net output #1: top-1 = 0.838666
I0122 16:27:28.572857 47756 solver.cpp:517]     Test net output #2: top-5 = 0.986778
I0122 16:27:28.581071 47756 solver.cpp:266] Iteration 11000 (92.1106 iter/s, 1.08565s/100 iter), loss = 0.120561
I0122 16:27:28.581089 47756 solver.cpp:285]     Train net output #0: loss = 0.120561 (* 1 = 0.120561 loss)
I0122 16:27:28.581094 47756 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:27:29.444306 47756 solver.cpp:266] Iteration 11100 (115.852 iter/s, 0.863172s/100 iter), loss = 0.0985752
I0122 16:27:29.444332 47756 solver.cpp:285]     Train net output #0: loss = 0.0985752 (* 1 = 0.0985752 loss)
I0122 16:27:29.444339 47756 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:27:30.305919 47756 solver.cpp:266] Iteration 11200 (116.071 iter/s, 0.861542s/100 iter), loss = 0.131365
I0122 16:27:30.305946 47756 solver.cpp:285]     Train net output #0: loss = 0.131365 (* 1 = 0.131365 loss)
I0122 16:27:30.305951 47756 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:27:31.173300 47756 solver.cpp:266] Iteration 11300 (115.299 iter/s, 0.86731s/100 iter), loss = 0.133723
I0122 16:27:31.173326 47756 solver.cpp:285]     Train net output #0: loss = 0.133723 (* 1 = 0.133723 loss)
I0122 16:27:31.173332 47756 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:27:32.045002 47756 solver.cpp:266] Iteration 11400 (114.728 iter/s, 0.871628s/100 iter), loss = 0.119201
I0122 16:27:32.045028 47756 solver.cpp:285]     Train net output #0: loss = 0.119201 (* 1 = 0.119201 loss)
I0122 16:27:32.045034 47756 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:27:32.916345 47756 solver.cpp:266] Iteration 11500 (114.775 iter/s, 0.871271s/100 iter), loss = 0.130943
I0122 16:27:32.916373 47756 solver.cpp:285]     Train net output #0: loss = 0.130943 (* 1 = 0.130943 loss)
I0122 16:27:32.916378 47756 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:27:33.781390 47756 solver.cpp:266] Iteration 11600 (115.611 iter/s, 0.864973s/100 iter), loss = 0.097345
I0122 16:27:33.781416 47756 solver.cpp:285]     Train net output #0: loss = 0.0973449 (* 1 = 0.0973449 loss)
I0122 16:27:33.781421 47756 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:27:34.670125 47756 solver.cpp:266] Iteration 11700 (112.529 iter/s, 0.888661s/100 iter), loss = 0.233492
I0122 16:27:34.670153 47756 solver.cpp:285]     Train net output #0: loss = 0.233492 (* 1 = 0.233492 loss)
I0122 16:27:34.670159 47756 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:27:35.545712 47756 solver.cpp:266] Iteration 11800 (114.219 iter/s, 0.875513s/100 iter), loss = 0.134589
I0122 16:27:35.545740 47756 solver.cpp:285]     Train net output #0: loss = 0.134589 (* 1 = 0.134589 loss)
I0122 16:27:35.545745 47756 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:27:36.431633 47756 solver.cpp:266] Iteration 11900 (112.886 iter/s, 0.885846s/100 iter), loss = 0.147108
I0122 16:27:36.431660 47756 solver.cpp:285]     Train net output #0: loss = 0.147108 (* 1 = 0.147108 loss)
I0122 16:27:36.431666 47756 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:27:37.302923 47756 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:27:37.524085 47756 solver.cpp:517]     Test net output #0: loss = 0.485201 (* 1 = 0.485201 loss)
I0122 16:27:37.524099 47756 solver.cpp:517]     Test net output #1: top-1 = 0.856111
I0122 16:27:37.524103 47756 solver.cpp:517]     Test net output #2: top-5 = 0.989556
I0122 16:27:37.532181 47756 solver.cpp:266] Iteration 12000 (90.8704 iter/s, 1.10047s/100 iter), loss = 0.0917297
I0122 16:27:37.532200 47756 solver.cpp:285]     Train net output #0: loss = 0.0917297 (* 1 = 0.0917297 loss)
I0122 16:27:37.532207 47756 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:27:38.394629 47756 solver.cpp:266] Iteration 12100 (115.958 iter/s, 0.862382s/100 iter), loss = 0.0922335
I0122 16:27:38.394654 47756 solver.cpp:285]     Train net output #0: loss = 0.0922335 (* 1 = 0.0922335 loss)
I0122 16:27:38.394660 47756 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:27:39.256546 47756 solver.cpp:266] Iteration 12200 (116.03 iter/s, 0.861849s/100 iter), loss = 0.232771
I0122 16:27:39.256572 47756 solver.cpp:285]     Train net output #0: loss = 0.232771 (* 1 = 0.232771 loss)
I0122 16:27:39.256577 47756 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:27:40.118822 47756 solver.cpp:266] Iteration 12300 (115.982 iter/s, 0.862206s/100 iter), loss = 0.20097
I0122 16:27:40.118875 47756 solver.cpp:285]     Train net output #0: loss = 0.20097 (* 1 = 0.20097 loss)
I0122 16:27:40.118881 47756 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:27:40.981374 47756 solver.cpp:266] Iteration 12400 (115.948 iter/s, 0.862454s/100 iter), loss = 0.134006
I0122 16:27:40.981401 47756 solver.cpp:285]     Train net output #0: loss = 0.134006 (* 1 = 0.134006 loss)
I0122 16:27:40.981406 47756 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:27:41.862354 47756 solver.cpp:266] Iteration 12500 (113.52 iter/s, 0.880906s/100 iter), loss = 0.106144
I0122 16:27:41.862381 47756 solver.cpp:285]     Train net output #0: loss = 0.106144 (* 1 = 0.106144 loss)
I0122 16:27:41.862387 47756 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:27:42.751550 47756 solver.cpp:266] Iteration 12600 (112.471 iter/s, 0.889121s/100 iter), loss = 0.19554
I0122 16:27:42.751577 47756 solver.cpp:285]     Train net output #0: loss = 0.19554 (* 1 = 0.19554 loss)
I0122 16:27:42.751582 47756 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:27:43.636626 47756 solver.cpp:266] Iteration 12700 (112.994 iter/s, 0.885003s/100 iter), loss = 0.199976
I0122 16:27:43.636654 47756 solver.cpp:285]     Train net output #0: loss = 0.199976 (* 1 = 0.199976 loss)
I0122 16:27:43.636660 47756 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:27:44.527216 47756 solver.cpp:266] Iteration 12800 (112.294 iter/s, 0.890516s/100 iter), loss = 0.13035
I0122 16:27:44.527245 47756 solver.cpp:285]     Train net output #0: loss = 0.13035 (* 1 = 0.13035 loss)
I0122 16:27:44.527249 47756 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:27:45.399158 47756 solver.cpp:266] Iteration 12900 (114.696 iter/s, 0.871868s/100 iter), loss = 0.15255
I0122 16:27:45.399188 47756 solver.cpp:285]     Train net output #0: loss = 0.15255 (* 1 = 0.15255 loss)
I0122 16:27:45.399195 47756 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:27:46.367377 47756 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:27:46.587147 47756 solver.cpp:517]     Test net output #0: loss = 0.447113 (* 1 = 0.447113 loss)
I0122 16:27:46.587162 47756 solver.cpp:517]     Test net output #1: top-1 = 0.857889
I0122 16:27:46.587167 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992111
I0122 16:27:46.595297 47756 solver.cpp:266] Iteration 13000 (83.6084 iter/s, 1.19605s/100 iter), loss = 0.10721
I0122 16:27:46.595315 47756 solver.cpp:285]     Train net output #0: loss = 0.10721 (* 1 = 0.10721 loss)
I0122 16:27:46.595321 47756 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:27:47.554237 47756 solver.cpp:266] Iteration 13100 (104.289 iter/s, 0.958872s/100 iter), loss = 0.163198
I0122 16:27:47.554268 47756 solver.cpp:285]     Train net output #0: loss = 0.163198 (* 1 = 0.163198 loss)
I0122 16:27:47.554314 47756 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:27:48.568779 47756 solver.cpp:266] Iteration 13200 (98.5792 iter/s, 1.01441s/100 iter), loss = 0.148558
I0122 16:27:48.568809 47756 solver.cpp:285]     Train net output #0: loss = 0.148558 (* 1 = 0.148558 loss)
I0122 16:27:48.568856 47756 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:27:49.597479 47756 solver.cpp:266] Iteration 13300 (97.2223 iter/s, 1.02857s/100 iter), loss = 0.118368
I0122 16:27:49.597601 47756 solver.cpp:285]     Train net output #0: loss = 0.118368 (* 1 = 0.118368 loss)
I0122 16:27:49.597643 47756 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:27:50.632398 47756 solver.cpp:266] Iteration 13400 (96.6461 iter/s, 1.0347s/100 iter), loss = 0.122102
I0122 16:27:50.632429 47756 solver.cpp:285]     Train net output #0: loss = 0.122102 (* 1 = 0.122102 loss)
I0122 16:27:50.632474 47756 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:27:51.665839 47756 solver.cpp:266] Iteration 13500 (96.7761 iter/s, 1.03331s/100 iter), loss = 0.0926915
I0122 16:27:51.665870 47756 solver.cpp:285]     Train net output #0: loss = 0.0926914 (* 1 = 0.0926914 loss)
I0122 16:27:51.665916 47756 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:27:52.574795 47756 solver.cpp:266] Iteration 13600 (110.031 iter/s, 0.908832s/100 iter), loss = 0.115279
I0122 16:27:52.574826 47756 solver.cpp:285]     Train net output #0: loss = 0.115279 (* 1 = 0.115279 loss)
I0122 16:27:52.574831 47756 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:27:53.437144 47756 solver.cpp:266] Iteration 13700 (115.973 iter/s, 0.862272s/100 iter), loss = 0.0949261
I0122 16:27:53.437173 47756 solver.cpp:285]     Train net output #0: loss = 0.094926 (* 1 = 0.094926 loss)
I0122 16:27:53.437180 47756 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:27:54.441174 47756 solver.cpp:266] Iteration 13800 (99.6068 iter/s, 1.00395s/100 iter), loss = 0.137759
I0122 16:27:54.441205 47756 solver.cpp:285]     Train net output #0: loss = 0.137759 (* 1 = 0.137759 loss)
I0122 16:27:54.441253 47756 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:27:55.348992 47756 solver.cpp:266] Iteration 13900 (110.17 iter/s, 0.907692s/100 iter), loss = 0.121172
I0122 16:27:55.349022 47756 solver.cpp:285]     Train net output #0: loss = 0.121172 (* 1 = 0.121172 loss)
I0122 16:27:55.349071 47756 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:27:56.374635 47756 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:27:56.639504 47756 solver.cpp:517]     Test net output #0: loss = 0.482273 (* 1 = 0.482273 loss)
I0122 16:27:56.639523 47756 solver.cpp:517]     Test net output #1: top-1 = 0.856778
I0122 16:27:56.639526 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0122 16:27:56.647631 47756 solver.cpp:266] Iteration 14000 (77.0118 iter/s, 1.2985s/100 iter), loss = 0.110577
I0122 16:27:56.647651 47756 solver.cpp:285]     Train net output #0: loss = 0.110577 (* 1 = 0.110577 loss)
I0122 16:27:56.647658 47756 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:27:57.509740 47756 solver.cpp:266] Iteration 14100 (116.003 iter/s, 0.862043s/100 iter), loss = 0.0923865
I0122 16:27:57.509770 47756 solver.cpp:285]     Train net output #0: loss = 0.0923864 (* 1 = 0.0923864 loss)
I0122 16:27:57.509776 47756 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:27:58.371069 47756 solver.cpp:266] Iteration 14200 (116.11 iter/s, 0.861253s/100 iter), loss = 0.132121
I0122 16:27:58.371099 47756 solver.cpp:285]     Train net output #0: loss = 0.13212 (* 1 = 0.13212 loss)
I0122 16:27:58.371105 47756 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:27:59.232723 47756 solver.cpp:266] Iteration 14300 (116.066 iter/s, 0.861578s/100 iter), loss = 0.127621
I0122 16:27:59.232753 47756 solver.cpp:285]     Train net output #0: loss = 0.127621 (* 1 = 0.127621 loss)
I0122 16:27:59.232758 47756 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:28:00.105259 47756 solver.cpp:266] Iteration 14400 (114.618 iter/s, 0.87246s/100 iter), loss = 0.125078
I0122 16:28:00.105288 47756 solver.cpp:285]     Train net output #0: loss = 0.125078 (* 1 = 0.125078 loss)
I0122 16:28:00.105293 47756 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:28:00.989851 47756 solver.cpp:266] Iteration 14500 (113.056 iter/s, 0.884517s/100 iter), loss = 0.0679767
I0122 16:28:00.989881 47756 solver.cpp:285]     Train net output #0: loss = 0.0679767 (* 1 = 0.0679767 loss)
I0122 16:28:00.989887 47756 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:28:01.859019 47756 solver.cpp:266] Iteration 14600 (115.063 iter/s, 0.869092s/100 iter), loss = 0.113201
I0122 16:28:01.859046 47756 solver.cpp:285]     Train net output #0: loss = 0.113201 (* 1 = 0.113201 loss)
I0122 16:28:01.859052 47756 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:28:02.736202 47756 solver.cpp:266] Iteration 14700 (114.011 iter/s, 0.87711s/100 iter), loss = 0.0662012
I0122 16:28:02.736230 47756 solver.cpp:285]     Train net output #0: loss = 0.0662011 (* 1 = 0.0662011 loss)
I0122 16:28:02.736237 47756 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:28:03.621592 47756 solver.cpp:266] Iteration 14800 (112.954 iter/s, 0.885316s/100 iter), loss = 0.166722
I0122 16:28:03.621623 47756 solver.cpp:285]     Train net output #0: loss = 0.166722 (* 1 = 0.166722 loss)
I0122 16:28:03.621628 47756 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:28:04.508113 47756 solver.cpp:266] Iteration 14900 (112.81 iter/s, 0.886444s/100 iter), loss = 0.0888793
I0122 16:28:04.508138 47756 solver.cpp:285]     Train net output #0: loss = 0.0888793 (* 1 = 0.0888793 loss)
I0122 16:28:04.508144 47756 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:28:05.363281 47756 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:28:05.582543 47756 solver.cpp:517]     Test net output #0: loss = 0.472337 (* 1 = 0.472337 loss)
I0122 16:28:05.582557 47756 solver.cpp:517]     Test net output #1: top-1 = 0.855444
I0122 16:28:05.582562 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0122 16:28:05.590646 47756 solver.cpp:266] Iteration 15000 (92.3825 iter/s, 1.08246s/100 iter), loss = 0.155279
I0122 16:28:05.590672 47756 solver.cpp:285]     Train net output #0: loss = 0.155279 (* 1 = 0.155279 loss)
I0122 16:28:05.590682 47756 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:28:06.452571 47756 solver.cpp:266] Iteration 15100 (116.027 iter/s, 0.861865s/100 iter), loss = 0.0862235
I0122 16:28:06.452597 47756 solver.cpp:285]     Train net output #0: loss = 0.0862234 (* 1 = 0.0862234 loss)
I0122 16:28:06.452602 47756 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:28:07.314100 47756 solver.cpp:266] Iteration 15200 (116.082 iter/s, 0.861459s/100 iter), loss = 0.209531
I0122 16:28:07.314127 47756 solver.cpp:285]     Train net output #0: loss = 0.209531 (* 1 = 0.209531 loss)
I0122 16:28:07.314132 47756 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:28:08.175642 47756 solver.cpp:266] Iteration 15300 (116.081 iter/s, 0.861469s/100 iter), loss = 0.134272
I0122 16:28:08.175667 47756 solver.cpp:285]     Train net output #0: loss = 0.134272 (* 1 = 0.134272 loss)
I0122 16:28:08.175673 47756 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:28:09.045317 47756 solver.cpp:266] Iteration 15400 (114.995 iter/s, 0.869605s/100 iter), loss = 0.13062
I0122 16:28:09.045347 47756 solver.cpp:285]     Train net output #0: loss = 0.13062 (* 1 = 0.13062 loss)
I0122 16:28:09.045353 47756 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:28:09.906695 47756 solver.cpp:266] Iteration 15500 (116.103 iter/s, 0.861305s/100 iter), loss = 0.115114
I0122 16:28:09.906723 47756 solver.cpp:285]     Train net output #0: loss = 0.115114 (* 1 = 0.115114 loss)
I0122 16:28:09.906728 47756 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:28:10.768625 47756 solver.cpp:266] Iteration 15600 (116.029 iter/s, 0.861857s/100 iter), loss = 0.127049
I0122 16:28:10.768652 47756 solver.cpp:285]     Train net output #0: loss = 0.127049 (* 1 = 0.127049 loss)
I0122 16:28:10.768657 47756 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:28:11.630008 47756 solver.cpp:266] Iteration 15700 (116.102 iter/s, 0.861312s/100 iter), loss = 0.103737
I0122 16:28:11.630033 47756 solver.cpp:285]     Train net output #0: loss = 0.103737 (* 1 = 0.103737 loss)
I0122 16:28:11.630039 47756 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:28:12.492012 47756 solver.cpp:266] Iteration 15800 (116.018 iter/s, 0.861933s/100 iter), loss = 0.0693029
I0122 16:28:12.492038 47756 solver.cpp:285]     Train net output #0: loss = 0.0693029 (* 1 = 0.0693029 loss)
I0122 16:28:12.492059 47756 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:28:13.353598 47756 solver.cpp:266] Iteration 15900 (116.074 iter/s, 0.861517s/100 iter), loss = 0.137388
I0122 16:28:13.353624 47756 solver.cpp:285]     Train net output #0: loss = 0.137388 (* 1 = 0.137388 loss)
I0122 16:28:13.353631 47756 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:28:14.206738 47756 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:28:14.426096 47756 solver.cpp:517]     Test net output #0: loss = 0.447903 (* 1 = 0.447903 loss)
I0122 16:28:14.426110 47756 solver.cpp:517]     Test net output #1: top-1 = 0.862
I0122 16:28:14.426115 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0122 16:28:14.434170 47756 solver.cpp:266] Iteration 16000 (92.5502 iter/s, 1.08049s/100 iter), loss = 0.141171
I0122 16:28:14.434187 47756 solver.cpp:285]     Train net output #0: loss = 0.141171 (* 1 = 0.141171 loss)
I0122 16:28:14.434193 47756 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:28:15.296205 47756 solver.cpp:266] Iteration 16100 (116.013 iter/s, 0.861972s/100 iter), loss = 0.128233
I0122 16:28:15.296229 47756 solver.cpp:285]     Train net output #0: loss = 0.128233 (* 1 = 0.128233 loss)
I0122 16:28:15.296236 47756 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:28:16.157765 47756 solver.cpp:266] Iteration 16200 (116.078 iter/s, 0.861491s/100 iter), loss = 0.0974227
I0122 16:28:16.157791 47756 solver.cpp:285]     Train net output #0: loss = 0.0974226 (* 1 = 0.0974226 loss)
I0122 16:28:16.157796 47756 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:28:17.020009 47756 solver.cpp:266] Iteration 16300 (115.986 iter/s, 0.862175s/100 iter), loss = 0.101219
I0122 16:28:17.020035 47756 solver.cpp:285]     Train net output #0: loss = 0.101219 (* 1 = 0.101219 loss)
I0122 16:28:17.020040 47756 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:28:17.881821 47756 solver.cpp:266] Iteration 16400 (116.044 iter/s, 0.861742s/100 iter), loss = 0.125195
I0122 16:28:17.881847 47756 solver.cpp:285]     Train net output #0: loss = 0.125195 (* 1 = 0.125195 loss)
I0122 16:28:17.881852 47756 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:28:18.743955 47756 solver.cpp:266] Iteration 16500 (116.001 iter/s, 0.862064s/100 iter), loss = 0.0760219
I0122 16:28:18.743981 47756 solver.cpp:285]     Train net output #0: loss = 0.0760219 (* 1 = 0.0760219 loss)
I0122 16:28:18.743988 47756 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:28:19.605620 47756 solver.cpp:266] Iteration 16600 (116.064 iter/s, 0.861595s/100 iter), loss = 0.131296
I0122 16:28:19.605768 47756 solver.cpp:285]     Train net output #0: loss = 0.131295 (* 1 = 0.131295 loss)
I0122 16:28:19.605775 47756 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:28:20.467531 47756 solver.cpp:266] Iteration 16700 (116.047 iter/s, 0.861721s/100 iter), loss = 0.116443
I0122 16:28:20.467556 47756 solver.cpp:285]     Train net output #0: loss = 0.116443 (* 1 = 0.116443 loss)
I0122 16:28:20.467562 47756 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:28:21.330349 47756 solver.cpp:266] Iteration 16800 (115.909 iter/s, 0.862749s/100 iter), loss = 0.0923985
I0122 16:28:21.330389 47756 solver.cpp:285]     Train net output #0: loss = 0.0923984 (* 1 = 0.0923984 loss)
I0122 16:28:21.330394 47756 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:28:22.194000 47756 solver.cpp:266] Iteration 16900 (115.799 iter/s, 0.863568s/100 iter), loss = 0.135188
I0122 16:28:22.194027 47756 solver.cpp:285]     Train net output #0: loss = 0.135188 (* 1 = 0.135188 loss)
I0122 16:28:22.194033 47756 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:28:23.064889 47756 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:28:23.290391 47756 solver.cpp:517]     Test net output #0: loss = 0.444096 (* 1 = 0.444096 loss)
I0122 16:28:23.290405 47756 solver.cpp:517]     Test net output #1: top-1 = 0.860444
I0122 16:28:23.290410 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992889
I0122 16:28:23.298490 47756 solver.cpp:266] Iteration 17000 (90.5461 iter/s, 1.10441s/100 iter), loss = 0.177951
I0122 16:28:23.298506 47756 solver.cpp:285]     Train net output #0: loss = 0.177951 (* 1 = 0.177951 loss)
I0122 16:28:23.298512 47756 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:28:24.160795 47756 solver.cpp:266] Iteration 17100 (115.976 iter/s, 0.862244s/100 iter), loss = 0.116343
I0122 16:28:24.160821 47756 solver.cpp:285]     Train net output #0: loss = 0.116343 (* 1 = 0.116343 loss)
I0122 16:28:24.160826 47756 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:28:25.022513 47756 solver.cpp:266] Iteration 17200 (116.057 iter/s, 0.861648s/100 iter), loss = 0.0688583
I0122 16:28:25.022541 47756 solver.cpp:285]     Train net output #0: loss = 0.0688583 (* 1 = 0.0688583 loss)
I0122 16:28:25.022547 47756 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:28:25.885102 47756 solver.cpp:266] Iteration 17300 (115.94 iter/s, 0.862518s/100 iter), loss = 0.140424
I0122 16:28:25.885128 47756 solver.cpp:285]     Train net output #0: loss = 0.140423 (* 1 = 0.140423 loss)
I0122 16:28:25.885134 47756 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:28:26.761219 47756 solver.cpp:266] Iteration 17400 (114.149 iter/s, 0.876045s/100 iter), loss = 0.110001
I0122 16:28:26.761251 47756 solver.cpp:285]     Train net output #0: loss = 0.110001 (* 1 = 0.110001 loss)
I0122 16:28:26.761296 47756 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:28:27.673962 47756 solver.cpp:266] Iteration 17500 (109.575 iter/s, 0.912619s/100 iter), loss = 0.10772
I0122 16:28:27.673993 47756 solver.cpp:285]     Train net output #0: loss = 0.107719 (* 1 = 0.107719 loss)
I0122 16:28:27.673998 47756 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:28:28.534790 47756 solver.cpp:266] Iteration 17600 (116.177 iter/s, 0.860754s/100 iter), loss = 0.0638681
I0122 16:28:28.534819 47756 solver.cpp:285]     Train net output #0: loss = 0.063868 (* 1 = 0.063868 loss)
I0122 16:28:28.534826 47756 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:28:29.396703 47756 solver.cpp:266] Iteration 17700 (116.031 iter/s, 0.861839s/100 iter), loss = 0.137853
I0122 16:28:29.396730 47756 solver.cpp:285]     Train net output #0: loss = 0.137853 (* 1 = 0.137853 loss)
I0122 16:28:29.396736 47756 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:28:30.258133 47756 solver.cpp:266] Iteration 17800 (116.096 iter/s, 0.861358s/100 iter), loss = 0.123569
I0122 16:28:30.258160 47756 solver.cpp:285]     Train net output #0: loss = 0.123569 (* 1 = 0.123569 loss)
I0122 16:28:30.258165 47756 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:28:31.119803 47756 solver.cpp:266] Iteration 17900 (116.063 iter/s, 0.861598s/100 iter), loss = 0.107367
I0122 16:28:31.119828 47756 solver.cpp:285]     Train net output #0: loss = 0.107367 (* 1 = 0.107367 loss)
I0122 16:28:31.119834 47756 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:28:31.972872 47756 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:28:32.192087 47756 solver.cpp:517]     Test net output #0: loss = 0.438464 (* 1 = 0.438464 loss)
I0122 16:28:32.192102 47756 solver.cpp:517]     Test net output #1: top-1 = 0.868
I0122 16:28:32.192106 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992333
I0122 16:28:32.200215 47756 solver.cpp:266] Iteration 18000 (92.5637 iter/s, 1.08034s/100 iter), loss = 0.0884972
I0122 16:28:32.200232 47756 solver.cpp:285]     Train net output #0: loss = 0.0884972 (* 1 = 0.0884972 loss)
I0122 16:28:32.200238 47756 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:28:33.062146 47756 solver.cpp:266] Iteration 18100 (116.027 iter/s, 0.86187s/100 iter), loss = 0.107857
I0122 16:28:33.062173 47756 solver.cpp:285]     Train net output #0: loss = 0.107857 (* 1 = 0.107857 loss)
I0122 16:28:33.062180 47756 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:28:33.924965 47756 solver.cpp:266] Iteration 18200 (115.909 iter/s, 0.862748s/100 iter), loss = 0.0828438
I0122 16:28:33.924994 47756 solver.cpp:285]     Train net output #0: loss = 0.0828438 (* 1 = 0.0828438 loss)
I0122 16:28:33.924999 47756 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:28:34.809159 47756 solver.cpp:266] Iteration 18300 (113.107 iter/s, 0.88412s/100 iter), loss = 0.109663
I0122 16:28:34.809188 47756 solver.cpp:285]     Train net output #0: loss = 0.109662 (* 1 = 0.109662 loss)
I0122 16:28:34.809195 47756 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:28:35.670630 47756 solver.cpp:266] Iteration 18400 (116.09 iter/s, 0.861398s/100 iter), loss = 0.099833
I0122 16:28:35.670658 47756 solver.cpp:285]     Train net output #0: loss = 0.099833 (* 1 = 0.099833 loss)
I0122 16:28:35.670663 47756 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:28:36.532279 47756 solver.cpp:266] Iteration 18500 (116.066 iter/s, 0.861577s/100 iter), loss = 0.105688
I0122 16:28:36.532306 47756 solver.cpp:285]     Train net output #0: loss = 0.105688 (* 1 = 0.105688 loss)
I0122 16:28:36.532312 47756 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:28:37.394426 47756 solver.cpp:266] Iteration 18600 (115.999 iter/s, 0.862077s/100 iter), loss = 0.0735313
I0122 16:28:37.394454 47756 solver.cpp:285]     Train net output #0: loss = 0.0735312 (* 1 = 0.0735312 loss)
I0122 16:28:37.394459 47756 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:28:38.256253 47756 solver.cpp:266] Iteration 18700 (116.042 iter/s, 0.861755s/100 iter), loss = 0.126484
I0122 16:28:38.256279 47756 solver.cpp:285]     Train net output #0: loss = 0.126484 (* 1 = 0.126484 loss)
I0122 16:28:38.256285 47756 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:28:39.118332 47756 solver.cpp:266] Iteration 18800 (116.008 iter/s, 0.862009s/100 iter), loss = 0.0645434
I0122 16:28:39.118360 47756 solver.cpp:285]     Train net output #0: loss = 0.0645434 (* 1 = 0.0645434 loss)
I0122 16:28:39.118366 47756 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:28:39.980269 47756 solver.cpp:266] Iteration 18900 (116.027 iter/s, 0.861867s/100 iter), loss = 0.0690636
I0122 16:28:39.980295 47756 solver.cpp:285]     Train net output #0: loss = 0.0690636 (* 1 = 0.0690636 loss)
I0122 16:28:39.980301 47756 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:28:40.834089 47756 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:28:41.053767 47756 solver.cpp:517]     Test net output #0: loss = 0.433059 (* 1 = 0.433059 loss)
I0122 16:28:41.053782 47756 solver.cpp:517]     Test net output #1: top-1 = 0.867778
I0122 16:28:41.053791 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992445
I0122 16:28:41.061862 47756 solver.cpp:266] Iteration 19000 (92.4626 iter/s, 1.08152s/100 iter), loss = 0.125006
I0122 16:28:41.061894 47756 solver.cpp:285]     Train net output #0: loss = 0.125006 (* 1 = 0.125006 loss)
I0122 16:28:41.061900 47756 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:28:41.935410 47756 solver.cpp:266] Iteration 19100 (114.486 iter/s, 0.873471s/100 iter), loss = 0.100648
I0122 16:28:41.935441 47756 solver.cpp:285]     Train net output #0: loss = 0.100648 (* 1 = 0.100648 loss)
I0122 16:28:41.935446 47756 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:28:42.823135 47756 solver.cpp:266] Iteration 19200 (112.657 iter/s, 0.88765s/100 iter), loss = 0.101587
I0122 16:28:42.823164 47756 solver.cpp:285]     Train net output #0: loss = 0.101587 (* 1 = 0.101587 loss)
I0122 16:28:42.823170 47756 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:28:43.686859 47756 solver.cpp:266] Iteration 19300 (115.788 iter/s, 0.863651s/100 iter), loss = 0.0874063
I0122 16:28:43.686888 47756 solver.cpp:285]     Train net output #0: loss = 0.0874063 (* 1 = 0.0874063 loss)
I0122 16:28:43.686894 47756 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:28:44.551741 47756 solver.cpp:266] Iteration 19400 (115.632 iter/s, 0.864809s/100 iter), loss = 0.0732481
I0122 16:28:44.551769 47756 solver.cpp:285]     Train net output #0: loss = 0.073248 (* 1 = 0.073248 loss)
I0122 16:28:44.551774 47756 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:28:45.417748 47756 solver.cpp:266] Iteration 19500 (115.482 iter/s, 0.865936s/100 iter), loss = 0.0707654
I0122 16:28:45.417775 47756 solver.cpp:285]     Train net output #0: loss = 0.0707654 (* 1 = 0.0707654 loss)
I0122 16:28:45.417780 47756 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:28:46.311168 47756 solver.cpp:266] Iteration 19600 (111.939 iter/s, 0.893347s/100 iter), loss = 0.105446
I0122 16:28:46.311199 47756 solver.cpp:285]     Train net output #0: loss = 0.105446 (* 1 = 0.105446 loss)
I0122 16:28:46.311246 47756 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:28:47.297241 47756 solver.cpp:266] Iteration 19700 (101.425 iter/s, 0.985947s/100 iter), loss = 0.0977899
I0122 16:28:47.297273 47756 solver.cpp:285]     Train net output #0: loss = 0.0977898 (* 1 = 0.0977898 loss)
I0122 16:28:47.297319 47756 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:28:48.207350 47756 solver.cpp:266] Iteration 19800 (109.892 iter/s, 0.909985s/100 iter), loss = 0.0701644
I0122 16:28:48.207381 47756 solver.cpp:285]     Train net output #0: loss = 0.0701643 (* 1 = 0.0701643 loss)
I0122 16:28:48.207386 47756 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:28:49.071590 47756 solver.cpp:266] Iteration 19900 (115.718 iter/s, 0.864167s/100 iter), loss = 0.137859
I0122 16:28:49.071619 47756 solver.cpp:285]     Train net output #0: loss = 0.137859 (* 1 = 0.137859 loss)
I0122 16:28:49.071625 47756 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:28:49.931282 47756 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/_iter_20000.caffemodel
I0122 16:28:49.994714 47756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.1/snapshots/_iter_20000.solverstate
I0122 16:28:50.006197 47756 solver.cpp:378] Iteration 20000, loss = 0.0117076
I0122 16:28:50.006227 47756 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:28:50.225901 47756 solver.cpp:517]     Test net output #0: loss = 0.432547 (* 1 = 0.432547 loss)
I0122 16:28:50.225920 47756 solver.cpp:517]     Test net output #1: top-1 = 0.868778
I0122 16:28:50.225924 47756 solver.cpp:517]     Test net output #2: top-5 = 0.992445
I0122 16:28:50.225929 47756 solver.cpp:386] Optimization Done (110.744 iter/s).
I0122 16:28:50.225934 47756 caffe_interface.cpp:530] Optimization Done.

# compression: second run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config2.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress2_miniVggNet.txt
I0122 16:28:50.779484 48361 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:28:50.828743 48361 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:28:52.248492 48361 pruning_runner.cpp:264] Compression complete 0.00181635%
I0122 16:28:52.873076 48361 pruning_runner.cpp:264] Compression complete 75.0005%
I0122 16:28:53.507972 48361 pruning_runner.cpp:264] Compression complete 96.0001%
I0122 16:28:54.150710 48361 pruning_runner.cpp:264] Compression complete 98%
I0122 16:28:54.783498 48361 pruning_runner.cpp:264] Compression complete 99.4924%
I0122 16:28:55.407228 48361 pruning_runner.cpp:264] Compression complete 99.8731%
I0122 16:28:56.062127 48361 pruning_runner.cpp:264] Compression complete 99.9365%
I0122 16:28:56.732396 48361 pruning_runner.cpp:264] Compression complete 99.9683%
I0122 16:28:57.383445 48361 pruning_runner.cpp:264] Compression complete 99.9841%
I0122 16:28:58.015967 48361 pruning_runner.cpp:264] Compression complete 99.998%
I0122 16:28:58.642240 48361 pruning_runner.cpp:264] Compression complete 99.9995%
I0122 16:28:59.276865 48361 pruning_runner.cpp:264] Compression complete 99.9998%
I0122 16:28:59.944653 48361 pruning_runner.cpp:264] Compression complete 99.9999%
I0122 16:29:00.569682 48361 pruning_runner.cpp:264] Compression complete 100%
I0122 16:29:01.200497 48361 pruning_runner.cpp:264] Compression complete 100%
I0122 16:29:01.824864 48361 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:29:01.825203 48361 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:29:01.825641 48361 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:29:01.825876 48361 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:29:01.826025 48361 layer_factory.hpp:77] Creating layer data
I0122 16:29:01.826076 48361 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:01.826467 48361 net.cpp:94] Creating Layer data
I0122 16:29:01.826478 48361 net.cpp:409] data -> data
I0122 16:29:01.826488 48361 net.cpp:409] data -> label
I0122 16:29:01.827491 49840 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:29:01.827538 49840 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:29:01.827675 48361 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:29:01.827786 48361 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:29:01.831012 48361 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:01.831058 48361 net.cpp:144] Setting up data
I0122 16:29:01.831066 48361 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:29:01.831071 48361 net.cpp:151] Top shape: 50 (50)
I0122 16:29:01.831074 48361 net.cpp:159] Memory required for data: 614600
I0122 16:29:01.831079 48361 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:29:01.831090 48361 net.cpp:94] Creating Layer label_data_1_split
I0122 16:29:01.831096 48361 net.cpp:435] label_data_1_split <- label
I0122 16:29:01.831106 48361 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:29:01.831118 48361 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:29:01.831130 48361 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:29:01.831231 48361 net.cpp:144] Setting up label_data_1_split
I0122 16:29:01.831239 48361 net.cpp:151] Top shape: 50 (50)
I0122 16:29:01.831243 48361 net.cpp:151] Top shape: 50 (50)
I0122 16:29:01.831248 48361 net.cpp:151] Top shape: 50 (50)
I0122 16:29:01.831251 48361 net.cpp:159] Memory required for data: 615200
I0122 16:29:01.831255 48361 layer_factory.hpp:77] Creating layer conv1
I0122 16:29:01.831267 48361 net.cpp:94] Creating Layer conv1
I0122 16:29:01.831274 48361 net.cpp:435] conv1 <- data
I0122 16:29:01.831280 48361 net.cpp:409] conv1 -> conv1
I0122 16:29:01.832407 48361 net.cpp:144] Setting up conv1
I0122 16:29:01.832419 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.832424 48361 net.cpp:159] Memory required for data: 7168800
I0122 16:29:01.832437 48361 layer_factory.hpp:77] Creating layer bn1
I0122 16:29:01.832446 48361 net.cpp:94] Creating Layer bn1
I0122 16:29:01.832450 48361 net.cpp:435] bn1 <- conv1
I0122 16:29:01.832458 48361 net.cpp:409] bn1 -> scale1
I0122 16:29:01.833186 48361 net.cpp:144] Setting up bn1
I0122 16:29:01.833194 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.833199 48361 net.cpp:159] Memory required for data: 13722400
I0122 16:29:01.833212 48361 layer_factory.hpp:77] Creating layer relu1
I0122 16:29:01.833220 48361 net.cpp:94] Creating Layer relu1
I0122 16:29:01.833225 48361 net.cpp:435] relu1 <- scale1
I0122 16:29:01.833230 48361 net.cpp:409] relu1 -> relu1
I0122 16:29:01.833252 48361 net.cpp:144] Setting up relu1
I0122 16:29:01.833257 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.833261 48361 net.cpp:159] Memory required for data: 20276000
I0122 16:29:01.833264 48361 layer_factory.hpp:77] Creating layer conv2
I0122 16:29:01.833276 48361 net.cpp:94] Creating Layer conv2
I0122 16:29:01.833281 48361 net.cpp:435] conv2 <- relu1
I0122 16:29:01.833287 48361 net.cpp:409] conv2 -> conv2
I0122 16:29:01.834301 48361 net.cpp:144] Setting up conv2
I0122 16:29:01.834314 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.834316 48361 net.cpp:159] Memory required for data: 26829600
I0122 16:29:01.834324 48361 layer_factory.hpp:77] Creating layer bn2
I0122 16:29:01.834332 48361 net.cpp:94] Creating Layer bn2
I0122 16:29:01.834337 48361 net.cpp:435] bn2 <- conv2
I0122 16:29:01.834343 48361 net.cpp:409] bn2 -> scale2
I0122 16:29:01.835043 48361 net.cpp:144] Setting up bn2
I0122 16:29:01.835050 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.835053 48361 net.cpp:159] Memory required for data: 33383200
I0122 16:29:01.835062 48361 layer_factory.hpp:77] Creating layer relu2
I0122 16:29:01.835069 48361 net.cpp:94] Creating Layer relu2
I0122 16:29:01.835072 48361 net.cpp:435] relu2 <- scale2
I0122 16:29:01.835078 48361 net.cpp:409] relu2 -> relu2
I0122 16:29:01.835158 48361 net.cpp:144] Setting up relu2
I0122 16:29:01.835165 48361 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:01.835167 48361 net.cpp:159] Memory required for data: 39936800
I0122 16:29:01.835182 48361 layer_factory.hpp:77] Creating layer pool1
I0122 16:29:01.835189 48361 net.cpp:94] Creating Layer pool1
I0122 16:29:01.835192 48361 net.cpp:435] pool1 <- relu2
I0122 16:29:01.835196 48361 net.cpp:409] pool1 -> pool1
I0122 16:29:01.835224 48361 net.cpp:144] Setting up pool1
I0122 16:29:01.835229 48361 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:29:01.835233 48361 net.cpp:159] Memory required for data: 41575200
I0122 16:29:01.835235 48361 layer_factory.hpp:77] Creating layer drop1
I0122 16:29:01.835242 48361 net.cpp:94] Creating Layer drop1
I0122 16:29:01.835245 48361 net.cpp:435] drop1 <- pool1
I0122 16:29:01.835249 48361 net.cpp:409] drop1 -> drop1
I0122 16:29:01.835273 48361 net.cpp:144] Setting up drop1
I0122 16:29:01.835279 48361 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:29:01.835283 48361 net.cpp:159] Memory required for data: 43213600
I0122 16:29:01.835285 48361 layer_factory.hpp:77] Creating layer conv3
I0122 16:29:01.835294 48361 net.cpp:94] Creating Layer conv3
I0122 16:29:01.835297 48361 net.cpp:435] conv3 <- drop1
I0122 16:29:01.835302 48361 net.cpp:409] conv3 -> conv3
I0122 16:29:01.836341 48361 net.cpp:144] Setting up conv3
I0122 16:29:01.836352 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.836356 48361 net.cpp:159] Memory required for data: 46490400
I0122 16:29:01.836362 48361 layer_factory.hpp:77] Creating layer bn3
I0122 16:29:01.836369 48361 net.cpp:94] Creating Layer bn3
I0122 16:29:01.836376 48361 net.cpp:435] bn3 <- conv3
I0122 16:29:01.836383 48361 net.cpp:409] bn3 -> scale3
I0122 16:29:01.836990 48361 net.cpp:144] Setting up bn3
I0122 16:29:01.836997 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.837000 48361 net.cpp:159] Memory required for data: 49767200
I0122 16:29:01.837013 48361 layer_factory.hpp:77] Creating layer relu3
I0122 16:29:01.837019 48361 net.cpp:94] Creating Layer relu3
I0122 16:29:01.837023 48361 net.cpp:435] relu3 <- scale3
I0122 16:29:01.837028 48361 net.cpp:409] relu3 -> relu3
I0122 16:29:01.837046 48361 net.cpp:144] Setting up relu3
I0122 16:29:01.837052 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.837055 48361 net.cpp:159] Memory required for data: 53044000
I0122 16:29:01.837059 48361 layer_factory.hpp:77] Creating layer conv4
I0122 16:29:01.837066 48361 net.cpp:94] Creating Layer conv4
I0122 16:29:01.837071 48361 net.cpp:435] conv4 <- relu3
I0122 16:29:01.837076 48361 net.cpp:409] conv4 -> conv4
I0122 16:29:01.837553 48361 net.cpp:144] Setting up conv4
I0122 16:29:01.837561 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.837565 48361 net.cpp:159] Memory required for data: 56320800
I0122 16:29:01.837570 48361 layer_factory.hpp:77] Creating layer bn4
I0122 16:29:01.837577 48361 net.cpp:94] Creating Layer bn4
I0122 16:29:01.837580 48361 net.cpp:435] bn4 <- conv4
I0122 16:29:01.837586 48361 net.cpp:409] bn4 -> scale4
I0122 16:29:01.838239 48361 net.cpp:144] Setting up bn4
I0122 16:29:01.838246 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.838249 48361 net.cpp:159] Memory required for data: 59597600
I0122 16:29:01.838258 48361 layer_factory.hpp:77] Creating layer relu4
I0122 16:29:01.838263 48361 net.cpp:94] Creating Layer relu4
I0122 16:29:01.838266 48361 net.cpp:435] relu4 <- scale4
I0122 16:29:01.838271 48361 net.cpp:409] relu4 -> relu4
I0122 16:29:01.838289 48361 net.cpp:144] Setting up relu4
I0122 16:29:01.838294 48361 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:01.838297 48361 net.cpp:159] Memory required for data: 62874400
I0122 16:29:01.838301 48361 layer_factory.hpp:77] Creating layer pool2
I0122 16:29:01.838307 48361 net.cpp:94] Creating Layer pool2
I0122 16:29:01.838311 48361 net.cpp:435] pool2 <- relu4
I0122 16:29:01.838315 48361 net.cpp:409] pool2 -> pool2
I0122 16:29:01.838408 48361 net.cpp:144] Setting up pool2
I0122 16:29:01.838414 48361 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:29:01.838418 48361 net.cpp:159] Memory required for data: 63693600
I0122 16:29:01.838419 48361 layer_factory.hpp:77] Creating layer drop2
I0122 16:29:01.838436 48361 net.cpp:94] Creating Layer drop2
I0122 16:29:01.838439 48361 net.cpp:435] drop2 <- pool2
I0122 16:29:01.838444 48361 net.cpp:409] drop2 -> drop2
I0122 16:29:01.838471 48361 net.cpp:144] Setting up drop2
I0122 16:29:01.838477 48361 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:29:01.838480 48361 net.cpp:159] Memory required for data: 64512800
I0122 16:29:01.838484 48361 layer_factory.hpp:77] Creating layer fc1
I0122 16:29:01.838490 48361 net.cpp:94] Creating Layer fc1
I0122 16:29:01.838493 48361 net.cpp:435] fc1 <- drop2
I0122 16:29:01.838500 48361 net.cpp:409] fc1 -> fc1
I0122 16:29:01.852630 48361 net.cpp:144] Setting up fc1
I0122 16:29:01.852649 48361 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:01.852653 48361 net.cpp:159] Memory required for data: 64615200
I0122 16:29:01.852659 48361 layer_factory.hpp:77] Creating layer bn5
I0122 16:29:01.852668 48361 net.cpp:94] Creating Layer bn5
I0122 16:29:01.852670 48361 net.cpp:435] bn5 <- fc1
I0122 16:29:01.852677 48361 net.cpp:409] bn5 -> scale5
I0122 16:29:01.853226 48361 net.cpp:144] Setting up bn5
I0122 16:29:01.853233 48361 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:01.853236 48361 net.cpp:159] Memory required for data: 64717600
I0122 16:29:01.853250 48361 layer_factory.hpp:77] Creating layer relu5
I0122 16:29:01.853257 48361 net.cpp:94] Creating Layer relu5
I0122 16:29:01.853260 48361 net.cpp:435] relu5 <- scale5
I0122 16:29:01.853266 48361 net.cpp:409] relu5 -> relu5
I0122 16:29:01.853284 48361 net.cpp:144] Setting up relu5
I0122 16:29:01.853289 48361 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:01.853292 48361 net.cpp:159] Memory required for data: 64820000
I0122 16:29:01.853296 48361 layer_factory.hpp:77] Creating layer drop3
I0122 16:29:01.853302 48361 net.cpp:94] Creating Layer drop3
I0122 16:29:01.853304 48361 net.cpp:435] drop3 <- relu5
I0122 16:29:01.853309 48361 net.cpp:409] drop3 -> drop3
I0122 16:29:01.853334 48361 net.cpp:144] Setting up drop3
I0122 16:29:01.853340 48361 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:01.853343 48361 net.cpp:159] Memory required for data: 64922400
I0122 16:29:01.853346 48361 layer_factory.hpp:77] Creating layer fc2
I0122 16:29:01.853353 48361 net.cpp:94] Creating Layer fc2
I0122 16:29:01.853358 48361 net.cpp:435] fc2 <- drop3
I0122 16:29:01.853363 48361 net.cpp:409] fc2 -> fc2
I0122 16:29:01.853490 48361 net.cpp:144] Setting up fc2
I0122 16:29:01.853497 48361 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:01.853499 48361 net.cpp:159] Memory required for data: 64924400
I0122 16:29:01.853504 48361 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:29:01.853509 48361 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:29:01.853513 48361 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:29:01.853518 48361 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:29:01.853524 48361 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:29:01.853530 48361 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:29:01.853565 48361 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:29:01.853571 48361 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:01.853574 48361 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:01.853579 48361 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:01.853580 48361 net.cpp:159] Memory required for data: 64930400
I0122 16:29:01.853583 48361 layer_factory.hpp:77] Creating layer loss
I0122 16:29:01.853588 48361 net.cpp:94] Creating Layer loss
I0122 16:29:01.853591 48361 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:29:01.853596 48361 net.cpp:435] loss <- label_data_1_split_0
I0122 16:29:01.853601 48361 net.cpp:409] loss -> loss
I0122 16:29:01.853610 48361 layer_factory.hpp:77] Creating layer loss
I0122 16:29:01.853677 48361 net.cpp:144] Setting up loss
I0122 16:29:01.853682 48361 net.cpp:151] Top shape: (1)
I0122 16:29:01.853685 48361 net.cpp:154]     with loss weight 1
I0122 16:29:01.853695 48361 net.cpp:159] Memory required for data: 64930404
I0122 16:29:01.853698 48361 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:29:01.853716 48361 net.cpp:94] Creating Layer accuracy-top1
I0122 16:29:01.853720 48361 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:29:01.853724 48361 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:29:01.853729 48361 net.cpp:409] accuracy-top1 -> top-1
I0122 16:29:01.853736 48361 net.cpp:144] Setting up accuracy-top1
I0122 16:29:01.853742 48361 net.cpp:151] Top shape: (1)
I0122 16:29:01.853745 48361 net.cpp:159] Memory required for data: 64930408
I0122 16:29:01.853747 48361 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:29:01.853752 48361 net.cpp:94] Creating Layer accuracy-top5
I0122 16:29:01.853757 48361 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:29:01.853761 48361 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:29:01.853766 48361 net.cpp:409] accuracy-top5 -> top-5
I0122 16:29:01.853772 48361 net.cpp:144] Setting up accuracy-top5
I0122 16:29:01.853778 48361 net.cpp:151] Top shape: (1)
I0122 16:29:01.853780 48361 net.cpp:159] Memory required for data: 64930412
I0122 16:29:01.853783 48361 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:29:01.853787 48361 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:29:01.853791 48361 net.cpp:220] loss needs backward computation.
I0122 16:29:01.853796 48361 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:29:01.853798 48361 net.cpp:220] fc2 needs backward computation.
I0122 16:29:01.853801 48361 net.cpp:220] drop3 needs backward computation.
I0122 16:29:01.853806 48361 net.cpp:220] relu5 needs backward computation.
I0122 16:29:01.853809 48361 net.cpp:220] bn5 needs backward computation.
I0122 16:29:01.853812 48361 net.cpp:220] fc1 needs backward computation.
I0122 16:29:01.853816 48361 net.cpp:220] drop2 needs backward computation.
I0122 16:29:01.853819 48361 net.cpp:220] pool2 needs backward computation.
I0122 16:29:01.853822 48361 net.cpp:220] relu4 needs backward computation.
I0122 16:29:01.853826 48361 net.cpp:220] bn4 needs backward computation.
I0122 16:29:01.853829 48361 net.cpp:220] conv4 needs backward computation.
I0122 16:29:01.853833 48361 net.cpp:220] relu3 needs backward computation.
I0122 16:29:01.853837 48361 net.cpp:220] bn3 needs backward computation.
I0122 16:29:01.853839 48361 net.cpp:220] conv3 needs backward computation.
I0122 16:29:01.853842 48361 net.cpp:220] drop1 needs backward computation.
I0122 16:29:01.853845 48361 net.cpp:220] pool1 needs backward computation.
I0122 16:29:01.853848 48361 net.cpp:220] relu2 needs backward computation.
I0122 16:29:01.853852 48361 net.cpp:220] bn2 needs backward computation.
I0122 16:29:01.853854 48361 net.cpp:220] conv2 needs backward computation.
I0122 16:29:01.853858 48361 net.cpp:220] relu1 needs backward computation.
I0122 16:29:01.853862 48361 net.cpp:220] bn1 needs backward computation.
I0122 16:29:01.853864 48361 net.cpp:220] conv1 needs backward computation.
I0122 16:29:01.853868 48361 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:29:01.853873 48361 net.cpp:222] data does not need backward computation.
I0122 16:29:01.853875 48361 net.cpp:264] This network produces output loss
I0122 16:29:01.853878 48361 net.cpp:264] This network produces output top-1
I0122 16:29:01.853881 48361 net.cpp:264] This network produces output top-5
I0122 16:29:01.853910 48361 net.cpp:284] Network initialization done.
I0122 16:29:01.856796 48361 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:29:01.863332 48361 caffe_interface.cpp:125] Batch 0, loss = 0.540569
I0122 16:29:01.863353 48361 caffe_interface.cpp:125] Batch 0, top-1 = 0.84
I0122 16:29:01.863358 48361 caffe_interface.cpp:125] Batch 0, top-5 = 0.98
I0122 16:29:01.864670 48361 caffe_interface.cpp:125] Batch 1, loss = 0.518254
I0122 16:29:01.864677 48361 caffe_interface.cpp:125] Batch 1, top-1 = 0.84
I0122 16:29:01.864681 48361 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0122 16:29:01.865980 48361 caffe_interface.cpp:125] Batch 2, loss = 0.387999
I0122 16:29:01.865988 48361 caffe_interface.cpp:125] Batch 2, top-1 = 0.84
I0122 16:29:01.866004 48361 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:29:01.867308 48361 caffe_interface.cpp:125] Batch 3, loss = 1.02814
I0122 16:29:01.867316 48361 caffe_interface.cpp:125] Batch 3, top-1 = 0.76
I0122 16:29:01.867321 48361 caffe_interface.cpp:125] Batch 3, top-5 = 0.98
I0122 16:29:01.868638 48361 caffe_interface.cpp:125] Batch 4, loss = 0.539231
I0122 16:29:01.868644 48361 caffe_interface.cpp:125] Batch 4, top-1 = 0.88
I0122 16:29:01.868649 48361 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0122 16:29:01.869956 48361 caffe_interface.cpp:125] Batch 5, loss = 0.578095
I0122 16:29:01.869964 48361 caffe_interface.cpp:125] Batch 5, top-1 = 0.9
I0122 16:29:01.869967 48361 caffe_interface.cpp:125] Batch 5, top-5 = 0.98
I0122 16:29:01.871278 48361 caffe_interface.cpp:125] Batch 6, loss = 0.55876
I0122 16:29:01.871292 48361 caffe_interface.cpp:125] Batch 6, top-1 = 0.8
I0122 16:29:01.871296 48361 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0122 16:29:01.872596 48361 caffe_interface.cpp:125] Batch 7, loss = 0.336365
I0122 16:29:01.872603 48361 caffe_interface.cpp:125] Batch 7, top-1 = 0.86
I0122 16:29:01.872607 48361 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0122 16:29:01.874166 48361 caffe_interface.cpp:125] Batch 8, loss = 0.555237
I0122 16:29:01.874174 48361 caffe_interface.cpp:125] Batch 8, top-1 = 0.84
I0122 16:29:01.874178 48361 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:29:01.875469 48361 caffe_interface.cpp:125] Batch 9, loss = 0.637305
I0122 16:29:01.875478 48361 caffe_interface.cpp:125] Batch 9, top-1 = 0.78
I0122 16:29:01.875481 48361 caffe_interface.cpp:125] Batch 9, top-5 = 0.94
I0122 16:29:01.876767 48361 caffe_interface.cpp:125] Batch 10, loss = 0.361499
I0122 16:29:01.876783 48361 caffe_interface.cpp:125] Batch 10, top-1 = 0.9
I0122 16:29:01.876787 48361 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:29:01.878072 48361 caffe_interface.cpp:125] Batch 11, loss = 0.717332
I0122 16:29:01.878082 48361 caffe_interface.cpp:125] Batch 11, top-1 = 0.82
I0122 16:29:01.878084 48361 caffe_interface.cpp:125] Batch 11, top-5 = 0.98
I0122 16:29:01.879391 48361 caffe_interface.cpp:125] Batch 12, loss = 0.963551
I0122 16:29:01.879400 48361 caffe_interface.cpp:125] Batch 12, top-1 = 0.76
I0122 16:29:01.879403 48361 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:29:01.880686 48361 caffe_interface.cpp:125] Batch 13, loss = 0.647031
I0122 16:29:01.880692 48361 caffe_interface.cpp:125] Batch 13, top-1 = 0.84
I0122 16:29:01.880697 48361 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0122 16:29:01.881991 48361 caffe_interface.cpp:125] Batch 14, loss = 0.486395
I0122 16:29:01.882000 48361 caffe_interface.cpp:125] Batch 14, top-1 = 0.92
I0122 16:29:01.882004 48361 caffe_interface.cpp:125] Batch 14, top-5 = 0.98
I0122 16:29:01.883294 48361 caffe_interface.cpp:125] Batch 15, loss = 0.386888
I0122 16:29:01.883302 48361 caffe_interface.cpp:125] Batch 15, top-1 = 0.86
I0122 16:29:01.883306 48361 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:29:01.884591 48361 caffe_interface.cpp:125] Batch 16, loss = 0.584514
I0122 16:29:01.884598 48361 caffe_interface.cpp:125] Batch 16, top-1 = 0.88
I0122 16:29:01.884603 48361 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0122 16:29:01.885891 48361 caffe_interface.cpp:125] Batch 17, loss = 0.247985
I0122 16:29:01.885900 48361 caffe_interface.cpp:125] Batch 17, top-1 = 0.88
I0122 16:29:01.885902 48361 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0122 16:29:01.887192 48361 caffe_interface.cpp:125] Batch 18, loss = 0.729753
I0122 16:29:01.887199 48361 caffe_interface.cpp:125] Batch 18, top-1 = 0.82
I0122 16:29:01.887203 48361 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0122 16:29:01.889422 48361 caffe_interface.cpp:125] Batch 19, loss = 0.367012
I0122 16:29:01.889430 48361 caffe_interface.cpp:125] Batch 19, top-1 = 0.86
I0122 16:29:01.889433 48361 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:29:01.890883 48361 caffe_interface.cpp:125] Batch 20, loss = 0.399734
I0122 16:29:01.890892 48361 caffe_interface.cpp:125] Batch 20, top-1 = 0.86
I0122 16:29:01.890908 48361 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0122 16:29:01.892390 48361 caffe_interface.cpp:125] Batch 21, loss = 0.73262
I0122 16:29:01.892397 48361 caffe_interface.cpp:125] Batch 21, top-1 = 0.78
I0122 16:29:01.892401 48361 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0122 16:29:01.893702 48361 caffe_interface.cpp:125] Batch 22, loss = 0.5932
I0122 16:29:01.893709 48361 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0122 16:29:01.893712 48361 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0122 16:29:01.895010 48361 caffe_interface.cpp:125] Batch 23, loss = 0.415827
I0122 16:29:01.895018 48361 caffe_interface.cpp:125] Batch 23, top-1 = 0.86
I0122 16:29:01.895022 48361 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:29:01.896318 48361 caffe_interface.cpp:125] Batch 24, loss = 0.737195
I0122 16:29:01.896334 48361 caffe_interface.cpp:125] Batch 24, top-1 = 0.84
I0122 16:29:01.896338 48361 caffe_interface.cpp:125] Batch 24, top-5 = 0.94
I0122 16:29:01.897651 48361 caffe_interface.cpp:125] Batch 25, loss = 0.601599
I0122 16:29:01.897658 48361 caffe_interface.cpp:125] Batch 25, top-1 = 0.86
I0122 16:29:01.897662 48361 caffe_interface.cpp:125] Batch 25, top-5 = 1
I0122 16:29:01.898968 48361 caffe_interface.cpp:125] Batch 26, loss = 0.629774
I0122 16:29:01.898977 48361 caffe_interface.cpp:125] Batch 26, top-1 = 0.82
I0122 16:29:01.898980 48361 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0122 16:29:01.900275 48361 caffe_interface.cpp:125] Batch 27, loss = 0.502246
I0122 16:29:01.900283 48361 caffe_interface.cpp:125] Batch 27, top-1 = 0.78
I0122 16:29:01.900287 48361 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:29:01.901587 48361 caffe_interface.cpp:125] Batch 28, loss = 0.607311
I0122 16:29:01.901594 48361 caffe_interface.cpp:125] Batch 28, top-1 = 0.78
I0122 16:29:01.901598 48361 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:29:01.902904 48361 caffe_interface.cpp:125] Batch 29, loss = 0.294137
I0122 16:29:01.902911 48361 caffe_interface.cpp:125] Batch 29, top-1 = 0.9
I0122 16:29:01.902915 48361 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:29:01.904217 48361 caffe_interface.cpp:125] Batch 30, loss = 0.295505
I0122 16:29:01.904224 48361 caffe_interface.cpp:125] Batch 30, top-1 = 0.9
I0122 16:29:01.904228 48361 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:29:01.905542 48361 caffe_interface.cpp:125] Batch 31, loss = 0.577297
I0122 16:29:01.905550 48361 caffe_interface.cpp:125] Batch 31, top-1 = 0.8
I0122 16:29:01.905555 48361 caffe_interface.cpp:125] Batch 31, top-5 = 1
I0122 16:29:01.907054 48361 caffe_interface.cpp:125] Batch 32, loss = 0.664182
I0122 16:29:01.907063 48361 caffe_interface.cpp:125] Batch 32, top-1 = 0.86
I0122 16:29:01.907066 48361 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:29:01.908349 48361 caffe_interface.cpp:125] Batch 33, loss = 0.944936
I0122 16:29:01.908356 48361 caffe_interface.cpp:125] Batch 33, top-1 = 0.84
I0122 16:29:01.908360 48361 caffe_interface.cpp:125] Batch 33, top-5 = 0.96
I0122 16:29:01.909657 48361 caffe_interface.cpp:125] Batch 34, loss = 0.377889
I0122 16:29:01.909672 48361 caffe_interface.cpp:125] Batch 34, top-1 = 0.88
I0122 16:29:01.909677 48361 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0122 16:29:01.910982 48361 caffe_interface.cpp:125] Batch 35, loss = 0.769173
I0122 16:29:01.910991 48361 caffe_interface.cpp:125] Batch 35, top-1 = 0.82
I0122 16:29:01.910995 48361 caffe_interface.cpp:125] Batch 35, top-5 = 0.98
I0122 16:29:01.912292 48361 caffe_interface.cpp:125] Batch 36, loss = 0.512586
I0122 16:29:01.912298 48361 caffe_interface.cpp:125] Batch 36, top-1 = 0.82
I0122 16:29:01.912302 48361 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:29:01.913588 48361 caffe_interface.cpp:125] Batch 37, loss = 0.829303
I0122 16:29:01.913595 48361 caffe_interface.cpp:125] Batch 37, top-1 = 0.78
I0122 16:29:01.913599 48361 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0122 16:29:01.914888 48361 caffe_interface.cpp:125] Batch 38, loss = 0.895494
I0122 16:29:01.914896 48361 caffe_interface.cpp:125] Batch 38, top-1 = 0.68
I0122 16:29:01.914911 48361 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0122 16:29:01.916194 48361 caffe_interface.cpp:125] Batch 39, loss = 0.490941
I0122 16:29:01.916203 48361 caffe_interface.cpp:125] Batch 39, top-1 = 0.84
I0122 16:29:01.916206 48361 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:29:01.917497 48361 caffe_interface.cpp:125] Batch 40, loss = 0.389211
I0122 16:29:01.917505 48361 caffe_interface.cpp:125] Batch 40, top-1 = 0.84
I0122 16:29:01.917508 48361 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:29:01.918805 48361 caffe_interface.cpp:125] Batch 41, loss = 0.911542
I0122 16:29:01.918813 48361 caffe_interface.cpp:125] Batch 41, top-1 = 0.76
I0122 16:29:01.918817 48361 caffe_interface.cpp:125] Batch 41, top-5 = 1
I0122 16:29:01.920096 48361 caffe_interface.cpp:125] Batch 42, loss = 0.677698
I0122 16:29:01.920104 48361 caffe_interface.cpp:125] Batch 42, top-1 = 0.8
I0122 16:29:01.920107 48361 caffe_interface.cpp:125] Batch 42, top-5 = 0.96
I0122 16:29:01.921394 48361 caffe_interface.cpp:125] Batch 43, loss = 0.506845
I0122 16:29:01.921402 48361 caffe_interface.cpp:125] Batch 43, top-1 = 0.84
I0122 16:29:01.921406 48361 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0122 16:29:01.923698 48361 caffe_interface.cpp:125] Batch 44, loss = 0.632264
I0122 16:29:01.923707 48361 caffe_interface.cpp:125] Batch 44, top-1 = 0.82
I0122 16:29:01.923710 48361 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0122 16:29:01.925177 48361 caffe_interface.cpp:125] Batch 45, loss = 0.506302
I0122 16:29:01.925184 48361 caffe_interface.cpp:125] Batch 45, top-1 = 0.86
I0122 16:29:01.925189 48361 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0122 16:29:01.926491 48361 caffe_interface.cpp:125] Batch 46, loss = 0.370479
I0122 16:29:01.926498 48361 caffe_interface.cpp:125] Batch 46, top-1 = 0.88
I0122 16:29:01.926502 48361 caffe_interface.cpp:125] Batch 46, top-5 = 0.98
I0122 16:29:01.927809 48361 caffe_interface.cpp:125] Batch 47, loss = 0.352064
I0122 16:29:01.927817 48361 caffe_interface.cpp:125] Batch 47, top-1 = 0.88
I0122 16:29:01.927821 48361 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0122 16:29:01.929134 48361 caffe_interface.cpp:125] Batch 48, loss = 0.50602
I0122 16:29:01.929149 48361 caffe_interface.cpp:125] Batch 48, top-1 = 0.84
I0122 16:29:01.929153 48361 caffe_interface.cpp:125] Batch 48, top-5 = 0.98
I0122 16:29:01.930438 48361 caffe_interface.cpp:125] Batch 49, loss = 0.637019
I0122 16:29:01.930446 48361 caffe_interface.cpp:125] Batch 49, top-1 = 0.88
I0122 16:29:01.930450 48361 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0122 16:29:01.931751 48361 caffe_interface.cpp:125] Batch 50, loss = 0.539361
I0122 16:29:01.931758 48361 caffe_interface.cpp:125] Batch 50, top-1 = 0.84
I0122 16:29:01.931762 48361 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:29:01.933054 48361 caffe_interface.cpp:125] Batch 51, loss = 0.289883
I0122 16:29:01.933063 48361 caffe_interface.cpp:125] Batch 51, top-1 = 0.88
I0122 16:29:01.933066 48361 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0122 16:29:01.934373 48361 caffe_interface.cpp:125] Batch 52, loss = 0.459235
I0122 16:29:01.934381 48361 caffe_interface.cpp:125] Batch 52, top-1 = 0.88
I0122 16:29:01.934386 48361 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0122 16:29:01.935673 48361 caffe_interface.cpp:125] Batch 53, loss = 0.82743
I0122 16:29:01.935681 48361 caffe_interface.cpp:125] Batch 53, top-1 = 0.74
I0122 16:29:01.935685 48361 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0122 16:29:01.936975 48361 caffe_interface.cpp:125] Batch 54, loss = 0.555195
I0122 16:29:01.936983 48361 caffe_interface.cpp:125] Batch 54, top-1 = 0.76
I0122 16:29:01.936987 48361 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0122 16:29:01.938272 48361 caffe_interface.cpp:125] Batch 55, loss = 0.198503
I0122 16:29:01.938280 48361 caffe_interface.cpp:125] Batch 55, top-1 = 0.94
I0122 16:29:01.938284 48361 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:29:01.939564 48361 caffe_interface.cpp:125] Batch 56, loss = 0.593186
I0122 16:29:01.939584 48361 caffe_interface.cpp:125] Batch 56, top-1 = 0.86
I0122 16:29:01.939589 48361 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0122 16:29:01.941143 48361 caffe_interface.cpp:125] Batch 57, loss = 0.657427
I0122 16:29:01.941151 48361 caffe_interface.cpp:125] Batch 57, top-1 = 0.84
I0122 16:29:01.941154 48361 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:29:01.942423 48361 caffe_interface.cpp:125] Batch 58, loss = 0.788021
I0122 16:29:01.942432 48361 caffe_interface.cpp:125] Batch 58, top-1 = 0.76
I0122 16:29:01.942435 48361 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0122 16:29:01.943711 48361 caffe_interface.cpp:125] Batch 59, loss = 0.596862
I0122 16:29:01.943719 48361 caffe_interface.cpp:125] Batch 59, top-1 = 0.8
I0122 16:29:01.943723 48361 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:29:01.944996 48361 caffe_interface.cpp:125] Batch 60, loss = 0.631072
I0122 16:29:01.945004 48361 caffe_interface.cpp:125] Batch 60, top-1 = 0.82
I0122 16:29:01.945008 48361 caffe_interface.cpp:125] Batch 60, top-5 = 0.98
I0122 16:29:01.946282 48361 caffe_interface.cpp:125] Batch 61, loss = 1.1937
I0122 16:29:01.946290 48361 caffe_interface.cpp:125] Batch 61, top-1 = 0.7
I0122 16:29:01.946295 48361 caffe_interface.cpp:125] Batch 61, top-5 = 0.98
I0122 16:29:01.947564 48361 caffe_interface.cpp:125] Batch 62, loss = 0.631623
I0122 16:29:01.947571 48361 caffe_interface.cpp:125] Batch 62, top-1 = 0.76
I0122 16:29:01.947576 48361 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:29:01.948854 48361 caffe_interface.cpp:125] Batch 63, loss = 0.539803
I0122 16:29:01.948863 48361 caffe_interface.cpp:125] Batch 63, top-1 = 0.84
I0122 16:29:01.948866 48361 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0122 16:29:01.950141 48361 caffe_interface.cpp:125] Batch 64, loss = 0.578446
I0122 16:29:01.950150 48361 caffe_interface.cpp:125] Batch 64, top-1 = 0.76
I0122 16:29:01.950153 48361 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:29:01.951428 48361 caffe_interface.cpp:125] Batch 65, loss = 0.462979
I0122 16:29:01.951437 48361 caffe_interface.cpp:125] Batch 65, top-1 = 0.86
I0122 16:29:01.951442 48361 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:29:01.952702 48361 caffe_interface.cpp:125] Batch 66, loss = 0.624428
I0122 16:29:01.952708 48361 caffe_interface.cpp:125] Batch 66, top-1 = 0.84
I0122 16:29:01.952713 48361 caffe_interface.cpp:125] Batch 66, top-5 = 1
I0122 16:29:01.953986 48361 caffe_interface.cpp:125] Batch 67, loss = 0.950937
I0122 16:29:01.953994 48361 caffe_interface.cpp:125] Batch 67, top-1 = 0.76
I0122 16:29:01.953999 48361 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0122 16:29:01.956272 48361 caffe_interface.cpp:125] Batch 68, loss = 0.325348
I0122 16:29:01.956279 48361 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I0122 16:29:01.956284 48361 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:29:01.957621 48361 caffe_interface.cpp:125] Batch 69, loss = 0.341695
I0122 16:29:01.957628 48361 caffe_interface.cpp:125] Batch 69, top-1 = 0.9
I0122 16:29:01.957633 48361 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0122 16:29:01.959095 48361 caffe_interface.cpp:125] Batch 70, loss = 0.451871
I0122 16:29:01.959102 48361 caffe_interface.cpp:125] Batch 70, top-1 = 0.88
I0122 16:29:01.959111 48361 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0122 16:29:01.960386 48361 caffe_interface.cpp:125] Batch 71, loss = 0.324143
I0122 16:29:01.960394 48361 caffe_interface.cpp:125] Batch 71, top-1 = 0.88
I0122 16:29:01.960398 48361 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:29:01.961679 48361 caffe_interface.cpp:125] Batch 72, loss = 0.755175
I0122 16:29:01.961694 48361 caffe_interface.cpp:125] Batch 72, top-1 = 0.76
I0122 16:29:01.961697 48361 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0122 16:29:01.962961 48361 caffe_interface.cpp:125] Batch 73, loss = 0.466243
I0122 16:29:01.962970 48361 caffe_interface.cpp:125] Batch 73, top-1 = 0.86
I0122 16:29:01.962973 48361 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0122 16:29:01.964246 48361 caffe_interface.cpp:125] Batch 74, loss = 1.10131
I0122 16:29:01.964262 48361 caffe_interface.cpp:125] Batch 74, top-1 = 0.72
I0122 16:29:01.964267 48361 caffe_interface.cpp:125] Batch 74, top-5 = 0.94
I0122 16:29:01.965540 48361 caffe_interface.cpp:125] Batch 75, loss = 0.428826
I0122 16:29:01.965555 48361 caffe_interface.cpp:125] Batch 75, top-1 = 0.88
I0122 16:29:01.965559 48361 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0122 16:29:01.966835 48361 caffe_interface.cpp:125] Batch 76, loss = 0.798394
I0122 16:29:01.966841 48361 caffe_interface.cpp:125] Batch 76, top-1 = 0.8
I0122 16:29:01.966845 48361 caffe_interface.cpp:125] Batch 76, top-5 = 0.94
I0122 16:29:01.968135 48361 caffe_interface.cpp:125] Batch 77, loss = 0.478379
I0122 16:29:01.968142 48361 caffe_interface.cpp:125] Batch 77, top-1 = 0.84
I0122 16:29:01.968147 48361 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0122 16:29:01.969437 48361 caffe_interface.cpp:125] Batch 78, loss = 0.492509
I0122 16:29:01.969445 48361 caffe_interface.cpp:125] Batch 78, top-1 = 0.8
I0122 16:29:01.969449 48361 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0122 16:29:01.970726 48361 caffe_interface.cpp:125] Batch 79, loss = 0.682127
I0122 16:29:01.970734 48361 caffe_interface.cpp:125] Batch 79, top-1 = 0.8
I0122 16:29:01.970737 48361 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0122 16:29:01.972012 48361 caffe_interface.cpp:125] Batch 80, loss = 0.292197
I0122 16:29:01.972018 48361 caffe_interface.cpp:125] Batch 80, top-1 = 0.92
I0122 16:29:01.972023 48361 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:29:01.973307 48361 caffe_interface.cpp:125] Batch 81, loss = 0.747265
I0122 16:29:01.973315 48361 caffe_interface.cpp:125] Batch 81, top-1 = 0.74
I0122 16:29:01.973318 48361 caffe_interface.cpp:125] Batch 81, top-5 = 1
I0122 16:29:01.974643 48361 caffe_interface.cpp:125] Batch 82, loss = 0.329725
I0122 16:29:01.974651 48361 caffe_interface.cpp:125] Batch 82, top-1 = 0.86
I0122 16:29:01.974654 48361 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0122 16:29:01.975921 48361 caffe_interface.cpp:125] Batch 83, loss = 0.773953
I0122 16:29:01.975929 48361 caffe_interface.cpp:125] Batch 83, top-1 = 0.82
I0122 16:29:01.975934 48361 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:29:01.977210 48361 caffe_interface.cpp:125] Batch 84, loss = 0.998928
I0122 16:29:01.977218 48361 caffe_interface.cpp:125] Batch 84, top-1 = 0.78
I0122 16:29:01.977222 48361 caffe_interface.cpp:125] Batch 84, top-5 = 0.98
I0122 16:29:01.978487 48361 caffe_interface.cpp:125] Batch 85, loss = 0.522418
I0122 16:29:01.978494 48361 caffe_interface.cpp:125] Batch 85, top-1 = 0.86
I0122 16:29:01.978497 48361 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:29:01.979775 48361 caffe_interface.cpp:125] Batch 86, loss = 0.473212
I0122 16:29:01.979784 48361 caffe_interface.cpp:125] Batch 86, top-1 = 0.82
I0122 16:29:01.979787 48361 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:29:01.981076 48361 caffe_interface.cpp:125] Batch 87, loss = 0.465247
I0122 16:29:01.981086 48361 caffe_interface.cpp:125] Batch 87, top-1 = 0.8
I0122 16:29:01.981088 48361 caffe_interface.cpp:125] Batch 87, top-5 = 0.98
I0122 16:29:01.982358 48361 caffe_interface.cpp:125] Batch 88, loss = 0.954745
I0122 16:29:01.982367 48361 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0122 16:29:01.982370 48361 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0122 16:29:01.983640 48361 caffe_interface.cpp:125] Batch 89, loss = 0.457957
I0122 16:29:01.983649 48361 caffe_interface.cpp:125] Batch 89, top-1 = 0.88
I0122 16:29:01.983652 48361 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0122 16:29:01.984921 48361 caffe_interface.cpp:125] Batch 90, loss = 0.44203
I0122 16:29:01.984928 48361 caffe_interface.cpp:125] Batch 90, top-1 = 0.86
I0122 16:29:01.984932 48361 caffe_interface.cpp:125] Batch 90, top-5 = 0.98
I0122 16:29:01.986202 48361 caffe_interface.cpp:125] Batch 91, loss = 0.409321
I0122 16:29:01.986212 48361 caffe_interface.cpp:125] Batch 91, top-1 = 0.84
I0122 16:29:01.986215 48361 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0122 16:29:01.987484 48361 caffe_interface.cpp:125] Batch 92, loss = 0.855044
I0122 16:29:01.987501 48361 caffe_interface.cpp:125] Batch 92, top-1 = 0.88
I0122 16:29:01.987505 48361 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:29:01.989696 48361 caffe_interface.cpp:125] Batch 93, loss = 0.5875
I0122 16:29:01.989704 48361 caffe_interface.cpp:125] Batch 93, top-1 = 0.86
I0122 16:29:01.989707 48361 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0122 16:29:01.991040 48361 caffe_interface.cpp:125] Batch 94, loss = 1.17166
I0122 16:29:01.991047 48361 caffe_interface.cpp:125] Batch 94, top-1 = 0.74
I0122 16:29:01.991051 48361 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0122 16:29:01.992609 48361 caffe_interface.cpp:125] Batch 95, loss = 0.597475
I0122 16:29:01.992616 48361 caffe_interface.cpp:125] Batch 95, top-1 = 0.88
I0122 16:29:01.992620 48361 caffe_interface.cpp:125] Batch 95, top-5 = 1
I0122 16:29:01.993916 48361 caffe_interface.cpp:125] Batch 96, loss = 0.809233
I0122 16:29:01.993924 48361 caffe_interface.cpp:125] Batch 96, top-1 = 0.78
I0122 16:29:01.993928 48361 caffe_interface.cpp:125] Batch 96, top-5 = 0.98
I0122 16:29:01.995219 48361 caffe_interface.cpp:125] Batch 97, loss = 0.607305
I0122 16:29:01.995227 48361 caffe_interface.cpp:125] Batch 97, top-1 = 0.82
I0122 16:29:01.995244 48361 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:29:01.996533 48361 caffe_interface.cpp:125] Batch 98, loss = 0.601446
I0122 16:29:01.996548 48361 caffe_interface.cpp:125] Batch 98, top-1 = 0.84
I0122 16:29:01.996551 48361 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:29:01.997834 48361 caffe_interface.cpp:125] Batch 99, loss = 0.622626
I0122 16:29:01.997843 48361 caffe_interface.cpp:125] Batch 99, top-1 = 0.82
I0122 16:29:01.997846 48361 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:29:01.999125 48361 caffe_interface.cpp:125] Batch 100, loss = 0.451906
I0122 16:29:01.999133 48361 caffe_interface.cpp:125] Batch 100, top-1 = 0.88
I0122 16:29:01.999136 48361 caffe_interface.cpp:125] Batch 100, top-5 = 0.98
I0122 16:29:02.000442 48361 caffe_interface.cpp:125] Batch 101, loss = 0.31731
I0122 16:29:02.000450 48361 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0122 16:29:02.000454 48361 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:29:02.001732 48361 caffe_interface.cpp:125] Batch 102, loss = 0.737087
I0122 16:29:02.001740 48361 caffe_interface.cpp:125] Batch 102, top-1 = 0.76
I0122 16:29:02.001744 48361 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0122 16:29:02.003034 48361 caffe_interface.cpp:125] Batch 103, loss = 0.744015
I0122 16:29:02.003042 48361 caffe_interface.cpp:125] Batch 103, top-1 = 0.82
I0122 16:29:02.003046 48361 caffe_interface.cpp:125] Batch 103, top-5 = 0.96
I0122 16:29:02.004334 48361 caffe_interface.cpp:125] Batch 104, loss = 0.513115
I0122 16:29:02.004341 48361 caffe_interface.cpp:125] Batch 104, top-1 = 0.86
I0122 16:29:02.004345 48361 caffe_interface.cpp:125] Batch 104, top-5 = 0.98
I0122 16:29:02.005614 48361 caffe_interface.cpp:125] Batch 105, loss = 0.496018
I0122 16:29:02.005620 48361 caffe_interface.cpp:125] Batch 105, top-1 = 0.88
I0122 16:29:02.005623 48361 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:29:02.007086 48361 caffe_interface.cpp:125] Batch 106, loss = 0.4032
I0122 16:29:02.007093 48361 caffe_interface.cpp:125] Batch 106, top-1 = 0.88
I0122 16:29:02.007097 48361 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:29:02.008358 48361 caffe_interface.cpp:125] Batch 107, loss = 0.753583
I0122 16:29:02.008365 48361 caffe_interface.cpp:125] Batch 107, top-1 = 0.78
I0122 16:29:02.008369 48361 caffe_interface.cpp:125] Batch 107, top-5 = 0.96
I0122 16:29:02.009632 48361 caffe_interface.cpp:125] Batch 108, loss = 0.524991
I0122 16:29:02.009639 48361 caffe_interface.cpp:125] Batch 108, top-1 = 0.78
I0122 16:29:02.009644 48361 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0122 16:29:02.010915 48361 caffe_interface.cpp:125] Batch 109, loss = 0.431198
I0122 16:29:02.010923 48361 caffe_interface.cpp:125] Batch 109, top-1 = 0.82
I0122 16:29:02.010926 48361 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:29:02.012202 48361 caffe_interface.cpp:125] Batch 110, loss = 0.878712
I0122 16:29:02.012210 48361 caffe_interface.cpp:125] Batch 110, top-1 = 0.8
I0122 16:29:02.012214 48361 caffe_interface.cpp:125] Batch 110, top-5 = 0.98
I0122 16:29:02.013494 48361 caffe_interface.cpp:125] Batch 111, loss = 0.749271
I0122 16:29:02.013502 48361 caffe_interface.cpp:125] Batch 111, top-1 = 0.76
I0122 16:29:02.013506 48361 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0122 16:29:02.014788 48361 caffe_interface.cpp:125] Batch 112, loss = 0.368239
I0122 16:29:02.014796 48361 caffe_interface.cpp:125] Batch 112, top-1 = 0.9
I0122 16:29:02.014801 48361 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0122 16:29:02.016072 48361 caffe_interface.cpp:125] Batch 113, loss = 0.569561
I0122 16:29:02.016078 48361 caffe_interface.cpp:125] Batch 113, top-1 = 0.88
I0122 16:29:02.016083 48361 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:29:02.017359 48361 caffe_interface.cpp:125] Batch 114, loss = 1.01631
I0122 16:29:02.017365 48361 caffe_interface.cpp:125] Batch 114, top-1 = 0.66
I0122 16:29:02.017369 48361 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0122 16:29:02.018640 48361 caffe_interface.cpp:125] Batch 115, loss = 0.239666
I0122 16:29:02.018648 48361 caffe_interface.cpp:125] Batch 115, top-1 = 0.94
I0122 16:29:02.018651 48361 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:29:02.019868 48361 caffe_interface.cpp:125] Batch 116, loss = 0.863221
I0122 16:29:02.019876 48361 caffe_interface.cpp:125] Batch 116, top-1 = 0.74
I0122 16:29:02.019881 48361 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0122 16:29:02.021085 48361 caffe_interface.cpp:125] Batch 117, loss = 0.706116
I0122 16:29:02.021095 48361 caffe_interface.cpp:125] Batch 117, top-1 = 0.78
I0122 16:29:02.021100 48361 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0122 16:29:02.023319 48361 caffe_interface.cpp:125] Batch 118, loss = 0.710238
I0122 16:29:02.023324 48361 caffe_interface.cpp:125] Batch 118, top-1 = 0.82
I0122 16:29:02.023327 48361 caffe_interface.cpp:125] Batch 118, top-5 = 0.98
I0122 16:29:02.024646 48361 caffe_interface.cpp:125] Batch 119, loss = 0.594564
I0122 16:29:02.024653 48361 caffe_interface.cpp:125] Batch 119, top-1 = 0.9
I0122 16:29:02.024664 48361 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0122 16:29:02.026013 48361 caffe_interface.cpp:125] Batch 120, loss = 0.331631
I0122 16:29:02.026021 48361 caffe_interface.cpp:125] Batch 120, top-1 = 0.88
I0122 16:29:02.026026 48361 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0122 16:29:02.027230 48361 caffe_interface.cpp:125] Batch 121, loss = 0.995164
I0122 16:29:02.027236 48361 caffe_interface.cpp:125] Batch 121, top-1 = 0.8
I0122 16:29:02.027240 48361 caffe_interface.cpp:125] Batch 121, top-5 = 0.94
I0122 16:29:02.028461 48361 caffe_interface.cpp:125] Batch 122, loss = 0.686755
I0122 16:29:02.028470 48361 caffe_interface.cpp:125] Batch 122, top-1 = 0.78
I0122 16:29:02.028473 48361 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:29:02.029682 48361 caffe_interface.cpp:125] Batch 123, loss = 0.609075
I0122 16:29:02.029690 48361 caffe_interface.cpp:125] Batch 123, top-1 = 0.88
I0122 16:29:02.029695 48361 caffe_interface.cpp:125] Batch 123, top-5 = 0.96
I0122 16:29:02.030907 48361 caffe_interface.cpp:125] Batch 124, loss = 0.365955
I0122 16:29:02.030915 48361 caffe_interface.cpp:125] Batch 124, top-1 = 0.88
I0122 16:29:02.030918 48361 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0122 16:29:02.032137 48361 caffe_interface.cpp:125] Batch 125, loss = 0.55832
I0122 16:29:02.032145 48361 caffe_interface.cpp:125] Batch 125, top-1 = 0.86
I0122 16:29:02.032148 48361 caffe_interface.cpp:125] Batch 125, top-5 = 1
I0122 16:29:02.033375 48361 caffe_interface.cpp:125] Batch 126, loss = 0.754202
I0122 16:29:02.033383 48361 caffe_interface.cpp:125] Batch 126, top-1 = 0.82
I0122 16:29:02.033387 48361 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:29:02.034608 48361 caffe_interface.cpp:125] Batch 127, loss = 0.566441
I0122 16:29:02.034617 48361 caffe_interface.cpp:125] Batch 127, top-1 = 0.88
I0122 16:29:02.034629 48361 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0122 16:29:02.035838 48361 caffe_interface.cpp:125] Batch 128, loss = 0.515934
I0122 16:29:02.035845 48361 caffe_interface.cpp:125] Batch 128, top-1 = 0.88
I0122 16:29:02.035850 48361 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0122 16:29:02.037062 48361 caffe_interface.cpp:125] Batch 129, loss = 0.68171
I0122 16:29:02.037070 48361 caffe_interface.cpp:125] Batch 129, top-1 = 0.78
I0122 16:29:02.037075 48361 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0122 16:29:02.038290 48361 caffe_interface.cpp:125] Batch 130, loss = 0.573357
I0122 16:29:02.038297 48361 caffe_interface.cpp:125] Batch 130, top-1 = 0.82
I0122 16:29:02.038301 48361 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:29:02.039510 48361 caffe_interface.cpp:125] Batch 131, loss = 0.730439
I0122 16:29:02.039517 48361 caffe_interface.cpp:125] Batch 131, top-1 = 0.78
I0122 16:29:02.039521 48361 caffe_interface.cpp:125] Batch 131, top-5 = 0.96
I0122 16:29:02.040983 48361 caffe_interface.cpp:125] Batch 132, loss = 0.779961
I0122 16:29:02.040992 48361 caffe_interface.cpp:125] Batch 132, top-1 = 0.84
I0122 16:29:02.040995 48361 caffe_interface.cpp:125] Batch 132, top-5 = 0.96
I0122 16:29:02.042203 48361 caffe_interface.cpp:125] Batch 133, loss = 1.01541
I0122 16:29:02.042212 48361 caffe_interface.cpp:125] Batch 133, top-1 = 0.8
I0122 16:29:02.042215 48361 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0122 16:29:02.043416 48361 caffe_interface.cpp:125] Batch 134, loss = 0.700213
I0122 16:29:02.043424 48361 caffe_interface.cpp:125] Batch 134, top-1 = 0.78
I0122 16:29:02.043427 48361 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:29:02.044642 48361 caffe_interface.cpp:125] Batch 135, loss = 0.776495
I0122 16:29:02.044651 48361 caffe_interface.cpp:125] Batch 135, top-1 = 0.76
I0122 16:29:02.044654 48361 caffe_interface.cpp:125] Batch 135, top-5 = 1
I0122 16:29:02.045850 48361 caffe_interface.cpp:125] Batch 136, loss = 0.453282
I0122 16:29:02.045857 48361 caffe_interface.cpp:125] Batch 136, top-1 = 0.88
I0122 16:29:02.045861 48361 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0122 16:29:02.047058 48361 caffe_interface.cpp:125] Batch 137, loss = 0.484235
I0122 16:29:02.047066 48361 caffe_interface.cpp:125] Batch 137, top-1 = 0.84
I0122 16:29:02.047070 48361 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:29:02.048279 48361 caffe_interface.cpp:125] Batch 138, loss = 0.749421
I0122 16:29:02.048285 48361 caffe_interface.cpp:125] Batch 138, top-1 = 0.74
I0122 16:29:02.048290 48361 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0122 16:29:02.049510 48361 caffe_interface.cpp:125] Batch 139, loss = 0.776725
I0122 16:29:02.049520 48361 caffe_interface.cpp:125] Batch 139, top-1 = 0.84
I0122 16:29:02.049522 48361 caffe_interface.cpp:125] Batch 139, top-5 = 0.96
I0122 16:29:02.050735 48361 caffe_interface.cpp:125] Batch 140, loss = 0.587273
I0122 16:29:02.050743 48361 caffe_interface.cpp:125] Batch 140, top-1 = 0.84
I0122 16:29:02.050747 48361 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0122 16:29:02.051967 48361 caffe_interface.cpp:125] Batch 141, loss = 0.630875
I0122 16:29:02.051975 48361 caffe_interface.cpp:125] Batch 141, top-1 = 0.78
I0122 16:29:02.051980 48361 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:29:02.053189 48361 caffe_interface.cpp:125] Batch 142, loss = 0.173994
I0122 16:29:02.053195 48361 caffe_interface.cpp:125] Batch 142, top-1 = 0.9
I0122 16:29:02.053200 48361 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:29:02.054397 48361 caffe_interface.cpp:125] Batch 143, loss = 0.632107
I0122 16:29:02.054405 48361 caffe_interface.cpp:125] Batch 143, top-1 = 0.78
I0122 16:29:02.054409 48361 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0122 16:29:02.056557 48361 caffe_interface.cpp:125] Batch 144, loss = 0.812061
I0122 16:29:02.056565 48361 caffe_interface.cpp:125] Batch 144, top-1 = 0.8
I0122 16:29:02.056568 48361 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0122 16:29:02.057898 48361 caffe_interface.cpp:125] Batch 145, loss = 0.661561
I0122 16:29:02.057926 48361 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0122 16:29:02.057930 48361 caffe_interface.cpp:125] Batch 145, top-5 = 0.98
I0122 16:29:02.059262 48361 caffe_interface.cpp:125] Batch 146, loss = 0.347248
I0122 16:29:02.059275 48361 caffe_interface.cpp:125] Batch 146, top-1 = 0.88
I0122 16:29:02.059279 48361 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0122 16:29:02.060513 48361 caffe_interface.cpp:125] Batch 147, loss = 0.751159
I0122 16:29:02.060524 48361 caffe_interface.cpp:125] Batch 147, top-1 = 0.8
I0122 16:29:02.060528 48361 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0122 16:29:02.061743 48361 caffe_interface.cpp:125] Batch 148, loss = 0.280726
I0122 16:29:02.061750 48361 caffe_interface.cpp:125] Batch 148, top-1 = 0.88
I0122 16:29:02.061754 48361 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:29:02.062969 48361 caffe_interface.cpp:125] Batch 149, loss = 0.387906
I0122 16:29:02.062978 48361 caffe_interface.cpp:125] Batch 149, top-1 = 0.82
I0122 16:29:02.062980 48361 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:29:02.064199 48361 caffe_interface.cpp:125] Batch 150, loss = 0.235944
I0122 16:29:02.064206 48361 caffe_interface.cpp:125] Batch 150, top-1 = 0.88
I0122 16:29:02.064210 48361 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0122 16:29:02.065443 48361 caffe_interface.cpp:125] Batch 151, loss = 0.629173
I0122 16:29:02.065450 48361 caffe_interface.cpp:125] Batch 151, top-1 = 0.74
I0122 16:29:02.065454 48361 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:29:02.066674 48361 caffe_interface.cpp:125] Batch 152, loss = 0.559685
I0122 16:29:02.066682 48361 caffe_interface.cpp:125] Batch 152, top-1 = 0.84
I0122 16:29:02.066686 48361 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0122 16:29:02.067898 48361 caffe_interface.cpp:125] Batch 153, loss = 0.312694
I0122 16:29:02.067904 48361 caffe_interface.cpp:125] Batch 153, top-1 = 0.88
I0122 16:29:02.067909 48361 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:29:02.069135 48361 caffe_interface.cpp:125] Batch 154, loss = 0.473244
I0122 16:29:02.069144 48361 caffe_interface.cpp:125] Batch 154, top-1 = 0.84
I0122 16:29:02.069147 48361 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:29:02.070361 48361 caffe_interface.cpp:125] Batch 155, loss = 0.299908
I0122 16:29:02.070369 48361 caffe_interface.cpp:125] Batch 155, top-1 = 0.86
I0122 16:29:02.070374 48361 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:29:02.071599 48361 caffe_interface.cpp:125] Batch 156, loss = 0.537417
I0122 16:29:02.071605 48361 caffe_interface.cpp:125] Batch 156, top-1 = 0.84
I0122 16:29:02.071609 48361 caffe_interface.cpp:125] Batch 156, top-5 = 1
I0122 16:29:02.072830 48361 caffe_interface.cpp:125] Batch 157, loss = 0.27942
I0122 16:29:02.072837 48361 caffe_interface.cpp:125] Batch 157, top-1 = 0.92
I0122 16:29:02.072841 48361 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:29:02.074311 48361 caffe_interface.cpp:125] Batch 158, loss = 0.554341
I0122 16:29:02.074317 48361 caffe_interface.cpp:125] Batch 158, top-1 = 0.78
I0122 16:29:02.074321 48361 caffe_interface.cpp:125] Batch 158, top-5 = 1
I0122 16:29:02.075526 48361 caffe_interface.cpp:125] Batch 159, loss = 0.701396
I0122 16:29:02.075534 48361 caffe_interface.cpp:125] Batch 159, top-1 = 0.8
I0122 16:29:02.075538 48361 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0122 16:29:02.076750 48361 caffe_interface.cpp:125] Batch 160, loss = 0.40987
I0122 16:29:02.076757 48361 caffe_interface.cpp:125] Batch 160, top-1 = 0.86
I0122 16:29:02.076761 48361 caffe_interface.cpp:125] Batch 160, top-5 = 0.98
I0122 16:29:02.077978 48361 caffe_interface.cpp:125] Batch 161, loss = 0.444565
I0122 16:29:02.077986 48361 caffe_interface.cpp:125] Batch 161, top-1 = 0.82
I0122 16:29:02.077989 48361 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0122 16:29:02.079205 48361 caffe_interface.cpp:125] Batch 162, loss = 0.601039
I0122 16:29:02.079211 48361 caffe_interface.cpp:125] Batch 162, top-1 = 0.9
I0122 16:29:02.079216 48361 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:29:02.080448 48361 caffe_interface.cpp:125] Batch 163, loss = 0.304325
I0122 16:29:02.080457 48361 caffe_interface.cpp:125] Batch 163, top-1 = 0.86
I0122 16:29:02.080461 48361 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:29:02.081688 48361 caffe_interface.cpp:125] Batch 164, loss = 1.10919
I0122 16:29:02.081696 48361 caffe_interface.cpp:125] Batch 164, top-1 = 0.74
I0122 16:29:02.081701 48361 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:29:02.082919 48361 caffe_interface.cpp:125] Batch 165, loss = 0.59287
I0122 16:29:02.082927 48361 caffe_interface.cpp:125] Batch 165, top-1 = 0.86
I0122 16:29:02.082931 48361 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:29:02.084141 48361 caffe_interface.cpp:125] Batch 166, loss = 0.37874
I0122 16:29:02.084149 48361 caffe_interface.cpp:125] Batch 166, top-1 = 0.9
I0122 16:29:02.084153 48361 caffe_interface.cpp:125] Batch 166, top-5 = 0.98
I0122 16:29:02.085366 48361 caffe_interface.cpp:125] Batch 167, loss = 0.885219
I0122 16:29:02.085374 48361 caffe_interface.cpp:125] Batch 167, top-1 = 0.78
I0122 16:29:02.085378 48361 caffe_interface.cpp:125] Batch 167, top-5 = 0.96
I0122 16:29:02.086589 48361 caffe_interface.cpp:125] Batch 168, loss = 0.316995
I0122 16:29:02.086596 48361 caffe_interface.cpp:125] Batch 168, top-1 = 0.9
I0122 16:29:02.086601 48361 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:29:02.087826 48361 caffe_interface.cpp:125] Batch 169, loss = 0.621563
I0122 16:29:02.087832 48361 caffe_interface.cpp:125] Batch 169, top-1 = 0.76
I0122 16:29:02.087836 48361 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0122 16:29:02.090102 48361 caffe_interface.cpp:125] Batch 170, loss = 0.30126
I0122 16:29:02.090108 48361 caffe_interface.cpp:125] Batch 170, top-1 = 0.86
I0122 16:29:02.090111 48361 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0122 16:29:02.091398 48361 caffe_interface.cpp:125] Batch 171, loss = 0.384777
I0122 16:29:02.091404 48361 caffe_interface.cpp:125] Batch 171, top-1 = 0.8
I0122 16:29:02.091409 48361 caffe_interface.cpp:125] Batch 171, top-5 = 1
I0122 16:29:02.092804 48361 caffe_interface.cpp:125] Batch 172, loss = 0.956275
I0122 16:29:02.092813 48361 caffe_interface.cpp:125] Batch 172, top-1 = 0.78
I0122 16:29:02.092815 48361 caffe_interface.cpp:125] Batch 172, top-5 = 0.96
I0122 16:29:02.094043 48361 caffe_interface.cpp:125] Batch 173, loss = 0.378828
I0122 16:29:02.094051 48361 caffe_interface.cpp:125] Batch 173, top-1 = 0.9
I0122 16:29:02.094055 48361 caffe_interface.cpp:125] Batch 173, top-5 = 0.98
I0122 16:29:02.095278 48361 caffe_interface.cpp:125] Batch 174, loss = 0.526369
I0122 16:29:02.095285 48361 caffe_interface.cpp:125] Batch 174, top-1 = 0.82
I0122 16:29:02.095289 48361 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:29:02.096504 48361 caffe_interface.cpp:125] Batch 175, loss = 0.640829
I0122 16:29:02.096513 48361 caffe_interface.cpp:125] Batch 175, top-1 = 0.78
I0122 16:29:02.096518 48361 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0122 16:29:02.097730 48361 caffe_interface.cpp:125] Batch 176, loss = 1.23348
I0122 16:29:02.097738 48361 caffe_interface.cpp:125] Batch 176, top-1 = 0.76
I0122 16:29:02.097741 48361 caffe_interface.cpp:125] Batch 176, top-5 = 0.96
I0122 16:29:02.098963 48361 caffe_interface.cpp:125] Batch 177, loss = 0.632596
I0122 16:29:02.098971 48361 caffe_interface.cpp:125] Batch 177, top-1 = 0.8
I0122 16:29:02.098974 48361 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0122 16:29:02.100203 48361 caffe_interface.cpp:125] Batch 178, loss = 0.547472
I0122 16:29:02.100210 48361 caffe_interface.cpp:125] Batch 178, top-1 = 0.88
I0122 16:29:02.100214 48361 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0122 16:29:02.101428 48361 caffe_interface.cpp:125] Batch 179, loss = 0.397035
I0122 16:29:02.101434 48361 caffe_interface.cpp:125] Batch 179, top-1 = 0.86
I0122 16:29:02.101438 48361 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:29:02.101441 48361 caffe_interface.cpp:130] Loss: 0.587223
I0122 16:29:02.101447 48361 caffe_interface.cpp:142] loss = 0.587223 (* 1 = 0.587223 loss)
I0122 16:29:02.101464 48361 caffe_interface.cpp:142] top-1 = 0.83
I0122 16:29:02.101469 48361 caffe_interface.cpp:142] top-5 = 0.989778
I0122 16:29:02.251040 48361 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/sparse.caffemodel
I0122 16:29:02.251075 48361 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.2:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.829999626    | -0.0326668024  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 60749          | -11.1713886%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 40636928       | -17.1582756%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config2.prototxt
## fine-tuning: second run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config2.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune2_miniVggNet.txt
I0122 16:29:02.483845 49871 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0122 16:29:02.661759 49871 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:29:02.662263 49871 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24873074688, dev_info[0]: total=25620447232 free=24873074688
I0122 16:29:02.662274 49871 caffe_interface.cpp:493] Using GPUs 0
I0122 16:29:02.662528 49871 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:29:03.243997 49871 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt"
type: "SGD"
I0122 16:29:03.244112 49871 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0122 16:29:03.244340 49871 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:29:03.244354 49871 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:29:03.244356 49871 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:29:03.244498 49871 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:29:03.244558 49871 layer_factory.hpp:77] Creating layer data
I0122 16:29:03.244638 49871 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:03.245144 49871 net.cpp:94] Creating Layer data
I0122 16:29:03.245153 49871 net.cpp:409] data -> data
I0122 16:29:03.245177 49871 net.cpp:409] data -> label
I0122 16:29:03.246697 49910 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:29:03.246750 49910 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:29:03.246836 49871 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:29:03.246913 49871 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:29:03.254438 49871 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:03.254482 49871 net.cpp:144] Setting up data
I0122 16:29:03.254490 49871 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:29:03.254493 49871 net.cpp:151] Top shape: 128 (128)
I0122 16:29:03.254496 49871 net.cpp:159] Memory required for data: 1573376
I0122 16:29:03.254501 49871 layer_factory.hpp:77] Creating layer conv1
I0122 16:29:03.254513 49871 net.cpp:94] Creating Layer conv1
I0122 16:29:03.254518 49871 net.cpp:435] conv1 <- data
I0122 16:29:03.254532 49871 net.cpp:409] conv1 -> conv1
I0122 16:29:03.255542 49871 net.cpp:144] Setting up conv1
I0122 16:29:03.255554 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.255558 49871 net.cpp:159] Memory required for data: 18350592
I0122 16:29:03.255573 49871 layer_factory.hpp:77] Creating layer bn1
I0122 16:29:03.255583 49871 net.cpp:94] Creating Layer bn1
I0122 16:29:03.255586 49871 net.cpp:435] bn1 <- conv1
I0122 16:29:03.255591 49871 net.cpp:409] bn1 -> scale1
I0122 16:29:03.256173 49871 net.cpp:144] Setting up bn1
I0122 16:29:03.256181 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.256183 49871 net.cpp:159] Memory required for data: 35127808
I0122 16:29:03.256193 49871 layer_factory.hpp:77] Creating layer relu1
I0122 16:29:03.256201 49871 net.cpp:94] Creating Layer relu1
I0122 16:29:03.256204 49871 net.cpp:435] relu1 <- scale1
I0122 16:29:03.256208 49871 net.cpp:409] relu1 -> relu1
I0122 16:29:03.256228 49871 net.cpp:144] Setting up relu1
I0122 16:29:03.256234 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.256237 49871 net.cpp:159] Memory required for data: 51905024
I0122 16:29:03.256239 49871 layer_factory.hpp:77] Creating layer conv2
I0122 16:29:03.256247 49871 net.cpp:94] Creating Layer conv2
I0122 16:29:03.256253 49871 net.cpp:435] conv2 <- relu1
I0122 16:29:03.256258 49871 net.cpp:409] conv2 -> conv2
I0122 16:29:03.257793 49871 net.cpp:144] Setting up conv2
I0122 16:29:03.257804 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.257807 49871 net.cpp:159] Memory required for data: 68682240
I0122 16:29:03.257817 49871 layer_factory.hpp:77] Creating layer bn2
I0122 16:29:03.257825 49871 net.cpp:94] Creating Layer bn2
I0122 16:29:03.257831 49871 net.cpp:435] bn2 <- conv2
I0122 16:29:03.257838 49871 net.cpp:409] bn2 -> scale2
I0122 16:29:03.258617 49871 net.cpp:144] Setting up bn2
I0122 16:29:03.258625 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.258627 49871 net.cpp:159] Memory required for data: 85459456
I0122 16:29:03.258636 49871 layer_factory.hpp:77] Creating layer relu2
I0122 16:29:03.258641 49871 net.cpp:94] Creating Layer relu2
I0122 16:29:03.258644 49871 net.cpp:435] relu2 <- scale2
I0122 16:29:03.258649 49871 net.cpp:409] relu2 -> relu2
I0122 16:29:03.258671 49871 net.cpp:144] Setting up relu2
I0122 16:29:03.258677 49871 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:29:03.258680 49871 net.cpp:159] Memory required for data: 102236672
I0122 16:29:03.258682 49871 layer_factory.hpp:77] Creating layer pool1
I0122 16:29:03.258688 49871 net.cpp:94] Creating Layer pool1
I0122 16:29:03.258699 49871 net.cpp:435] pool1 <- relu2
I0122 16:29:03.258703 49871 net.cpp:409] pool1 -> pool1
I0122 16:29:03.258874 49871 net.cpp:144] Setting up pool1
I0122 16:29:03.258882 49871 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:29:03.258884 49871 net.cpp:159] Memory required for data: 106430976
I0122 16:29:03.258888 49871 layer_factory.hpp:77] Creating layer drop1
I0122 16:29:03.258893 49871 net.cpp:94] Creating Layer drop1
I0122 16:29:03.258896 49871 net.cpp:435] drop1 <- pool1
I0122 16:29:03.258913 49871 net.cpp:409] drop1 -> drop1
I0122 16:29:03.258957 49871 net.cpp:144] Setting up drop1
I0122 16:29:03.258962 49871 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:29:03.258965 49871 net.cpp:159] Memory required for data: 110625280
I0122 16:29:03.258968 49871 layer_factory.hpp:77] Creating layer conv3
I0122 16:29:03.258976 49871 net.cpp:94] Creating Layer conv3
I0122 16:29:03.258980 49871 net.cpp:435] conv3 <- drop1
I0122 16:29:03.258986 49871 net.cpp:409] conv3 -> conv3
I0122 16:29:03.259958 49871 net.cpp:144] Setting up conv3
I0122 16:29:03.259969 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.259972 49871 net.cpp:159] Memory required for data: 119013888
I0122 16:29:03.259979 49871 layer_factory.hpp:77] Creating layer bn3
I0122 16:29:03.259985 49871 net.cpp:94] Creating Layer bn3
I0122 16:29:03.259990 49871 net.cpp:435] bn3 <- conv3
I0122 16:29:03.259996 49871 net.cpp:409] bn3 -> scale3
I0122 16:29:03.260612 49871 net.cpp:144] Setting up bn3
I0122 16:29:03.260617 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.260622 49871 net.cpp:159] Memory required for data: 127402496
I0122 16:29:03.260632 49871 layer_factory.hpp:77] Creating layer relu3
I0122 16:29:03.260638 49871 net.cpp:94] Creating Layer relu3
I0122 16:29:03.260641 49871 net.cpp:435] relu3 <- scale3
I0122 16:29:03.260645 49871 net.cpp:409] relu3 -> relu3
I0122 16:29:03.260663 49871 net.cpp:144] Setting up relu3
I0122 16:29:03.260669 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.260673 49871 net.cpp:159] Memory required for data: 135791104
I0122 16:29:03.260676 49871 layer_factory.hpp:77] Creating layer conv4
I0122 16:29:03.260684 49871 net.cpp:94] Creating Layer conv4
I0122 16:29:03.260689 49871 net.cpp:435] conv4 <- relu3
I0122 16:29:03.260694 49871 net.cpp:409] conv4 -> conv4
I0122 16:29:03.261106 49871 net.cpp:144] Setting up conv4
I0122 16:29:03.261113 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.261117 49871 net.cpp:159] Memory required for data: 144179712
I0122 16:29:03.261122 49871 layer_factory.hpp:77] Creating layer bn4
I0122 16:29:03.261128 49871 net.cpp:94] Creating Layer bn4
I0122 16:29:03.261132 49871 net.cpp:435] bn4 <- conv4
I0122 16:29:03.261137 49871 net.cpp:409] bn4 -> scale4
I0122 16:29:03.261783 49871 net.cpp:144] Setting up bn4
I0122 16:29:03.261790 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.261792 49871 net.cpp:159] Memory required for data: 152568320
I0122 16:29:03.261801 49871 layer_factory.hpp:77] Creating layer relu4
I0122 16:29:03.261806 49871 net.cpp:94] Creating Layer relu4
I0122 16:29:03.261808 49871 net.cpp:435] relu4 <- scale4
I0122 16:29:03.261812 49871 net.cpp:409] relu4 -> relu4
I0122 16:29:03.261839 49871 net.cpp:144] Setting up relu4
I0122 16:29:03.261845 49871 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:29:03.261848 49871 net.cpp:159] Memory required for data: 160956928
I0122 16:29:03.261850 49871 layer_factory.hpp:77] Creating layer pool2
I0122 16:29:03.261857 49871 net.cpp:94] Creating Layer pool2
I0122 16:29:03.261859 49871 net.cpp:435] pool2 <- relu4
I0122 16:29:03.261864 49871 net.cpp:409] pool2 -> pool2
I0122 16:29:03.261893 49871 net.cpp:144] Setting up pool2
I0122 16:29:03.261899 49871 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:29:03.261904 49871 net.cpp:159] Memory required for data: 163054080
I0122 16:29:03.261914 49871 layer_factory.hpp:77] Creating layer drop2
I0122 16:29:03.261919 49871 net.cpp:94] Creating Layer drop2
I0122 16:29:03.261922 49871 net.cpp:435] drop2 <- pool2
I0122 16:29:03.261926 49871 net.cpp:409] drop2 -> drop2
I0122 16:29:03.261952 49871 net.cpp:144] Setting up drop2
I0122 16:29:03.261957 49871 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:29:03.261960 49871 net.cpp:159] Memory required for data: 165151232
I0122 16:29:03.261963 49871 layer_factory.hpp:77] Creating layer fc1
I0122 16:29:03.261970 49871 net.cpp:94] Creating Layer fc1
I0122 16:29:03.261974 49871 net.cpp:435] fc1 <- drop2
I0122 16:29:03.261979 49871 net.cpp:409] fc1 -> fc1
I0122 16:29:03.276142 49871 net.cpp:144] Setting up fc1
I0122 16:29:03.276160 49871 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:29:03.276165 49871 net.cpp:159] Memory required for data: 165413376
I0122 16:29:03.276172 49871 layer_factory.hpp:77] Creating layer bn5
I0122 16:29:03.276180 49871 net.cpp:94] Creating Layer bn5
I0122 16:29:03.276183 49871 net.cpp:435] bn5 <- fc1
I0122 16:29:03.276190 49871 net.cpp:409] bn5 -> scale5
I0122 16:29:03.276747 49871 net.cpp:144] Setting up bn5
I0122 16:29:03.276754 49871 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:29:03.276757 49871 net.cpp:159] Memory required for data: 165675520
I0122 16:29:03.276769 49871 layer_factory.hpp:77] Creating layer relu5
I0122 16:29:03.276777 49871 net.cpp:94] Creating Layer relu5
I0122 16:29:03.276779 49871 net.cpp:435] relu5 <- scale5
I0122 16:29:03.276785 49871 net.cpp:409] relu5 -> relu5
I0122 16:29:03.276804 49871 net.cpp:144] Setting up relu5
I0122 16:29:03.276813 49871 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:29:03.276815 49871 net.cpp:159] Memory required for data: 165937664
I0122 16:29:03.276818 49871 layer_factory.hpp:77] Creating layer drop3
I0122 16:29:03.276823 49871 net.cpp:94] Creating Layer drop3
I0122 16:29:03.276826 49871 net.cpp:435] drop3 <- relu5
I0122 16:29:03.276830 49871 net.cpp:409] drop3 -> drop3
I0122 16:29:03.276856 49871 net.cpp:144] Setting up drop3
I0122 16:29:03.276861 49871 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:29:03.276865 49871 net.cpp:159] Memory required for data: 166199808
I0122 16:29:03.276867 49871 layer_factory.hpp:77] Creating layer fc2
I0122 16:29:03.276872 49871 net.cpp:94] Creating Layer fc2
I0122 16:29:03.276875 49871 net.cpp:435] fc2 <- drop3
I0122 16:29:03.276881 49871 net.cpp:409] fc2 -> fc2
I0122 16:29:03.277034 49871 net.cpp:144] Setting up fc2
I0122 16:29:03.277040 49871 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:29:03.277043 49871 net.cpp:159] Memory required for data: 166204928
I0122 16:29:03.277048 49871 layer_factory.hpp:77] Creating layer loss
I0122 16:29:03.277055 49871 net.cpp:94] Creating Layer loss
I0122 16:29:03.277058 49871 net.cpp:435] loss <- fc2
I0122 16:29:03.277061 49871 net.cpp:435] loss <- label
I0122 16:29:03.277066 49871 net.cpp:409] loss -> loss
I0122 16:29:03.277073 49871 layer_factory.hpp:77] Creating layer loss
I0122 16:29:03.277990 49871 net.cpp:144] Setting up loss
I0122 16:29:03.278010 49871 net.cpp:151] Top shape: (1)
I0122 16:29:03.278013 49871 net.cpp:154]     with loss weight 1
I0122 16:29:03.278023 49871 net.cpp:159] Memory required for data: 166204932
I0122 16:29:03.278025 49871 net.cpp:220] loss needs backward computation.
I0122 16:29:03.278040 49871 net.cpp:220] fc2 needs backward computation.
I0122 16:29:03.278044 49871 net.cpp:220] drop3 needs backward computation.
I0122 16:29:03.278046 49871 net.cpp:220] relu5 needs backward computation.
I0122 16:29:03.278050 49871 net.cpp:220] bn5 needs backward computation.
I0122 16:29:03.278053 49871 net.cpp:220] fc1 needs backward computation.
I0122 16:29:03.278064 49871 net.cpp:220] drop2 needs backward computation.
I0122 16:29:03.278067 49871 net.cpp:220] pool2 needs backward computation.
I0122 16:29:03.278070 49871 net.cpp:220] relu4 needs backward computation.
I0122 16:29:03.278074 49871 net.cpp:220] bn4 needs backward computation.
I0122 16:29:03.278077 49871 net.cpp:220] conv4 needs backward computation.
I0122 16:29:03.278080 49871 net.cpp:220] relu3 needs backward computation.
I0122 16:29:03.278084 49871 net.cpp:220] bn3 needs backward computation.
I0122 16:29:03.278087 49871 net.cpp:220] conv3 needs backward computation.
I0122 16:29:03.278091 49871 net.cpp:220] drop1 needs backward computation.
I0122 16:29:03.278093 49871 net.cpp:220] pool1 needs backward computation.
I0122 16:29:03.278096 49871 net.cpp:220] relu2 needs backward computation.
I0122 16:29:03.278100 49871 net.cpp:220] bn2 needs backward computation.
I0122 16:29:03.278103 49871 net.cpp:220] conv2 needs backward computation.
I0122 16:29:03.278106 49871 net.cpp:220] relu1 needs backward computation.
I0122 16:29:03.278128 49871 net.cpp:220] bn1 needs backward computation.
I0122 16:29:03.278131 49871 net.cpp:220] conv1 needs backward computation.
I0122 16:29:03.278136 49871 net.cpp:222] data does not need backward computation.
I0122 16:29:03.278139 49871 net.cpp:264] This network produces output loss
I0122 16:29:03.278157 49871 net.cpp:284] Network initialization done.
I0122 16:29:03.278470 49871 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/net_finetune.prototxt
I0122 16:29:03.278504 49871 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:29:03.278702 49871 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:29:03.278800 49871 layer_factory.hpp:77] Creating layer data
I0122 16:29:03.278841 49871 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:03.279732 49871 net.cpp:94] Creating Layer data
I0122 16:29:03.279742 49871 net.cpp:409] data -> data
I0122 16:29:03.279753 49871 net.cpp:409] data -> label
I0122 16:29:03.280740 49940 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:29:03.280774 49940 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:29:03.280846 49871 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:29:03.280925 49871 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:29:03.284464 49871 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:29:03.284518 49871 net.cpp:144] Setting up data
I0122 16:29:03.284525 49871 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:29:03.284529 49871 net.cpp:151] Top shape: 50 (50)
I0122 16:29:03.284533 49871 net.cpp:159] Memory required for data: 614600
I0122 16:29:03.284536 49871 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:29:03.284546 49871 net.cpp:94] Creating Layer label_data_1_split
I0122 16:29:03.284551 49871 net.cpp:435] label_data_1_split <- label
I0122 16:29:03.284559 49871 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:29:03.284566 49871 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:29:03.284571 49871 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:29:03.284693 49871 net.cpp:144] Setting up label_data_1_split
I0122 16:29:03.284699 49871 net.cpp:151] Top shape: 50 (50)
I0122 16:29:03.284703 49871 net.cpp:151] Top shape: 50 (50)
I0122 16:29:03.284708 49871 net.cpp:151] Top shape: 50 (50)
I0122 16:29:03.284709 49871 net.cpp:159] Memory required for data: 615200
I0122 16:29:03.284713 49871 layer_factory.hpp:77] Creating layer conv1
I0122 16:29:03.284723 49871 net.cpp:94] Creating Layer conv1
I0122 16:29:03.284727 49871 net.cpp:435] conv1 <- data
I0122 16:29:03.284734 49871 net.cpp:409] conv1 -> conv1
I0122 16:29:03.285063 49871 net.cpp:144] Setting up conv1
I0122 16:29:03.285069 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.285073 49871 net.cpp:159] Memory required for data: 7168800
I0122 16:29:03.285081 49871 layer_factory.hpp:77] Creating layer bn1
I0122 16:29:03.285089 49871 net.cpp:94] Creating Layer bn1
I0122 16:29:03.285094 49871 net.cpp:435] bn1 <- conv1
I0122 16:29:03.285099 49871 net.cpp:409] bn1 -> scale1
I0122 16:29:03.285749 49871 net.cpp:144] Setting up bn1
I0122 16:29:03.285756 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.285759 49871 net.cpp:159] Memory required for data: 13722400
I0122 16:29:03.285770 49871 layer_factory.hpp:77] Creating layer relu1
I0122 16:29:03.285778 49871 net.cpp:94] Creating Layer relu1
I0122 16:29:03.285780 49871 net.cpp:435] relu1 <- scale1
I0122 16:29:03.285785 49871 net.cpp:409] relu1 -> relu1
I0122 16:29:03.285804 49871 net.cpp:144] Setting up relu1
I0122 16:29:03.285809 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.285811 49871 net.cpp:159] Memory required for data: 20276000
I0122 16:29:03.285815 49871 layer_factory.hpp:77] Creating layer conv2
I0122 16:29:03.285821 49871 net.cpp:94] Creating Layer conv2
I0122 16:29:03.285826 49871 net.cpp:435] conv2 <- relu1
I0122 16:29:03.285831 49871 net.cpp:409] conv2 -> conv2
I0122 16:29:03.286118 49871 net.cpp:144] Setting up conv2
I0122 16:29:03.286124 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.286128 49871 net.cpp:159] Memory required for data: 26829600
I0122 16:29:03.286134 49871 layer_factory.hpp:77] Creating layer bn2
I0122 16:29:03.286140 49871 net.cpp:94] Creating Layer bn2
I0122 16:29:03.286144 49871 net.cpp:435] bn2 <- conv2
I0122 16:29:03.286149 49871 net.cpp:409] bn2 -> scale2
I0122 16:29:03.287142 49871 net.cpp:144] Setting up bn2
I0122 16:29:03.287148 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.287151 49871 net.cpp:159] Memory required for data: 33383200
I0122 16:29:03.287159 49871 layer_factory.hpp:77] Creating layer relu2
I0122 16:29:03.287164 49871 net.cpp:94] Creating Layer relu2
I0122 16:29:03.287170 49871 net.cpp:435] relu2 <- scale2
I0122 16:29:03.287175 49871 net.cpp:409] relu2 -> relu2
I0122 16:29:03.287221 49871 net.cpp:144] Setting up relu2
I0122 16:29:03.287227 49871 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:29:03.287230 49871 net.cpp:159] Memory required for data: 39936800
I0122 16:29:03.287233 49871 layer_factory.hpp:77] Creating layer pool1
I0122 16:29:03.287238 49871 net.cpp:94] Creating Layer pool1
I0122 16:29:03.287245 49871 net.cpp:435] pool1 <- relu2
I0122 16:29:03.287250 49871 net.cpp:409] pool1 -> pool1
I0122 16:29:03.287317 49871 net.cpp:144] Setting up pool1
I0122 16:29:03.287331 49871 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:29:03.287333 49871 net.cpp:159] Memory required for data: 41575200
I0122 16:29:03.287338 49871 layer_factory.hpp:77] Creating layer drop1
I0122 16:29:03.287343 49871 net.cpp:94] Creating Layer drop1
I0122 16:29:03.287345 49871 net.cpp:435] drop1 <- pool1
I0122 16:29:03.287349 49871 net.cpp:409] drop1 -> drop1
I0122 16:29:03.287411 49871 net.cpp:144] Setting up drop1
I0122 16:29:03.287417 49871 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:29:03.287420 49871 net.cpp:159] Memory required for data: 43213600
I0122 16:29:03.287423 49871 layer_factory.hpp:77] Creating layer conv3
I0122 16:29:03.287431 49871 net.cpp:94] Creating Layer conv3
I0122 16:29:03.287436 49871 net.cpp:435] conv3 <- drop1
I0122 16:29:03.287441 49871 net.cpp:409] conv3 -> conv3
I0122 16:29:03.287811 49871 net.cpp:144] Setting up conv3
I0122 16:29:03.287818 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.287822 49871 net.cpp:159] Memory required for data: 46490400
I0122 16:29:03.287825 49871 layer_factory.hpp:77] Creating layer bn3
I0122 16:29:03.287832 49871 net.cpp:94] Creating Layer bn3
I0122 16:29:03.287838 49871 net.cpp:435] bn3 <- conv3
I0122 16:29:03.287843 49871 net.cpp:409] bn3 -> scale3
I0122 16:29:03.288503 49871 net.cpp:144] Setting up bn3
I0122 16:29:03.288511 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.288514 49871 net.cpp:159] Memory required for data: 49767200
I0122 16:29:03.288524 49871 layer_factory.hpp:77] Creating layer relu3
I0122 16:29:03.288530 49871 net.cpp:94] Creating Layer relu3
I0122 16:29:03.288533 49871 net.cpp:435] relu3 <- scale3
I0122 16:29:03.288538 49871 net.cpp:409] relu3 -> relu3
I0122 16:29:03.288555 49871 net.cpp:144] Setting up relu3
I0122 16:29:03.288560 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.288563 49871 net.cpp:159] Memory required for data: 53044000
I0122 16:29:03.288566 49871 layer_factory.hpp:77] Creating layer conv4
I0122 16:29:03.288574 49871 net.cpp:94] Creating Layer conv4
I0122 16:29:03.288578 49871 net.cpp:435] conv4 <- relu3
I0122 16:29:03.288584 49871 net.cpp:409] conv4 -> conv4
I0122 16:29:03.289022 49871 net.cpp:144] Setting up conv4
I0122 16:29:03.289029 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.289032 49871 net.cpp:159] Memory required for data: 56320800
I0122 16:29:03.289036 49871 layer_factory.hpp:77] Creating layer bn4
I0122 16:29:03.289046 49871 net.cpp:94] Creating Layer bn4
I0122 16:29:03.289050 49871 net.cpp:435] bn4 <- conv4
I0122 16:29:03.289057 49871 net.cpp:409] bn4 -> scale4
I0122 16:29:03.289747 49871 net.cpp:144] Setting up bn4
I0122 16:29:03.289754 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.289757 49871 net.cpp:159] Memory required for data: 59597600
I0122 16:29:03.289764 49871 layer_factory.hpp:77] Creating layer relu4
I0122 16:29:03.289769 49871 net.cpp:94] Creating Layer relu4
I0122 16:29:03.289772 49871 net.cpp:435] relu4 <- scale4
I0122 16:29:03.289778 49871 net.cpp:409] relu4 -> relu4
I0122 16:29:03.289795 49871 net.cpp:144] Setting up relu4
I0122 16:29:03.289803 49871 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:29:03.289806 49871 net.cpp:159] Memory required for data: 62874400
I0122 16:29:03.289809 49871 layer_factory.hpp:77] Creating layer pool2
I0122 16:29:03.289815 49871 net.cpp:94] Creating Layer pool2
I0122 16:29:03.289819 49871 net.cpp:435] pool2 <- relu4
I0122 16:29:03.289824 49871 net.cpp:409] pool2 -> pool2
I0122 16:29:03.289924 49871 net.cpp:144] Setting up pool2
I0122 16:29:03.289930 49871 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:29:03.289934 49871 net.cpp:159] Memory required for data: 63693600
I0122 16:29:03.289937 49871 layer_factory.hpp:77] Creating layer drop2
I0122 16:29:03.289942 49871 net.cpp:94] Creating Layer drop2
I0122 16:29:03.289945 49871 net.cpp:435] drop2 <- pool2
I0122 16:29:03.289950 49871 net.cpp:409] drop2 -> drop2
I0122 16:29:03.289983 49871 net.cpp:144] Setting up drop2
I0122 16:29:03.289989 49871 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:29:03.290004 49871 net.cpp:159] Memory required for data: 64512800
I0122 16:29:03.290007 49871 layer_factory.hpp:77] Creating layer fc1
I0122 16:29:03.290014 49871 net.cpp:94] Creating Layer fc1
I0122 16:29:03.290017 49871 net.cpp:435] fc1 <- drop2
I0122 16:29:03.290024 49871 net.cpp:409] fc1 -> fc1
I0122 16:29:03.304014 49871 net.cpp:144] Setting up fc1
I0122 16:29:03.304033 49871 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:03.304035 49871 net.cpp:159] Memory required for data: 64615200
I0122 16:29:03.304042 49871 layer_factory.hpp:77] Creating layer bn5
I0122 16:29:03.304051 49871 net.cpp:94] Creating Layer bn5
I0122 16:29:03.304056 49871 net.cpp:435] bn5 <- fc1
I0122 16:29:03.304062 49871 net.cpp:409] bn5 -> scale5
I0122 16:29:03.304611 49871 net.cpp:144] Setting up bn5
I0122 16:29:03.304617 49871 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:03.304620 49871 net.cpp:159] Memory required for data: 64717600
I0122 16:29:03.304633 49871 layer_factory.hpp:77] Creating layer relu5
I0122 16:29:03.304641 49871 net.cpp:94] Creating Layer relu5
I0122 16:29:03.304643 49871 net.cpp:435] relu5 <- scale5
I0122 16:29:03.304648 49871 net.cpp:409] relu5 -> relu5
I0122 16:29:03.304667 49871 net.cpp:144] Setting up relu5
I0122 16:29:03.304672 49871 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:03.304677 49871 net.cpp:159] Memory required for data: 64820000
I0122 16:29:03.304679 49871 layer_factory.hpp:77] Creating layer drop3
I0122 16:29:03.304683 49871 net.cpp:94] Creating Layer drop3
I0122 16:29:03.304687 49871 net.cpp:435] drop3 <- relu5
I0122 16:29:03.304692 49871 net.cpp:409] drop3 -> drop3
I0122 16:29:03.304721 49871 net.cpp:144] Setting up drop3
I0122 16:29:03.304726 49871 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:29:03.304728 49871 net.cpp:159] Memory required for data: 64922400
I0122 16:29:03.304731 49871 layer_factory.hpp:77] Creating layer fc2
I0122 16:29:03.304738 49871 net.cpp:94] Creating Layer fc2
I0122 16:29:03.304742 49871 net.cpp:435] fc2 <- drop3
I0122 16:29:03.304747 49871 net.cpp:409] fc2 -> fc2
I0122 16:29:03.304885 49871 net.cpp:144] Setting up fc2
I0122 16:29:03.304890 49871 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:03.304893 49871 net.cpp:159] Memory required for data: 64924400
I0122 16:29:03.304898 49871 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:29:03.304903 49871 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:29:03.304906 49871 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:29:03.304911 49871 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:29:03.304919 49871 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:29:03.304924 49871 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:29:03.304963 49871 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:29:03.304968 49871 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:03.304971 49871 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:03.304975 49871 net.cpp:151] Top shape: 50 10 (500)
I0122 16:29:03.304977 49871 net.cpp:159] Memory required for data: 64930400
I0122 16:29:03.304980 49871 layer_factory.hpp:77] Creating layer loss
I0122 16:29:03.304985 49871 net.cpp:94] Creating Layer loss
I0122 16:29:03.304987 49871 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:29:03.304991 49871 net.cpp:435] loss <- label_data_1_split_0
I0122 16:29:03.304997 49871 net.cpp:409] loss -> loss
I0122 16:29:03.305004 49871 layer_factory.hpp:77] Creating layer loss
I0122 16:29:03.305076 49871 net.cpp:144] Setting up loss
I0122 16:29:03.305083 49871 net.cpp:151] Top shape: (1)
I0122 16:29:03.305085 49871 net.cpp:154]     with loss weight 1
I0122 16:29:03.305094 49871 net.cpp:159] Memory required for data: 64930404
I0122 16:29:03.305096 49871 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:29:03.305104 49871 net.cpp:94] Creating Layer accuracy-top1
I0122 16:29:03.305109 49871 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:29:03.305111 49871 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:29:03.305117 49871 net.cpp:409] accuracy-top1 -> top-1
I0122 16:29:03.305135 49871 net.cpp:144] Setting up accuracy-top1
I0122 16:29:03.305140 49871 net.cpp:151] Top shape: (1)
I0122 16:29:03.305141 49871 net.cpp:159] Memory required for data: 64930408
I0122 16:29:03.305145 49871 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:29:03.305150 49871 net.cpp:94] Creating Layer accuracy-top5
I0122 16:29:03.305152 49871 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:29:03.305155 49871 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:29:03.305161 49871 net.cpp:409] accuracy-top5 -> top-5
I0122 16:29:03.305167 49871 net.cpp:144] Setting up accuracy-top5
I0122 16:29:03.305172 49871 net.cpp:151] Top shape: (1)
I0122 16:29:03.305174 49871 net.cpp:159] Memory required for data: 64930412
I0122 16:29:03.305177 49871 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:29:03.305181 49871 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:29:03.305184 49871 net.cpp:220] loss needs backward computation.
I0122 16:29:03.305188 49871 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:29:03.305191 49871 net.cpp:220] fc2 needs backward computation.
I0122 16:29:03.305194 49871 net.cpp:220] drop3 needs backward computation.
I0122 16:29:03.305197 49871 net.cpp:220] relu5 needs backward computation.
I0122 16:29:03.305200 49871 net.cpp:220] bn5 needs backward computation.
I0122 16:29:03.305203 49871 net.cpp:220] fc1 needs backward computation.
I0122 16:29:03.305207 49871 net.cpp:220] drop2 needs backward computation.
I0122 16:29:03.305209 49871 net.cpp:220] pool2 needs backward computation.
I0122 16:29:03.305212 49871 net.cpp:220] relu4 needs backward computation.
I0122 16:29:03.305215 49871 net.cpp:220] bn4 needs backward computation.
I0122 16:29:03.305218 49871 net.cpp:220] conv4 needs backward computation.
I0122 16:29:03.305222 49871 net.cpp:220] relu3 needs backward computation.
I0122 16:29:03.305224 49871 net.cpp:220] bn3 needs backward computation.
I0122 16:29:03.305227 49871 net.cpp:220] conv3 needs backward computation.
I0122 16:29:03.305230 49871 net.cpp:220] drop1 needs backward computation.
I0122 16:29:03.305233 49871 net.cpp:220] pool1 needs backward computation.
I0122 16:29:03.305236 49871 net.cpp:220] relu2 needs backward computation.
I0122 16:29:03.305239 49871 net.cpp:220] bn2 needs backward computation.
I0122 16:29:03.305243 49871 net.cpp:220] conv2 needs backward computation.
I0122 16:29:03.305245 49871 net.cpp:220] relu1 needs backward computation.
I0122 16:29:03.305248 49871 net.cpp:220] bn1 needs backward computation.
I0122 16:29:03.305251 49871 net.cpp:220] conv1 needs backward computation.
I0122 16:29:03.305255 49871 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:29:03.305259 49871 net.cpp:222] data does not need backward computation.
I0122 16:29:03.305261 49871 net.cpp:264] This network produces output loss
I0122 16:29:03.305264 49871 net.cpp:264] This network produces output top-1
I0122 16:29:03.305269 49871 net.cpp:264] This network produces output top-5
I0122 16:29:03.305290 49871 net.cpp:284] Network initialization done.
I0122 16:29:03.305392 49871 solver.cpp:63] Solver scaffolding done.
I0122 16:29:03.306550 49871 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/sparse.caffemodel
I0122 16:29:03.364380 49871 caffe_interface.cpp:527] Starting Optimization
I0122 16:29:03.364398 49871 solver.cpp:335] Solving 
I0122 16:29:03.364400 49871 solver.cpp:336] Learning Rate Policy: poly
I0122 16:29:03.365641 49871 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:29:03.591739 49871 solver.cpp:517]     Test net output #0: loss = 0.587223 (* 1 = 0.587223 loss)
I0122 16:29:03.591758 49871 solver.cpp:517]     Test net output #1: top-1 = 0.83
I0122 16:29:03.591763 49871 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0122 16:29:03.609283 49871 solver.cpp:266] Iteration 0 (0 iter/s, 0.244841s/100 iter), loss = 0.117625
I0122 16:29:03.609316 49871 solver.cpp:285]     Train net output #0: loss = 0.117625 (* 1 = 0.117625 loss)
I0122 16:29:03.609341 49871 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:29:04.473338 49871 solver.cpp:266] Iteration 100 (115.744 iter/s, 0.863976s/100 iter), loss = 0.166025
I0122 16:29:04.473366 49871 solver.cpp:285]     Train net output #0: loss = 0.166025 (* 1 = 0.166025 loss)
I0122 16:29:04.473372 49871 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:29:05.336613 49871 solver.cpp:266] Iteration 200 (115.848 iter/s, 0.863203s/100 iter), loss = 0.19342
I0122 16:29:05.336642 49871 solver.cpp:285]     Train net output #0: loss = 0.19342 (* 1 = 0.19342 loss)
I0122 16:29:05.336647 49871 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:29:06.199862 49871 solver.cpp:266] Iteration 300 (115.851 iter/s, 0.863176s/100 iter), loss = 0.190621
I0122 16:29:06.199889 49871 solver.cpp:285]     Train net output #0: loss = 0.190621 (* 1 = 0.190621 loss)
I0122 16:29:06.199895 49871 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:29:07.062142 49871 solver.cpp:266] Iteration 400 (115.981 iter/s, 0.862209s/100 iter), loss = 0.12365
I0122 16:29:07.062171 49871 solver.cpp:285]     Train net output #0: loss = 0.12365 (* 1 = 0.12365 loss)
I0122 16:29:07.062177 49871 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:29:07.924767 49871 solver.cpp:266] Iteration 500 (115.935 iter/s, 0.862551s/100 iter), loss = 0.233684
I0122 16:29:07.924796 49871 solver.cpp:285]     Train net output #0: loss = 0.233684 (* 1 = 0.233684 loss)
I0122 16:29:07.924803 49871 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:29:08.787425 49871 solver.cpp:266] Iteration 600 (115.931 iter/s, 0.862584s/100 iter), loss = 0.126135
I0122 16:29:08.787453 49871 solver.cpp:285]     Train net output #0: loss = 0.126135 (* 1 = 0.126135 loss)
I0122 16:29:08.787458 49871 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:29:09.649339 49871 solver.cpp:266] Iteration 700 (116.031 iter/s, 0.861841s/100 iter), loss = 0.397419
I0122 16:29:09.649366 49871 solver.cpp:285]     Train net output #0: loss = 0.397419 (* 1 = 0.397419 loss)
I0122 16:29:09.649372 49871 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:29:10.511420 49871 solver.cpp:266] Iteration 800 (116.008 iter/s, 0.862009s/100 iter), loss = 0.314943
I0122 16:29:10.511448 49871 solver.cpp:285]     Train net output #0: loss = 0.314943 (* 1 = 0.314943 loss)
I0122 16:29:10.511453 49871 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:29:11.374001 49871 solver.cpp:266] Iteration 900 (115.941 iter/s, 0.862508s/100 iter), loss = 0.234329
I0122 16:29:11.374028 49871 solver.cpp:285]     Train net output #0: loss = 0.234329 (* 1 = 0.234329 loss)
I0122 16:29:11.374034 49871 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:29:12.228250 49871 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:29:12.446743 49871 solver.cpp:517]     Test net output #0: loss = 1.01335 (* 1 = 1.01335 loss)
I0122 16:29:12.446760 49871 solver.cpp:517]     Test net output #1: top-1 = 0.774889
I0122 16:29:12.446765 49871 solver.cpp:517]     Test net output #2: top-5 = 0.974667
I0122 16:29:12.454798 49871 solver.cpp:266] Iteration 1000 (92.5309 iter/s, 1.08072s/100 iter), loss = 0.244012
I0122 16:29:12.454816 49871 solver.cpp:285]     Train net output #0: loss = 0.244012 (* 1 = 0.244012 loss)
I0122 16:29:12.454823 49871 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:29:13.322033 49871 solver.cpp:266] Iteration 1100 (115.317 iter/s, 0.867172s/100 iter), loss = 0.149365
I0122 16:29:13.322062 49871 solver.cpp:285]     Train net output #0: loss = 0.149365 (* 1 = 0.149365 loss)
I0122 16:29:13.322067 49871 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:29:14.205337 49871 solver.cpp:266] Iteration 1200 (113.221 iter/s, 0.883227s/100 iter), loss = 0.218391
I0122 16:29:14.205377 49871 solver.cpp:285]     Train net output #0: loss = 0.218391 (* 1 = 0.218391 loss)
I0122 16:29:14.205384 49871 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:29:15.067473 49871 solver.cpp:266] Iteration 1300 (116.002 iter/s, 0.862053s/100 iter), loss = 0.252697
I0122 16:29:15.067502 49871 solver.cpp:285]     Train net output #0: loss = 0.252697 (* 1 = 0.252697 loss)
I0122 16:29:15.067531 49871 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:29:15.929595 49871 solver.cpp:266] Iteration 1400 (116.003 iter/s, 0.862049s/100 iter), loss = 0.222047
I0122 16:29:15.929623 49871 solver.cpp:285]     Train net output #0: loss = 0.222047 (* 1 = 0.222047 loss)
I0122 16:29:15.929630 49871 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:29:16.814635 49871 solver.cpp:266] Iteration 1500 (112.999 iter/s, 0.884964s/100 iter), loss = 0.135362
I0122 16:29:16.814663 49871 solver.cpp:285]     Train net output #0: loss = 0.135362 (* 1 = 0.135362 loss)
I0122 16:29:16.814668 49871 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:29:17.676648 49871 solver.cpp:266] Iteration 1600 (116.017 iter/s, 0.86194s/100 iter), loss = 0.301385
I0122 16:29:17.676676 49871 solver.cpp:285]     Train net output #0: loss = 0.301385 (* 1 = 0.301385 loss)
I0122 16:29:17.676682 49871 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:29:18.538856 49871 solver.cpp:266] Iteration 1700 (115.991 iter/s, 0.862134s/100 iter), loss = 0.154529
I0122 16:29:18.538883 49871 solver.cpp:285]     Train net output #0: loss = 0.154529 (* 1 = 0.154529 loss)
I0122 16:29:18.538889 49871 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:29:19.402526 49871 solver.cpp:266] Iteration 1800 (115.795 iter/s, 0.863598s/100 iter), loss = 0.198496
I0122 16:29:19.402555 49871 solver.cpp:285]     Train net output #0: loss = 0.198496 (* 1 = 0.198496 loss)
I0122 16:29:19.402561 49871 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:29:20.265178 49871 solver.cpp:266] Iteration 1900 (115.931 iter/s, 0.862579s/100 iter), loss = 0.184648
I0122 16:29:20.265208 49871 solver.cpp:285]     Train net output #0: loss = 0.184648 (* 1 = 0.184648 loss)
I0122 16:29:20.265213 49871 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:29:21.119382 49871 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:29:21.338191 49871 solver.cpp:517]     Test net output #0: loss = 0.606436 (* 1 = 0.606436 loss)
I0122 16:29:21.338205 49871 solver.cpp:517]     Test net output #1: top-1 = 0.826333
I0122 16:29:21.338209 49871 solver.cpp:517]     Test net output #2: top-5 = 0.985111
I0122 16:29:21.346262 49871 solver.cpp:266] Iteration 2000 (92.5065 iter/s, 1.081s/100 iter), loss = 0.170806
I0122 16:29:21.346282 49871 solver.cpp:285]     Train net output #0: loss = 0.170806 (* 1 = 0.170806 loss)
I0122 16:29:21.346287 49871 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:29:22.230231 49871 solver.cpp:266] Iteration 2100 (113.135 iter/s, 0.883902s/100 iter), loss = 0.194425
I0122 16:29:22.230259 49871 solver.cpp:285]     Train net output #0: loss = 0.194425 (* 1 = 0.194425 loss)
I0122 16:29:22.230265 49871 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:29:23.092561 49871 solver.cpp:266] Iteration 2200 (115.975 iter/s, 0.862256s/100 iter), loss = 0.18965
I0122 16:29:23.092589 49871 solver.cpp:285]     Train net output #0: loss = 0.18965 (* 1 = 0.18965 loss)
I0122 16:29:23.092595 49871 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:29:23.954859 49871 solver.cpp:266] Iteration 2300 (115.979 iter/s, 0.862225s/100 iter), loss = 0.233348
I0122 16:29:23.954888 49871 solver.cpp:285]     Train net output #0: loss = 0.233348 (* 1 = 0.233348 loss)
I0122 16:29:23.954895 49871 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:29:24.816682 49871 solver.cpp:266] Iteration 2400 (116.043 iter/s, 0.86175s/100 iter), loss = 0.196409
I0122 16:29:24.816711 49871 solver.cpp:285]     Train net output #0: loss = 0.196409 (* 1 = 0.196409 loss)
I0122 16:29:24.816716 49871 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:29:25.678548 49871 solver.cpp:266] Iteration 2500 (116.037 iter/s, 0.861792s/100 iter), loss = 0.20626
I0122 16:29:25.678578 49871 solver.cpp:285]     Train net output #0: loss = 0.20626 (* 1 = 0.20626 loss)
I0122 16:29:25.678584 49871 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:29:26.541103 49871 solver.cpp:266] Iteration 2600 (115.945 iter/s, 0.862481s/100 iter), loss = 0.138991
I0122 16:29:26.541149 49871 solver.cpp:285]     Train net output #0: loss = 0.138991 (* 1 = 0.138991 loss)
I0122 16:29:26.541155 49871 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:29:27.403414 49871 solver.cpp:266] Iteration 2700 (115.98 iter/s, 0.86222s/100 iter), loss = 0.202293
I0122 16:29:27.403440 49871 solver.cpp:285]     Train net output #0: loss = 0.202293 (* 1 = 0.202293 loss)
I0122 16:29:27.403445 49871 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:29:28.265460 49871 solver.cpp:266] Iteration 2800 (116.012 iter/s, 0.861977s/100 iter), loss = 0.26672
I0122 16:29:28.265488 49871 solver.cpp:285]     Train net output #0: loss = 0.26672 (* 1 = 0.26672 loss)
I0122 16:29:28.265494 49871 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:29:29.127171 49871 solver.cpp:266] Iteration 2900 (116.058 iter/s, 0.861638s/100 iter), loss = 0.215909
I0122 16:29:29.127197 49871 solver.cpp:285]     Train net output #0: loss = 0.215909 (* 1 = 0.215909 loss)
I0122 16:29:29.127203 49871 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:29:29.981220 49871 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:29:30.200059 49871 solver.cpp:517]     Test net output #0: loss = 0.753193 (* 1 = 0.753193 loss)
I0122 16:29:30.200073 49871 solver.cpp:517]     Test net output #1: top-1 = 0.807
I0122 16:29:30.200078 49871 solver.cpp:517]     Test net output #2: top-5 = 0.981445
I0122 16:29:30.208165 49871 solver.cpp:266] Iteration 3000 (92.5141 iter/s, 1.08092s/100 iter), loss = 0.16231
I0122 16:29:30.208184 49871 solver.cpp:285]     Train net output #0: loss = 0.16231 (* 1 = 0.16231 loss)
I0122 16:29:30.208189 49871 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:29:31.070870 49871 solver.cpp:266] Iteration 3100 (115.923 iter/s, 0.862643s/100 iter), loss = 0.131119
I0122 16:29:31.070896 49871 solver.cpp:285]     Train net output #0: loss = 0.131119 (* 1 = 0.131119 loss)
I0122 16:29:31.070901 49871 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:29:31.933733 49871 solver.cpp:266] Iteration 3200 (115.903 iter/s, 0.862792s/100 iter), loss = 0.157694
I0122 16:29:31.933760 49871 solver.cpp:285]     Train net output #0: loss = 0.157694 (* 1 = 0.157694 loss)
I0122 16:29:31.933765 49871 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:29:32.806361 49871 solver.cpp:266] Iteration 3300 (114.606 iter/s, 0.872556s/100 iter), loss = 0.0912388
I0122 16:29:32.806545 49871 solver.cpp:285]     Train net output #0: loss = 0.0912388 (* 1 = 0.0912388 loss)
I0122 16:29:32.806579 49871 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:29:33.717890 49871 solver.cpp:266] Iteration 3400 (109.738 iter/s, 0.911265s/100 iter), loss = 0.21546
I0122 16:29:33.717921 49871 solver.cpp:285]     Train net output #0: loss = 0.21546 (* 1 = 0.21546 loss)
I0122 16:29:33.717926 49871 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:29:34.734737 49871 solver.cpp:266] Iteration 3500 (98.351 iter/s, 1.01677s/100 iter), loss = 0.179485
I0122 16:29:34.734767 49871 solver.cpp:285]     Train net output #0: loss = 0.179485 (* 1 = 0.179485 loss)
I0122 16:29:34.734814 49871 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:29:35.769698 49871 solver.cpp:266] Iteration 3600 (96.6341 iter/s, 1.03483s/100 iter), loss = 0.207087
I0122 16:29:35.769728 49871 solver.cpp:285]     Train net output #0: loss = 0.207087 (* 1 = 0.207087 loss)
I0122 16:29:35.769774 49871 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:29:36.742553 49871 solver.cpp:266] Iteration 3700 (102.804 iter/s, 0.97273s/100 iter), loss = 0.206947
I0122 16:29:36.742581 49871 solver.cpp:285]     Train net output #0: loss = 0.206947 (* 1 = 0.206947 loss)
I0122 16:29:36.742588 49871 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:29:37.763206 49871 solver.cpp:266] Iteration 3800 (97.9843 iter/s, 1.02057s/100 iter), loss = 0.160942
I0122 16:29:37.763234 49871 solver.cpp:285]     Train net output #0: loss = 0.160942 (* 1 = 0.160942 loss)
I0122 16:29:37.763283 49871 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:29:38.671500 49871 solver.cpp:266] Iteration 3900 (110.111 iter/s, 0.908173s/100 iter), loss = 0.135273
I0122 16:29:38.671528 49871 solver.cpp:285]     Train net output #0: loss = 0.135273 (* 1 = 0.135273 loss)
I0122 16:29:38.671535 49871 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:29:39.525558 49871 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:29:39.744231 49871 solver.cpp:517]     Test net output #0: loss = 0.53774 (* 1 = 0.53774 loss)
I0122 16:29:39.744246 49871 solver.cpp:517]     Test net output #1: top-1 = 0.839222
I0122 16:29:39.744251 49871 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0122 16:29:39.752332 49871 solver.cpp:266] Iteration 4000 (92.5281 iter/s, 1.08075s/100 iter), loss = 0.112141
I0122 16:29:39.752351 49871 solver.cpp:285]     Train net output #0: loss = 0.112141 (* 1 = 0.112141 loss)
I0122 16:29:39.752357 49871 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:29:40.692656 49871 solver.cpp:266] Iteration 4100 (106.354 iter/s, 0.940256s/100 iter), loss = 0.189022
I0122 16:29:40.692687 49871 solver.cpp:285]     Train net output #0: loss = 0.189022 (* 1 = 0.189022 loss)
I0122 16:29:40.692734 49871 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:29:41.634948 49871 solver.cpp:266] Iteration 4200 (106.138 iter/s, 0.942166s/100 iter), loss = 0.214933
I0122 16:29:41.634976 49871 solver.cpp:285]     Train net output #0: loss = 0.214933 (* 1 = 0.214933 loss)
I0122 16:29:41.635025 49871 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:29:42.574679 49871 solver.cpp:266] Iteration 4300 (106.427 iter/s, 0.939607s/100 iter), loss = 0.161962
I0122 16:29:42.574707 49871 solver.cpp:285]     Train net output #0: loss = 0.161962 (* 1 = 0.161962 loss)
I0122 16:29:42.574757 49871 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:29:43.536350 49871 solver.cpp:266] Iteration 4400 (103.999 iter/s, 0.961546s/100 iter), loss = 0.294427
I0122 16:29:43.536379 49871 solver.cpp:285]     Train net output #0: loss = 0.294427 (* 1 = 0.294427 loss)
I0122 16:29:43.536386 49871 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:29:44.399092 49871 solver.cpp:266] Iteration 4500 (115.919 iter/s, 0.862668s/100 iter), loss = 0.17088
I0122 16:29:44.399121 49871 solver.cpp:285]     Train net output #0: loss = 0.17088 (* 1 = 0.17088 loss)
I0122 16:29:44.399127 49871 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:29:45.261234 49871 solver.cpp:266] Iteration 4600 (116 iter/s, 0.862068s/100 iter), loss = 0.190582
I0122 16:29:45.261283 49871 solver.cpp:285]     Train net output #0: loss = 0.190582 (* 1 = 0.190582 loss)
I0122 16:29:45.261289 49871 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:29:46.123566 49871 solver.cpp:266] Iteration 4700 (115.977 iter/s, 0.862239s/100 iter), loss = 0.240496
I0122 16:29:46.123594 49871 solver.cpp:285]     Train net output #0: loss = 0.240496 (* 1 = 0.240496 loss)
I0122 16:29:46.123600 49871 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:29:46.985975 49871 solver.cpp:266] Iteration 4800 (115.964 iter/s, 0.862337s/100 iter), loss = 0.192233
I0122 16:29:46.986002 49871 solver.cpp:285]     Train net output #0: loss = 0.192233 (* 1 = 0.192233 loss)
I0122 16:29:46.986008 49871 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:29:47.848232 49871 solver.cpp:266] Iteration 4900 (115.984 iter/s, 0.862184s/100 iter), loss = 0.286771
I0122 16:29:47.848259 49871 solver.cpp:285]     Train net output #0: loss = 0.286771 (* 1 = 0.286771 loss)
I0122 16:29:47.848265 49871 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:29:48.702261 49871 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:29:48.921380 49871 solver.cpp:517]     Test net output #0: loss = 0.530054 (* 1 = 0.530054 loss)
I0122 16:29:48.921396 49871 solver.cpp:517]     Test net output #1: top-1 = 0.832
I0122 16:29:48.921401 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991445
I0122 16:29:48.929486 49871 solver.cpp:266] Iteration 5000 (92.4919 iter/s, 1.08118s/100 iter), loss = 0.148277
I0122 16:29:48.929505 49871 solver.cpp:285]     Train net output #0: loss = 0.148277 (* 1 = 0.148277 loss)
I0122 16:29:48.929512 49871 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:29:49.791765 49871 solver.cpp:266] Iteration 5100 (115.98 iter/s, 0.862216s/100 iter), loss = 0.141612
I0122 16:29:49.791792 49871 solver.cpp:285]     Train net output #0: loss = 0.141612 (* 1 = 0.141612 loss)
I0122 16:29:49.791797 49871 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:29:50.653725 49871 solver.cpp:266] Iteration 5200 (116.024 iter/s, 0.86189s/100 iter), loss = 0.252357
I0122 16:29:50.653754 49871 solver.cpp:285]     Train net output #0: loss = 0.252357 (* 1 = 0.252357 loss)
I0122 16:29:50.653759 49871 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:29:51.515329 49871 solver.cpp:266] Iteration 5300 (116.072 iter/s, 0.861532s/100 iter), loss = 0.22775
I0122 16:29:51.515357 49871 solver.cpp:285]     Train net output #0: loss = 0.22775 (* 1 = 0.22775 loss)
I0122 16:29:51.515363 49871 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:29:52.377521 49871 solver.cpp:266] Iteration 5400 (115.993 iter/s, 0.86212s/100 iter), loss = 0.200963
I0122 16:29:52.377548 49871 solver.cpp:285]     Train net output #0: loss = 0.200963 (* 1 = 0.200963 loss)
I0122 16:29:52.377554 49871 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:29:53.239943 49871 solver.cpp:266] Iteration 5500 (115.962 iter/s, 0.862349s/100 iter), loss = 0.192257
I0122 16:29:53.239970 49871 solver.cpp:285]     Train net output #0: loss = 0.192257 (* 1 = 0.192257 loss)
I0122 16:29:53.239976 49871 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:29:54.102026 49871 solver.cpp:266] Iteration 5600 (116.008 iter/s, 0.862012s/100 iter), loss = 0.15424
I0122 16:29:54.102056 49871 solver.cpp:285]     Train net output #0: loss = 0.15424 (* 1 = 0.15424 loss)
I0122 16:29:54.102061 49871 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:29:54.964282 49871 solver.cpp:266] Iteration 5700 (115.985 iter/s, 0.862184s/100 iter), loss = 0.166288
I0122 16:29:54.964311 49871 solver.cpp:285]     Train net output #0: loss = 0.166288 (* 1 = 0.166288 loss)
I0122 16:29:54.964318 49871 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:29:55.826151 49871 solver.cpp:266] Iteration 5800 (116.037 iter/s, 0.861796s/100 iter), loss = 0.141526
I0122 16:29:55.826179 49871 solver.cpp:285]     Train net output #0: loss = 0.141526 (* 1 = 0.141526 loss)
I0122 16:29:55.826201 49871 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:29:56.688527 49871 solver.cpp:266] Iteration 5900 (115.968 iter/s, 0.862305s/100 iter), loss = 0.186777
I0122 16:29:56.688554 49871 solver.cpp:285]     Train net output #0: loss = 0.186777 (* 1 = 0.186777 loss)
I0122 16:29:56.688560 49871 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:29:57.542042 49871 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:29:57.760740 49871 solver.cpp:517]     Test net output #0: loss = 0.529195 (* 1 = 0.529195 loss)
I0122 16:29:57.760754 49871 solver.cpp:517]     Test net output #1: top-1 = 0.845222
I0122 16:29:57.760758 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:29:57.768824 49871 solver.cpp:266] Iteration 6000 (92.5737 iter/s, 1.08022s/100 iter), loss = 0.147822
I0122 16:29:57.768842 49871 solver.cpp:285]     Train net output #0: loss = 0.147822 (* 1 = 0.147822 loss)
I0122 16:29:57.768848 49871 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:29:58.631371 49871 solver.cpp:266] Iteration 6100 (115.944 iter/s, 0.862484s/100 iter), loss = 0.142312
I0122 16:29:58.631399 49871 solver.cpp:285]     Train net output #0: loss = 0.142312 (* 1 = 0.142312 loss)
I0122 16:29:58.631407 49871 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:29:59.493336 49871 solver.cpp:266] Iteration 6200 (116.024 iter/s, 0.861893s/100 iter), loss = 0.162155
I0122 16:29:59.493364 49871 solver.cpp:285]     Train net output #0: loss = 0.162155 (* 1 = 0.162155 loss)
I0122 16:29:59.493371 49871 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:30:00.359117 49871 solver.cpp:266] Iteration 6300 (115.512 iter/s, 0.865708s/100 iter), loss = 0.125004
I0122 16:30:00.359145 49871 solver.cpp:285]     Train net output #0: loss = 0.125004 (* 1 = 0.125004 loss)
I0122 16:30:00.359150 49871 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:30:01.280570 49871 solver.cpp:266] Iteration 6400 (108.533 iter/s, 0.921378s/100 iter), loss = 0.1552
I0122 16:30:01.280601 49871 solver.cpp:285]     Train net output #0: loss = 0.1552 (* 1 = 0.1552 loss)
I0122 16:30:01.280606 49871 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:30:02.175527 49871 solver.cpp:266] Iteration 6500 (111.747 iter/s, 0.894882s/100 iter), loss = 0.25785
I0122 16:30:02.175556 49871 solver.cpp:285]     Train net output #0: loss = 0.25785 (* 1 = 0.25785 loss)
I0122 16:30:02.175655 49871 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:30:03.067746 49871 solver.cpp:266] Iteration 6600 (112.102 iter/s, 0.892047s/100 iter), loss = 0.19251
I0122 16:30:03.067898 49871 solver.cpp:285]     Train net output #0: loss = 0.19251 (* 1 = 0.19251 loss)
I0122 16:30:03.067909 49871 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:30:03.969954 49871 solver.cpp:266] Iteration 6700 (110.863 iter/s, 0.902014s/100 iter), loss = 0.147542
I0122 16:30:03.969983 49871 solver.cpp:285]     Train net output #0: loss = 0.147542 (* 1 = 0.147542 loss)
I0122 16:30:03.969988 49871 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:30:04.885499 49871 solver.cpp:266] Iteration 6800 (109.234 iter/s, 0.915468s/100 iter), loss = 0.170808
I0122 16:30:04.885529 49871 solver.cpp:285]     Train net output #0: loss = 0.170808 (* 1 = 0.170808 loss)
I0122 16:30:04.885535 49871 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:30:05.756330 49871 solver.cpp:266] Iteration 6900 (114.843 iter/s, 0.870755s/100 iter), loss = 0.161986
I0122 16:30:05.756359 49871 solver.cpp:285]     Train net output #0: loss = 0.161986 (* 1 = 0.161986 loss)
I0122 16:30:05.756366 49871 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:30:06.709290 49871 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:30:07.033143 49871 solver.cpp:517]     Test net output #0: loss = 0.506048 (* 1 = 0.506048 loss)
I0122 16:30:07.033160 49871 solver.cpp:517]     Test net output #1: top-1 = 0.843666
I0122 16:30:07.033164 49871 solver.cpp:517]     Test net output #2: top-5 = 0.989667
I0122 16:30:07.042877 49871 solver.cpp:266] Iteration 7000 (77.7328 iter/s, 1.28646s/100 iter), loss = 0.241834
I0122 16:30:07.042904 49871 solver.cpp:285]     Train net output #0: loss = 0.241834 (* 1 = 0.241834 loss)
I0122 16:30:07.042959 49871 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:30:08.041821 49871 solver.cpp:266] Iteration 7100 (100.119 iter/s, 0.998814s/100 iter), loss = 0.190941
I0122 16:30:08.041851 49871 solver.cpp:285]     Train net output #0: loss = 0.190941 (* 1 = 0.190941 loss)
I0122 16:30:08.041898 49871 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:30:08.929666 49871 solver.cpp:266] Iteration 7200 (112.648 iter/s, 0.887723s/100 iter), loss = 0.118056
I0122 16:30:08.929694 49871 solver.cpp:285]     Train net output #0: loss = 0.118056 (* 1 = 0.118056 loss)
I0122 16:30:08.929699 49871 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:30:09.906533 49871 solver.cpp:266] Iteration 7300 (102.376 iter/s, 0.97679s/100 iter), loss = 0.225947
I0122 16:30:09.906561 49871 solver.cpp:285]     Train net output #0: loss = 0.225947 (* 1 = 0.225947 loss)
I0122 16:30:09.906567 49871 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:30:10.884117 49871 solver.cpp:266] Iteration 7400 (102.301 iter/s, 0.977509s/100 iter), loss = 0.12096
I0122 16:30:10.884147 49871 solver.cpp:285]     Train net output #0: loss = 0.12096 (* 1 = 0.12096 loss)
I0122 16:30:10.884153 49871 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:30:11.829998 49871 solver.cpp:266] Iteration 7500 (105.73 iter/s, 0.945803s/100 iter), loss = 0.178743
I0122 16:30:11.830029 49871 solver.cpp:285]     Train net output #0: loss = 0.178743 (* 1 = 0.178743 loss)
I0122 16:30:11.830075 49871 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:30:12.771870 49871 solver.cpp:266] Iteration 7600 (106.185 iter/s, 0.941748s/100 iter), loss = 0.175126
I0122 16:30:12.771900 49871 solver.cpp:285]     Train net output #0: loss = 0.175126 (* 1 = 0.175126 loss)
I0122 16:30:12.771947 49871 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:30:13.679754 49871 solver.cpp:266] Iteration 7700 (110.161 iter/s, 0.907762s/100 iter), loss = 0.152546
I0122 16:30:13.679782 49871 solver.cpp:285]     Train net output #0: loss = 0.152546 (* 1 = 0.152546 loss)
I0122 16:30:13.679788 49871 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:30:14.542114 49871 solver.cpp:266] Iteration 7800 (115.971 iter/s, 0.862287s/100 iter), loss = 0.174974
I0122 16:30:14.542155 49871 solver.cpp:285]     Train net output #0: loss = 0.174974 (* 1 = 0.174974 loss)
I0122 16:30:14.542160 49871 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:30:15.404534 49871 solver.cpp:266] Iteration 7900 (115.964 iter/s, 0.862336s/100 iter), loss = 0.124327
I0122 16:30:15.404583 49871 solver.cpp:285]     Train net output #0: loss = 0.124327 (* 1 = 0.124327 loss)
I0122 16:30:15.404589 49871 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:30:16.362768 49871 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:30:16.581339 49871 solver.cpp:517]     Test net output #0: loss = 0.554689 (* 1 = 0.554689 loss)
I0122 16:30:16.581358 49871 solver.cpp:517]     Test net output #1: top-1 = 0.835889
I0122 16:30:16.581362 49871 solver.cpp:517]     Test net output #2: top-5 = 0.992111
I0122 16:30:16.589442 49871 solver.cpp:266] Iteration 8000 (84.402 iter/s, 1.18481s/100 iter), loss = 0.128144
I0122 16:30:16.589462 49871 solver.cpp:285]     Train net output #0: loss = 0.128144 (* 1 = 0.128144 loss)
I0122 16:30:16.589467 49871 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:30:17.452018 49871 solver.cpp:266] Iteration 8100 (115.94 iter/s, 0.862514s/100 iter), loss = 0.147505
I0122 16:30:17.452045 49871 solver.cpp:285]     Train net output #0: loss = 0.147505 (* 1 = 0.147505 loss)
I0122 16:30:17.452051 49871 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:30:18.314246 49871 solver.cpp:266] Iteration 8200 (115.988 iter/s, 0.862158s/100 iter), loss = 0.102925
I0122 16:30:18.314273 49871 solver.cpp:285]     Train net output #0: loss = 0.102925 (* 1 = 0.102925 loss)
I0122 16:30:18.314278 49871 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:30:19.176915 49871 solver.cpp:266] Iteration 8300 (115.929 iter/s, 0.862598s/100 iter), loss = 0.185115
I0122 16:30:19.176942 49871 solver.cpp:285]     Train net output #0: loss = 0.185115 (* 1 = 0.185115 loss)
I0122 16:30:19.176949 49871 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:30:20.038807 49871 solver.cpp:266] Iteration 8400 (116.033 iter/s, 0.861822s/100 iter), loss = 0.0945253
I0122 16:30:20.038835 49871 solver.cpp:285]     Train net output #0: loss = 0.0945253 (* 1 = 0.0945253 loss)
I0122 16:30:20.038839 49871 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:30:20.902133 49871 solver.cpp:266] Iteration 8500 (115.84 iter/s, 0.863258s/100 iter), loss = 0.167965
I0122 16:30:20.902163 49871 solver.cpp:285]     Train net output #0: loss = 0.167965 (* 1 = 0.167965 loss)
I0122 16:30:20.902168 49871 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:30:21.764865 49871 solver.cpp:266] Iteration 8600 (115.92 iter/s, 0.862661s/100 iter), loss = 0.152923
I0122 16:30:21.764892 49871 solver.cpp:285]     Train net output #0: loss = 0.152923 (* 1 = 0.152923 loss)
I0122 16:30:21.764899 49871 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:30:22.626940 49871 solver.cpp:266] Iteration 8700 (116.009 iter/s, 0.862004s/100 iter), loss = 0.146129
I0122 16:30:22.626967 49871 solver.cpp:285]     Train net output #0: loss = 0.146129 (* 1 = 0.146129 loss)
I0122 16:30:22.626973 49871 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:30:23.489666 49871 solver.cpp:266] Iteration 8800 (115.921 iter/s, 0.862657s/100 iter), loss = 0.207843
I0122 16:30:23.489693 49871 solver.cpp:285]     Train net output #0: loss = 0.207843 (* 1 = 0.207843 loss)
I0122 16:30:23.489698 49871 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:30:24.352442 49871 solver.cpp:266] Iteration 8900 (115.914 iter/s, 0.862706s/100 iter), loss = 0.0846083
I0122 16:30:24.352468 49871 solver.cpp:285]     Train net output #0: loss = 0.0846083 (* 1 = 0.0846083 loss)
I0122 16:30:24.352474 49871 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:30:25.207140 49871 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:30:25.425977 49871 solver.cpp:517]     Test net output #0: loss = 0.485303 (* 1 = 0.485303 loss)
I0122 16:30:25.425990 49871 solver.cpp:517]     Test net output #1: top-1 = 0.850222
I0122 16:30:25.426002 49871 solver.cpp:517]     Test net output #2: top-5 = 0.990444
I0122 16:30:25.434101 49871 solver.cpp:266] Iteration 9000 (92.4568 iter/s, 1.08159s/100 iter), loss = 0.118761
I0122 16:30:25.434119 49871 solver.cpp:285]     Train net output #0: loss = 0.118761 (* 1 = 0.118761 loss)
I0122 16:30:25.434144 49871 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:30:26.296130 49871 solver.cpp:266] Iteration 9100 (116.013 iter/s, 0.861969s/100 iter), loss = 0.109011
I0122 16:30:26.296155 49871 solver.cpp:285]     Train net output #0: loss = 0.109011 (* 1 = 0.109011 loss)
I0122 16:30:26.296161 49871 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:30:27.157506 49871 solver.cpp:266] Iteration 9200 (116.102 iter/s, 0.861309s/100 iter), loss = 0.184681
I0122 16:30:27.157531 49871 solver.cpp:285]     Train net output #0: loss = 0.184681 (* 1 = 0.184681 loss)
I0122 16:30:27.157536 49871 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:30:28.019999 49871 solver.cpp:266] Iteration 9300 (115.952 iter/s, 0.862427s/100 iter), loss = 0.115923
I0122 16:30:28.020026 49871 solver.cpp:285]     Train net output #0: loss = 0.115923 (* 1 = 0.115923 loss)
I0122 16:30:28.020033 49871 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:30:28.884763 49871 solver.cpp:266] Iteration 9400 (115.648 iter/s, 0.864692s/100 iter), loss = 0.120891
I0122 16:30:28.884789 49871 solver.cpp:285]     Train net output #0: loss = 0.120891 (* 1 = 0.120891 loss)
I0122 16:30:28.884795 49871 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:30:29.762799 49871 solver.cpp:266] Iteration 9500 (113.9 iter/s, 0.877966s/100 iter), loss = 0.0880095
I0122 16:30:29.762828 49871 solver.cpp:285]     Train net output #0: loss = 0.0880096 (* 1 = 0.0880096 loss)
I0122 16:30:29.762835 49871 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:30:30.628471 49871 solver.cpp:266] Iteration 9600 (115.527 iter/s, 0.8656s/100 iter), loss = 0.152067
I0122 16:30:30.628501 49871 solver.cpp:285]     Train net output #0: loss = 0.152067 (* 1 = 0.152067 loss)
I0122 16:30:30.628506 49871 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:30:31.491487 49871 solver.cpp:266] Iteration 9700 (115.882 iter/s, 0.862944s/100 iter), loss = 0.139092
I0122 16:30:31.491516 49871 solver.cpp:285]     Train net output #0: loss = 0.139092 (* 1 = 0.139092 loss)
I0122 16:30:31.491521 49871 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:30:32.392069 49871 solver.cpp:266] Iteration 9800 (111.048 iter/s, 0.900509s/100 iter), loss = 0.154971
I0122 16:30:32.392100 49871 solver.cpp:285]     Train net output #0: loss = 0.154971 (* 1 = 0.154971 loss)
I0122 16:30:32.392107 49871 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:30:33.307113 49871 solver.cpp:266] Iteration 9900 (109.294 iter/s, 0.914967s/100 iter), loss = 0.147806
I0122 16:30:33.307262 49871 solver.cpp:285]     Train net output #0: loss = 0.147806 (* 1 = 0.147806 loss)
I0122 16:30:33.307305 49871 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:30:34.220329 49871 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:30:34.438885 49871 solver.cpp:517]     Test net output #0: loss = 0.526976 (* 1 = 0.526976 loss)
I0122 16:30:34.438900 49871 solver.cpp:517]     Test net output #1: top-1 = 0.840222
I0122 16:30:34.438905 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:30:34.447010 49871 solver.cpp:266] Iteration 10000 (87.7457 iter/s, 1.13966s/100 iter), loss = 0.180154
I0122 16:30:34.447028 49871 solver.cpp:285]     Train net output #0: loss = 0.180154 (* 1 = 0.180154 loss)
I0122 16:30:34.447034 49871 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:30:35.309473 49871 solver.cpp:266] Iteration 10100 (115.955 iter/s, 0.862401s/100 iter), loss = 0.125818
I0122 16:30:35.309501 49871 solver.cpp:285]     Train net output #0: loss = 0.125818 (* 1 = 0.125818 loss)
I0122 16:30:35.309507 49871 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:30:36.171455 49871 solver.cpp:266] Iteration 10200 (116.021 iter/s, 0.861911s/100 iter), loss = 0.144516
I0122 16:30:36.171483 49871 solver.cpp:285]     Train net output #0: loss = 0.144516 (* 1 = 0.144516 loss)
I0122 16:30:36.171489 49871 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:30:37.033597 49871 solver.cpp:266] Iteration 10300 (116 iter/s, 0.862072s/100 iter), loss = 0.0835461
I0122 16:30:37.033627 49871 solver.cpp:285]     Train net output #0: loss = 0.0835461 (* 1 = 0.0835461 loss)
I0122 16:30:37.033632 49871 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:30:37.978487 49871 solver.cpp:266] Iteration 10400 (105.841 iter/s, 0.944814s/100 iter), loss = 0.121533
I0122 16:30:37.978516 49871 solver.cpp:285]     Train net output #0: loss = 0.121533 (* 1 = 0.121533 loss)
I0122 16:30:37.978564 49871 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:30:39.005036 49871 solver.cpp:266] Iteration 10500 (97.4256 iter/s, 1.02642s/100 iter), loss = 0.108218
I0122 16:30:39.005064 49871 solver.cpp:285]     Train net output #0: loss = 0.108218 (* 1 = 0.108218 loss)
I0122 16:30:39.005071 49871 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:30:40.034147 49871 solver.cpp:266] Iteration 10600 (97.1786 iter/s, 1.02903s/100 iter), loss = 0.116991
I0122 16:30:40.034176 49871 solver.cpp:285]     Train net output #0: loss = 0.116991 (* 1 = 0.116991 loss)
I0122 16:30:40.034224 49871 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:30:41.070487 49871 solver.cpp:266] Iteration 10700 (96.5052 iter/s, 1.03621s/100 iter), loss = 0.113088
I0122 16:30:41.070516 49871 solver.cpp:285]     Train net output #0: loss = 0.113088 (* 1 = 0.113088 loss)
I0122 16:30:41.070564 49871 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:30:42.073390 49871 solver.cpp:266] Iteration 10800 (99.7227 iter/s, 1.00278s/100 iter), loss = 0.131328
I0122 16:30:42.073421 49871 solver.cpp:285]     Train net output #0: loss = 0.131328 (* 1 = 0.131328 loss)
I0122 16:30:42.073426 49871 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:30:42.939687 49871 solver.cpp:266] Iteration 10900 (115.443 iter/s, 0.866225s/100 iter), loss = 0.143542
I0122 16:30:42.939714 49871 solver.cpp:285]     Train net output #0: loss = 0.143542 (* 1 = 0.143542 loss)
I0122 16:30:42.939721 49871 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:30:43.839442 49871 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:30:44.085124 49871 solver.cpp:517]     Test net output #0: loss = 0.507037 (* 1 = 0.507037 loss)
I0122 16:30:44.085141 49871 solver.cpp:517]     Test net output #1: top-1 = 0.845222
I0122 16:30:44.085146 49871 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:30:44.094866 49871 solver.cpp:266] Iteration 11000 (86.5725 iter/s, 1.1551s/100 iter), loss = 0.138531
I0122 16:30:44.094883 49871 solver.cpp:285]     Train net output #0: loss = 0.138531 (* 1 = 0.138531 loss)
I0122 16:30:44.094949 49871 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:30:45.130518 49871 solver.cpp:266] Iteration 11100 (96.5695 iter/s, 1.03552s/100 iter), loss = 0.116267
I0122 16:30:45.130548 49871 solver.cpp:285]     Train net output #0: loss = 0.116267 (* 1 = 0.116267 loss)
I0122 16:30:45.130594 49871 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:30:46.166647 49871 solver.cpp:266] Iteration 11200 (96.5246 iter/s, 1.03601s/100 iter), loss = 0.101694
I0122 16:30:46.166678 49871 solver.cpp:285]     Train net output #0: loss = 0.101694 (* 1 = 0.101694 loss)
I0122 16:30:46.166724 49871 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:30:47.190003 49871 solver.cpp:266] Iteration 11300 (97.7295 iter/s, 1.02323s/100 iter), loss = 0.122833
I0122 16:30:47.190032 49871 solver.cpp:285]     Train net output #0: loss = 0.122833 (* 1 = 0.122833 loss)
I0122 16:30:47.190081 49871 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:30:48.212976 49871 solver.cpp:266] Iteration 11400 (97.7662 iter/s, 1.02285s/100 iter), loss = 0.217948
I0122 16:30:48.213004 49871 solver.cpp:285]     Train net output #0: loss = 0.217948 (* 1 = 0.217948 loss)
I0122 16:30:48.213053 49871 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:30:49.206764 49871 solver.cpp:266] Iteration 11500 (100.638 iter/s, 0.993664s/100 iter), loss = 0.0961581
I0122 16:30:49.206792 49871 solver.cpp:285]     Train net output #0: loss = 0.0961581 (* 1 = 0.0961581 loss)
I0122 16:30:49.206842 49871 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:30:50.178390 49871 solver.cpp:266] Iteration 11600 (102.933 iter/s, 0.971503s/100 iter), loss = 0.130979
I0122 16:30:50.178421 49871 solver.cpp:285]     Train net output #0: loss = 0.130979 (* 1 = 0.130979 loss)
I0122 16:30:50.178467 49871 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:30:51.160008 49871 solver.cpp:266] Iteration 11700 (101.886 iter/s, 0.981494s/100 iter), loss = 0.189772
I0122 16:30:51.160037 49871 solver.cpp:285]     Train net output #0: loss = 0.189772 (* 1 = 0.189772 loss)
I0122 16:30:51.160084 49871 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:30:52.195355 49871 solver.cpp:266] Iteration 11800 (96.5977 iter/s, 1.03522s/100 iter), loss = 0.152754
I0122 16:30:52.195384 49871 solver.cpp:285]     Train net output #0: loss = 0.152754 (* 1 = 0.152754 loss)
I0122 16:30:52.195432 49871 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:30:53.231644 49871 solver.cpp:266] Iteration 11900 (96.5098 iter/s, 1.03616s/100 iter), loss = 0.135823
I0122 16:30:53.231673 49871 solver.cpp:285]     Train net output #0: loss = 0.135823 (* 1 = 0.135823 loss)
I0122 16:30:53.231721 49871 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:30:54.250943 49871 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:30:54.568810 49871 solver.cpp:517]     Test net output #0: loss = 0.455732 (* 1 = 0.455732 loss)
I0122 16:30:54.568828 49871 solver.cpp:517]     Test net output #1: top-1 = 0.861333
I0122 16:30:54.568832 49871 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0122 16:30:54.578506 49871 solver.cpp:266] Iteration 12000 (74.2547 iter/s, 1.34672s/100 iter), loss = 0.121605
I0122 16:30:54.578524 49871 solver.cpp:285]     Train net output #0: loss = 0.121605 (* 1 = 0.121605 loss)
I0122 16:30:54.578578 49871 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:30:55.608031 49871 solver.cpp:266] Iteration 12100 (97.1431 iter/s, 1.02941s/100 iter), loss = 0.0815503
I0122 16:30:55.608062 49871 solver.cpp:285]     Train net output #0: loss = 0.0815503 (* 1 = 0.0815503 loss)
I0122 16:30:55.608109 49871 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:30:56.642556 49871 solver.cpp:266] Iteration 12200 (96.6745 iter/s, 1.0344s/100 iter), loss = 0.21078
I0122 16:30:56.642585 49871 solver.cpp:285]     Train net output #0: loss = 0.21078 (* 1 = 0.21078 loss)
I0122 16:30:56.642632 49871 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:30:57.583528 49871 solver.cpp:266] Iteration 12300 (106.287 iter/s, 0.940851s/100 iter), loss = 0.170435
I0122 16:30:57.583575 49871 solver.cpp:285]     Train net output #0: loss = 0.170435 (* 1 = 0.170435 loss)
I0122 16:30:57.583612 49871 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:30:58.515146 49871 solver.cpp:266] Iteration 12400 (107.355 iter/s, 0.931489s/100 iter), loss = 0.117404
I0122 16:30:58.515185 49871 solver.cpp:285]     Train net output #0: loss = 0.117404 (* 1 = 0.117404 loss)
I0122 16:30:58.515192 49871 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:30:59.378720 49871 solver.cpp:266] Iteration 12500 (115.809 iter/s, 0.863493s/100 iter), loss = 0.141115
I0122 16:30:59.378748 49871 solver.cpp:285]     Train net output #0: loss = 0.141115 (* 1 = 0.141115 loss)
I0122 16:30:59.378756 49871 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:31:00.242844 49871 solver.cpp:266] Iteration 12600 (115.734 iter/s, 0.864054s/100 iter), loss = 0.184456
I0122 16:31:00.242872 49871 solver.cpp:285]     Train net output #0: loss = 0.184456 (* 1 = 0.184456 loss)
I0122 16:31:00.242878 49871 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:31:01.105130 49871 solver.cpp:266] Iteration 12700 (115.98 iter/s, 0.862217s/100 iter), loss = 0.180057
I0122 16:31:01.105159 49871 solver.cpp:285]     Train net output #0: loss = 0.180057 (* 1 = 0.180057 loss)
I0122 16:31:01.105163 49871 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:31:01.970715 49871 solver.cpp:266] Iteration 12800 (115.538 iter/s, 0.865516s/100 iter), loss = 0.134688
I0122 16:31:01.970743 49871 solver.cpp:285]     Train net output #0: loss = 0.134688 (* 1 = 0.134688 loss)
I0122 16:31:01.970748 49871 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:31:02.833250 49871 solver.cpp:266] Iteration 12900 (115.947 iter/s, 0.862465s/100 iter), loss = 0.186105
I0122 16:31:02.833277 49871 solver.cpp:285]     Train net output #0: loss = 0.186105 (* 1 = 0.186105 loss)
I0122 16:31:02.833283 49871 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:31:03.687464 49871 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:31:03.907011 49871 solver.cpp:517]     Test net output #0: loss = 0.458997 (* 1 = 0.458997 loss)
I0122 16:31:03.907025 49871 solver.cpp:517]     Test net output #1: top-1 = 0.860111
I0122 16:31:03.907030 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0122 16:31:03.915132 49871 solver.cpp:266] Iteration 13000 (92.4377 iter/s, 1.08181s/100 iter), loss = 0.155896
I0122 16:31:03.915150 49871 solver.cpp:285]     Train net output #0: loss = 0.155896 (* 1 = 0.155896 loss)
I0122 16:31:03.915156 49871 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:31:04.788437 49871 solver.cpp:266] Iteration 13100 (114.515 iter/s, 0.873246s/100 iter), loss = 0.125718
I0122 16:31:04.788465 49871 solver.cpp:285]     Train net output #0: loss = 0.125718 (* 1 = 0.125718 loss)
I0122 16:31:04.788470 49871 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:31:05.675377 49871 solver.cpp:266] Iteration 13200 (112.756 iter/s, 0.88687s/100 iter), loss = 0.174101
I0122 16:31:05.675406 49871 solver.cpp:285]     Train net output #0: loss = 0.174101 (* 1 = 0.174101 loss)
I0122 16:31:05.675411 49871 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:31:06.539809 49871 solver.cpp:266] Iteration 13300 (115.692 iter/s, 0.864362s/100 iter), loss = 0.141085
I0122 16:31:06.539839 49871 solver.cpp:285]     Train net output #0: loss = 0.141085 (* 1 = 0.141085 loss)
I0122 16:31:06.539845 49871 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:31:07.426652 49871 solver.cpp:266] Iteration 13400 (112.769 iter/s, 0.886772s/100 iter), loss = 0.117614
I0122 16:31:07.426679 49871 solver.cpp:285]     Train net output #0: loss = 0.117614 (* 1 = 0.117614 loss)
I0122 16:31:07.426686 49871 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:31:08.369247 49871 solver.cpp:266] Iteration 13500 (106.098 iter/s, 0.942525s/100 iter), loss = 0.0918576
I0122 16:31:08.369276 49871 solver.cpp:285]     Train net output #0: loss = 0.0918577 (* 1 = 0.0918577 loss)
I0122 16:31:08.369282 49871 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:31:09.261008 49871 solver.cpp:266] Iteration 13600 (112.147 iter/s, 0.89169s/100 iter), loss = 0.152024
I0122 16:31:09.261037 49871 solver.cpp:285]     Train net output #0: loss = 0.152024 (* 1 = 0.152024 loss)
I0122 16:31:09.261042 49871 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:31:10.126200 49871 solver.cpp:266] Iteration 13700 (115.59 iter/s, 0.865123s/100 iter), loss = 0.0960973
I0122 16:31:10.126229 49871 solver.cpp:285]     Train net output #0: loss = 0.0960973 (* 1 = 0.0960973 loss)
I0122 16:31:10.126235 49871 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:31:11.018752 49871 solver.cpp:266] Iteration 13800 (112.047 iter/s, 0.892483s/100 iter), loss = 0.100509
I0122 16:31:11.018784 49871 solver.cpp:285]     Train net output #0: loss = 0.100509 (* 1 = 0.100509 loss)
I0122 16:31:11.018790 49871 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:31:11.895316 49871 solver.cpp:266] Iteration 13900 (114.091 iter/s, 0.876493s/100 iter), loss = 0.117948
I0122 16:31:11.895345 49871 solver.cpp:285]     Train net output #0: loss = 0.117948 (* 1 = 0.117948 loss)
I0122 16:31:11.895350 49871 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:31:12.763626 49871 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:31:12.982031 49871 solver.cpp:517]     Test net output #0: loss = 0.463571 (* 1 = 0.463571 loss)
I0122 16:31:12.982046 49871 solver.cpp:517]     Test net output #1: top-1 = 0.861333
I0122 16:31:12.982050 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:31:12.990134 49871 solver.cpp:266] Iteration 14000 (91.3455 iter/s, 1.09474s/100 iter), loss = 0.137882
I0122 16:31:12.990152 49871 solver.cpp:285]     Train net output #0: loss = 0.137882 (* 1 = 0.137882 loss)
I0122 16:31:12.990159 49871 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:31:13.869375 49871 solver.cpp:266] Iteration 14100 (113.742 iter/s, 0.879181s/100 iter), loss = 0.0723839
I0122 16:31:13.869406 49871 solver.cpp:285]     Train net output #0: loss = 0.072384 (* 1 = 0.072384 loss)
I0122 16:31:13.869426 49871 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:31:14.747453 49871 solver.cpp:266] Iteration 14200 (113.894 iter/s, 0.878007s/100 iter), loss = 0.130243
I0122 16:31:14.747483 49871 solver.cpp:285]     Train net output #0: loss = 0.130244 (* 1 = 0.130244 loss)
I0122 16:31:14.747488 49871 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:31:15.619616 49871 solver.cpp:266] Iteration 14300 (114.667 iter/s, 0.872093s/100 iter), loss = 0.12157
I0122 16:31:15.619647 49871 solver.cpp:285]     Train net output #0: loss = 0.12157 (* 1 = 0.12157 loss)
I0122 16:31:15.619652 49871 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:31:16.502125 49871 solver.cpp:266] Iteration 14400 (113.323 iter/s, 0.882436s/100 iter), loss = 0.13992
I0122 16:31:16.502154 49871 solver.cpp:285]     Train net output #0: loss = 0.13992 (* 1 = 0.13992 loss)
I0122 16:31:16.502161 49871 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:31:17.422078 49871 solver.cpp:266] Iteration 14500 (108.71 iter/s, 0.919882s/100 iter), loss = 0.0970815
I0122 16:31:17.422107 49871 solver.cpp:285]     Train net output #0: loss = 0.0970816 (* 1 = 0.0970816 loss)
I0122 16:31:17.422116 49871 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:31:18.323102 49871 solver.cpp:266] Iteration 14600 (110.994 iter/s, 0.900953s/100 iter), loss = 0.0940514
I0122 16:31:18.323133 49871 solver.cpp:285]     Train net output #0: loss = 0.0940515 (* 1 = 0.0940515 loss)
I0122 16:31:18.323139 49871 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:31:19.188972 49871 solver.cpp:266] Iteration 14700 (115.5 iter/s, 0.8658s/100 iter), loss = 0.0568434
I0122 16:31:19.189000 49871 solver.cpp:285]     Train net output #0: loss = 0.0568434 (* 1 = 0.0568434 loss)
I0122 16:31:19.189005 49871 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:31:20.089026 49871 solver.cpp:266] Iteration 14800 (111.113 iter/s, 0.899983s/100 iter), loss = 0.16954
I0122 16:31:20.089056 49871 solver.cpp:285]     Train net output #0: loss = 0.16954 (* 1 = 0.16954 loss)
I0122 16:31:20.089061 49871 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:31:20.953934 49871 solver.cpp:266] Iteration 14900 (115.628 iter/s, 0.86484s/100 iter), loss = 0.0891279
I0122 16:31:20.953963 49871 solver.cpp:285]     Train net output #0: loss = 0.0891279 (* 1 = 0.0891279 loss)
I0122 16:31:20.953985 49871 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:31:21.809271 49871 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:31:22.029355 49871 solver.cpp:517]     Test net output #0: loss = 0.501093 (* 1 = 0.501093 loss)
I0122 16:31:22.029371 49871 solver.cpp:517]     Test net output #1: top-1 = 0.851222
I0122 16:31:22.029376 49871 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:31:22.037427 49871 solver.cpp:266] Iteration 15000 (92.3003 iter/s, 1.08342s/100 iter), loss = 0.128036
I0122 16:31:22.037446 49871 solver.cpp:285]     Train net output #0: loss = 0.128036 (* 1 = 0.128036 loss)
I0122 16:31:22.037451 49871 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:31:22.900101 49871 solver.cpp:266] Iteration 15100 (115.926 iter/s, 0.862616s/100 iter), loss = 0.0806022
I0122 16:31:22.900130 49871 solver.cpp:285]     Train net output #0: loss = 0.0806023 (* 1 = 0.0806023 loss)
I0122 16:31:22.900135 49871 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:31:23.762392 49871 solver.cpp:266] Iteration 15200 (115.979 iter/s, 0.862223s/100 iter), loss = 0.19485
I0122 16:31:23.762431 49871 solver.cpp:285]     Train net output #0: loss = 0.19485 (* 1 = 0.19485 loss)
I0122 16:31:23.762437 49871 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:31:24.625497 49871 solver.cpp:266] Iteration 15300 (115.871 iter/s, 0.863027s/100 iter), loss = 0.104586
I0122 16:31:24.625525 49871 solver.cpp:285]     Train net output #0: loss = 0.104586 (* 1 = 0.104586 loss)
I0122 16:31:24.625531 49871 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:31:25.501365 49871 solver.cpp:266] Iteration 15400 (114.182 iter/s, 0.875799s/100 iter), loss = 0.116037
I0122 16:31:25.501412 49871 solver.cpp:285]     Train net output #0: loss = 0.116037 (* 1 = 0.116037 loss)
I0122 16:31:25.501417 49871 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:31:26.376135 49871 solver.cpp:266] Iteration 15500 (114.327 iter/s, 0.874683s/100 iter), loss = 0.110849
I0122 16:31:26.376163 49871 solver.cpp:285]     Train net output #0: loss = 0.110849 (* 1 = 0.110849 loss)
I0122 16:31:26.376168 49871 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:31:27.268007 49871 solver.cpp:266] Iteration 15600 (112.132 iter/s, 0.891803s/100 iter), loss = 0.119458
I0122 16:31:27.268038 49871 solver.cpp:285]     Train net output #0: loss = 0.119458 (* 1 = 0.119458 loss)
I0122 16:31:27.268043 49871 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:31:28.137660 49871 solver.cpp:266] Iteration 15700 (114.998 iter/s, 0.869583s/100 iter), loss = 0.115703
I0122 16:31:28.137687 49871 solver.cpp:285]     Train net output #0: loss = 0.115703 (* 1 = 0.115703 loss)
I0122 16:31:28.137693 49871 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:31:29.032105 49871 solver.cpp:266] Iteration 15800 (111.81 iter/s, 0.894376s/100 iter), loss = 0.0738312
I0122 16:31:29.032135 49871 solver.cpp:285]     Train net output #0: loss = 0.0738312 (* 1 = 0.0738312 loss)
I0122 16:31:29.032140 49871 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:31:29.928457 49871 solver.cpp:266] Iteration 15900 (111.572 iter/s, 0.896281s/100 iter), loss = 0.196136
I0122 16:31:29.928484 49871 solver.cpp:285]     Train net output #0: loss = 0.196136 (* 1 = 0.196136 loss)
I0122 16:31:29.928489 49871 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:31:30.790014 49871 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:31:31.008126 49871 solver.cpp:517]     Test net output #0: loss = 0.471736 (* 1 = 0.471736 loss)
I0122 16:31:31.008148 49871 solver.cpp:517]     Test net output #1: top-1 = 0.857555
I0122 16:31:31.008152 49871 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0122 16:31:31.016230 49871 solver.cpp:266] Iteration 16000 (91.9369 iter/s, 1.0877s/100 iter), loss = 0.176201
I0122 16:31:31.016247 49871 solver.cpp:285]     Train net output #0: loss = 0.176201 (* 1 = 0.176201 loss)
I0122 16:31:31.016265 49871 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:31:31.908843 49871 solver.cpp:266] Iteration 16100 (112.038 iter/s, 0.892553s/100 iter), loss = 0.124265
I0122 16:31:31.908870 49871 solver.cpp:285]     Train net output #0: loss = 0.124265 (* 1 = 0.124265 loss)
I0122 16:31:31.908876 49871 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:31:32.825268 49871 solver.cpp:266] Iteration 16200 (109.128 iter/s, 0.916356s/100 iter), loss = 0.104392
I0122 16:31:32.825295 49871 solver.cpp:285]     Train net output #0: loss = 0.104392 (* 1 = 0.104392 loss)
I0122 16:31:32.825301 49871 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:31:33.741363 49871 solver.cpp:266] Iteration 16300 (109.167 iter/s, 0.916024s/100 iter), loss = 0.116305
I0122 16:31:33.741535 49871 solver.cpp:285]     Train net output #0: loss = 0.116305 (* 1 = 0.116305 loss)
I0122 16:31:33.741542 49871 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:31:34.648850 49871 solver.cpp:266] Iteration 16400 (110.22 iter/s, 0.907277s/100 iter), loss = 0.108643
I0122 16:31:34.648880 49871 solver.cpp:285]     Train net output #0: loss = 0.108643 (* 1 = 0.108643 loss)
I0122 16:31:34.648885 49871 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:31:35.513993 49871 solver.cpp:266] Iteration 16500 (115.597 iter/s, 0.865075s/100 iter), loss = 0.0924277
I0122 16:31:35.514021 49871 solver.cpp:285]     Train net output #0: loss = 0.0924277 (* 1 = 0.0924277 loss)
I0122 16:31:35.514029 49871 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:31:36.378742 49871 solver.cpp:266] Iteration 16600 (115.65 iter/s, 0.86468s/100 iter), loss = 0.182949
I0122 16:31:36.378770 49871 solver.cpp:285]     Train net output #0: loss = 0.182949 (* 1 = 0.182949 loss)
I0122 16:31:36.378777 49871 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:31:37.249729 49871 solver.cpp:266] Iteration 16700 (114.821 iter/s, 0.87092s/100 iter), loss = 0.107515
I0122 16:31:37.249756 49871 solver.cpp:285]     Train net output #0: loss = 0.107515 (* 1 = 0.107515 loss)
I0122 16:31:37.249763 49871 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:31:38.113930 49871 solver.cpp:266] Iteration 16800 (115.723 iter/s, 0.864134s/100 iter), loss = 0.0714535
I0122 16:31:38.113958 49871 solver.cpp:285]     Train net output #0: loss = 0.0714535 (* 1 = 0.0714535 loss)
I0122 16:31:38.113963 49871 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:31:38.989024 49871 solver.cpp:266] Iteration 16900 (114.282 iter/s, 0.875027s/100 iter), loss = 0.141704
I0122 16:31:38.989053 49871 solver.cpp:285]     Train net output #0: loss = 0.141704 (* 1 = 0.141704 loss)
I0122 16:31:38.989058 49871 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:31:39.872313 49871 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:31:40.102636 49871 solver.cpp:517]     Test net output #0: loss = 0.450863 (* 1 = 0.450863 loss)
I0122 16:31:40.102653 49871 solver.cpp:517]     Test net output #1: top-1 = 0.862222
I0122 16:31:40.102658 49871 solver.cpp:517]     Test net output #2: top-5 = 0.993445
I0122 16:31:40.110975 49871 solver.cpp:266] Iteration 17000 (89.1364 iter/s, 1.12188s/100 iter), loss = 0.11741
I0122 16:31:40.110993 49871 solver.cpp:285]     Train net output #0: loss = 0.11741 (* 1 = 0.11741 loss)
I0122 16:31:40.110999 49871 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:31:40.990708 49871 solver.cpp:266] Iteration 17100 (113.678 iter/s, 0.879676s/100 iter), loss = 0.142495
I0122 16:31:40.990737 49871 solver.cpp:285]     Train net output #0: loss = 0.142495 (* 1 = 0.142495 loss)
I0122 16:31:40.990742 49871 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:31:41.874219 49871 solver.cpp:266] Iteration 17200 (113.194 iter/s, 0.883442s/100 iter), loss = 0.080969
I0122 16:31:41.874248 49871 solver.cpp:285]     Train net output #0: loss = 0.0809691 (* 1 = 0.0809691 loss)
I0122 16:31:41.874254 49871 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:31:42.747295 49871 solver.cpp:266] Iteration 17300 (114.547 iter/s, 0.873007s/100 iter), loss = 0.175965
I0122 16:31:42.747323 49871 solver.cpp:285]     Train net output #0: loss = 0.175965 (* 1 = 0.175965 loss)
I0122 16:31:42.747329 49871 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:31:43.615640 49871 solver.cpp:266] Iteration 17400 (115.171 iter/s, 0.868278s/100 iter), loss = 0.10526
I0122 16:31:43.615669 49871 solver.cpp:285]     Train net output #0: loss = 0.10526 (* 1 = 0.10526 loss)
I0122 16:31:43.615674 49871 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:31:44.478096 49871 solver.cpp:266] Iteration 17500 (115.957 iter/s, 0.862389s/100 iter), loss = 0.106112
I0122 16:31:44.478124 49871 solver.cpp:285]     Train net output #0: loss = 0.106112 (* 1 = 0.106112 loss)
I0122 16:31:44.478130 49871 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:31:45.340581 49871 solver.cpp:266] Iteration 17600 (115.953 iter/s, 0.862418s/100 iter), loss = 0.045341
I0122 16:31:45.340610 49871 solver.cpp:285]     Train net output #0: loss = 0.0453411 (* 1 = 0.0453411 loss)
I0122 16:31:45.340615 49871 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:31:46.202646 49871 solver.cpp:266] Iteration 17700 (116.01 iter/s, 0.861997s/100 iter), loss = 0.13386
I0122 16:31:46.202673 49871 solver.cpp:285]     Train net output #0: loss = 0.13386 (* 1 = 0.13386 loss)
I0122 16:31:46.202679 49871 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:31:47.066262 49871 solver.cpp:266] Iteration 17800 (115.801 iter/s, 0.863549s/100 iter), loss = 0.145162
I0122 16:31:47.066289 49871 solver.cpp:285]     Train net output #0: loss = 0.145162 (* 1 = 0.145162 loss)
I0122 16:31:47.066295 49871 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:31:47.930641 49871 solver.cpp:266] Iteration 17900 (115.699 iter/s, 0.864313s/100 iter), loss = 0.0995434
I0122 16:31:47.930670 49871 solver.cpp:285]     Train net output #0: loss = 0.0995434 (* 1 = 0.0995434 loss)
I0122 16:31:47.930676 49871 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:31:48.784365 49871 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:31:49.002768 49871 solver.cpp:517]     Test net output #0: loss = 0.440328 (* 1 = 0.440328 loss)
I0122 16:31:49.002782 49871 solver.cpp:517]     Test net output #1: top-1 = 0.865333
I0122 16:31:49.002787 49871 solver.cpp:517]     Test net output #2: top-5 = 0.992111
I0122 16:31:49.010869 49871 solver.cpp:266] Iteration 18000 (92.5792 iter/s, 1.08016s/100 iter), loss = 0.10484
I0122 16:31:49.010887 49871 solver.cpp:285]     Train net output #0: loss = 0.10484 (* 1 = 0.10484 loss)
I0122 16:31:49.010895 49871 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:31:49.873395 49871 solver.cpp:266] Iteration 18100 (115.946 iter/s, 0.86247s/100 iter), loss = 0.129712
I0122 16:31:49.873421 49871 solver.cpp:285]     Train net output #0: loss = 0.129712 (* 1 = 0.129712 loss)
I0122 16:31:49.873427 49871 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:31:50.738179 49871 solver.cpp:266] Iteration 18200 (115.645 iter/s, 0.864719s/100 iter), loss = 0.0994682
I0122 16:31:50.738207 49871 solver.cpp:285]     Train net output #0: loss = 0.0994682 (* 1 = 0.0994682 loss)
I0122 16:31:50.738214 49871 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:31:51.601296 49871 solver.cpp:266] Iteration 18300 (115.868 iter/s, 0.863052s/100 iter), loss = 0.117972
I0122 16:31:51.601323 49871 solver.cpp:285]     Train net output #0: loss = 0.117972 (* 1 = 0.117972 loss)
I0122 16:31:51.601328 49871 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:31:52.463500 49871 solver.cpp:266] Iteration 18400 (115.991 iter/s, 0.862137s/100 iter), loss = 0.109225
I0122 16:31:52.463527 49871 solver.cpp:285]     Train net output #0: loss = 0.109225 (* 1 = 0.109225 loss)
I0122 16:31:52.463532 49871 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:31:53.325832 49871 solver.cpp:266] Iteration 18500 (115.973 iter/s, 0.862267s/100 iter), loss = 0.121819
I0122 16:31:53.325860 49871 solver.cpp:285]     Train net output #0: loss = 0.121819 (* 1 = 0.121819 loss)
I0122 16:31:53.325865 49871 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:31:54.192721 49871 solver.cpp:266] Iteration 18600 (115.364 iter/s, 0.866823s/100 iter), loss = 0.110825
I0122 16:31:54.192749 49871 solver.cpp:285]     Train net output #0: loss = 0.110825 (* 1 = 0.110825 loss)
I0122 16:31:54.192755 49871 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:31:55.056145 49871 solver.cpp:266] Iteration 18700 (115.827 iter/s, 0.863356s/100 iter), loss = 0.129598
I0122 16:31:55.056172 49871 solver.cpp:285]     Train net output #0: loss = 0.129598 (* 1 = 0.129598 loss)
I0122 16:31:55.056179 49871 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:31:55.921995 49871 solver.cpp:266] Iteration 18800 (115.502 iter/s, 0.865784s/100 iter), loss = 0.0783593
I0122 16:31:55.922024 49871 solver.cpp:285]     Train net output #0: loss = 0.0783593 (* 1 = 0.0783593 loss)
I0122 16:31:55.922062 49871 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:31:56.817999 49871 solver.cpp:266] Iteration 18900 (111.615 iter/s, 0.895938s/100 iter), loss = 0.100514
I0122 16:31:56.818027 49871 solver.cpp:285]     Train net output #0: loss = 0.100514 (* 1 = 0.100514 loss)
I0122 16:31:56.818034 49871 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:31:57.673187 49871 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:31:57.891541 49871 solver.cpp:517]     Test net output #0: loss = 0.436705 (* 1 = 0.436705 loss)
I0122 16:31:57.891556 49871 solver.cpp:517]     Test net output #1: top-1 = 0.866777
I0122 16:31:57.891561 49871 solver.cpp:517]     Test net output #2: top-5 = 0.993
I0122 16:31:57.899657 49871 solver.cpp:266] Iteration 19000 (92.4568 iter/s, 1.08159s/100 iter), loss = 0.156997
I0122 16:31:57.899675 49871 solver.cpp:285]     Train net output #0: loss = 0.156997 (* 1 = 0.156997 loss)
I0122 16:31:57.899680 49871 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:31:58.761399 49871 solver.cpp:266] Iteration 19100 (116.052 iter/s, 0.861684s/100 iter), loss = 0.107361
I0122 16:31:58.761425 49871 solver.cpp:285]     Train net output #0: loss = 0.107361 (* 1 = 0.107361 loss)
I0122 16:31:58.761430 49871 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:31:59.622541 49871 solver.cpp:266] Iteration 19200 (116.133 iter/s, 0.86108s/100 iter), loss = 0.125743
I0122 16:31:59.622567 49871 solver.cpp:285]     Train net output #0: loss = 0.125743 (* 1 = 0.125743 loss)
I0122 16:31:59.622573 49871 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:32:00.519158 49871 solver.cpp:266] Iteration 19300 (111.539 iter/s, 0.896551s/100 iter), loss = 0.127917
I0122 16:32:00.519188 49871 solver.cpp:285]     Train net output #0: loss = 0.127917 (* 1 = 0.127917 loss)
I0122 16:32:00.519194 49871 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:32:01.381371 49871 solver.cpp:266] Iteration 19400 (115.99 iter/s, 0.862146s/100 iter), loss = 0.0771518
I0122 16:32:01.381398 49871 solver.cpp:285]     Train net output #0: loss = 0.0771519 (* 1 = 0.0771519 loss)
I0122 16:32:01.381404 49871 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:32:02.244717 49871 solver.cpp:266] Iteration 19500 (115.837 iter/s, 0.86328s/100 iter), loss = 0.0811082
I0122 16:32:02.244745 49871 solver.cpp:285]     Train net output #0: loss = 0.0811084 (* 1 = 0.0811084 loss)
I0122 16:32:02.244750 49871 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:32:03.137392 49871 solver.cpp:266] Iteration 19600 (112.031 iter/s, 0.892607s/100 iter), loss = 0.127742
I0122 16:32:03.137420 49871 solver.cpp:285]     Train net output #0: loss = 0.127743 (* 1 = 0.127743 loss)
I0122 16:32:03.137425 49871 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:32:04.001327 49871 solver.cpp:266] Iteration 19700 (115.758 iter/s, 0.863869s/100 iter), loss = 0.129509
I0122 16:32:04.001487 49871 solver.cpp:285]     Train net output #0: loss = 0.12951 (* 1 = 0.12951 loss)
I0122 16:32:04.001495 49871 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:32:04.863862 49871 solver.cpp:266] Iteration 19800 (115.964 iter/s, 0.862337s/100 iter), loss = 0.0915381
I0122 16:32:04.863889 49871 solver.cpp:285]     Train net output #0: loss = 0.0915382 (* 1 = 0.0915382 loss)
I0122 16:32:04.863895 49871 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:32:05.726277 49871 solver.cpp:266] Iteration 19900 (115.962 iter/s, 0.86235s/100 iter), loss = 0.0981275
I0122 16:32:05.726305 49871 solver.cpp:285]     Train net output #0: loss = 0.0981276 (* 1 = 0.0981276 loss)
I0122 16:32:05.726310 49871 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:32:06.580284 49871 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/_iter_20000.caffemodel
I0122 16:32:06.648105 49871 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.2/snapshots/_iter_20000.solverstate
I0122 16:32:06.659987 49871 solver.cpp:378] Iteration 20000, loss = 0.0119331
I0122 16:32:06.660009 49871 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:32:06.878235 49871 solver.cpp:517]     Test net output #0: loss = 0.437248 (* 1 = 0.437248 loss)
I0122 16:32:06.878252 49871 solver.cpp:517]     Test net output #1: top-1 = 0.867555
I0122 16:32:06.878255 49871 solver.cpp:517]     Test net output #2: top-5 = 0.992445
I0122 16:32:06.878260 49871 solver.cpp:386] Optimization Done (109.682 iter/s).
I0122 16:32:06.878264 49871 caffe_interface.cpp:530] Optimization Done.

## compression: third run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config3.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress3_miniVggNet.txt
I0122 16:32:07.420711 50515 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:32:07.472528 50515 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:32:08.880134 50515 pruning_runner.cpp:264] Compression complete 0.000942904%
I0122 16:32:09.508322 50515 pruning_runner.cpp:264] Compression complete 50.0005%
I0122 16:32:10.137353 50515 pruning_runner.cpp:264] Compression complete 80.0003%
I0122 16:32:10.766327 50515 pruning_runner.cpp:264] Compression complete 90.0001%
I0122 16:32:11.392443 50515 pruning_runner.cpp:264] Compression complete 97.2973%
I0122 16:32:12.025358 50515 pruning_runner.cpp:264] Compression complete 98.6302%
I0122 16:32:12.655166 50515 pruning_runner.cpp:264] Compression complete 99.9144%
I0122 16:32:13.282248 50515 pruning_runner.cpp:264] Compression complete 99.9786%
I0122 16:32:13.939162 50515 pruning_runner.cpp:264] Compression complete 99.9946%
I0122 16:32:14.570616 50515 pruning_runner.cpp:264] Compression complete 99.9973%
I0122 16:32:15.197098 50515 pruning_runner.cpp:264] Compression complete 99.9987%
I0122 16:32:15.839248 50515 pruning_runner.cpp:264] Compression complete 99.9993%
I0122 16:32:16.456344 50515 pruning_runner.cpp:264] Compression complete 99.9997%
I0122 16:32:17.082489 50515 pruning_runner.cpp:264] Compression complete 100%
I0122 16:32:17.709988 50515 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:32:17.710346 50515 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:32:17.710821 50515 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:32:17.711074 50515 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:32:17.711225 50515 layer_factory.hpp:77] Creating layer data
I0122 16:32:17.711279 50515 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:17.711740 50515 net.cpp:94] Creating Layer data
I0122 16:32:17.711750 50515 net.cpp:409] data -> data
I0122 16:32:17.711762 50515 net.cpp:409] data -> label
I0122 16:32:17.712786 51903 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:32:17.712817 51903 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:32:17.712954 50515 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:32:17.713070 50515 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:32:17.716480 50515 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:17.716527 50515 net.cpp:144] Setting up data
I0122 16:32:17.716537 50515 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:32:17.716544 50515 net.cpp:151] Top shape: 50 (50)
I0122 16:32:17.716547 50515 net.cpp:159] Memory required for data: 614600
I0122 16:32:17.716552 50515 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:32:17.716562 50515 net.cpp:94] Creating Layer label_data_1_split
I0122 16:32:17.716586 50515 net.cpp:435] label_data_1_split <- label
I0122 16:32:17.716594 50515 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:32:17.716606 50515 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:32:17.716615 50515 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:32:17.716714 50515 net.cpp:144] Setting up label_data_1_split
I0122 16:32:17.716722 50515 net.cpp:151] Top shape: 50 (50)
I0122 16:32:17.716727 50515 net.cpp:151] Top shape: 50 (50)
I0122 16:32:17.716732 50515 net.cpp:151] Top shape: 50 (50)
I0122 16:32:17.716735 50515 net.cpp:159] Memory required for data: 615200
I0122 16:32:17.716739 50515 layer_factory.hpp:77] Creating layer conv1
I0122 16:32:17.716753 50515 net.cpp:94] Creating Layer conv1
I0122 16:32:17.716758 50515 net.cpp:435] conv1 <- data
I0122 16:32:17.716766 50515 net.cpp:409] conv1 -> conv1
I0122 16:32:17.717936 50515 net.cpp:144] Setting up conv1
I0122 16:32:17.717950 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.717954 50515 net.cpp:159] Memory required for data: 7168800
I0122 16:32:17.717968 50515 layer_factory.hpp:77] Creating layer bn1
I0122 16:32:17.717979 50515 net.cpp:94] Creating Layer bn1
I0122 16:32:17.717986 50515 net.cpp:435] bn1 <- conv1
I0122 16:32:17.717993 50515 net.cpp:409] bn1 -> scale1
I0122 16:32:17.718664 50515 net.cpp:144] Setting up bn1
I0122 16:32:17.718672 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.718674 50515 net.cpp:159] Memory required for data: 13722400
I0122 16:32:17.718686 50515 layer_factory.hpp:77] Creating layer relu1
I0122 16:32:17.718694 50515 net.cpp:94] Creating Layer relu1
I0122 16:32:17.718698 50515 net.cpp:435] relu1 <- scale1
I0122 16:32:17.718701 50515 net.cpp:409] relu1 -> relu1
I0122 16:32:17.718719 50515 net.cpp:144] Setting up relu1
I0122 16:32:17.718725 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.718729 50515 net.cpp:159] Memory required for data: 20276000
I0122 16:32:17.718730 50515 layer_factory.hpp:77] Creating layer conv2
I0122 16:32:17.718739 50515 net.cpp:94] Creating Layer conv2
I0122 16:32:17.718744 50515 net.cpp:435] conv2 <- relu1
I0122 16:32:17.718750 50515 net.cpp:409] conv2 -> conv2
I0122 16:32:17.719682 50515 net.cpp:144] Setting up conv2
I0122 16:32:17.719694 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.719697 50515 net.cpp:159] Memory required for data: 26829600
I0122 16:32:17.719707 50515 layer_factory.hpp:77] Creating layer bn2
I0122 16:32:17.719715 50515 net.cpp:94] Creating Layer bn2
I0122 16:32:17.719718 50515 net.cpp:435] bn2 <- conv2
I0122 16:32:17.719725 50515 net.cpp:409] bn2 -> scale2
I0122 16:32:17.720464 50515 net.cpp:144] Setting up bn2
I0122 16:32:17.720471 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.720474 50515 net.cpp:159] Memory required for data: 33383200
I0122 16:32:17.720484 50515 layer_factory.hpp:77] Creating layer relu2
I0122 16:32:17.720489 50515 net.cpp:94] Creating Layer relu2
I0122 16:32:17.720494 50515 net.cpp:435] relu2 <- scale2
I0122 16:32:17.720497 50515 net.cpp:409] relu2 -> relu2
I0122 16:32:17.720522 50515 net.cpp:144] Setting up relu2
I0122 16:32:17.720528 50515 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:17.720531 50515 net.cpp:159] Memory required for data: 39936800
I0122 16:32:17.720535 50515 layer_factory.hpp:77] Creating layer pool1
I0122 16:32:17.720553 50515 net.cpp:94] Creating Layer pool1
I0122 16:32:17.720556 50515 net.cpp:435] pool1 <- relu2
I0122 16:32:17.720561 50515 net.cpp:409] pool1 -> pool1
I0122 16:32:17.720597 50515 net.cpp:144] Setting up pool1
I0122 16:32:17.720603 50515 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:32:17.720607 50515 net.cpp:159] Memory required for data: 41575200
I0122 16:32:17.720610 50515 layer_factory.hpp:77] Creating layer drop1
I0122 16:32:17.720614 50515 net.cpp:94] Creating Layer drop1
I0122 16:32:17.720618 50515 net.cpp:435] drop1 <- pool1
I0122 16:32:17.720623 50515 net.cpp:409] drop1 -> drop1
I0122 16:32:17.720686 50515 net.cpp:144] Setting up drop1
I0122 16:32:17.720692 50515 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:32:17.720696 50515 net.cpp:159] Memory required for data: 43213600
I0122 16:32:17.720698 50515 layer_factory.hpp:77] Creating layer conv3
I0122 16:32:17.720708 50515 net.cpp:94] Creating Layer conv3
I0122 16:32:17.720712 50515 net.cpp:435] conv3 <- drop1
I0122 16:32:17.720718 50515 net.cpp:409] conv3 -> conv3
I0122 16:32:17.721693 50515 net.cpp:144] Setting up conv3
I0122 16:32:17.721704 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.721706 50515 net.cpp:159] Memory required for data: 46490400
I0122 16:32:17.721714 50515 layer_factory.hpp:77] Creating layer bn3
I0122 16:32:17.721720 50515 net.cpp:94] Creating Layer bn3
I0122 16:32:17.721725 50515 net.cpp:435] bn3 <- conv3
I0122 16:32:17.721731 50515 net.cpp:409] bn3 -> scale3
I0122 16:32:17.722419 50515 net.cpp:144] Setting up bn3
I0122 16:32:17.722427 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.722431 50515 net.cpp:159] Memory required for data: 49767200
I0122 16:32:17.722442 50515 layer_factory.hpp:77] Creating layer relu3
I0122 16:32:17.722450 50515 net.cpp:94] Creating Layer relu3
I0122 16:32:17.722453 50515 net.cpp:435] relu3 <- scale3
I0122 16:32:17.722458 50515 net.cpp:409] relu3 -> relu3
I0122 16:32:17.722476 50515 net.cpp:144] Setting up relu3
I0122 16:32:17.722483 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.722486 50515 net.cpp:159] Memory required for data: 53044000
I0122 16:32:17.722489 50515 layer_factory.hpp:77] Creating layer conv4
I0122 16:32:17.722497 50515 net.cpp:94] Creating Layer conv4
I0122 16:32:17.722503 50515 net.cpp:435] conv4 <- relu3
I0122 16:32:17.722508 50515 net.cpp:409] conv4 -> conv4
I0122 16:32:17.722954 50515 net.cpp:144] Setting up conv4
I0122 16:32:17.722962 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.722966 50515 net.cpp:159] Memory required for data: 56320800
I0122 16:32:17.722971 50515 layer_factory.hpp:77] Creating layer bn4
I0122 16:32:17.722977 50515 net.cpp:94] Creating Layer bn4
I0122 16:32:17.722982 50515 net.cpp:435] bn4 <- conv4
I0122 16:32:17.722988 50515 net.cpp:409] bn4 -> scale4
I0122 16:32:17.723645 50515 net.cpp:144] Setting up bn4
I0122 16:32:17.723651 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.723655 50515 net.cpp:159] Memory required for data: 59597600
I0122 16:32:17.723664 50515 layer_factory.hpp:77] Creating layer relu4
I0122 16:32:17.723668 50515 net.cpp:94] Creating Layer relu4
I0122 16:32:17.723671 50515 net.cpp:435] relu4 <- scale4
I0122 16:32:17.723676 50515 net.cpp:409] relu4 -> relu4
I0122 16:32:17.723693 50515 net.cpp:144] Setting up relu4
I0122 16:32:17.723700 50515 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:17.723702 50515 net.cpp:159] Memory required for data: 62874400
I0122 16:32:17.723706 50515 layer_factory.hpp:77] Creating layer pool2
I0122 16:32:17.723713 50515 net.cpp:94] Creating Layer pool2
I0122 16:32:17.723716 50515 net.cpp:435] pool2 <- relu4
I0122 16:32:17.723721 50515 net.cpp:409] pool2 -> pool2
I0122 16:32:17.723752 50515 net.cpp:144] Setting up pool2
I0122 16:32:17.723758 50515 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:32:17.723762 50515 net.cpp:159] Memory required for data: 63693600
I0122 16:32:17.723764 50515 layer_factory.hpp:77] Creating layer drop2
I0122 16:32:17.723769 50515 net.cpp:94] Creating Layer drop2
I0122 16:32:17.723784 50515 net.cpp:435] drop2 <- pool2
I0122 16:32:17.723788 50515 net.cpp:409] drop2 -> drop2
I0122 16:32:17.723850 50515 net.cpp:144] Setting up drop2
I0122 16:32:17.723856 50515 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:32:17.723860 50515 net.cpp:159] Memory required for data: 64512800
I0122 16:32:17.723862 50515 layer_factory.hpp:77] Creating layer fc1
I0122 16:32:17.723870 50515 net.cpp:94] Creating Layer fc1
I0122 16:32:17.723875 50515 net.cpp:435] fc1 <- drop2
I0122 16:32:17.723881 50515 net.cpp:409] fc1 -> fc1
I0122 16:32:17.737828 50515 net.cpp:144] Setting up fc1
I0122 16:32:17.737862 50515 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:17.737865 50515 net.cpp:159] Memory required for data: 64615200
I0122 16:32:17.737874 50515 layer_factory.hpp:77] Creating layer bn5
I0122 16:32:17.737881 50515 net.cpp:94] Creating Layer bn5
I0122 16:32:17.737885 50515 net.cpp:435] bn5 <- fc1
I0122 16:32:17.737891 50515 net.cpp:409] bn5 -> scale5
I0122 16:32:17.738445 50515 net.cpp:144] Setting up bn5
I0122 16:32:17.738452 50515 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:17.738456 50515 net.cpp:159] Memory required for data: 64717600
I0122 16:32:17.738467 50515 layer_factory.hpp:77] Creating layer relu5
I0122 16:32:17.738474 50515 net.cpp:94] Creating Layer relu5
I0122 16:32:17.738478 50515 net.cpp:435] relu5 <- scale5
I0122 16:32:17.738482 50515 net.cpp:409] relu5 -> relu5
I0122 16:32:17.738500 50515 net.cpp:144] Setting up relu5
I0122 16:32:17.738507 50515 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:17.738509 50515 net.cpp:159] Memory required for data: 64820000
I0122 16:32:17.738512 50515 layer_factory.hpp:77] Creating layer drop3
I0122 16:32:17.738517 50515 net.cpp:94] Creating Layer drop3
I0122 16:32:17.738521 50515 net.cpp:435] drop3 <- relu5
I0122 16:32:17.738525 50515 net.cpp:409] drop3 -> drop3
I0122 16:32:17.738550 50515 net.cpp:144] Setting up drop3
I0122 16:32:17.738555 50515 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:17.738559 50515 net.cpp:159] Memory required for data: 64922400
I0122 16:32:17.738561 50515 layer_factory.hpp:77] Creating layer fc2
I0122 16:32:17.738567 50515 net.cpp:94] Creating Layer fc2
I0122 16:32:17.738570 50515 net.cpp:435] fc2 <- drop3
I0122 16:32:17.738575 50515 net.cpp:409] fc2 -> fc2
I0122 16:32:17.738701 50515 net.cpp:144] Setting up fc2
I0122 16:32:17.738706 50515 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:17.738710 50515 net.cpp:159] Memory required for data: 64924400
I0122 16:32:17.738715 50515 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:32:17.738720 50515 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:32:17.738723 50515 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:32:17.738729 50515 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:32:17.738737 50515 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:32:17.738742 50515 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:32:17.738777 50515 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:32:17.738782 50515 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:17.738787 50515 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:17.738790 50515 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:17.738792 50515 net.cpp:159] Memory required for data: 64930400
I0122 16:32:17.738795 50515 layer_factory.hpp:77] Creating layer loss
I0122 16:32:17.738801 50515 net.cpp:94] Creating Layer loss
I0122 16:32:17.738804 50515 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:32:17.738808 50515 net.cpp:435] loss <- label_data_1_split_0
I0122 16:32:17.738812 50515 net.cpp:409] loss -> loss
I0122 16:32:17.738821 50515 layer_factory.hpp:77] Creating layer loss
I0122 16:32:17.738886 50515 net.cpp:144] Setting up loss
I0122 16:32:17.738891 50515 net.cpp:151] Top shape: (1)
I0122 16:32:17.738894 50515 net.cpp:154]     with loss weight 1
I0122 16:32:17.738906 50515 net.cpp:159] Memory required for data: 64930404
I0122 16:32:17.738909 50515 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:32:17.738915 50515 net.cpp:94] Creating Layer accuracy-top1
I0122 16:32:17.738929 50515 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:32:17.738934 50515 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:32:17.738940 50515 net.cpp:409] accuracy-top1 -> top-1
I0122 16:32:17.738946 50515 net.cpp:144] Setting up accuracy-top1
I0122 16:32:17.738950 50515 net.cpp:151] Top shape: (1)
I0122 16:32:17.738953 50515 net.cpp:159] Memory required for data: 64930408
I0122 16:32:17.738956 50515 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:32:17.738961 50515 net.cpp:94] Creating Layer accuracy-top5
I0122 16:32:17.738967 50515 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:32:17.738971 50515 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:32:17.738976 50515 net.cpp:409] accuracy-top5 -> top-5
I0122 16:32:17.738982 50515 net.cpp:144] Setting up accuracy-top5
I0122 16:32:17.738986 50515 net.cpp:151] Top shape: (1)
I0122 16:32:17.738989 50515 net.cpp:159] Memory required for data: 64930412
I0122 16:32:17.738991 50515 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:32:17.738996 50515 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:32:17.738999 50515 net.cpp:220] loss needs backward computation.
I0122 16:32:17.739004 50515 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:32:17.739007 50515 net.cpp:220] fc2 needs backward computation.
I0122 16:32:17.739010 50515 net.cpp:220] drop3 needs backward computation.
I0122 16:32:17.739013 50515 net.cpp:220] relu5 needs backward computation.
I0122 16:32:17.739017 50515 net.cpp:220] bn5 needs backward computation.
I0122 16:32:17.739019 50515 net.cpp:220] fc1 needs backward computation.
I0122 16:32:17.739022 50515 net.cpp:220] drop2 needs backward computation.
I0122 16:32:17.739027 50515 net.cpp:220] pool2 needs backward computation.
I0122 16:32:17.739032 50515 net.cpp:220] relu4 needs backward computation.
I0122 16:32:17.739034 50515 net.cpp:220] bn4 needs backward computation.
I0122 16:32:17.739037 50515 net.cpp:220] conv4 needs backward computation.
I0122 16:32:17.739040 50515 net.cpp:220] relu3 needs backward computation.
I0122 16:32:17.739044 50515 net.cpp:220] bn3 needs backward computation.
I0122 16:32:17.739048 50515 net.cpp:220] conv3 needs backward computation.
I0122 16:32:17.739050 50515 net.cpp:220] drop1 needs backward computation.
I0122 16:32:17.739053 50515 net.cpp:220] pool1 needs backward computation.
I0122 16:32:17.739056 50515 net.cpp:220] relu2 needs backward computation.
I0122 16:32:17.739059 50515 net.cpp:220] bn2 needs backward computation.
I0122 16:32:17.739063 50515 net.cpp:220] conv2 needs backward computation.
I0122 16:32:17.739066 50515 net.cpp:220] relu1 needs backward computation.
I0122 16:32:17.739069 50515 net.cpp:220] bn1 needs backward computation.
I0122 16:32:17.739073 50515 net.cpp:220] conv1 needs backward computation.
I0122 16:32:17.739076 50515 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:32:17.739080 50515 net.cpp:222] data does not need backward computation.
I0122 16:32:17.739084 50515 net.cpp:264] This network produces output loss
I0122 16:32:17.739087 50515 net.cpp:264] This network produces output top-1
I0122 16:32:17.739090 50515 net.cpp:264] This network produces output top-5
I0122 16:32:17.739111 50515 net.cpp:284] Network initialization done.
I0122 16:32:17.741994 50515 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:32:17.748620 50515 caffe_interface.cpp:125] Batch 0, loss = 0.609263
I0122 16:32:17.748641 50515 caffe_interface.cpp:125] Batch 0, top-1 = 0.84
I0122 16:32:17.748646 50515 caffe_interface.cpp:125] Batch 0, top-5 = 0.98
I0122 16:32:17.749982 50515 caffe_interface.cpp:125] Batch 1, loss = 0.543786
I0122 16:32:17.749995 50515 caffe_interface.cpp:125] Batch 1, top-1 = 0.86
I0122 16:32:17.750000 50515 caffe_interface.cpp:125] Batch 1, top-5 = 0.98
I0122 16:32:17.751287 50515 caffe_interface.cpp:125] Batch 2, loss = 0.373077
I0122 16:32:17.751296 50515 caffe_interface.cpp:125] Batch 2, top-1 = 0.88
I0122 16:32:17.751300 50515 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:32:17.752604 50515 caffe_interface.cpp:125] Batch 3, loss = 0.634932
I0122 16:32:17.752617 50515 caffe_interface.cpp:125] Batch 3, top-1 = 0.82
I0122 16:32:17.752622 50515 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0122 16:32:17.753923 50515 caffe_interface.cpp:125] Batch 4, loss = 0.4767
I0122 16:32:17.753932 50515 caffe_interface.cpp:125] Batch 4, top-1 = 0.88
I0122 16:32:17.753935 50515 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0122 16:32:17.755214 50515 caffe_interface.cpp:125] Batch 5, loss = 0.663601
I0122 16:32:17.755221 50515 caffe_interface.cpp:125] Batch 5, top-1 = 0.84
I0122 16:32:17.755224 50515 caffe_interface.cpp:125] Batch 5, top-5 = 0.96
I0122 16:32:17.756500 50515 caffe_interface.cpp:125] Batch 6, loss = 0.473753
I0122 16:32:17.756508 50515 caffe_interface.cpp:125] Batch 6, top-1 = 0.86
I0122 16:32:17.756511 50515 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0122 16:32:17.757798 50515 caffe_interface.cpp:125] Batch 7, loss = 0.393441
I0122 16:32:17.757807 50515 caffe_interface.cpp:125] Batch 7, top-1 = 0.88
I0122 16:32:17.757810 50515 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0122 16:32:17.759093 50515 caffe_interface.cpp:125] Batch 8, loss = 0.447446
I0122 16:32:17.759100 50515 caffe_interface.cpp:125] Batch 8, top-1 = 0.78
I0122 16:32:17.759104 50515 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:32:17.760643 50515 caffe_interface.cpp:125] Batch 9, loss = 0.550034
I0122 16:32:17.760651 50515 caffe_interface.cpp:125] Batch 9, top-1 = 0.84
I0122 16:32:17.760654 50515 caffe_interface.cpp:125] Batch 9, top-5 = 0.98
I0122 16:32:17.761939 50515 caffe_interface.cpp:125] Batch 10, loss = 0.500475
I0122 16:32:17.761946 50515 caffe_interface.cpp:125] Batch 10, top-1 = 0.88
I0122 16:32:17.761950 50515 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:32:17.763216 50515 caffe_interface.cpp:125] Batch 11, loss = 0.56615
I0122 16:32:17.763222 50515 caffe_interface.cpp:125] Batch 11, top-1 = 0.84
I0122 16:32:17.763226 50515 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:32:17.764502 50515 caffe_interface.cpp:125] Batch 12, loss = 0.89119
I0122 16:32:17.764508 50515 caffe_interface.cpp:125] Batch 12, top-1 = 0.72
I0122 16:32:17.764511 50515 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:32:17.765774 50515 caffe_interface.cpp:125] Batch 13, loss = 0.648793
I0122 16:32:17.765781 50515 caffe_interface.cpp:125] Batch 13, top-1 = 0.82
I0122 16:32:17.765784 50515 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0122 16:32:17.767782 50515 caffe_interface.cpp:125] Batch 14, loss = 0.703893
I0122 16:32:17.767791 50515 caffe_interface.cpp:125] Batch 14, top-1 = 0.86
I0122 16:32:17.767792 50515 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0122 16:32:17.769284 50515 caffe_interface.cpp:125] Batch 15, loss = 0.61484
I0122 16:32:17.769290 50515 caffe_interface.cpp:125] Batch 15, top-1 = 0.8
I0122 16:32:17.769294 50515 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:32:17.770597 50515 caffe_interface.cpp:125] Batch 16, loss = 0.756162
I0122 16:32:17.770606 50515 caffe_interface.cpp:125] Batch 16, top-1 = 0.76
I0122 16:32:17.770608 50515 caffe_interface.cpp:125] Batch 16, top-5 = 1
I0122 16:32:17.771895 50515 caffe_interface.cpp:125] Batch 17, loss = 0.338402
I0122 16:32:17.771903 50515 caffe_interface.cpp:125] Batch 17, top-1 = 0.84
I0122 16:32:17.771906 50515 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0122 16:32:17.773183 50515 caffe_interface.cpp:125] Batch 18, loss = 0.672937
I0122 16:32:17.773190 50515 caffe_interface.cpp:125] Batch 18, top-1 = 0.8
I0122 16:32:17.773195 50515 caffe_interface.cpp:125] Batch 18, top-5 = 0.98
I0122 16:32:17.775576 50515 caffe_interface.cpp:125] Batch 19, loss = 0.554887
I0122 16:32:17.775584 50515 caffe_interface.cpp:125] Batch 19, top-1 = 0.84
I0122 16:32:17.775588 50515 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:32:17.776950 50515 caffe_interface.cpp:125] Batch 20, loss = 0.603766
I0122 16:32:17.776957 50515 caffe_interface.cpp:125] Batch 20, top-1 = 0.78
I0122 16:32:17.776962 50515 caffe_interface.cpp:125] Batch 20, top-5 = 0.98
I0122 16:32:17.778272 50515 caffe_interface.cpp:125] Batch 21, loss = 0.785636
I0122 16:32:17.778287 50515 caffe_interface.cpp:125] Batch 21, top-1 = 0.76
I0122 16:32:17.778291 50515 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0122 16:32:17.779556 50515 caffe_interface.cpp:125] Batch 22, loss = 0.448201
I0122 16:32:17.779564 50515 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0122 16:32:17.779567 50515 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0122 16:32:17.780843 50515 caffe_interface.cpp:125] Batch 23, loss = 0.275616
I0122 16:32:17.780850 50515 caffe_interface.cpp:125] Batch 23, top-1 = 0.92
I0122 16:32:17.780854 50515 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:32:17.782136 50515 caffe_interface.cpp:125] Batch 24, loss = 0.640423
I0122 16:32:17.782145 50515 caffe_interface.cpp:125] Batch 24, top-1 = 0.82
I0122 16:32:17.782147 50515 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0122 16:32:17.783428 50515 caffe_interface.cpp:125] Batch 25, loss = 0.3592
I0122 16:32:17.783443 50515 caffe_interface.cpp:125] Batch 25, top-1 = 0.86
I0122 16:32:17.783447 50515 caffe_interface.cpp:125] Batch 25, top-5 = 1
I0122 16:32:17.785012 50515 caffe_interface.cpp:125] Batch 26, loss = 0.559677
I0122 16:32:17.785020 50515 caffe_interface.cpp:125] Batch 26, top-1 = 0.82
I0122 16:32:17.785023 50515 caffe_interface.cpp:125] Batch 26, top-5 = 0.98
I0122 16:32:17.786509 50515 caffe_interface.cpp:125] Batch 27, loss = 0.369329
I0122 16:32:17.786517 50515 caffe_interface.cpp:125] Batch 27, top-1 = 0.9
I0122 16:32:17.786521 50515 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:32:17.787788 50515 caffe_interface.cpp:125] Batch 28, loss = 0.629241
I0122 16:32:17.787796 50515 caffe_interface.cpp:125] Batch 28, top-1 = 0.8
I0122 16:32:17.787799 50515 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:32:17.789095 50515 caffe_interface.cpp:125] Batch 29, loss = 0.32641
I0122 16:32:17.789103 50515 caffe_interface.cpp:125] Batch 29, top-1 = 0.92
I0122 16:32:17.789105 50515 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:32:17.790388 50515 caffe_interface.cpp:125] Batch 30, loss = 0.498874
I0122 16:32:17.790397 50515 caffe_interface.cpp:125] Batch 30, top-1 = 0.84
I0122 16:32:17.790401 50515 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:32:17.791684 50515 caffe_interface.cpp:125] Batch 31, loss = 0.549634
I0122 16:32:17.791692 50515 caffe_interface.cpp:125] Batch 31, top-1 = 0.86
I0122 16:32:17.791697 50515 caffe_interface.cpp:125] Batch 31, top-5 = 0.96
I0122 16:32:17.793174 50515 caffe_interface.cpp:125] Batch 32, loss = 0.608957
I0122 16:32:17.793180 50515 caffe_interface.cpp:125] Batch 32, top-1 = 0.76
I0122 16:32:17.793184 50515 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:32:17.794456 50515 caffe_interface.cpp:125] Batch 33, loss = 0.82679
I0122 16:32:17.794466 50515 caffe_interface.cpp:125] Batch 33, top-1 = 0.84
I0122 16:32:17.794468 50515 caffe_interface.cpp:125] Batch 33, top-5 = 0.98
I0122 16:32:17.795732 50515 caffe_interface.cpp:125] Batch 34, loss = 0.515081
I0122 16:32:17.795739 50515 caffe_interface.cpp:125] Batch 34, top-1 = 0.9
I0122 16:32:17.795742 50515 caffe_interface.cpp:125] Batch 34, top-5 = 1
I0122 16:32:17.797009 50515 caffe_interface.cpp:125] Batch 35, loss = 0.636005
I0122 16:32:17.797022 50515 caffe_interface.cpp:125] Batch 35, top-1 = 0.88
I0122 16:32:17.797026 50515 caffe_interface.cpp:125] Batch 35, top-5 = 0.98
I0122 16:32:17.798305 50515 caffe_interface.cpp:125] Batch 36, loss = 0.589538
I0122 16:32:17.798312 50515 caffe_interface.cpp:125] Batch 36, top-1 = 0.78
I0122 16:32:17.798316 50515 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:32:17.799576 50515 caffe_interface.cpp:125] Batch 37, loss = 0.621649
I0122 16:32:17.799582 50515 caffe_interface.cpp:125] Batch 37, top-1 = 0.86
I0122 16:32:17.799587 50515 caffe_interface.cpp:125] Batch 37, top-5 = 0.98
I0122 16:32:17.800853 50515 caffe_interface.cpp:125] Batch 38, loss = 0.649228
I0122 16:32:17.800861 50515 caffe_interface.cpp:125] Batch 38, top-1 = 0.76
I0122 16:32:17.800864 50515 caffe_interface.cpp:125] Batch 38, top-5 = 1
I0122 16:32:17.802165 50515 caffe_interface.cpp:125] Batch 39, loss = 0.39541
I0122 16:32:17.802172 50515 caffe_interface.cpp:125] Batch 39, top-1 = 0.84
I0122 16:32:17.802177 50515 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:32:17.803439 50515 caffe_interface.cpp:125] Batch 40, loss = 0.249143
I0122 16:32:17.803457 50515 caffe_interface.cpp:125] Batch 40, top-1 = 0.9
I0122 16:32:17.803459 50515 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:32:17.804718 50515 caffe_interface.cpp:125] Batch 41, loss = 0.827726
I0122 16:32:17.804724 50515 caffe_interface.cpp:125] Batch 41, top-1 = 0.8
I0122 16:32:17.804728 50515 caffe_interface.cpp:125] Batch 41, top-5 = 0.96
I0122 16:32:17.806000 50515 caffe_interface.cpp:125] Batch 42, loss = 0.804479
I0122 16:32:17.806008 50515 caffe_interface.cpp:125] Batch 42, top-1 = 0.74
I0122 16:32:17.806011 50515 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0122 16:32:17.808313 50515 caffe_interface.cpp:125] Batch 43, loss = 0.675319
I0122 16:32:17.808321 50515 caffe_interface.cpp:125] Batch 43, top-1 = 0.78
I0122 16:32:17.808326 50515 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0122 16:32:17.809664 50515 caffe_interface.cpp:125] Batch 44, loss = 0.87591
I0122 16:32:17.809672 50515 caffe_interface.cpp:125] Batch 44, top-1 = 0.76
I0122 16:32:17.809675 50515 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0122 16:32:17.811103 50515 caffe_interface.cpp:125] Batch 45, loss = 0.516527
I0122 16:32:17.811110 50515 caffe_interface.cpp:125] Batch 45, top-1 = 0.82
I0122 16:32:17.811115 50515 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0122 16:32:17.812408 50515 caffe_interface.cpp:125] Batch 46, loss = 0.214311
I0122 16:32:17.812417 50515 caffe_interface.cpp:125] Batch 46, top-1 = 0.94
I0122 16:32:17.812419 50515 caffe_interface.cpp:125] Batch 46, top-5 = 1
I0122 16:32:17.813691 50515 caffe_interface.cpp:125] Batch 47, loss = 0.427379
I0122 16:32:17.813699 50515 caffe_interface.cpp:125] Batch 47, top-1 = 0.84
I0122 16:32:17.813704 50515 caffe_interface.cpp:125] Batch 47, top-5 = 0.98
I0122 16:32:17.814975 50515 caffe_interface.cpp:125] Batch 48, loss = 0.431116
I0122 16:32:17.814983 50515 caffe_interface.cpp:125] Batch 48, top-1 = 0.86
I0122 16:32:17.814986 50515 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:32:17.816262 50515 caffe_interface.cpp:125] Batch 49, loss = 0.378116
I0122 16:32:17.816268 50515 caffe_interface.cpp:125] Batch 49, top-1 = 0.82
I0122 16:32:17.816272 50515 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0122 16:32:17.817564 50515 caffe_interface.cpp:125] Batch 50, loss = 0.573259
I0122 16:32:17.817572 50515 caffe_interface.cpp:125] Batch 50, top-1 = 0.82
I0122 16:32:17.817575 50515 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:32:17.818856 50515 caffe_interface.cpp:125] Batch 51, loss = 0.516965
I0122 16:32:17.818863 50515 caffe_interface.cpp:125] Batch 51, top-1 = 0.84
I0122 16:32:17.818867 50515 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0122 16:32:17.820129 50515 caffe_interface.cpp:125] Batch 52, loss = 0.765657
I0122 16:32:17.820137 50515 caffe_interface.cpp:125] Batch 52, top-1 = 0.78
I0122 16:32:17.820140 50515 caffe_interface.cpp:125] Batch 52, top-5 = 0.98
I0122 16:32:17.821429 50515 caffe_interface.cpp:125] Batch 53, loss = 0.652871
I0122 16:32:17.821436 50515 caffe_interface.cpp:125] Batch 53, top-1 = 0.7
I0122 16:32:17.821439 50515 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0122 16:32:17.822729 50515 caffe_interface.cpp:125] Batch 54, loss = 0.618323
I0122 16:32:17.822738 50515 caffe_interface.cpp:125] Batch 54, top-1 = 0.8
I0122 16:32:17.822742 50515 caffe_interface.cpp:125] Batch 54, top-5 = 0.98
I0122 16:32:17.824026 50515 caffe_interface.cpp:125] Batch 55, loss = 0.286309
I0122 16:32:17.824034 50515 caffe_interface.cpp:125] Batch 55, top-1 = 0.82
I0122 16:32:17.824038 50515 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:32:17.825309 50515 caffe_interface.cpp:125] Batch 56, loss = 0.658974
I0122 16:32:17.825325 50515 caffe_interface.cpp:125] Batch 56, top-1 = 0.78
I0122 16:32:17.825337 50515 caffe_interface.cpp:125] Batch 56, top-5 = 0.98
I0122 16:32:17.826882 50515 caffe_interface.cpp:125] Batch 57, loss = 0.232594
I0122 16:32:17.826890 50515 caffe_interface.cpp:125] Batch 57, top-1 = 0.92
I0122 16:32:17.826894 50515 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:32:17.828161 50515 caffe_interface.cpp:125] Batch 58, loss = 0.671627
I0122 16:32:17.828168 50515 caffe_interface.cpp:125] Batch 58, top-1 = 0.76
I0122 16:32:17.828171 50515 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0122 16:32:17.829443 50515 caffe_interface.cpp:125] Batch 59, loss = 0.377598
I0122 16:32:17.829450 50515 caffe_interface.cpp:125] Batch 59, top-1 = 0.9
I0122 16:32:17.829455 50515 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:32:17.830723 50515 caffe_interface.cpp:125] Batch 60, loss = 0.580074
I0122 16:32:17.830730 50515 caffe_interface.cpp:125] Batch 60, top-1 = 0.8
I0122 16:32:17.830734 50515 caffe_interface.cpp:125] Batch 60, top-5 = 1
I0122 16:32:17.831995 50515 caffe_interface.cpp:125] Batch 61, loss = 0.786159
I0122 16:32:17.832001 50515 caffe_interface.cpp:125] Batch 61, top-1 = 0.78
I0122 16:32:17.832005 50515 caffe_interface.cpp:125] Batch 61, top-5 = 0.96
I0122 16:32:17.833288 50515 caffe_interface.cpp:125] Batch 62, loss = 0.645535
I0122 16:32:17.833297 50515 caffe_interface.cpp:125] Batch 62, top-1 = 0.76
I0122 16:32:17.833299 50515 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:32:17.834571 50515 caffe_interface.cpp:125] Batch 63, loss = 0.475129
I0122 16:32:17.834579 50515 caffe_interface.cpp:125] Batch 63, top-1 = 0.9
I0122 16:32:17.834583 50515 caffe_interface.cpp:125] Batch 63, top-5 = 1
I0122 16:32:17.835855 50515 caffe_interface.cpp:125] Batch 64, loss = 0.322608
I0122 16:32:17.835861 50515 caffe_interface.cpp:125] Batch 64, top-1 = 0.88
I0122 16:32:17.835865 50515 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:32:17.837138 50515 caffe_interface.cpp:125] Batch 65, loss = 0.576311
I0122 16:32:17.837146 50515 caffe_interface.cpp:125] Batch 65, top-1 = 0.82
I0122 16:32:17.837149 50515 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:32:17.838425 50515 caffe_interface.cpp:125] Batch 66, loss = 0.757382
I0122 16:32:17.838440 50515 caffe_interface.cpp:125] Batch 66, top-1 = 0.76
I0122 16:32:17.838443 50515 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0122 16:32:17.839720 50515 caffe_interface.cpp:125] Batch 67, loss = 0.852109
I0122 16:32:17.839726 50515 caffe_interface.cpp:125] Batch 67, top-1 = 0.74
I0122 16:32:17.839731 50515 caffe_interface.cpp:125] Batch 67, top-5 = 0.98
I0122 16:32:17.842077 50515 caffe_interface.cpp:125] Batch 68, loss = 0.217679
I0122 16:32:17.842085 50515 caffe_interface.cpp:125] Batch 68, top-1 = 0.92
I0122 16:32:17.842089 50515 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:32:17.843487 50515 caffe_interface.cpp:125] Batch 69, loss = 0.640222
I0122 16:32:17.843493 50515 caffe_interface.cpp:125] Batch 69, top-1 = 0.86
I0122 16:32:17.843503 50515 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0122 16:32:17.844971 50515 caffe_interface.cpp:125] Batch 70, loss = 0.280745
I0122 16:32:17.844980 50515 caffe_interface.cpp:125] Batch 70, top-1 = 0.88
I0122 16:32:17.844982 50515 caffe_interface.cpp:125] Batch 70, top-5 = 0.98
I0122 16:32:17.846251 50515 caffe_interface.cpp:125] Batch 71, loss = 0.505703
I0122 16:32:17.846258 50515 caffe_interface.cpp:125] Batch 71, top-1 = 0.82
I0122 16:32:17.846262 50515 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:32:17.847543 50515 caffe_interface.cpp:125] Batch 72, loss = 0.578825
I0122 16:32:17.847549 50515 caffe_interface.cpp:125] Batch 72, top-1 = 0.8
I0122 16:32:17.847553 50515 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0122 16:32:17.848824 50515 caffe_interface.cpp:125] Batch 73, loss = 0.624191
I0122 16:32:17.848831 50515 caffe_interface.cpp:125] Batch 73, top-1 = 0.74
I0122 16:32:17.848835 50515 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0122 16:32:17.850109 50515 caffe_interface.cpp:125] Batch 74, loss = 0.838489
I0122 16:32:17.850117 50515 caffe_interface.cpp:125] Batch 74, top-1 = 0.78
I0122 16:32:17.850139 50515 caffe_interface.cpp:125] Batch 74, top-5 = 0.94
I0122 16:32:17.851423 50515 caffe_interface.cpp:125] Batch 75, loss = 0.561056
I0122 16:32:17.851429 50515 caffe_interface.cpp:125] Batch 75, top-1 = 0.84
I0122 16:32:17.851433 50515 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0122 16:32:17.852726 50515 caffe_interface.cpp:125] Batch 76, loss = 0.663569
I0122 16:32:17.852733 50515 caffe_interface.cpp:125] Batch 76, top-1 = 0.8
I0122 16:32:17.852744 50515 caffe_interface.cpp:125] Batch 76, top-5 = 0.98
I0122 16:32:17.854053 50515 caffe_interface.cpp:125] Batch 77, loss = 0.708346
I0122 16:32:17.854060 50515 caffe_interface.cpp:125] Batch 77, top-1 = 0.82
I0122 16:32:17.854064 50515 caffe_interface.cpp:125] Batch 77, top-5 = 1
I0122 16:32:17.855355 50515 caffe_interface.cpp:125] Batch 78, loss = 0.53418
I0122 16:32:17.855362 50515 caffe_interface.cpp:125] Batch 78, top-1 = 0.82
I0122 16:32:17.855366 50515 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0122 16:32:17.856653 50515 caffe_interface.cpp:125] Batch 79, loss = 0.750759
I0122 16:32:17.856662 50515 caffe_interface.cpp:125] Batch 79, top-1 = 0.74
I0122 16:32:17.856665 50515 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0122 16:32:17.857954 50515 caffe_interface.cpp:125] Batch 80, loss = 0.264689
I0122 16:32:17.857962 50515 caffe_interface.cpp:125] Batch 80, top-1 = 0.88
I0122 16:32:17.857965 50515 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:32:17.859238 50515 caffe_interface.cpp:125] Batch 81, loss = 0.637957
I0122 16:32:17.859246 50515 caffe_interface.cpp:125] Batch 81, top-1 = 0.78
I0122 16:32:17.859249 50515 caffe_interface.cpp:125] Batch 81, top-5 = 0.98
I0122 16:32:17.860786 50515 caffe_interface.cpp:125] Batch 82, loss = 0.667957
I0122 16:32:17.860795 50515 caffe_interface.cpp:125] Batch 82, top-1 = 0.78
I0122 16:32:17.860797 50515 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0122 16:32:17.862066 50515 caffe_interface.cpp:125] Batch 83, loss = 0.69383
I0122 16:32:17.862073 50515 caffe_interface.cpp:125] Batch 83, top-1 = 0.86
I0122 16:32:17.862077 50515 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:32:17.863348 50515 caffe_interface.cpp:125] Batch 84, loss = 0.75946
I0122 16:32:17.863358 50515 caffe_interface.cpp:125] Batch 84, top-1 = 0.76
I0122 16:32:17.863361 50515 caffe_interface.cpp:125] Batch 84, top-5 = 0.96
I0122 16:32:17.864635 50515 caffe_interface.cpp:125] Batch 85, loss = 0.466867
I0122 16:32:17.864642 50515 caffe_interface.cpp:125] Batch 85, top-1 = 0.88
I0122 16:32:17.864645 50515 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:32:17.865907 50515 caffe_interface.cpp:125] Batch 86, loss = 0.467661
I0122 16:32:17.865916 50515 caffe_interface.cpp:125] Batch 86, top-1 = 0.84
I0122 16:32:17.865919 50515 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:32:17.867174 50515 caffe_interface.cpp:125] Batch 87, loss = 0.503877
I0122 16:32:17.867182 50515 caffe_interface.cpp:125] Batch 87, top-1 = 0.84
I0122 16:32:17.867185 50515 caffe_interface.cpp:125] Batch 87, top-5 = 0.98
I0122 16:32:17.868459 50515 caffe_interface.cpp:125] Batch 88, loss = 0.849587
I0122 16:32:17.868475 50515 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0122 16:32:17.868479 50515 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0122 16:32:17.869751 50515 caffe_interface.cpp:125] Batch 89, loss = 0.511418
I0122 16:32:17.869760 50515 caffe_interface.cpp:125] Batch 89, top-1 = 0.82
I0122 16:32:17.869762 50515 caffe_interface.cpp:125] Batch 89, top-5 = 0.98
I0122 16:32:17.871024 50515 caffe_interface.cpp:125] Batch 90, loss = 0.68233
I0122 16:32:17.871038 50515 caffe_interface.cpp:125] Batch 90, top-1 = 0.74
I0122 16:32:17.871042 50515 caffe_interface.cpp:125] Batch 90, top-5 = 0.98
I0122 16:32:17.872328 50515 caffe_interface.cpp:125] Batch 91, loss = 0.695861
I0122 16:32:17.872336 50515 caffe_interface.cpp:125] Batch 91, top-1 = 0.8
I0122 16:32:17.872339 50515 caffe_interface.cpp:125] Batch 91, top-5 = 0.98
I0122 16:32:17.873595 50515 caffe_interface.cpp:125] Batch 92, loss = 0.927946
I0122 16:32:17.873602 50515 caffe_interface.cpp:125] Batch 92, top-1 = 0.82
I0122 16:32:17.873615 50515 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:32:17.875972 50515 caffe_interface.cpp:125] Batch 93, loss = 0.459202
I0122 16:32:17.875980 50515 caffe_interface.cpp:125] Batch 93, top-1 = 0.82
I0122 16:32:17.875984 50515 caffe_interface.cpp:125] Batch 93, top-5 = 1
I0122 16:32:17.877379 50515 caffe_interface.cpp:125] Batch 94, loss = 1.00033
I0122 16:32:17.877396 50515 caffe_interface.cpp:125] Batch 94, top-1 = 0.68
I0122 16:32:17.877399 50515 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0122 16:32:17.878877 50515 caffe_interface.cpp:125] Batch 95, loss = 0.493701
I0122 16:32:17.878885 50515 caffe_interface.cpp:125] Batch 95, top-1 = 0.82
I0122 16:32:17.878890 50515 caffe_interface.cpp:125] Batch 95, top-5 = 0.98
I0122 16:32:17.880147 50515 caffe_interface.cpp:125] Batch 96, loss = 0.55813
I0122 16:32:17.880156 50515 caffe_interface.cpp:125] Batch 96, top-1 = 0.82
I0122 16:32:17.880159 50515 caffe_interface.cpp:125] Batch 96, top-5 = 0.98
I0122 16:32:17.881435 50515 caffe_interface.cpp:125] Batch 97, loss = 0.473007
I0122 16:32:17.881443 50515 caffe_interface.cpp:125] Batch 97, top-1 = 0.8
I0122 16:32:17.881445 50515 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:32:17.882732 50515 caffe_interface.cpp:125] Batch 98, loss = 0.603483
I0122 16:32:17.882741 50515 caffe_interface.cpp:125] Batch 98, top-1 = 0.86
I0122 16:32:17.882745 50515 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:32:17.884030 50515 caffe_interface.cpp:125] Batch 99, loss = 0.562358
I0122 16:32:17.884037 50515 caffe_interface.cpp:125] Batch 99, top-1 = 0.8
I0122 16:32:17.884042 50515 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:32:17.885325 50515 caffe_interface.cpp:125] Batch 100, loss = 0.346704
I0122 16:32:17.885332 50515 caffe_interface.cpp:125] Batch 100, top-1 = 0.9
I0122 16:32:17.885335 50515 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0122 16:32:17.886631 50515 caffe_interface.cpp:125] Batch 101, loss = 0.367521
I0122 16:32:17.886637 50515 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0122 16:32:17.886641 50515 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:32:17.887923 50515 caffe_interface.cpp:125] Batch 102, loss = 0.756972
I0122 16:32:17.887930 50515 caffe_interface.cpp:125] Batch 102, top-1 = 0.72
I0122 16:32:17.887934 50515 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0122 16:32:17.889199 50515 caffe_interface.cpp:125] Batch 103, loss = 0.558617
I0122 16:32:17.889206 50515 caffe_interface.cpp:125] Batch 103, top-1 = 0.88
I0122 16:32:17.889210 50515 caffe_interface.cpp:125] Batch 103, top-5 = 0.96
I0122 16:32:17.890486 50515 caffe_interface.cpp:125] Batch 104, loss = 0.556484
I0122 16:32:17.890494 50515 caffe_interface.cpp:125] Batch 104, top-1 = 0.84
I0122 16:32:17.890497 50515 caffe_interface.cpp:125] Batch 104, top-5 = 0.98
I0122 16:32:17.891777 50515 caffe_interface.cpp:125] Batch 105, loss = 0.718994
I0122 16:32:17.891783 50515 caffe_interface.cpp:125] Batch 105, top-1 = 0.82
I0122 16:32:17.891788 50515 caffe_interface.cpp:125] Batch 105, top-5 = 0.98
I0122 16:32:17.893251 50515 caffe_interface.cpp:125] Batch 106, loss = 0.62295
I0122 16:32:17.893260 50515 caffe_interface.cpp:125] Batch 106, top-1 = 0.8
I0122 16:32:17.893262 50515 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:32:17.894536 50515 caffe_interface.cpp:125] Batch 107, loss = 0.934743
I0122 16:32:17.894542 50515 caffe_interface.cpp:125] Batch 107, top-1 = 0.76
I0122 16:32:17.894546 50515 caffe_interface.cpp:125] Batch 107, top-5 = 0.98
I0122 16:32:17.895813 50515 caffe_interface.cpp:125] Batch 108, loss = 0.476923
I0122 16:32:17.895820 50515 caffe_interface.cpp:125] Batch 108, top-1 = 0.82
I0122 16:32:17.895823 50515 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0122 16:32:17.897085 50515 caffe_interface.cpp:125] Batch 109, loss = 0.433604
I0122 16:32:17.897092 50515 caffe_interface.cpp:125] Batch 109, top-1 = 0.86
I0122 16:32:17.897095 50515 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:32:17.898365 50515 caffe_interface.cpp:125] Batch 110, loss = 0.647387
I0122 16:32:17.898382 50515 caffe_interface.cpp:125] Batch 110, top-1 = 0.8
I0122 16:32:17.898386 50515 caffe_interface.cpp:125] Batch 110, top-5 = 0.98
I0122 16:32:17.899641 50515 caffe_interface.cpp:125] Batch 111, loss = 0.767395
I0122 16:32:17.899649 50515 caffe_interface.cpp:125] Batch 111, top-1 = 0.8
I0122 16:32:17.899653 50515 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0122 16:32:17.900926 50515 caffe_interface.cpp:125] Batch 112, loss = 0.502816
I0122 16:32:17.900933 50515 caffe_interface.cpp:125] Batch 112, top-1 = 0.92
I0122 16:32:17.900936 50515 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0122 16:32:17.902207 50515 caffe_interface.cpp:125] Batch 113, loss = 0.586707
I0122 16:32:17.902215 50515 caffe_interface.cpp:125] Batch 113, top-1 = 0.78
I0122 16:32:17.902217 50515 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:32:17.903478 50515 caffe_interface.cpp:125] Batch 114, loss = 0.933031
I0122 16:32:17.903486 50515 caffe_interface.cpp:125] Batch 114, top-1 = 0.7
I0122 16:32:17.903489 50515 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0122 16:32:17.904758 50515 caffe_interface.cpp:125] Batch 115, loss = 0.293803
I0122 16:32:17.904768 50515 caffe_interface.cpp:125] Batch 115, top-1 = 0.92
I0122 16:32:17.904772 50515 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:32:17.906041 50515 caffe_interface.cpp:125] Batch 116, loss = 0.773646
I0122 16:32:17.906049 50515 caffe_interface.cpp:125] Batch 116, top-1 = 0.78
I0122 16:32:17.906052 50515 caffe_interface.cpp:125] Batch 116, top-5 = 0.98
I0122 16:32:17.907331 50515 caffe_interface.cpp:125] Batch 117, loss = 0.585078
I0122 16:32:17.907338 50515 caffe_interface.cpp:125] Batch 117, top-1 = 0.78
I0122 16:32:17.907341 50515 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0122 16:32:17.909725 50515 caffe_interface.cpp:125] Batch 118, loss = 0.502409
I0122 16:32:17.909732 50515 caffe_interface.cpp:125] Batch 118, top-1 = 0.82
I0122 16:32:17.909734 50515 caffe_interface.cpp:125] Batch 118, top-5 = 0.98
I0122 16:32:17.911190 50515 caffe_interface.cpp:125] Batch 119, loss = 0.856194
I0122 16:32:17.911197 50515 caffe_interface.cpp:125] Batch 119, top-1 = 0.76
I0122 16:32:17.911201 50515 caffe_interface.cpp:125] Batch 119, top-5 = 0.96
I0122 16:32:17.912624 50515 caffe_interface.cpp:125] Batch 120, loss = 0.629596
I0122 16:32:17.912631 50515 caffe_interface.cpp:125] Batch 120, top-1 = 0.84
I0122 16:32:17.912634 50515 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0122 16:32:17.913920 50515 caffe_interface.cpp:125] Batch 121, loss = 0.855154
I0122 16:32:17.913928 50515 caffe_interface.cpp:125] Batch 121, top-1 = 0.76
I0122 16:32:17.913931 50515 caffe_interface.cpp:125] Batch 121, top-5 = 0.98
I0122 16:32:17.915218 50515 caffe_interface.cpp:125] Batch 122, loss = 0.438937
I0122 16:32:17.915233 50515 caffe_interface.cpp:125] Batch 122, top-1 = 0.86
I0122 16:32:17.915237 50515 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:32:17.916509 50515 caffe_interface.cpp:125] Batch 123, loss = 0.497449
I0122 16:32:17.916517 50515 caffe_interface.cpp:125] Batch 123, top-1 = 0.76
I0122 16:32:17.916522 50515 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0122 16:32:17.917814 50515 caffe_interface.cpp:125] Batch 124, loss = 0.549026
I0122 16:32:17.917822 50515 caffe_interface.cpp:125] Batch 124, top-1 = 0.8
I0122 16:32:17.917826 50515 caffe_interface.cpp:125] Batch 124, top-5 = 1
I0122 16:32:17.919102 50515 caffe_interface.cpp:125] Batch 125, loss = 0.617145
I0122 16:32:17.919118 50515 caffe_interface.cpp:125] Batch 125, top-1 = 0.8
I0122 16:32:17.919121 50515 caffe_interface.cpp:125] Batch 125, top-5 = 1
I0122 16:32:17.920399 50515 caffe_interface.cpp:125] Batch 126, loss = 0.553607
I0122 16:32:17.920408 50515 caffe_interface.cpp:125] Batch 126, top-1 = 0.82
I0122 16:32:17.920410 50515 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:32:17.921699 50515 caffe_interface.cpp:125] Batch 127, loss = 0.546332
I0122 16:32:17.921706 50515 caffe_interface.cpp:125] Batch 127, top-1 = 0.82
I0122 16:32:17.921710 50515 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:32:17.923010 50515 caffe_interface.cpp:125] Batch 128, loss = 0.480946
I0122 16:32:17.923018 50515 caffe_interface.cpp:125] Batch 128, top-1 = 0.8
I0122 16:32:17.923022 50515 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0122 16:32:17.924291 50515 caffe_interface.cpp:125] Batch 129, loss = 0.759456
I0122 16:32:17.924299 50515 caffe_interface.cpp:125] Batch 129, top-1 = 0.8
I0122 16:32:17.924302 50515 caffe_interface.cpp:125] Batch 129, top-5 = 0.98
I0122 16:32:17.925572 50515 caffe_interface.cpp:125] Batch 130, loss = 0.671361
I0122 16:32:17.925580 50515 caffe_interface.cpp:125] Batch 130, top-1 = 0.7
I0122 16:32:17.925583 50515 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:32:17.927120 50515 caffe_interface.cpp:125] Batch 131, loss = 0.626931
I0122 16:32:17.927129 50515 caffe_interface.cpp:125] Batch 131, top-1 = 0.84
I0122 16:32:17.927132 50515 caffe_interface.cpp:125] Batch 131, top-5 = 0.98
I0122 16:32:17.928400 50515 caffe_interface.cpp:125] Batch 132, loss = 0.778182
I0122 16:32:17.928406 50515 caffe_interface.cpp:125] Batch 132, top-1 = 0.76
I0122 16:32:17.928411 50515 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0122 16:32:17.929673 50515 caffe_interface.cpp:125] Batch 133, loss = 0.872402
I0122 16:32:17.929680 50515 caffe_interface.cpp:125] Batch 133, top-1 = 0.76
I0122 16:32:17.929683 50515 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0122 16:32:17.930948 50515 caffe_interface.cpp:125] Batch 134, loss = 0.467597
I0122 16:32:17.930956 50515 caffe_interface.cpp:125] Batch 134, top-1 = 0.82
I0122 16:32:17.930960 50515 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:32:17.932224 50515 caffe_interface.cpp:125] Batch 135, loss = 0.871282
I0122 16:32:17.932232 50515 caffe_interface.cpp:125] Batch 135, top-1 = 0.78
I0122 16:32:17.932236 50515 caffe_interface.cpp:125] Batch 135, top-5 = 1
I0122 16:32:17.933506 50515 caffe_interface.cpp:125] Batch 136, loss = 0.639739
I0122 16:32:17.933513 50515 caffe_interface.cpp:125] Batch 136, top-1 = 0.82
I0122 16:32:17.933516 50515 caffe_interface.cpp:125] Batch 136, top-5 = 0.98
I0122 16:32:17.934784 50515 caffe_interface.cpp:125] Batch 137, loss = 0.445355
I0122 16:32:17.934792 50515 caffe_interface.cpp:125] Batch 137, top-1 = 0.8
I0122 16:32:17.934795 50515 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:32:17.936072 50515 caffe_interface.cpp:125] Batch 138, loss = 0.78199
I0122 16:32:17.936085 50515 caffe_interface.cpp:125] Batch 138, top-1 = 0.78
I0122 16:32:17.936089 50515 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0122 16:32:17.937371 50515 caffe_interface.cpp:125] Batch 139, loss = 1.0246
I0122 16:32:17.937378 50515 caffe_interface.cpp:125] Batch 139, top-1 = 0.8
I0122 16:32:17.937382 50515 caffe_interface.cpp:125] Batch 139, top-5 = 0.96
I0122 16:32:17.938649 50515 caffe_interface.cpp:125] Batch 140, loss = 0.559141
I0122 16:32:17.938657 50515 caffe_interface.cpp:125] Batch 140, top-1 = 0.86
I0122 16:32:17.938661 50515 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0122 16:32:17.939934 50515 caffe_interface.cpp:125] Batch 141, loss = 0.571197
I0122 16:32:17.939941 50515 caffe_interface.cpp:125] Batch 141, top-1 = 0.82
I0122 16:32:17.939944 50515 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:32:17.941210 50515 caffe_interface.cpp:125] Batch 142, loss = 0.262774
I0122 16:32:17.941217 50515 caffe_interface.cpp:125] Batch 142, top-1 = 0.9
I0122 16:32:17.941220 50515 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:32:17.943554 50515 caffe_interface.cpp:125] Batch 143, loss = 0.656242
I0122 16:32:17.943562 50515 caffe_interface.cpp:125] Batch 143, top-1 = 0.78
I0122 16:32:17.943565 50515 caffe_interface.cpp:125] Batch 143, top-5 = 0.98
I0122 16:32:17.944957 50515 caffe_interface.cpp:125] Batch 144, loss = 0.617441
I0122 16:32:17.944963 50515 caffe_interface.cpp:125] Batch 144, top-1 = 0.84
I0122 16:32:17.944967 50515 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0122 16:32:17.946445 50515 caffe_interface.cpp:125] Batch 145, loss = 0.532991
I0122 16:32:17.946456 50515 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0122 16:32:17.946468 50515 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0122 16:32:17.947753 50515 caffe_interface.cpp:125] Batch 146, loss = 0.403883
I0122 16:32:17.947762 50515 caffe_interface.cpp:125] Batch 146, top-1 = 0.82
I0122 16:32:17.947764 50515 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0122 16:32:17.949038 50515 caffe_interface.cpp:125] Batch 147, loss = 0.772805
I0122 16:32:17.949044 50515 caffe_interface.cpp:125] Batch 147, top-1 = 0.76
I0122 16:32:17.949049 50515 caffe_interface.cpp:125] Batch 147, top-5 = 0.98
I0122 16:32:17.950340 50515 caffe_interface.cpp:125] Batch 148, loss = 0.230673
I0122 16:32:17.950347 50515 caffe_interface.cpp:125] Batch 148, top-1 = 0.9
I0122 16:32:17.950350 50515 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:32:17.951642 50515 caffe_interface.cpp:125] Batch 149, loss = 0.409011
I0122 16:32:17.951656 50515 caffe_interface.cpp:125] Batch 149, top-1 = 0.86
I0122 16:32:17.951659 50515 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:32:17.952947 50515 caffe_interface.cpp:125] Batch 150, loss = 0.201936
I0122 16:32:17.952955 50515 caffe_interface.cpp:125] Batch 150, top-1 = 0.92
I0122 16:32:17.952958 50515 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0122 16:32:17.954236 50515 caffe_interface.cpp:125] Batch 151, loss = 0.72155
I0122 16:32:17.954243 50515 caffe_interface.cpp:125] Batch 151, top-1 = 0.74
I0122 16:32:17.954248 50515 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:32:17.955526 50515 caffe_interface.cpp:125] Batch 152, loss = 0.367202
I0122 16:32:17.955533 50515 caffe_interface.cpp:125] Batch 152, top-1 = 0.92
I0122 16:32:17.955538 50515 caffe_interface.cpp:125] Batch 152, top-5 = 1
I0122 16:32:17.956825 50515 caffe_interface.cpp:125] Batch 153, loss = 0.381739
I0122 16:32:17.956835 50515 caffe_interface.cpp:125] Batch 153, top-1 = 0.8
I0122 16:32:17.956838 50515 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:32:17.958107 50515 caffe_interface.cpp:125] Batch 154, loss = 0.735044
I0122 16:32:17.958115 50515 caffe_interface.cpp:125] Batch 154, top-1 = 0.76
I0122 16:32:17.958119 50515 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:32:17.959404 50515 caffe_interface.cpp:125] Batch 155, loss = 0.547364
I0122 16:32:17.959411 50515 caffe_interface.cpp:125] Batch 155, top-1 = 0.82
I0122 16:32:17.959416 50515 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:32:17.960769 50515 caffe_interface.cpp:125] Batch 156, loss = 0.471067
I0122 16:32:17.960779 50515 caffe_interface.cpp:125] Batch 156, top-1 = 0.8
I0122 16:32:17.960783 50515 caffe_interface.cpp:125] Batch 156, top-5 = 1
I0122 16:32:17.962045 50515 caffe_interface.cpp:125] Batch 157, loss = 0.296098
I0122 16:32:17.962052 50515 caffe_interface.cpp:125] Batch 157, top-1 = 0.94
I0122 16:32:17.962056 50515 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:32:17.963322 50515 caffe_interface.cpp:125] Batch 158, loss = 0.561316
I0122 16:32:17.963330 50515 caffe_interface.cpp:125] Batch 158, top-1 = 0.82
I0122 16:32:17.963333 50515 caffe_interface.cpp:125] Batch 158, top-5 = 0.98
I0122 16:32:17.964607 50515 caffe_interface.cpp:125] Batch 159, loss = 0.682343
I0122 16:32:17.964614 50515 caffe_interface.cpp:125] Batch 159, top-1 = 0.78
I0122 16:32:17.964618 50515 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0122 16:32:17.965903 50515 caffe_interface.cpp:125] Batch 160, loss = 0.536869
I0122 16:32:17.965914 50515 caffe_interface.cpp:125] Batch 160, top-1 = 0.84
I0122 16:32:17.965919 50515 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0122 16:32:17.967178 50515 caffe_interface.cpp:125] Batch 161, loss = 0.419651
I0122 16:32:17.967186 50515 caffe_interface.cpp:125] Batch 161, top-1 = 0.84
I0122 16:32:17.967190 50515 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0122 16:32:17.968467 50515 caffe_interface.cpp:125] Batch 162, loss = 0.699767
I0122 16:32:17.968477 50515 caffe_interface.cpp:125] Batch 162, top-1 = 0.84
I0122 16:32:17.968480 50515 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:32:17.969753 50515 caffe_interface.cpp:125] Batch 163, loss = 0.63096
I0122 16:32:17.969777 50515 caffe_interface.cpp:125] Batch 163, top-1 = 0.78
I0122 16:32:17.969781 50515 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:32:17.971062 50515 caffe_interface.cpp:125] Batch 164, loss = 0.659085
I0122 16:32:17.971071 50515 caffe_interface.cpp:125] Batch 164, top-1 = 0.72
I0122 16:32:17.971073 50515 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:32:17.972352 50515 caffe_interface.cpp:125] Batch 165, loss = 0.465108
I0122 16:32:17.972358 50515 caffe_interface.cpp:125] Batch 165, top-1 = 0.84
I0122 16:32:17.972362 50515 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:32:17.973623 50515 caffe_interface.cpp:125] Batch 166, loss = 0.299111
I0122 16:32:17.973630 50515 caffe_interface.cpp:125] Batch 166, top-1 = 0.86
I0122 16:32:17.973634 50515 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:32:17.974897 50515 caffe_interface.cpp:125] Batch 167, loss = 0.627472
I0122 16:32:17.974905 50515 caffe_interface.cpp:125] Batch 167, top-1 = 0.74
I0122 16:32:17.974908 50515 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0122 16:32:17.977291 50515 caffe_interface.cpp:125] Batch 168, loss = 0.288331
I0122 16:32:17.977296 50515 caffe_interface.cpp:125] Batch 168, top-1 = 0.92
I0122 16:32:17.977299 50515 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:32:17.979255 50515 caffe_interface.cpp:125] Batch 169, loss = 0.587561
I0122 16:32:17.979262 50515 caffe_interface.cpp:125] Batch 169, top-1 = 0.76
I0122 16:32:17.979265 50515 caffe_interface.cpp:125] Batch 169, top-5 = 1
I0122 16:32:17.980489 50515 caffe_interface.cpp:125] Batch 170, loss = 0.473033
I0122 16:32:17.980496 50515 caffe_interface.cpp:125] Batch 170, top-1 = 0.84
I0122 16:32:17.980499 50515 caffe_interface.cpp:125] Batch 170, top-5 = 0.98
I0122 16:32:17.982054 50515 caffe_interface.cpp:125] Batch 171, loss = 0.489759
I0122 16:32:17.982062 50515 caffe_interface.cpp:125] Batch 171, top-1 = 0.84
I0122 16:32:17.982067 50515 caffe_interface.cpp:125] Batch 171, top-5 = 0.98
I0122 16:32:17.983284 50515 caffe_interface.cpp:125] Batch 172, loss = 1.12843
I0122 16:32:17.983291 50515 caffe_interface.cpp:125] Batch 172, top-1 = 0.74
I0122 16:32:17.983295 50515 caffe_interface.cpp:125] Batch 172, top-5 = 0.96
I0122 16:32:17.984496 50515 caffe_interface.cpp:125] Batch 173, loss = 0.374635
I0122 16:32:17.984503 50515 caffe_interface.cpp:125] Batch 173, top-1 = 0.82
I0122 16:32:17.984506 50515 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0122 16:32:17.985728 50515 caffe_interface.cpp:125] Batch 174, loss = 0.370115
I0122 16:32:17.985736 50515 caffe_interface.cpp:125] Batch 174, top-1 = 0.82
I0122 16:32:17.985739 50515 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:32:17.986953 50515 caffe_interface.cpp:125] Batch 175, loss = 0.776306
I0122 16:32:17.986960 50515 caffe_interface.cpp:125] Batch 175, top-1 = 0.72
I0122 16:32:17.986964 50515 caffe_interface.cpp:125] Batch 175, top-5 = 0.96
I0122 16:32:17.988181 50515 caffe_interface.cpp:125] Batch 176, loss = 0.906174
I0122 16:32:17.988188 50515 caffe_interface.cpp:125] Batch 176, top-1 = 0.78
I0122 16:32:17.988191 50515 caffe_interface.cpp:125] Batch 176, top-5 = 0.98
I0122 16:32:17.989403 50515 caffe_interface.cpp:125] Batch 177, loss = 0.405142
I0122 16:32:17.989409 50515 caffe_interface.cpp:125] Batch 177, top-1 = 0.84
I0122 16:32:17.989413 50515 caffe_interface.cpp:125] Batch 177, top-5 = 1
I0122 16:32:17.991248 50515 caffe_interface.cpp:125] Batch 178, loss = 0.705309
I0122 16:32:17.991255 50515 caffe_interface.cpp:125] Batch 178, top-1 = 0.76
I0122 16:32:17.991258 50515 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0122 16:32:17.992472 50515 caffe_interface.cpp:125] Batch 179, loss = 0.405109
I0122 16:32:17.992480 50515 caffe_interface.cpp:125] Batch 179, top-1 = 0.84
I0122 16:32:17.992485 50515 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:32:17.992486 50515 caffe_interface.cpp:130] Loss: 0.576634
I0122 16:32:17.992492 50515 caffe_interface.cpp:142] loss = 0.576634 (* 1 = 0.576634 loss)
I0122 16:32:17.992497 50515 caffe_interface.cpp:142] top-1 = 0.818
I0122 16:32:17.992513 50515 caffe_interface.cpp:142] top-5 = 0.989556
I0122 16:32:18.141600 50515 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/sparse.caffemodel
I0122 16:32:18.141628 50515 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.3:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.817999959    | -0.0446664691  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 56497          | -17.3887615%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 35541504       | -27.5457153%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config3.prototxt
## fine-tuning: third run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config3.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune3_miniVggNet.txt
I0122 16:32:18.377364 51935 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0122 16:32:18.558234 51935 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:32:18.558746 51935 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24879628288, dev_info[0]: total=25620447232 free=24879628288
I0122 16:32:18.558758 51935 caffe_interface.cpp:493] Using GPUs 0
I0122 16:32:18.559010 51935 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:32:19.157496 51935 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt"
type: "SGD"
I0122 16:32:19.157613 51935 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0122 16:32:19.157866 51935 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:32:19.157881 51935 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:32:19.157886 51935 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:32:19.158051 51935 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:32:19.158123 51935 layer_factory.hpp:77] Creating layer data
I0122 16:32:19.158213 51935 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:19.158684 51935 net.cpp:94] Creating Layer data
I0122 16:32:19.158692 51935 net.cpp:409] data -> data
I0122 16:32:19.158712 51935 net.cpp:409] data -> label
I0122 16:32:19.160166 51976 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:32:19.160212 51976 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:32:19.160372 51935 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:32:19.160465 51935 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:32:19.168311 51935 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:19.168357 51935 net.cpp:144] Setting up data
I0122 16:32:19.168365 51935 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:32:19.168367 51935 net.cpp:151] Top shape: 128 (128)
I0122 16:32:19.168370 51935 net.cpp:159] Memory required for data: 1573376
I0122 16:32:19.168375 51935 layer_factory.hpp:77] Creating layer conv1
I0122 16:32:19.168387 51935 net.cpp:94] Creating Layer conv1
I0122 16:32:19.168390 51935 net.cpp:435] conv1 <- data
I0122 16:32:19.168403 51935 net.cpp:409] conv1 -> conv1
I0122 16:32:19.169379 51935 net.cpp:144] Setting up conv1
I0122 16:32:19.169391 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.169394 51935 net.cpp:159] Memory required for data: 18350592
I0122 16:32:19.169409 51935 layer_factory.hpp:77] Creating layer bn1
I0122 16:32:19.169420 51935 net.cpp:94] Creating Layer bn1
I0122 16:32:19.169422 51935 net.cpp:435] bn1 <- conv1
I0122 16:32:19.169428 51935 net.cpp:409] bn1 -> scale1
I0122 16:32:19.170015 51935 net.cpp:144] Setting up bn1
I0122 16:32:19.170022 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.170025 51935 net.cpp:159] Memory required for data: 35127808
I0122 16:32:19.170037 51935 layer_factory.hpp:77] Creating layer relu1
I0122 16:32:19.170043 51935 net.cpp:94] Creating Layer relu1
I0122 16:32:19.170047 51935 net.cpp:435] relu1 <- scale1
I0122 16:32:19.170051 51935 net.cpp:409] relu1 -> relu1
I0122 16:32:19.170073 51935 net.cpp:144] Setting up relu1
I0122 16:32:19.170078 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.170081 51935 net.cpp:159] Memory required for data: 51905024
I0122 16:32:19.170084 51935 layer_factory.hpp:77] Creating layer conv2
I0122 16:32:19.170091 51935 net.cpp:94] Creating Layer conv2
I0122 16:32:19.170096 51935 net.cpp:435] conv2 <- relu1
I0122 16:32:19.170101 51935 net.cpp:409] conv2 -> conv2
I0122 16:32:19.171605 51935 net.cpp:144] Setting up conv2
I0122 16:32:19.171614 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.171617 51935 net.cpp:159] Memory required for data: 68682240
I0122 16:32:19.171625 51935 layer_factory.hpp:77] Creating layer bn2
I0122 16:32:19.171631 51935 net.cpp:94] Creating Layer bn2
I0122 16:32:19.171633 51935 net.cpp:435] bn2 <- conv2
I0122 16:32:19.171638 51935 net.cpp:409] bn2 -> scale2
I0122 16:32:19.172361 51935 net.cpp:144] Setting up bn2
I0122 16:32:19.172367 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.172370 51935 net.cpp:159] Memory required for data: 85459456
I0122 16:32:19.172379 51935 layer_factory.hpp:77] Creating layer relu2
I0122 16:32:19.172384 51935 net.cpp:94] Creating Layer relu2
I0122 16:32:19.172386 51935 net.cpp:435] relu2 <- scale2
I0122 16:32:19.172391 51935 net.cpp:409] relu2 -> relu2
I0122 16:32:19.172422 51935 net.cpp:144] Setting up relu2
I0122 16:32:19.172427 51935 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:32:19.172430 51935 net.cpp:159] Memory required for data: 102236672
I0122 16:32:19.172433 51935 layer_factory.hpp:77] Creating layer pool1
I0122 16:32:19.172439 51935 net.cpp:94] Creating Layer pool1
I0122 16:32:19.172442 51935 net.cpp:435] pool1 <- relu2
I0122 16:32:19.172446 51935 net.cpp:409] pool1 -> pool1
I0122 16:32:19.172484 51935 net.cpp:144] Setting up pool1
I0122 16:32:19.172490 51935 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:32:19.172493 51935 net.cpp:159] Memory required for data: 106430976
I0122 16:32:19.172495 51935 layer_factory.hpp:77] Creating layer drop1
I0122 16:32:19.172502 51935 net.cpp:94] Creating Layer drop1
I0122 16:32:19.172504 51935 net.cpp:435] drop1 <- pool1
I0122 16:32:19.172520 51935 net.cpp:409] drop1 -> drop1
I0122 16:32:19.172557 51935 net.cpp:144] Setting up drop1
I0122 16:32:19.172564 51935 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:32:19.172566 51935 net.cpp:159] Memory required for data: 110625280
I0122 16:32:19.172569 51935 layer_factory.hpp:77] Creating layer conv3
I0122 16:32:19.172576 51935 net.cpp:94] Creating Layer conv3
I0122 16:32:19.172580 51935 net.cpp:435] conv3 <- drop1
I0122 16:32:19.172586 51935 net.cpp:409] conv3 -> conv3
I0122 16:32:19.173669 51935 net.cpp:144] Setting up conv3
I0122 16:32:19.173681 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.173686 51935 net.cpp:159] Memory required for data: 119013888
I0122 16:32:19.173691 51935 layer_factory.hpp:77] Creating layer bn3
I0122 16:32:19.173701 51935 net.cpp:94] Creating Layer bn3
I0122 16:32:19.173703 51935 net.cpp:435] bn3 <- conv3
I0122 16:32:19.173708 51935 net.cpp:409] bn3 -> scale3
I0122 16:32:19.174345 51935 net.cpp:144] Setting up bn3
I0122 16:32:19.174353 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.174356 51935 net.cpp:159] Memory required for data: 127402496
I0122 16:32:19.174368 51935 layer_factory.hpp:77] Creating layer relu3
I0122 16:32:19.174374 51935 net.cpp:94] Creating Layer relu3
I0122 16:32:19.174377 51935 net.cpp:435] relu3 <- scale3
I0122 16:32:19.174381 51935 net.cpp:409] relu3 -> relu3
I0122 16:32:19.174399 51935 net.cpp:144] Setting up relu3
I0122 16:32:19.174405 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.174408 51935 net.cpp:159] Memory required for data: 135791104
I0122 16:32:19.174412 51935 layer_factory.hpp:77] Creating layer conv4
I0122 16:32:19.174418 51935 net.cpp:94] Creating Layer conv4
I0122 16:32:19.174424 51935 net.cpp:435] conv4 <- relu3
I0122 16:32:19.174429 51935 net.cpp:409] conv4 -> conv4
I0122 16:32:19.174847 51935 net.cpp:144] Setting up conv4
I0122 16:32:19.174854 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.174857 51935 net.cpp:159] Memory required for data: 144179712
I0122 16:32:19.174862 51935 layer_factory.hpp:77] Creating layer bn4
I0122 16:32:19.174868 51935 net.cpp:94] Creating Layer bn4
I0122 16:32:19.174872 51935 net.cpp:435] bn4 <- conv4
I0122 16:32:19.174878 51935 net.cpp:409] bn4 -> scale4
I0122 16:32:19.175472 51935 net.cpp:144] Setting up bn4
I0122 16:32:19.175480 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.175483 51935 net.cpp:159] Memory required for data: 152568320
I0122 16:32:19.175493 51935 layer_factory.hpp:77] Creating layer relu4
I0122 16:32:19.175496 51935 net.cpp:94] Creating Layer relu4
I0122 16:32:19.175499 51935 net.cpp:435] relu4 <- scale4
I0122 16:32:19.175503 51935 net.cpp:409] relu4 -> relu4
I0122 16:32:19.175520 51935 net.cpp:144] Setting up relu4
I0122 16:32:19.175526 51935 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:32:19.175529 51935 net.cpp:159] Memory required for data: 160956928
I0122 16:32:19.175531 51935 layer_factory.hpp:77] Creating layer pool2
I0122 16:32:19.175537 51935 net.cpp:94] Creating Layer pool2
I0122 16:32:19.175539 51935 net.cpp:435] pool2 <- relu4
I0122 16:32:19.175544 51935 net.cpp:409] pool2 -> pool2
I0122 16:32:19.175570 51935 net.cpp:144] Setting up pool2
I0122 16:32:19.175576 51935 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:32:19.175580 51935 net.cpp:159] Memory required for data: 163054080
I0122 16:32:19.175581 51935 layer_factory.hpp:77] Creating layer drop2
I0122 16:32:19.175587 51935 net.cpp:94] Creating Layer drop2
I0122 16:32:19.175592 51935 net.cpp:435] drop2 <- pool2
I0122 16:32:19.175597 51935 net.cpp:409] drop2 -> drop2
I0122 16:32:19.175621 51935 net.cpp:144] Setting up drop2
I0122 16:32:19.175626 51935 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:32:19.175629 51935 net.cpp:159] Memory required for data: 165151232
I0122 16:32:19.175632 51935 layer_factory.hpp:77] Creating layer fc1
I0122 16:32:19.175638 51935 net.cpp:94] Creating Layer fc1
I0122 16:32:19.175642 51935 net.cpp:435] fc1 <- drop2
I0122 16:32:19.175647 51935 net.cpp:409] fc1 -> fc1
I0122 16:32:19.189826 51935 net.cpp:144] Setting up fc1
I0122 16:32:19.189844 51935 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:32:19.189846 51935 net.cpp:159] Memory required for data: 165413376
I0122 16:32:19.189855 51935 layer_factory.hpp:77] Creating layer bn5
I0122 16:32:19.189864 51935 net.cpp:94] Creating Layer bn5
I0122 16:32:19.189867 51935 net.cpp:435] bn5 <- fc1
I0122 16:32:19.189874 51935 net.cpp:409] bn5 -> scale5
I0122 16:32:19.190416 51935 net.cpp:144] Setting up bn5
I0122 16:32:19.190423 51935 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:32:19.190426 51935 net.cpp:159] Memory required for data: 165675520
I0122 16:32:19.190439 51935 layer_factory.hpp:77] Creating layer relu5
I0122 16:32:19.190444 51935 net.cpp:94] Creating Layer relu5
I0122 16:32:19.190448 51935 net.cpp:435] relu5 <- scale5
I0122 16:32:19.190452 51935 net.cpp:409] relu5 -> relu5
I0122 16:32:19.190470 51935 net.cpp:144] Setting up relu5
I0122 16:32:19.190477 51935 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:32:19.190479 51935 net.cpp:159] Memory required for data: 165937664
I0122 16:32:19.190482 51935 layer_factory.hpp:77] Creating layer drop3
I0122 16:32:19.190487 51935 net.cpp:94] Creating Layer drop3
I0122 16:32:19.190490 51935 net.cpp:435] drop3 <- relu5
I0122 16:32:19.190495 51935 net.cpp:409] drop3 -> drop3
I0122 16:32:19.190521 51935 net.cpp:144] Setting up drop3
I0122 16:32:19.190528 51935 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:32:19.190531 51935 net.cpp:159] Memory required for data: 166199808
I0122 16:32:19.190534 51935 layer_factory.hpp:77] Creating layer fc2
I0122 16:32:19.190541 51935 net.cpp:94] Creating Layer fc2
I0122 16:32:19.190543 51935 net.cpp:435] fc2 <- drop3
I0122 16:32:19.190548 51935 net.cpp:409] fc2 -> fc2
I0122 16:32:19.190686 51935 net.cpp:144] Setting up fc2
I0122 16:32:19.190691 51935 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:32:19.190695 51935 net.cpp:159] Memory required for data: 166204928
I0122 16:32:19.190699 51935 layer_factory.hpp:77] Creating layer loss
I0122 16:32:19.190706 51935 net.cpp:94] Creating Layer loss
I0122 16:32:19.190708 51935 net.cpp:435] loss <- fc2
I0122 16:32:19.190712 51935 net.cpp:435] loss <- label
I0122 16:32:19.190717 51935 net.cpp:409] loss -> loss
I0122 16:32:19.190726 51935 layer_factory.hpp:77] Creating layer loss
I0122 16:32:19.191485 51935 net.cpp:144] Setting up loss
I0122 16:32:19.191496 51935 net.cpp:151] Top shape: (1)
I0122 16:32:19.191499 51935 net.cpp:154]     with loss weight 1
I0122 16:32:19.191512 51935 net.cpp:159] Memory required for data: 166204932
I0122 16:32:19.191516 51935 net.cpp:220] loss needs backward computation.
I0122 16:32:19.191527 51935 net.cpp:220] fc2 needs backward computation.
I0122 16:32:19.191531 51935 net.cpp:220] drop3 needs backward computation.
I0122 16:32:19.191534 51935 net.cpp:220] relu5 needs backward computation.
I0122 16:32:19.191537 51935 net.cpp:220] bn5 needs backward computation.
I0122 16:32:19.191541 51935 net.cpp:220] fc1 needs backward computation.
I0122 16:32:19.191545 51935 net.cpp:220] drop2 needs backward computation.
I0122 16:32:19.191548 51935 net.cpp:220] pool2 needs backward computation.
I0122 16:32:19.191551 51935 net.cpp:220] relu4 needs backward computation.
I0122 16:32:19.191555 51935 net.cpp:220] bn4 needs backward computation.
I0122 16:32:19.191560 51935 net.cpp:220] conv4 needs backward computation.
I0122 16:32:19.191562 51935 net.cpp:220] relu3 needs backward computation.
I0122 16:32:19.191565 51935 net.cpp:220] bn3 needs backward computation.
I0122 16:32:19.191568 51935 net.cpp:220] conv3 needs backward computation.
I0122 16:32:19.191572 51935 net.cpp:220] drop1 needs backward computation.
I0122 16:32:19.191576 51935 net.cpp:220] pool1 needs backward computation.
I0122 16:32:19.191578 51935 net.cpp:220] relu2 needs backward computation.
I0122 16:32:19.191581 51935 net.cpp:220] bn2 needs backward computation.
I0122 16:32:19.191584 51935 net.cpp:220] conv2 needs backward computation.
I0122 16:32:19.191587 51935 net.cpp:220] relu1 needs backward computation.
I0122 16:32:19.191602 51935 net.cpp:220] bn1 needs backward computation.
I0122 16:32:19.191606 51935 net.cpp:220] conv1 needs backward computation.
I0122 16:32:19.191610 51935 net.cpp:222] data does not need backward computation.
I0122 16:32:19.191613 51935 net.cpp:264] This network produces output loss
I0122 16:32:19.191632 51935 net.cpp:284] Network initialization done.
I0122 16:32:19.191947 51935 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/net_finetune.prototxt
I0122 16:32:19.191982 51935 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:32:19.192176 51935 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:32:19.192275 51935 layer_factory.hpp:77] Creating layer data
I0122 16:32:19.192315 51935 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:19.193776 51935 net.cpp:94] Creating Layer data
I0122 16:32:19.193807 51935 net.cpp:409] data -> data
I0122 16:32:19.193830 51935 net.cpp:409] data -> label
I0122 16:32:19.194557 52006 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:32:19.194586 52006 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:32:19.194712 51935 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:32:19.194944 51935 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:32:19.200752 51935 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:32:19.200839 51935 net.cpp:144] Setting up data
I0122 16:32:19.200850 51935 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:32:19.200860 51935 net.cpp:151] Top shape: 50 (50)
I0122 16:32:19.200866 51935 net.cpp:159] Memory required for data: 614600
I0122 16:32:19.200875 51935 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:32:19.200896 51935 net.cpp:94] Creating Layer label_data_1_split
I0122 16:32:19.200903 51935 net.cpp:435] label_data_1_split <- label
I0122 16:32:19.200913 51935 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:32:19.200932 51935 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:32:19.200944 51935 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:32:19.201083 51935 net.cpp:144] Setting up label_data_1_split
I0122 16:32:19.201093 51935 net.cpp:151] Top shape: 50 (50)
I0122 16:32:19.201100 51935 net.cpp:151] Top shape: 50 (50)
I0122 16:32:19.201107 51935 net.cpp:151] Top shape: 50 (50)
I0122 16:32:19.201112 51935 net.cpp:159] Memory required for data: 615200
I0122 16:32:19.201117 51935 layer_factory.hpp:77] Creating layer conv1
I0122 16:32:19.201134 51935 net.cpp:94] Creating Layer conv1
I0122 16:32:19.201143 51935 net.cpp:435] conv1 <- data
I0122 16:32:19.201151 51935 net.cpp:409] conv1 -> conv1
I0122 16:32:19.201638 51935 net.cpp:144] Setting up conv1
I0122 16:32:19.201651 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.201656 51935 net.cpp:159] Memory required for data: 7168800
I0122 16:32:19.201673 51935 layer_factory.hpp:77] Creating layer bn1
I0122 16:32:19.201686 51935 net.cpp:94] Creating Layer bn1
I0122 16:32:19.201694 51935 net.cpp:435] bn1 <- conv1
I0122 16:32:19.201704 51935 net.cpp:409] bn1 -> scale1
I0122 16:32:19.203375 51935 net.cpp:144] Setting up bn1
I0122 16:32:19.203388 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.203395 51935 net.cpp:159] Memory required for data: 13722400
I0122 16:32:19.203416 51935 layer_factory.hpp:77] Creating layer relu1
I0122 16:32:19.203428 51935 net.cpp:94] Creating Layer relu1
I0122 16:32:19.203438 51935 net.cpp:435] relu1 <- scale1
I0122 16:32:19.203447 51935 net.cpp:409] relu1 -> relu1
I0122 16:32:19.203529 51935 net.cpp:144] Setting up relu1
I0122 16:32:19.203547 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.203552 51935 net.cpp:159] Memory required for data: 20276000
I0122 16:32:19.203558 51935 layer_factory.hpp:77] Creating layer conv2
I0122 16:32:19.203572 51935 net.cpp:94] Creating Layer conv2
I0122 16:32:19.203579 51935 net.cpp:435] conv2 <- relu1
I0122 16:32:19.203588 51935 net.cpp:409] conv2 -> conv2
I0122 16:32:19.204299 51935 net.cpp:144] Setting up conv2
I0122 16:32:19.204314 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.204320 51935 net.cpp:159] Memory required for data: 26829600
I0122 16:32:19.204339 51935 layer_factory.hpp:77] Creating layer bn2
I0122 16:32:19.204355 51935 net.cpp:94] Creating Layer bn2
I0122 16:32:19.204365 51935 net.cpp:435] bn2 <- conv2
I0122 16:32:19.204377 51935 net.cpp:409] bn2 -> scale2
I0122 16:32:19.205835 51935 net.cpp:144] Setting up bn2
I0122 16:32:19.205849 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.205854 51935 net.cpp:159] Memory required for data: 33383200
I0122 16:32:19.205870 51935 layer_factory.hpp:77] Creating layer relu2
I0122 16:32:19.205881 51935 net.cpp:94] Creating Layer relu2
I0122 16:32:19.205888 51935 net.cpp:435] relu2 <- scale2
I0122 16:32:19.205899 51935 net.cpp:409] relu2 -> relu2
I0122 16:32:19.205955 51935 net.cpp:144] Setting up relu2
I0122 16:32:19.205971 51935 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:32:19.205979 51935 net.cpp:159] Memory required for data: 39936800
I0122 16:32:19.205986 51935 layer_factory.hpp:77] Creating layer pool1
I0122 16:32:19.206001 51935 net.cpp:94] Creating Layer pool1
I0122 16:32:19.206013 51935 net.cpp:435] pool1 <- relu2
I0122 16:32:19.206037 51935 net.cpp:409] pool1 -> pool1
I0122 16:32:19.206110 51935 net.cpp:144] Setting up pool1
I0122 16:32:19.206146 51935 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:32:19.206154 51935 net.cpp:159] Memory required for data: 41575200
I0122 16:32:19.206161 51935 layer_factory.hpp:77] Creating layer drop1
I0122 16:32:19.206171 51935 net.cpp:94] Creating Layer drop1
I0122 16:32:19.206176 51935 net.cpp:435] drop1 <- pool1
I0122 16:32:19.206184 51935 net.cpp:409] drop1 -> drop1
I0122 16:32:19.206243 51935 net.cpp:144] Setting up drop1
I0122 16:32:19.206254 51935 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:32:19.206264 51935 net.cpp:159] Memory required for data: 43213600
I0122 16:32:19.206271 51935 layer_factory.hpp:77] Creating layer conv3
I0122 16:32:19.206305 51935 net.cpp:94] Creating Layer conv3
I0122 16:32:19.206315 51935 net.cpp:435] conv3 <- drop1
I0122 16:32:19.206329 51935 net.cpp:409] conv3 -> conv3
I0122 16:32:19.207082 51935 net.cpp:144] Setting up conv3
I0122 16:32:19.207093 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.207095 51935 net.cpp:159] Memory required for data: 46490400
I0122 16:32:19.207103 51935 layer_factory.hpp:77] Creating layer bn3
I0122 16:32:19.207113 51935 net.cpp:94] Creating Layer bn3
I0122 16:32:19.207121 51935 net.cpp:435] bn3 <- conv3
I0122 16:32:19.207132 51935 net.cpp:409] bn3 -> scale3
I0122 16:32:19.208117 51935 net.cpp:144] Setting up bn3
I0122 16:32:19.208125 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.208129 51935 net.cpp:159] Memory required for data: 49767200
I0122 16:32:19.208153 51935 layer_factory.hpp:77] Creating layer relu3
I0122 16:32:19.208160 51935 net.cpp:94] Creating Layer relu3
I0122 16:32:19.208164 51935 net.cpp:435] relu3 <- scale3
I0122 16:32:19.208171 51935 net.cpp:409] relu3 -> relu3
I0122 16:32:19.208196 51935 net.cpp:144] Setting up relu3
I0122 16:32:19.208204 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.208206 51935 net.cpp:159] Memory required for data: 53044000
I0122 16:32:19.208210 51935 layer_factory.hpp:77] Creating layer conv4
I0122 16:32:19.208220 51935 net.cpp:94] Creating Layer conv4
I0122 16:32:19.208225 51935 net.cpp:435] conv4 <- relu3
I0122 16:32:19.208231 51935 net.cpp:409] conv4 -> conv4
I0122 16:32:19.208973 51935 net.cpp:144] Setting up conv4
I0122 16:32:19.208988 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.208992 51935 net.cpp:159] Memory required for data: 56320800
I0122 16:32:19.209003 51935 layer_factory.hpp:77] Creating layer bn4
I0122 16:32:19.209013 51935 net.cpp:94] Creating Layer bn4
I0122 16:32:19.209019 51935 net.cpp:435] bn4 <- conv4
I0122 16:32:19.209029 51935 net.cpp:409] bn4 -> scale4
I0122 16:32:19.210013 51935 net.cpp:144] Setting up bn4
I0122 16:32:19.210023 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.210027 51935 net.cpp:159] Memory required for data: 59597600
I0122 16:32:19.210038 51935 layer_factory.hpp:77] Creating layer relu4
I0122 16:32:19.210047 51935 net.cpp:94] Creating Layer relu4
I0122 16:32:19.210052 51935 net.cpp:435] relu4 <- scale4
I0122 16:32:19.210057 51935 net.cpp:409] relu4 -> relu4
I0122 16:32:19.210081 51935 net.cpp:144] Setting up relu4
I0122 16:32:19.210088 51935 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:32:19.210093 51935 net.cpp:159] Memory required for data: 62874400
I0122 16:32:19.210096 51935 layer_factory.hpp:77] Creating layer pool2
I0122 16:32:19.210103 51935 net.cpp:94] Creating Layer pool2
I0122 16:32:19.210108 51935 net.cpp:435] pool2 <- relu4
I0122 16:32:19.210114 51935 net.cpp:409] pool2 -> pool2
I0122 16:32:19.210193 51935 net.cpp:144] Setting up pool2
I0122 16:32:19.210201 51935 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:32:19.210203 51935 net.cpp:159] Memory required for data: 63693600
I0122 16:32:19.210207 51935 layer_factory.hpp:77] Creating layer drop2
I0122 16:32:19.210213 51935 net.cpp:94] Creating Layer drop2
I0122 16:32:19.210217 51935 net.cpp:435] drop2 <- pool2
I0122 16:32:19.210223 51935 net.cpp:409] drop2 -> drop2
I0122 16:32:19.210279 51935 net.cpp:144] Setting up drop2
I0122 16:32:19.210289 51935 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:32:19.210316 51935 net.cpp:159] Memory required for data: 64512800
I0122 16:32:19.210320 51935 layer_factory.hpp:77] Creating layer fc1
I0122 16:32:19.210332 51935 net.cpp:94] Creating Layer fc1
I0122 16:32:19.210336 51935 net.cpp:435] fc1 <- drop2
I0122 16:32:19.210343 51935 net.cpp:409] fc1 -> fc1
I0122 16:32:19.226389 51935 net.cpp:144] Setting up fc1
I0122 16:32:19.226410 51935 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:19.226413 51935 net.cpp:159] Memory required for data: 64615200
I0122 16:32:19.226420 51935 layer_factory.hpp:77] Creating layer bn5
I0122 16:32:19.226429 51935 net.cpp:94] Creating Layer bn5
I0122 16:32:19.226433 51935 net.cpp:435] bn5 <- fc1
I0122 16:32:19.226439 51935 net.cpp:409] bn5 -> scale5
I0122 16:32:19.227021 51935 net.cpp:144] Setting up bn5
I0122 16:32:19.227030 51935 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:19.227032 51935 net.cpp:159] Memory required for data: 64717600
I0122 16:32:19.227044 51935 layer_factory.hpp:77] Creating layer relu5
I0122 16:32:19.227051 51935 net.cpp:94] Creating Layer relu5
I0122 16:32:19.227054 51935 net.cpp:435] relu5 <- scale5
I0122 16:32:19.227061 51935 net.cpp:409] relu5 -> relu5
I0122 16:32:19.227079 51935 net.cpp:144] Setting up relu5
I0122 16:32:19.227084 51935 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:19.227087 51935 net.cpp:159] Memory required for data: 64820000
I0122 16:32:19.227090 51935 layer_factory.hpp:77] Creating layer drop3
I0122 16:32:19.227095 51935 net.cpp:94] Creating Layer drop3
I0122 16:32:19.227098 51935 net.cpp:435] drop3 <- relu5
I0122 16:32:19.227102 51935 net.cpp:409] drop3 -> drop3
I0122 16:32:19.227130 51935 net.cpp:144] Setting up drop3
I0122 16:32:19.227134 51935 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:32:19.227138 51935 net.cpp:159] Memory required for data: 64922400
I0122 16:32:19.227139 51935 layer_factory.hpp:77] Creating layer fc2
I0122 16:32:19.227146 51935 net.cpp:94] Creating Layer fc2
I0122 16:32:19.227149 51935 net.cpp:435] fc2 <- drop3
I0122 16:32:19.227154 51935 net.cpp:409] fc2 -> fc2
I0122 16:32:19.227293 51935 net.cpp:144] Setting up fc2
I0122 16:32:19.227299 51935 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:19.227301 51935 net.cpp:159] Memory required for data: 64924400
I0122 16:32:19.227306 51935 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:32:19.227311 51935 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:32:19.227314 51935 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:32:19.227319 51935 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:32:19.227325 51935 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:32:19.227330 51935 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:32:19.227370 51935 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:32:19.227375 51935 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:19.227378 51935 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:19.227381 51935 net.cpp:151] Top shape: 50 10 (500)
I0122 16:32:19.227385 51935 net.cpp:159] Memory required for data: 64930400
I0122 16:32:19.227387 51935 layer_factory.hpp:77] Creating layer loss
I0122 16:32:19.227391 51935 net.cpp:94] Creating Layer loss
I0122 16:32:19.227394 51935 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:32:19.227399 51935 net.cpp:435] loss <- label_data_1_split_0
I0122 16:32:19.227403 51935 net.cpp:409] loss -> loss
I0122 16:32:19.227411 51935 layer_factory.hpp:77] Creating layer loss
I0122 16:32:19.227481 51935 net.cpp:144] Setting up loss
I0122 16:32:19.227488 51935 net.cpp:151] Top shape: (1)
I0122 16:32:19.227489 51935 net.cpp:154]     with loss weight 1
I0122 16:32:19.227501 51935 net.cpp:159] Memory required for data: 64930404
I0122 16:32:19.227504 51935 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:32:19.227510 51935 net.cpp:94] Creating Layer accuracy-top1
I0122 16:32:19.227514 51935 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:32:19.227516 51935 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:32:19.227521 51935 net.cpp:409] accuracy-top1 -> top-1
I0122 16:32:19.227542 51935 net.cpp:144] Setting up accuracy-top1
I0122 16:32:19.227546 51935 net.cpp:151] Top shape: (1)
I0122 16:32:19.227548 51935 net.cpp:159] Memory required for data: 64930408
I0122 16:32:19.227551 51935 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:32:19.227556 51935 net.cpp:94] Creating Layer accuracy-top5
I0122 16:32:19.227560 51935 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:32:19.227563 51935 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:32:19.227567 51935 net.cpp:409] accuracy-top5 -> top-5
I0122 16:32:19.227574 51935 net.cpp:144] Setting up accuracy-top5
I0122 16:32:19.227577 51935 net.cpp:151] Top shape: (1)
I0122 16:32:19.227579 51935 net.cpp:159] Memory required for data: 64930412
I0122 16:32:19.227583 51935 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:32:19.227587 51935 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:32:19.227591 51935 net.cpp:220] loss needs backward computation.
I0122 16:32:19.227594 51935 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:32:19.227597 51935 net.cpp:220] fc2 needs backward computation.
I0122 16:32:19.227602 51935 net.cpp:220] drop3 needs backward computation.
I0122 16:32:19.227604 51935 net.cpp:220] relu5 needs backward computation.
I0122 16:32:19.227607 51935 net.cpp:220] bn5 needs backward computation.
I0122 16:32:19.227610 51935 net.cpp:220] fc1 needs backward computation.
I0122 16:32:19.227614 51935 net.cpp:220] drop2 needs backward computation.
I0122 16:32:19.227617 51935 net.cpp:220] pool2 needs backward computation.
I0122 16:32:19.227620 51935 net.cpp:220] relu4 needs backward computation.
I0122 16:32:19.227623 51935 net.cpp:220] bn4 needs backward computation.
I0122 16:32:19.227627 51935 net.cpp:220] conv4 needs backward computation.
I0122 16:32:19.227630 51935 net.cpp:220] relu3 needs backward computation.
I0122 16:32:19.227633 51935 net.cpp:220] bn3 needs backward computation.
I0122 16:32:19.227636 51935 net.cpp:220] conv3 needs backward computation.
I0122 16:32:19.227640 51935 net.cpp:220] drop1 needs backward computation.
I0122 16:32:19.227643 51935 net.cpp:220] pool1 needs backward computation.
I0122 16:32:19.227646 51935 net.cpp:220] relu2 needs backward computation.
I0122 16:32:19.227649 51935 net.cpp:220] bn2 needs backward computation.
I0122 16:32:19.227653 51935 net.cpp:220] conv2 needs backward computation.
I0122 16:32:19.227655 51935 net.cpp:220] relu1 needs backward computation.
I0122 16:32:19.227658 51935 net.cpp:220] bn1 needs backward computation.
I0122 16:32:19.227661 51935 net.cpp:220] conv1 needs backward computation.
I0122 16:32:19.227666 51935 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:32:19.227670 51935 net.cpp:222] data does not need backward computation.
I0122 16:32:19.227672 51935 net.cpp:264] This network produces output loss
I0122 16:32:19.227675 51935 net.cpp:264] This network produces output top-1
I0122 16:32:19.227680 51935 net.cpp:264] This network produces output top-5
I0122 16:32:19.227701 51935 net.cpp:284] Network initialization done.
I0122 16:32:19.227808 51935 solver.cpp:63] Solver scaffolding done.
I0122 16:32:19.228956 51935 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/sparse.caffemodel
I0122 16:32:19.286207 51935 caffe_interface.cpp:527] Starting Optimization
I0122 16:32:19.286226 51935 solver.cpp:335] Solving 
I0122 16:32:19.286229 51935 solver.cpp:336] Learning Rate Policy: poly
I0122 16:32:19.287451 51935 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:32:19.516693 51935 solver.cpp:517]     Test net output #0: loss = 0.576634 (* 1 = 0.576634 loss)
I0122 16:32:19.516713 51935 solver.cpp:517]     Test net output #1: top-1 = 0.818
I0122 16:32:19.516718 51935 solver.cpp:517]     Test net output #2: top-5 = 0.989556
I0122 16:32:19.534011 51935 solver.cpp:266] Iteration 0 (0 iter/s, 0.247741s/100 iter), loss = 0.0839301
I0122 16:32:19.534045 51935 solver.cpp:285]     Train net output #0: loss = 0.0839301 (* 1 = 0.0839301 loss)
I0122 16:32:19.534070 51935 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:32:20.398279 51935 solver.cpp:266] Iteration 100 (115.715 iter/s, 0.864193s/100 iter), loss = 0.128191
I0122 16:32:20.398306 51935 solver.cpp:285]     Train net output #0: loss = 0.128191 (* 1 = 0.128191 loss)
I0122 16:32:20.398313 51935 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:32:21.268836 51935 solver.cpp:266] Iteration 200 (114.878 iter/s, 0.87049s/100 iter), loss = 0.180767
I0122 16:32:21.268865 51935 solver.cpp:285]     Train net output #0: loss = 0.180767 (* 1 = 0.180767 loss)
I0122 16:32:21.268872 51935 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:32:22.139436 51935 solver.cpp:266] Iteration 300 (114.873 iter/s, 0.870529s/100 iter), loss = 0.135559
I0122 16:32:22.139464 51935 solver.cpp:285]     Train net output #0: loss = 0.135559 (* 1 = 0.135559 loss)
I0122 16:32:22.139470 51935 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:32:23.006115 51935 solver.cpp:266] Iteration 400 (115.392 iter/s, 0.86661s/100 iter), loss = 0.0825847
I0122 16:32:23.006146 51935 solver.cpp:285]     Train net output #0: loss = 0.0825847 (* 1 = 0.0825847 loss)
I0122 16:32:23.006152 51935 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:32:23.869240 51935 solver.cpp:266] Iteration 500 (115.868 iter/s, 0.863054s/100 iter), loss = 0.141202
I0122 16:32:23.869267 51935 solver.cpp:285]     Train net output #0: loss = 0.141202 (* 1 = 0.141202 loss)
I0122 16:32:23.869273 51935 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:32:24.732426 51935 solver.cpp:266] Iteration 600 (115.859 iter/s, 0.863117s/100 iter), loss = 0.135666
I0122 16:32:24.732455 51935 solver.cpp:285]     Train net output #0: loss = 0.135666 (* 1 = 0.135666 loss)
I0122 16:32:24.732460 51935 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:32:25.601415 51935 solver.cpp:266] Iteration 700 (115.086 iter/s, 0.868918s/100 iter), loss = 0.189001
I0122 16:32:25.601444 51935 solver.cpp:285]     Train net output #0: loss = 0.189001 (* 1 = 0.189001 loss)
I0122 16:32:25.601449 51935 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:32:26.466096 51935 solver.cpp:266] Iteration 800 (115.659 iter/s, 0.864612s/100 iter), loss = 0.189535
I0122 16:32:26.466125 51935 solver.cpp:285]     Train net output #0: loss = 0.189535 (* 1 = 0.189535 loss)
I0122 16:32:26.466131 51935 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:32:27.338574 51935 solver.cpp:266] Iteration 900 (114.625 iter/s, 0.872407s/100 iter), loss = 0.163068
I0122 16:32:27.338603 51935 solver.cpp:285]     Train net output #0: loss = 0.163068 (* 1 = 0.163068 loss)
I0122 16:32:27.338609 51935 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:32:28.203181 51935 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:32:28.427692 51935 solver.cpp:517]     Test net output #0: loss = 1.05925 (* 1 = 1.05925 loss)
I0122 16:32:28.427712 51935 solver.cpp:517]     Test net output #1: top-1 = 0.771334
I0122 16:32:28.427717 51935 solver.cpp:517]     Test net output #2: top-5 = 0.979223
I0122 16:32:28.435829 51935 solver.cpp:266] Iteration 1000 (91.1428 iter/s, 1.09718s/100 iter), loss = 0.142086
I0122 16:32:28.435847 51935 solver.cpp:285]     Train net output #0: loss = 0.142086 (* 1 = 0.142086 loss)
I0122 16:32:28.435855 51935 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:32:29.303652 51935 solver.cpp:266] Iteration 1100 (115.239 iter/s, 0.867761s/100 iter), loss = 0.248138
I0122 16:32:29.303679 51935 solver.cpp:285]     Train net output #0: loss = 0.248138 (* 1 = 0.248138 loss)
I0122 16:32:29.303685 51935 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:32:30.168123 51935 solver.cpp:266] Iteration 1200 (115.687 iter/s, 0.864401s/100 iter), loss = 0.166965
I0122 16:32:30.168153 51935 solver.cpp:285]     Train net output #0: loss = 0.166965 (* 1 = 0.166965 loss)
I0122 16:32:30.168157 51935 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:32:31.033069 51935 solver.cpp:266] Iteration 1300 (115.624 iter/s, 0.864876s/100 iter), loss = 0.175899
I0122 16:32:31.033126 51935 solver.cpp:285]     Train net output #0: loss = 0.175899 (* 1 = 0.175899 loss)
I0122 16:32:31.033134 51935 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:32:31.898948 51935 solver.cpp:266] Iteration 1400 (115.503 iter/s, 0.865781s/100 iter), loss = 0.16642
I0122 16:32:31.898977 51935 solver.cpp:285]     Train net output #0: loss = 0.16642 (* 1 = 0.16642 loss)
I0122 16:32:31.898983 51935 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:32:32.768576 51935 solver.cpp:266] Iteration 1500 (115.001 iter/s, 0.869558s/100 iter), loss = 0.194137
I0122 16:32:32.768605 51935 solver.cpp:285]     Train net output #0: loss = 0.194137 (* 1 = 0.194137 loss)
I0122 16:32:32.768610 51935 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:32:33.633354 51935 solver.cpp:266] Iteration 1600 (115.646 iter/s, 0.864708s/100 iter), loss = 0.277798
I0122 16:32:33.633384 51935 solver.cpp:285]     Train net output #0: loss = 0.277798 (* 1 = 0.277798 loss)
I0122 16:32:33.633389 51935 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:32:34.500228 51935 solver.cpp:266] Iteration 1700 (115.367 iter/s, 0.866801s/100 iter), loss = 0.111948
I0122 16:32:34.500257 51935 solver.cpp:285]     Train net output #0: loss = 0.111948 (* 1 = 0.111948 loss)
I0122 16:32:34.500263 51935 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:32:35.364545 51935 solver.cpp:266] Iteration 1800 (115.708 iter/s, 0.864246s/100 iter), loss = 0.183004
I0122 16:32:35.364573 51935 solver.cpp:285]     Train net output #0: loss = 0.183004 (* 1 = 0.183004 loss)
I0122 16:32:35.364579 51935 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:32:36.228351 51935 solver.cpp:266] Iteration 1900 (115.776 iter/s, 0.863737s/100 iter), loss = 0.167284
I0122 16:32:36.228379 51935 solver.cpp:285]     Train net output #0: loss = 0.167284 (* 1 = 0.167284 loss)
I0122 16:32:36.228385 51935 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:32:37.092717 51935 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:32:37.317687 51935 solver.cpp:517]     Test net output #0: loss = 0.801479 (* 1 = 0.801479 loss)
I0122 16:32:37.317704 51935 solver.cpp:517]     Test net output #1: top-1 = 0.799889
I0122 16:32:37.317709 51935 solver.cpp:517]     Test net output #2: top-5 = 0.979778
I0122 16:32:37.325803 51935 solver.cpp:266] Iteration 2000 (91.1264 iter/s, 1.09738s/100 iter), loss = 0.194772
I0122 16:32:37.325822 51935 solver.cpp:285]     Train net output #0: loss = 0.194772 (* 1 = 0.194772 loss)
I0122 16:32:37.325829 51935 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:32:38.203650 51935 solver.cpp:266] Iteration 2100 (113.923 iter/s, 0.877784s/100 iter), loss = 0.184637
I0122 16:32:38.203680 51935 solver.cpp:285]     Train net output #0: loss = 0.184637 (* 1 = 0.184637 loss)
I0122 16:32:38.203686 51935 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:32:39.068222 51935 solver.cpp:266] Iteration 2200 (115.674 iter/s, 0.864501s/100 iter), loss = 0.130307
I0122 16:32:39.068249 51935 solver.cpp:285]     Train net output #0: loss = 0.130307 (* 1 = 0.130307 loss)
I0122 16:32:39.068254 51935 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:32:39.932715 51935 solver.cpp:266] Iteration 2300 (115.684 iter/s, 0.864424s/100 iter), loss = 0.173402
I0122 16:32:39.932744 51935 solver.cpp:285]     Train net output #0: loss = 0.173402 (* 1 = 0.173402 loss)
I0122 16:32:39.932749 51935 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:32:40.806212 51935 solver.cpp:266] Iteration 2400 (114.492 iter/s, 0.873427s/100 iter), loss = 0.182968
I0122 16:32:40.806241 51935 solver.cpp:285]     Train net output #0: loss = 0.182968 (* 1 = 0.182968 loss)
I0122 16:32:40.806247 51935 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:32:41.670156 51935 solver.cpp:266] Iteration 2500 (115.758 iter/s, 0.863872s/100 iter), loss = 0.175845
I0122 16:32:41.670188 51935 solver.cpp:285]     Train net output #0: loss = 0.175845 (* 1 = 0.175845 loss)
I0122 16:32:41.670195 51935 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:32:42.550807 51935 solver.cpp:266] Iteration 2600 (113.562 iter/s, 0.880577s/100 iter), loss = 0.133853
I0122 16:32:42.550856 51935 solver.cpp:285]     Train net output #0: loss = 0.133853 (* 1 = 0.133853 loss)
I0122 16:32:42.550863 51935 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:32:43.491880 51935 solver.cpp:266] Iteration 2700 (106.272 iter/s, 0.940979s/100 iter), loss = 0.227497
I0122 16:32:43.491912 51935 solver.cpp:285]     Train net output #0: loss = 0.227497 (* 1 = 0.227497 loss)
I0122 16:32:43.491957 51935 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:32:44.526571 51935 solver.cpp:266] Iteration 2800 (96.6589 iter/s, 1.03457s/100 iter), loss = 0.108315
I0122 16:32:44.526602 51935 solver.cpp:285]     Train net output #0: loss = 0.108315 (* 1 = 0.108315 loss)
I0122 16:32:44.526648 51935 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:32:45.415417 51935 solver.cpp:266] Iteration 2900 (112.52 iter/s, 0.888729s/100 iter), loss = 0.189266
I0122 16:32:45.415447 51935 solver.cpp:285]     Train net output #0: loss = 0.189266 (* 1 = 0.189266 loss)
I0122 16:32:45.415493 51935 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:32:46.378444 51935 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:32:46.602514 51935 solver.cpp:517]     Test net output #0: loss = 0.63207 (* 1 = 0.63207 loss)
I0122 16:32:46.602531 51935 solver.cpp:517]     Test net output #1: top-1 = 0.823667
I0122 16:32:46.602536 51935 solver.cpp:517]     Test net output #2: top-5 = 0.986778
I0122 16:32:46.610671 51935 solver.cpp:266] Iteration 3000 (83.6731 iter/s, 1.19513s/100 iter), loss = 0.14978
I0122 16:32:46.610689 51935 solver.cpp:285]     Train net output #0: loss = 0.14978 (* 1 = 0.14978 loss)
I0122 16:32:46.610697 51935 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:32:47.473914 51935 solver.cpp:266] Iteration 3100 (115.85 iter/s, 0.863183s/100 iter), loss = 0.115474
I0122 16:32:47.473944 51935 solver.cpp:285]     Train net output #0: loss = 0.115474 (* 1 = 0.115474 loss)
I0122 16:32:47.473949 51935 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:32:48.438120 51935 solver.cpp:266] Iteration 3200 (103.72 iter/s, 0.964131s/100 iter), loss = 0.158955
I0122 16:32:48.438299 51935 solver.cpp:285]     Train net output #0: loss = 0.158955 (* 1 = 0.158955 loss)
I0122 16:32:48.438308 51935 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:32:49.301162 51935 solver.cpp:266] Iteration 3300 (115.898 iter/s, 0.862824s/100 iter), loss = 0.110817
I0122 16:32:49.301192 51935 solver.cpp:285]     Train net output #0: loss = 0.110817 (* 1 = 0.110817 loss)
I0122 16:32:49.301198 51935 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:32:50.164628 51935 solver.cpp:266] Iteration 3400 (115.822 iter/s, 0.863396s/100 iter), loss = 0.201928
I0122 16:32:50.164654 51935 solver.cpp:285]     Train net output #0: loss = 0.201928 (* 1 = 0.201928 loss)
I0122 16:32:50.164659 51935 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:32:51.028600 51935 solver.cpp:266] Iteration 3500 (115.754 iter/s, 0.863905s/100 iter), loss = 0.200832
I0122 16:32:51.028627 51935 solver.cpp:285]     Train net output #0: loss = 0.200832 (* 1 = 0.200832 loss)
I0122 16:32:51.028632 51935 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:32:52.023510 51935 solver.cpp:266] Iteration 3600 (100.519 iter/s, 0.994836s/100 iter), loss = 0.261239
I0122 16:32:52.023540 51935 solver.cpp:285]     Train net output #0: loss = 0.261239 (* 1 = 0.261239 loss)
I0122 16:32:52.023546 51935 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:32:52.887042 51935 solver.cpp:266] Iteration 3700 (115.813 iter/s, 0.863458s/100 iter), loss = 0.256899
I0122 16:32:52.887070 51935 solver.cpp:285]     Train net output #0: loss = 0.256899 (* 1 = 0.256899 loss)
I0122 16:32:52.887092 51935 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:32:53.869899 51935 solver.cpp:266] Iteration 3800 (101.752 iter/s, 0.982781s/100 iter), loss = 0.136884
I0122 16:32:53.869943 51935 solver.cpp:285]     Train net output #0: loss = 0.136884 (* 1 = 0.136884 loss)
I0122 16:32:53.869987 51935 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:32:54.795969 51935 solver.cpp:266] Iteration 3900 (107.999 iter/s, 0.925939s/100 iter), loss = 0.135709
I0122 16:32:54.796000 51935 solver.cpp:285]     Train net output #0: loss = 0.135709 (* 1 = 0.135709 loss)
I0122 16:32:54.796046 51935 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:32:55.711323 51935 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:32:55.934209 51935 solver.cpp:517]     Test net output #0: loss = 0.655176 (* 1 = 0.655176 loss)
I0122 16:32:55.934227 51935 solver.cpp:517]     Test net output #1: top-1 = 0.815111
I0122 16:32:55.934231 51935 solver.cpp:517]     Test net output #2: top-5 = 0.988222
I0122 16:32:55.942288 51935 solver.cpp:266] Iteration 4000 (87.2452 iter/s, 1.1462s/100 iter), loss = 0.156594
I0122 16:32:55.942306 51935 solver.cpp:285]     Train net output #0: loss = 0.156594 (* 1 = 0.156594 loss)
I0122 16:32:55.942312 51935 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:32:56.805799 51935 solver.cpp:266] Iteration 4100 (115.814 iter/s, 0.863451s/100 iter), loss = 0.14671
I0122 16:32:56.805830 51935 solver.cpp:285]     Train net output #0: loss = 0.14671 (* 1 = 0.14671 loss)
I0122 16:32:56.805835 51935 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:32:57.707067 51935 solver.cpp:266] Iteration 4200 (110.964 iter/s, 0.901194s/100 iter), loss = 0.212899
I0122 16:32:57.707108 51935 solver.cpp:285]     Train net output #0: loss = 0.212899 (* 1 = 0.212899 loss)
I0122 16:32:57.707150 51935 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:32:58.651165 51935 solver.cpp:266] Iteration 4300 (105.935 iter/s, 0.943976s/100 iter), loss = 0.259219
I0122 16:32:58.651196 51935 solver.cpp:285]     Train net output #0: loss = 0.259219 (* 1 = 0.259219 loss)
I0122 16:32:58.651201 51935 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:32:59.533843 51935 solver.cpp:266] Iteration 4400 (113.301 iter/s, 0.882605s/100 iter), loss = 0.185221
I0122 16:32:59.533871 51935 solver.cpp:285]     Train net output #0: loss = 0.185221 (* 1 = 0.185221 loss)
I0122 16:32:59.533919 51935 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:33:00.499840 51935 solver.cpp:266] Iteration 4500 (103.533 iter/s, 0.965877s/100 iter), loss = 0.128033
I0122 16:33:00.499891 51935 solver.cpp:285]     Train net output #0: loss = 0.128033 (* 1 = 0.128033 loss)
I0122 16:33:00.499898 51935 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:33:01.486043 51935 solver.cpp:266] Iteration 4600 (101.409 iter/s, 0.986107s/100 iter), loss = 0.129399
I0122 16:33:01.486074 51935 solver.cpp:285]     Train net output #0: loss = 0.129399 (* 1 = 0.129399 loss)
I0122 16:33:01.486080 51935 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:33:02.350090 51935 solver.cpp:266] Iteration 4700 (115.744 iter/s, 0.863974s/100 iter), loss = 0.205583
I0122 16:33:02.350118 51935 solver.cpp:285]     Train net output #0: loss = 0.205583 (* 1 = 0.205583 loss)
I0122 16:33:02.350124 51935 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:33:03.335242 51935 solver.cpp:266] Iteration 4800 (101.515 iter/s, 0.985077s/100 iter), loss = 0.126097
I0122 16:33:03.335283 51935 solver.cpp:285]     Train net output #0: loss = 0.126097 (* 1 = 0.126097 loss)
I0122 16:33:03.335289 51935 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:33:04.199091 51935 solver.cpp:266] Iteration 4900 (115.772 iter/s, 0.863767s/100 iter), loss = 0.310997
I0122 16:33:04.199121 51935 solver.cpp:285]     Train net output #0: loss = 0.310997 (* 1 = 0.310997 loss)
I0122 16:33:04.199128 51935 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:33:05.054203 51935 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:33:05.276616 51935 solver.cpp:517]     Test net output #0: loss = 0.523714 (* 1 = 0.523714 loss)
I0122 16:33:05.276633 51935 solver.cpp:517]     Test net output #1: top-1 = 0.843889
I0122 16:33:05.276638 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:33:05.284739 51935 solver.cpp:266] Iteration 5000 (92.1173 iter/s, 1.08557s/100 iter), loss = 0.192881
I0122 16:33:05.284759 51935 solver.cpp:285]     Train net output #0: loss = 0.192881 (* 1 = 0.192881 loss)
I0122 16:33:05.284765 51935 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:33:06.148416 51935 solver.cpp:266] Iteration 5100 (115.792 iter/s, 0.863618s/100 iter), loss = 0.158756
I0122 16:33:06.148445 51935 solver.cpp:285]     Train net output #0: loss = 0.158756 (* 1 = 0.158756 loss)
I0122 16:33:06.148450 51935 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:33:07.011890 51935 solver.cpp:266] Iteration 5200 (115.821 iter/s, 0.863404s/100 iter), loss = 0.250207
I0122 16:33:07.011919 51935 solver.cpp:285]     Train net output #0: loss = 0.250207 (* 1 = 0.250207 loss)
I0122 16:33:07.011924 51935 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:33:07.875604 51935 solver.cpp:266] Iteration 5300 (115.788 iter/s, 0.863645s/100 iter), loss = 0.183284
I0122 16:33:07.875633 51935 solver.cpp:285]     Train net output #0: loss = 0.183284 (* 1 = 0.183284 loss)
I0122 16:33:07.875638 51935 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:33:08.738975 51935 solver.cpp:266] Iteration 5400 (115.834 iter/s, 0.863302s/100 iter), loss = 0.200436
I0122 16:33:08.739002 51935 solver.cpp:285]     Train net output #0: loss = 0.200436 (* 1 = 0.200436 loss)
I0122 16:33:08.739007 51935 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:33:09.602306 51935 solver.cpp:266] Iteration 5500 (115.839 iter/s, 0.863264s/100 iter), loss = 0.145816
I0122 16:33:09.602334 51935 solver.cpp:285]     Train net output #0: loss = 0.145816 (* 1 = 0.145816 loss)
I0122 16:33:09.602340 51935 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:33:10.466097 51935 solver.cpp:266] Iteration 5600 (115.778 iter/s, 0.863722s/100 iter), loss = 0.1601
I0122 16:33:10.466135 51935 solver.cpp:285]     Train net output #0: loss = 0.1601 (* 1 = 0.1601 loss)
I0122 16:33:10.466141 51935 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:33:11.330229 51935 solver.cpp:266] Iteration 5700 (115.734 iter/s, 0.864052s/100 iter), loss = 0.190152
I0122 16:33:11.330258 51935 solver.cpp:285]     Train net output #0: loss = 0.190152 (* 1 = 0.190152 loss)
I0122 16:33:11.330283 51935 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:33:12.193820 51935 solver.cpp:266] Iteration 5800 (115.805 iter/s, 0.863523s/100 iter), loss = 0.0890067
I0122 16:33:12.193850 51935 solver.cpp:285]     Train net output #0: loss = 0.0890067 (* 1 = 0.0890067 loss)
I0122 16:33:12.193856 51935 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:33:13.057695 51935 solver.cpp:266] Iteration 5900 (115.767 iter/s, 0.863805s/100 iter), loss = 0.192423
I0122 16:33:13.057724 51935 solver.cpp:285]     Train net output #0: loss = 0.192423 (* 1 = 0.192423 loss)
I0122 16:33:13.057770 51935 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:33:13.919904 51935 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:33:14.144992 51935 solver.cpp:517]     Test net output #0: loss = 0.563497 (* 1 = 0.563497 loss)
I0122 16:33:14.145009 51935 solver.cpp:517]     Test net output #1: top-1 = 0.840778
I0122 16:33:14.145015 51935 solver.cpp:517]     Test net output #2: top-5 = 0.990334
I0122 16:33:14.153120 51935 solver.cpp:266] Iteration 6000 (91.2985 iter/s, 1.09531s/100 iter), loss = 0.162551
I0122 16:33:14.153139 51935 solver.cpp:285]     Train net output #0: loss = 0.162551 (* 1 = 0.162551 loss)
I0122 16:33:14.153146 51935 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:33:15.017858 51935 solver.cpp:266] Iteration 6100 (115.65 iter/s, 0.864678s/100 iter), loss = 0.175813
I0122 16:33:15.017889 51935 solver.cpp:285]     Train net output #0: loss = 0.175813 (* 1 = 0.175813 loss)
I0122 16:33:15.017895 51935 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:33:15.904634 51935 solver.cpp:266] Iteration 6200 (112.777 iter/s, 0.886704s/100 iter), loss = 0.170887
I0122 16:33:15.904675 51935 solver.cpp:285]     Train net output #0: loss = 0.170887 (* 1 = 0.170887 loss)
I0122 16:33:15.904681 51935 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:33:16.774749 51935 solver.cpp:266] Iteration 6300 (114.938 iter/s, 0.870033s/100 iter), loss = 0.157321
I0122 16:33:16.774777 51935 solver.cpp:285]     Train net output #0: loss = 0.157321 (* 1 = 0.157321 loss)
I0122 16:33:16.774782 51935 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:33:17.641293 51935 solver.cpp:266] Iteration 6400 (115.41 iter/s, 0.866475s/100 iter), loss = 0.120218
I0122 16:33:17.641319 51935 solver.cpp:285]     Train net output #0: loss = 0.120218 (* 1 = 0.120218 loss)
I0122 16:33:17.641324 51935 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:33:18.520953 51935 solver.cpp:266] Iteration 6500 (113.689 iter/s, 0.879593s/100 iter), loss = 0.235302
I0122 16:33:18.521068 51935 solver.cpp:285]     Train net output #0: loss = 0.235302 (* 1 = 0.235302 loss)
I0122 16:33:18.521076 51935 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:33:19.387898 51935 solver.cpp:266] Iteration 6600 (115.368 iter/s, 0.866793s/100 iter), loss = 0.19564
I0122 16:33:19.387924 51935 solver.cpp:285]     Train net output #0: loss = 0.19564 (* 1 = 0.19564 loss)
I0122 16:33:19.387930 51935 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:33:20.274247 51935 solver.cpp:266] Iteration 6700 (112.831 iter/s, 0.886283s/100 iter), loss = 0.16322
I0122 16:33:20.274274 51935 solver.cpp:285]     Train net output #0: loss = 0.16322 (* 1 = 0.16322 loss)
I0122 16:33:20.274281 51935 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:33:21.138094 51935 solver.cpp:266] Iteration 6800 (115.77 iter/s, 0.863781s/100 iter), loss = 0.187327
I0122 16:33:21.138123 51935 solver.cpp:285]     Train net output #0: loss = 0.187327 (* 1 = 0.187327 loss)
I0122 16:33:21.138128 51935 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:33:22.001991 51935 solver.cpp:266] Iteration 6900 (115.764 iter/s, 0.863829s/100 iter), loss = 0.183408
I0122 16:33:22.002018 51935 solver.cpp:285]     Train net output #0: loss = 0.183408 (* 1 = 0.183408 loss)
I0122 16:33:22.002024 51935 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:33:22.857461 51935 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:33:23.080380 51935 solver.cpp:517]     Test net output #0: loss = 0.523645 (* 1 = 0.523645 loss)
I0122 16:33:23.080396 51935 solver.cpp:517]     Test net output #1: top-1 = 0.845889
I0122 16:33:23.080401 51935 solver.cpp:517]     Test net output #2: top-5 = 0.989667
I0122 16:33:23.088501 51935 solver.cpp:266] Iteration 7000 (92.0439 iter/s, 1.08644s/100 iter), loss = 0.154586
I0122 16:33:23.088519 51935 solver.cpp:285]     Train net output #0: loss = 0.154586 (* 1 = 0.154586 loss)
I0122 16:33:23.088526 51935 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:33:23.952267 51935 solver.cpp:266] Iteration 7100 (115.78 iter/s, 0.863709s/100 iter), loss = 0.218258
I0122 16:33:23.952292 51935 solver.cpp:285]     Train net output #0: loss = 0.218258 (* 1 = 0.218258 loss)
I0122 16:33:23.952297 51935 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:33:24.815527 51935 solver.cpp:266] Iteration 7200 (115.849 iter/s, 0.863196s/100 iter), loss = 0.12412
I0122 16:33:24.815555 51935 solver.cpp:285]     Train net output #0: loss = 0.12412 (* 1 = 0.12412 loss)
I0122 16:33:24.815562 51935 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:33:25.679625 51935 solver.cpp:266] Iteration 7300 (115.737 iter/s, 0.864031s/100 iter), loss = 0.205636
I0122 16:33:25.679652 51935 solver.cpp:285]     Train net output #0: loss = 0.205636 (* 1 = 0.205636 loss)
I0122 16:33:25.679658 51935 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:33:26.544270 51935 solver.cpp:266] Iteration 7400 (115.663 iter/s, 0.86458s/100 iter), loss = 0.126055
I0122 16:33:26.544296 51935 solver.cpp:285]     Train net output #0: loss = 0.126055 (* 1 = 0.126055 loss)
I0122 16:33:26.544301 51935 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:33:27.408776 51935 solver.cpp:266] Iteration 7500 (115.682 iter/s, 0.864439s/100 iter), loss = 0.194788
I0122 16:33:27.408800 51935 solver.cpp:285]     Train net output #0: loss = 0.194788 (* 1 = 0.194788 loss)
I0122 16:33:27.408805 51935 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:33:28.280117 51935 solver.cpp:266] Iteration 7600 (114.774 iter/s, 0.871278s/100 iter), loss = 0.169201
I0122 16:33:28.280145 51935 solver.cpp:285]     Train net output #0: loss = 0.169201 (* 1 = 0.169201 loss)
I0122 16:33:28.280150 51935 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:33:29.168591 51935 solver.cpp:266] Iteration 7700 (112.561 iter/s, 0.888404s/100 iter), loss = 0.148452
I0122 16:33:29.168619 51935 solver.cpp:285]     Train net output #0: loss = 0.148452 (* 1 = 0.148452 loss)
I0122 16:33:29.168625 51935 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:33:30.032855 51935 solver.cpp:266] Iteration 7800 (115.715 iter/s, 0.864196s/100 iter), loss = 0.17122
I0122 16:33:30.032903 51935 solver.cpp:285]     Train net output #0: loss = 0.17122 (* 1 = 0.17122 loss)
I0122 16:33:30.032909 51935 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:33:30.896378 51935 solver.cpp:266] Iteration 7900 (115.816 iter/s, 0.863437s/100 iter), loss = 0.128548
I0122 16:33:30.896406 51935 solver.cpp:285]     Train net output #0: loss = 0.128548 (* 1 = 0.128548 loss)
I0122 16:33:30.896412 51935 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:33:31.806777 51935 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:33:32.155804 51935 solver.cpp:517]     Test net output #0: loss = 0.584514 (* 1 = 0.584514 loss)
I0122 16:33:32.155823 51935 solver.cpp:517]     Test net output #1: top-1 = 0.828444
I0122 16:33:32.155827 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0122 16:33:32.165444 51935 solver.cpp:266] Iteration 8000 (78.803 iter/s, 1.26899s/100 iter), loss = 0.130926
I0122 16:33:32.165464 51935 solver.cpp:285]     Train net output #0: loss = 0.130926 (* 1 = 0.130926 loss)
I0122 16:33:32.165527 51935 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:33:33.106969 51935 solver.cpp:266] Iteration 8100 (106.224 iter/s, 0.941402s/100 iter), loss = 0.197424
I0122 16:33:33.107000 51935 solver.cpp:285]     Train net output #0: loss = 0.197424 (* 1 = 0.197424 loss)
I0122 16:33:33.107007 51935 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:33:34.082473 51935 solver.cpp:266] Iteration 8200 (102.519 iter/s, 0.975428s/100 iter), loss = 0.0870009
I0122 16:33:34.082502 51935 solver.cpp:285]     Train net output #0: loss = 0.0870009 (* 1 = 0.0870009 loss)
I0122 16:33:34.082509 51935 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:33:34.952920 51935 solver.cpp:266] Iteration 8300 (114.893 iter/s, 0.870378s/100 iter), loss = 0.124967
I0122 16:33:34.952951 51935 solver.cpp:285]     Train net output #0: loss = 0.124967 (* 1 = 0.124967 loss)
I0122 16:33:34.952996 51935 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:33:35.932654 51935 solver.cpp:266] Iteration 8400 (102.081 iter/s, 0.979614s/100 iter), loss = 0.159538
I0122 16:33:35.932685 51935 solver.cpp:285]     Train net output #0: loss = 0.159538 (* 1 = 0.159538 loss)
I0122 16:33:35.932690 51935 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:33:36.796366 51935 solver.cpp:266] Iteration 8500 (115.789 iter/s, 0.863643s/100 iter), loss = 0.167787
I0122 16:33:36.796396 51935 solver.cpp:285]     Train net output #0: loss = 0.167787 (* 1 = 0.167787 loss)
I0122 16:33:36.796401 51935 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:33:37.660907 51935 solver.cpp:266] Iteration 8600 (115.677 iter/s, 0.864473s/100 iter), loss = 0.147524
I0122 16:33:37.660936 51935 solver.cpp:285]     Train net output #0: loss = 0.147524 (* 1 = 0.147524 loss)
I0122 16:33:37.660941 51935 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:33:38.525713 51935 solver.cpp:266] Iteration 8700 (115.642 iter/s, 0.864739s/100 iter), loss = 0.135179
I0122 16:33:38.525741 51935 solver.cpp:285]     Train net output #0: loss = 0.135179 (* 1 = 0.135179 loss)
I0122 16:33:38.525746 51935 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:33:39.390147 51935 solver.cpp:266] Iteration 8800 (115.692 iter/s, 0.864367s/100 iter), loss = 0.140119
I0122 16:33:39.390177 51935 solver.cpp:285]     Train net output #0: loss = 0.140119 (* 1 = 0.140119 loss)
I0122 16:33:39.390182 51935 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:33:40.254940 51935 solver.cpp:266] Iteration 8900 (115.644 iter/s, 0.864724s/100 iter), loss = 0.125281
I0122 16:33:40.254968 51935 solver.cpp:285]     Train net output #0: loss = 0.125281 (* 1 = 0.125281 loss)
I0122 16:33:40.254976 51935 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:33:41.112295 51935 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:33:41.335290 51935 solver.cpp:517]     Test net output #0: loss = 0.47593 (* 1 = 0.47593 loss)
I0122 16:33:41.335307 51935 solver.cpp:517]     Test net output #1: top-1 = 0.855667
I0122 16:33:41.335333 51935 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0122 16:33:41.343451 51935 solver.cpp:266] Iteration 9000 (91.8747 iter/s, 1.08844s/100 iter), loss = 0.139078
I0122 16:33:41.343468 51935 solver.cpp:285]     Train net output #0: loss = 0.139078 (* 1 = 0.139078 loss)
I0122 16:33:41.343474 51935 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:33:42.207804 51935 solver.cpp:266] Iteration 9100 (115.701 iter/s, 0.864296s/100 iter), loss = 0.0898916
I0122 16:33:42.207834 51935 solver.cpp:285]     Train net output #0: loss = 0.0898916 (* 1 = 0.0898916 loss)
I0122 16:33:42.207840 51935 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:33:43.193466 51935 solver.cpp:266] Iteration 9200 (101.462 iter/s, 0.985589s/100 iter), loss = 0.126023
I0122 16:33:43.193495 51935 solver.cpp:285]     Train net output #0: loss = 0.126023 (* 1 = 0.126023 loss)
I0122 16:33:43.193500 51935 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:33:44.058650 51935 solver.cpp:266] Iteration 9300 (115.592 iter/s, 0.865115s/100 iter), loss = 0.0995335
I0122 16:33:44.058681 51935 solver.cpp:285]     Train net output #0: loss = 0.0995335 (* 1 = 0.0995335 loss)
I0122 16:33:44.058686 51935 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:33:44.923779 51935 solver.cpp:266] Iteration 9400 (115.599 iter/s, 0.865059s/100 iter), loss = 0.0856126
I0122 16:33:44.923808 51935 solver.cpp:285]     Train net output #0: loss = 0.0856126 (* 1 = 0.0856126 loss)
I0122 16:33:44.923815 51935 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:33:45.788390 51935 solver.cpp:266] Iteration 9500 (115.668 iter/s, 0.864543s/100 iter), loss = 0.129344
I0122 16:33:45.788420 51935 solver.cpp:285]     Train net output #0: loss = 0.129344 (* 1 = 0.129344 loss)
I0122 16:33:45.788425 51935 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:33:46.653894 51935 solver.cpp:266] Iteration 9600 (115.549 iter/s, 0.865434s/100 iter), loss = 0.133007
I0122 16:33:46.653925 51935 solver.cpp:285]     Train net output #0: loss = 0.133007 (* 1 = 0.133007 loss)
I0122 16:33:46.653930 51935 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:33:47.518610 51935 solver.cpp:266] Iteration 9700 (115.654 iter/s, 0.864644s/100 iter), loss = 0.135056
I0122 16:33:47.518638 51935 solver.cpp:285]     Train net output #0: loss = 0.135056 (* 1 = 0.135056 loss)
I0122 16:33:47.518645 51935 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:33:48.383476 51935 solver.cpp:266] Iteration 9800 (115.634 iter/s, 0.864797s/100 iter), loss = 0.127847
I0122 16:33:48.383504 51935 solver.cpp:285]     Train net output #0: loss = 0.127847 (* 1 = 0.127847 loss)
I0122 16:33:48.383509 51935 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:33:49.248852 51935 solver.cpp:266] Iteration 9900 (115.566 iter/s, 0.865309s/100 iter), loss = 0.233266
I0122 16:33:49.248998 51935 solver.cpp:285]     Train net output #0: loss = 0.233266 (* 1 = 0.233266 loss)
I0122 16:33:49.249006 51935 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:33:50.113751 51935 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:33:50.338368 51935 solver.cpp:517]     Test net output #0: loss = 0.525237 (* 1 = 0.525237 loss)
I0122 16:33:50.338387 51935 solver.cpp:517]     Test net output #1: top-1 = 0.839555
I0122 16:33:50.338390 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0122 16:33:50.346760 51935 solver.cpp:266] Iteration 10000 (91.0979 iter/s, 1.09772s/100 iter), loss = 0.193576
I0122 16:33:50.346778 51935 solver.cpp:285]     Train net output #0: loss = 0.193576 (* 1 = 0.193576 loss)
I0122 16:33:50.346786 51935 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:33:51.218094 51935 solver.cpp:266] Iteration 10100 (114.774 iter/s, 0.871275s/100 iter), loss = 0.171071
I0122 16:33:51.218124 51935 solver.cpp:285]     Train net output #0: loss = 0.171071 (* 1 = 0.171071 loss)
I0122 16:33:51.218130 51935 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:33:52.087000 51935 solver.cpp:266] Iteration 10200 (115.096 iter/s, 0.868836s/100 iter), loss = 0.139698
I0122 16:33:52.087030 51935 solver.cpp:285]     Train net output #0: loss = 0.139698 (* 1 = 0.139698 loss)
I0122 16:33:52.087036 51935 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:33:52.970649 51935 solver.cpp:266] Iteration 10300 (113.176 iter/s, 0.883579s/100 iter), loss = 0.0969909
I0122 16:33:52.970679 51935 solver.cpp:285]     Train net output #0: loss = 0.096991 (* 1 = 0.096991 loss)
I0122 16:33:52.970685 51935 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:33:53.849483 51935 solver.cpp:266] Iteration 10400 (113.796 iter/s, 0.878763s/100 iter), loss = 0.149835
I0122 16:33:53.849514 51935 solver.cpp:285]     Train net output #0: loss = 0.149835 (* 1 = 0.149835 loss)
I0122 16:33:53.849519 51935 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:33:54.740533 51935 solver.cpp:266] Iteration 10500 (112.236 iter/s, 0.890979s/100 iter), loss = 0.11117
I0122 16:33:54.740561 51935 solver.cpp:285]     Train net output #0: loss = 0.11117 (* 1 = 0.11117 loss)
I0122 16:33:54.740566 51935 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:33:55.627233 51935 solver.cpp:266] Iteration 10600 (112.786 iter/s, 0.886632s/100 iter), loss = 0.144383
I0122 16:33:55.627261 51935 solver.cpp:285]     Train net output #0: loss = 0.144383 (* 1 = 0.144383 loss)
I0122 16:33:55.627266 51935 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:33:56.506428 51935 solver.cpp:266] Iteration 10700 (113.749 iter/s, 0.879125s/100 iter), loss = 0.144688
I0122 16:33:56.506455 51935 solver.cpp:285]     Train net output #0: loss = 0.144688 (* 1 = 0.144688 loss)
I0122 16:33:56.506461 51935 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:33:57.383342 51935 solver.cpp:266] Iteration 10800 (114.045 iter/s, 0.876846s/100 iter), loss = 0.136123
I0122 16:33:57.383371 51935 solver.cpp:285]     Train net output #0: loss = 0.136123 (* 1 = 0.136123 loss)
I0122 16:33:57.383378 51935 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:33:58.253626 51935 solver.cpp:266] Iteration 10900 (114.914 iter/s, 0.870217s/100 iter), loss = 0.172079
I0122 16:33:58.253654 51935 solver.cpp:285]     Train net output #0: loss = 0.172079 (* 1 = 0.172079 loss)
I0122 16:33:58.253659 51935 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:33:59.116374 51935 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:33:59.347934 51935 solver.cpp:517]     Test net output #0: loss = 0.545626 (* 1 = 0.545626 loss)
I0122 16:33:59.347950 51935 solver.cpp:517]     Test net output #1: top-1 = 0.833222
I0122 16:33:59.347954 51935 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0122 16:33:59.356709 51935 solver.cpp:266] Iteration 11000 (90.6609 iter/s, 1.10301s/100 iter), loss = 0.187761
I0122 16:33:59.356727 51935 solver.cpp:285]     Train net output #0: loss = 0.187761 (* 1 = 0.187761 loss)
I0122 16:33:59.356734 51935 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:34:00.240545 51935 solver.cpp:266] Iteration 11100 (113.151 iter/s, 0.883776s/100 iter), loss = 0.113608
I0122 16:34:00.240573 51935 solver.cpp:285]     Train net output #0: loss = 0.113608 (* 1 = 0.113608 loss)
I0122 16:34:00.240578 51935 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:34:01.160012 51935 solver.cpp:266] Iteration 11200 (108.767 iter/s, 0.919399s/100 iter), loss = 0.0933453
I0122 16:34:01.160042 51935 solver.cpp:285]     Train net output #0: loss = 0.0933454 (* 1 = 0.0933454 loss)
I0122 16:34:01.160046 51935 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:34:02.026607 51935 solver.cpp:266] Iteration 11300 (115.403 iter/s, 0.866526s/100 iter), loss = 0.13475
I0122 16:34:02.026638 51935 solver.cpp:285]     Train net output #0: loss = 0.13475 (* 1 = 0.13475 loss)
I0122 16:34:02.026644 51935 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:34:02.909613 51935 solver.cpp:266] Iteration 11400 (113.259 iter/s, 0.882935s/100 iter), loss = 0.168161
I0122 16:34:02.909641 51935 solver.cpp:285]     Train net output #0: loss = 0.168161 (* 1 = 0.168161 loss)
I0122 16:34:02.909646 51935 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:34:03.856819 51935 solver.cpp:266] Iteration 11500 (105.581 iter/s, 0.947136s/100 iter), loss = 0.163453
I0122 16:34:03.856861 51935 solver.cpp:285]     Train net output #0: loss = 0.163453 (* 1 = 0.163453 loss)
I0122 16:34:03.856868 51935 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:34:04.750037 51935 solver.cpp:266] Iteration 11600 (111.965 iter/s, 0.893136s/100 iter), loss = 0.108909
I0122 16:34:04.750068 51935 solver.cpp:285]     Train net output #0: loss = 0.108909 (* 1 = 0.108909 loss)
I0122 16:34:04.750074 51935 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:34:05.654594 51935 solver.cpp:266] Iteration 11700 (110.56 iter/s, 0.904485s/100 iter), loss = 0.139738
I0122 16:34:05.654625 51935 solver.cpp:285]     Train net output #0: loss = 0.139738 (* 1 = 0.139738 loss)
I0122 16:34:05.654671 51935 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:34:06.538494 51935 solver.cpp:266] Iteration 11800 (113.15 iter/s, 0.883785s/100 iter), loss = 0.15919
I0122 16:34:06.538527 51935 solver.cpp:285]     Train net output #0: loss = 0.15919 (* 1 = 0.15919 loss)
I0122 16:34:06.538532 51935 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:34:07.427652 51935 solver.cpp:266] Iteration 11900 (112.475 iter/s, 0.889086s/100 iter), loss = 0.133334
I0122 16:34:07.427683 51935 solver.cpp:285]     Train net output #0: loss = 0.133334 (* 1 = 0.133334 loss)
I0122 16:34:07.427729 51935 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:34:08.452627 51935 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:34:08.801479 51935 solver.cpp:517]     Test net output #0: loss = 0.475913 (* 1 = 0.475913 loss)
I0122 16:34:08.801499 51935 solver.cpp:517]     Test net output #1: top-1 = 0.856333
I0122 16:34:08.801503 51935 solver.cpp:517]     Test net output #2: top-5 = 0.990556
I0122 16:34:08.811141 51935 solver.cpp:266] Iteration 12000 (72.2878 iter/s, 1.38336s/100 iter), loss = 0.144511
I0122 16:34:08.811161 51935 solver.cpp:285]     Train net output #0: loss = 0.144511 (* 1 = 0.144511 loss)
I0122 16:34:08.811224 51935 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:34:09.845820 51935 solver.cpp:266] Iteration 12100 (96.6601 iter/s, 1.03455s/100 iter), loss = 0.125118
I0122 16:34:09.845851 51935 solver.cpp:285]     Train net output #0: loss = 0.125118 (* 1 = 0.125118 loss)
I0122 16:34:09.845897 51935 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:34:10.870349 51935 solver.cpp:266] Iteration 12200 (97.6174 iter/s, 1.02441s/100 iter), loss = 0.214444
I0122 16:34:10.870379 51935 solver.cpp:285]     Train net output #0: loss = 0.214444 (* 1 = 0.214444 loss)
I0122 16:34:10.870426 51935 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:34:11.904573 51935 solver.cpp:266] Iteration 12300 (96.7023 iter/s, 1.0341s/100 iter), loss = 0.14721
I0122 16:34:11.904625 51935 solver.cpp:285]     Train net output #0: loss = 0.14721 (* 1 = 0.14721 loss)
I0122 16:34:11.904662 51935 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:34:12.939712 51935 solver.cpp:266] Iteration 12400 (96.618 iter/s, 1.035s/100 iter), loss = 0.132803
I0122 16:34:12.939743 51935 solver.cpp:285]     Train net output #0: loss = 0.132803 (* 1 = 0.132803 loss)
I0122 16:34:12.939790 51935 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:34:13.938522 51935 solver.cpp:266] Iteration 12500 (100.131 iter/s, 0.998689s/100 iter), loss = 0.111135
I0122 16:34:13.938555 51935 solver.cpp:285]     Train net output #0: loss = 0.111135 (* 1 = 0.111135 loss)
I0122 16:34:13.938599 51935 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:34:14.935328 51935 solver.cpp:266] Iteration 12600 (100.333 iter/s, 0.996685s/100 iter), loss = 0.205584
I0122 16:34:14.935359 51935 solver.cpp:285]     Train net output #0: loss = 0.205584 (* 1 = 0.205584 loss)
I0122 16:34:14.935364 51935 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:34:15.821435 51935 solver.cpp:266] Iteration 12700 (112.862 iter/s, 0.886038s/100 iter), loss = 0.183703
I0122 16:34:15.821465 51935 solver.cpp:285]     Train net output #0: loss = 0.183703 (* 1 = 0.183703 loss)
I0122 16:34:15.821470 51935 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:34:16.697551 51935 solver.cpp:266] Iteration 12800 (114.149 iter/s, 0.876047s/100 iter), loss = 0.207457
I0122 16:34:16.697580 51935 solver.cpp:285]     Train net output #0: loss = 0.207457 (* 1 = 0.207457 loss)
I0122 16:34:16.697587 51935 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:34:17.561524 51935 solver.cpp:266] Iteration 12900 (115.754 iter/s, 0.863905s/100 iter), loss = 0.139748
I0122 16:34:17.561554 51935 solver.cpp:285]     Train net output #0: loss = 0.139748 (* 1 = 0.139748 loss)
I0122 16:34:17.561560 51935 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:34:18.450352 51935 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:34:18.670096 51935 solver.cpp:517]     Test net output #0: loss = 0.475525 (* 1 = 0.475525 loss)
I0122 16:34:18.670114 51935 solver.cpp:517]     Test net output #1: top-1 = 0.859222
I0122 16:34:18.670119 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991445
I0122 16:34:18.678200 51935 solver.cpp:266] Iteration 13000 (89.5574 iter/s, 1.1166s/100 iter), loss = 0.168798
I0122 16:34:18.678218 51935 solver.cpp:285]     Train net output #0: loss = 0.168798 (* 1 = 0.168798 loss)
I0122 16:34:18.678225 51935 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:34:19.541788 51935 solver.cpp:266] Iteration 13100 (115.804 iter/s, 0.863531s/100 iter), loss = 0.117385
I0122 16:34:19.541909 51935 solver.cpp:285]     Train net output #0: loss = 0.117385 (* 1 = 0.117385 loss)
I0122 16:34:19.541918 51935 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:34:20.405313 51935 solver.cpp:266] Iteration 13200 (115.825 iter/s, 0.86337s/100 iter), loss = 0.179305
I0122 16:34:20.405354 51935 solver.cpp:285]     Train net output #0: loss = 0.179305 (* 1 = 0.179305 loss)
I0122 16:34:20.405360 51935 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:34:21.268633 51935 solver.cpp:266] Iteration 13300 (115.843 iter/s, 0.863241s/100 iter), loss = 0.166448
I0122 16:34:21.268663 51935 solver.cpp:285]     Train net output #0: loss = 0.166448 (* 1 = 0.166448 loss)
I0122 16:34:21.268671 51935 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:34:22.132720 51935 solver.cpp:266] Iteration 13400 (115.738 iter/s, 0.864017s/100 iter), loss = 0.126821
I0122 16:34:22.132750 51935 solver.cpp:285]     Train net output #0: loss = 0.126821 (* 1 = 0.126821 loss)
I0122 16:34:22.132755 51935 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:34:23.007194 51935 solver.cpp:266] Iteration 13500 (114.364 iter/s, 0.874403s/100 iter), loss = 0.114336
I0122 16:34:23.007222 51935 solver.cpp:285]     Train net output #0: loss = 0.114336 (* 1 = 0.114336 loss)
I0122 16:34:23.007227 51935 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:34:23.873772 51935 solver.cpp:266] Iteration 13600 (115.405 iter/s, 0.866511s/100 iter), loss = 0.122975
I0122 16:34:23.873803 51935 solver.cpp:285]     Train net output #0: loss = 0.122975 (* 1 = 0.122975 loss)
I0122 16:34:23.873809 51935 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:34:24.736968 51935 solver.cpp:266] Iteration 13700 (115.858 iter/s, 0.863126s/100 iter), loss = 0.0814902
I0122 16:34:24.736999 51935 solver.cpp:285]     Train net output #0: loss = 0.0814902 (* 1 = 0.0814902 loss)
I0122 16:34:24.737005 51935 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:34:25.605173 51935 solver.cpp:266] Iteration 13800 (115.189 iter/s, 0.868136s/100 iter), loss = 0.115989
I0122 16:34:25.605203 51935 solver.cpp:285]     Train net output #0: loss = 0.115989 (* 1 = 0.115989 loss)
I0122 16:34:25.605209 51935 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:34:26.468384 51935 solver.cpp:266] Iteration 13900 (115.856 iter/s, 0.863142s/100 iter), loss = 0.120911
I0122 16:34:26.468413 51935 solver.cpp:285]     Train net output #0: loss = 0.120911 (* 1 = 0.120911 loss)
I0122 16:34:26.468418 51935 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:34:27.326685 51935 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:34:27.547601 51935 solver.cpp:517]     Test net output #0: loss = 0.479535 (* 1 = 0.479535 loss)
I0122 16:34:27.547618 51935 solver.cpp:517]     Test net output #1: top-1 = 0.855333
I0122 16:34:27.547623 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:34:27.555737 51935 solver.cpp:266] Iteration 14000 (91.9727 iter/s, 1.08728s/100 iter), loss = 0.0801235
I0122 16:34:27.555765 51935 solver.cpp:285]     Train net output #0: loss = 0.0801236 (* 1 = 0.0801236 loss)
I0122 16:34:27.555773 51935 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:34:28.419811 51935 solver.cpp:266] Iteration 14100 (115.739 iter/s, 0.864016s/100 iter), loss = 0.0826325
I0122 16:34:28.419840 51935 solver.cpp:285]     Train net output #0: loss = 0.0826326 (* 1 = 0.0826326 loss)
I0122 16:34:28.419847 51935 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:34:29.283109 51935 solver.cpp:266] Iteration 14200 (115.844 iter/s, 0.86323s/100 iter), loss = 0.147897
I0122 16:34:29.283138 51935 solver.cpp:285]     Train net output #0: loss = 0.147897 (* 1 = 0.147897 loss)
I0122 16:34:29.283144 51935 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:34:30.146363 51935 solver.cpp:266] Iteration 14300 (115.85 iter/s, 0.863186s/100 iter), loss = 0.116641
I0122 16:34:30.146391 51935 solver.cpp:285]     Train net output #0: loss = 0.116641 (* 1 = 0.116641 loss)
I0122 16:34:30.146396 51935 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:34:31.009889 51935 solver.cpp:266] Iteration 14400 (115.813 iter/s, 0.86346s/100 iter), loss = 0.104195
I0122 16:34:31.009922 51935 solver.cpp:285]     Train net output #0: loss = 0.104195 (* 1 = 0.104195 loss)
I0122 16:34:31.009927 51935 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:34:31.880312 51935 solver.cpp:266] Iteration 14500 (114.896 iter/s, 0.870352s/100 iter), loss = 0.0970165
I0122 16:34:31.880342 51935 solver.cpp:285]     Train net output #0: loss = 0.0970165 (* 1 = 0.0970165 loss)
I0122 16:34:31.880347 51935 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:34:32.763100 51935 solver.cpp:266] Iteration 14600 (113.286 iter/s, 0.882718s/100 iter), loss = 0.0850525
I0122 16:34:32.763130 51935 solver.cpp:285]     Train net output #0: loss = 0.0850526 (* 1 = 0.0850526 loss)
I0122 16:34:32.763135 51935 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:34:33.631276 51935 solver.cpp:266] Iteration 14700 (115.193 iter/s, 0.868107s/100 iter), loss = 0.0751529
I0122 16:34:33.631305 51935 solver.cpp:285]     Train net output #0: loss = 0.075153 (* 1 = 0.075153 loss)
I0122 16:34:33.631310 51935 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:34:34.528987 51935 solver.cpp:266] Iteration 14800 (111.403 iter/s, 0.897642s/100 iter), loss = 0.147684
I0122 16:34:34.529016 51935 solver.cpp:285]     Train net output #0: loss = 0.147684 (* 1 = 0.147684 loss)
I0122 16:34:34.529022 51935 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:34:35.392395 51935 solver.cpp:266] Iteration 14900 (115.829 iter/s, 0.863339s/100 iter), loss = 0.098625
I0122 16:34:35.392424 51935 solver.cpp:285]     Train net output #0: loss = 0.0986251 (* 1 = 0.0986251 loss)
I0122 16:34:35.392431 51935 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:34:36.247690 51935 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:34:36.471946 51935 solver.cpp:517]     Test net output #0: loss = 0.475289 (* 1 = 0.475289 loss)
I0122 16:34:36.471961 51935 solver.cpp:517]     Test net output #1: top-1 = 0.860111
I0122 16:34:36.471966 51935 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:34:36.480217 51935 solver.cpp:266] Iteration 15000 (91.9328 iter/s, 1.08775s/100 iter), loss = 0.151214
I0122 16:34:36.480237 51935 solver.cpp:285]     Train net output #0: loss = 0.151214 (* 1 = 0.151214 loss)
I0122 16:34:36.480242 51935 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:34:37.364711 51935 solver.cpp:266] Iteration 15100 (113.066 iter/s, 0.884436s/100 iter), loss = 0.084567
I0122 16:34:37.364742 51935 solver.cpp:285]     Train net output #0: loss = 0.0845671 (* 1 = 0.0845671 loss)
I0122 16:34:37.364747 51935 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:34:38.227344 51935 solver.cpp:266] Iteration 15200 (115.933 iter/s, 0.862566s/100 iter), loss = 0.196233
I0122 16:34:38.227373 51935 solver.cpp:285]     Train net output #0: loss = 0.196233 (* 1 = 0.196233 loss)
I0122 16:34:38.227378 51935 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:34:39.092162 51935 solver.cpp:266] Iteration 15300 (115.64 iter/s, 0.864751s/100 iter), loss = 0.138036
I0122 16:34:39.092188 51935 solver.cpp:285]     Train net output #0: loss = 0.138036 (* 1 = 0.138036 loss)
I0122 16:34:39.092195 51935 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:34:39.959630 51935 solver.cpp:266] Iteration 15400 (115.287 iter/s, 0.867402s/100 iter), loss = 0.119546
I0122 16:34:39.959662 51935 solver.cpp:285]     Train net output #0: loss = 0.119546 (* 1 = 0.119546 loss)
I0122 16:34:39.959667 51935 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:34:40.846915 51935 solver.cpp:266] Iteration 15500 (112.712 iter/s, 0.887214s/100 iter), loss = 0.102434
I0122 16:34:40.846945 51935 solver.cpp:285]     Train net output #0: loss = 0.102434 (* 1 = 0.102434 loss)
I0122 16:34:40.846949 51935 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:34:41.710199 51935 solver.cpp:266] Iteration 15600 (115.846 iter/s, 0.863218s/100 iter), loss = 0.12386
I0122 16:34:41.710229 51935 solver.cpp:285]     Train net output #0: loss = 0.12386 (* 1 = 0.12386 loss)
I0122 16:34:41.710256 51935 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:34:42.573319 51935 solver.cpp:266] Iteration 15700 (115.868 iter/s, 0.863052s/100 iter), loss = 0.0979872
I0122 16:34:42.573348 51935 solver.cpp:285]     Train net output #0: loss = 0.0979873 (* 1 = 0.0979873 loss)
I0122 16:34:42.573354 51935 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:34:43.436903 51935 solver.cpp:266] Iteration 15800 (115.805 iter/s, 0.863518s/100 iter), loss = 0.0757238
I0122 16:34:43.436931 51935 solver.cpp:285]     Train net output #0: loss = 0.0757238 (* 1 = 0.0757238 loss)
I0122 16:34:43.436938 51935 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:34:44.302637 51935 solver.cpp:266] Iteration 15900 (115.518 iter/s, 0.865667s/100 iter), loss = 0.194586
I0122 16:34:44.302677 51935 solver.cpp:285]     Train net output #0: loss = 0.194586 (* 1 = 0.194586 loss)
I0122 16:34:44.302700 51935 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:34:45.158726 51935 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:34:45.378710 51935 solver.cpp:517]     Test net output #0: loss = 0.461265 (* 1 = 0.461265 loss)
I0122 16:34:45.378726 51935 solver.cpp:517]     Test net output #1: top-1 = 0.858222
I0122 16:34:45.378729 51935 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0122 16:34:45.386770 51935 solver.cpp:266] Iteration 16000 (92.2466 iter/s, 1.08405s/100 iter), loss = 0.112772
I0122 16:34:45.386799 51935 solver.cpp:285]     Train net output #0: loss = 0.112772 (* 1 = 0.112772 loss)
I0122 16:34:45.386806 51935 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:34:46.250654 51935 solver.cpp:266] Iteration 16100 (115.765 iter/s, 0.863818s/100 iter), loss = 0.13705
I0122 16:34:46.250682 51935 solver.cpp:285]     Train net output #0: loss = 0.13705 (* 1 = 0.13705 loss)
I0122 16:34:46.250689 51935 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:34:47.113610 51935 solver.cpp:266] Iteration 16200 (115.89 iter/s, 0.86289s/100 iter), loss = 0.090888
I0122 16:34:47.113638 51935 solver.cpp:285]     Train net output #0: loss = 0.0908881 (* 1 = 0.0908881 loss)
I0122 16:34:47.113644 51935 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:34:47.978152 51935 solver.cpp:266] Iteration 16300 (115.677 iter/s, 0.864476s/100 iter), loss = 0.104936
I0122 16:34:47.978180 51935 solver.cpp:285]     Train net output #0: loss = 0.104936 (* 1 = 0.104936 loss)
I0122 16:34:47.978185 51935 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:34:48.843355 51935 solver.cpp:266] Iteration 16400 (115.589 iter/s, 0.865137s/100 iter), loss = 0.141377
I0122 16:34:48.843386 51935 solver.cpp:285]     Train net output #0: loss = 0.141377 (* 1 = 0.141377 loss)
I0122 16:34:48.843391 51935 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:34:49.706578 51935 solver.cpp:266] Iteration 16500 (115.854 iter/s, 0.863153s/100 iter), loss = 0.133767
I0122 16:34:49.706682 51935 solver.cpp:285]     Train net output #0: loss = 0.133767 (* 1 = 0.133767 loss)
I0122 16:34:49.706691 51935 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:34:50.570789 51935 solver.cpp:266] Iteration 16600 (115.731 iter/s, 0.86407s/100 iter), loss = 0.134292
I0122 16:34:50.570816 51935 solver.cpp:285]     Train net output #0: loss = 0.134292 (* 1 = 0.134292 loss)
I0122 16:34:50.570822 51935 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:34:51.434211 51935 solver.cpp:266] Iteration 16700 (115.827 iter/s, 0.863356s/100 iter), loss = 0.102333
I0122 16:34:51.434239 51935 solver.cpp:285]     Train net output #0: loss = 0.102334 (* 1 = 0.102334 loss)
I0122 16:34:51.434245 51935 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:34:52.298960 51935 solver.cpp:266] Iteration 16800 (115.649 iter/s, 0.864682s/100 iter), loss = 0.0895557
I0122 16:34:52.298991 51935 solver.cpp:285]     Train net output #0: loss = 0.0895559 (* 1 = 0.0895559 loss)
I0122 16:34:52.298997 51935 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:34:53.164201 51935 solver.cpp:266] Iteration 16900 (115.584 iter/s, 0.865172s/100 iter), loss = 0.1129
I0122 16:34:53.164230 51935 solver.cpp:285]     Train net output #0: loss = 0.1129 (* 1 = 0.1129 loss)
I0122 16:34:53.164237 51935 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:34:54.019677 51935 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:34:54.240366 51935 solver.cpp:517]     Test net output #0: loss = 0.455806 (* 1 = 0.455806 loss)
I0122 16:34:54.240382 51935 solver.cpp:517]     Test net output #1: top-1 = 0.860444
I0122 16:34:54.240386 51935 solver.cpp:517]     Test net output #2: top-5 = 0.993334
I0122 16:34:54.248484 51935 solver.cpp:266] Iteration 17000 (92.233 iter/s, 1.08421s/100 iter), loss = 0.133991
I0122 16:34:54.248504 51935 solver.cpp:285]     Train net output #0: loss = 0.133991 (* 1 = 0.133991 loss)
I0122 16:34:54.248510 51935 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:34:55.112452 51935 solver.cpp:266] Iteration 17100 (115.753 iter/s, 0.863908s/100 iter), loss = 0.110854
I0122 16:34:55.112481 51935 solver.cpp:285]     Train net output #0: loss = 0.110854 (* 1 = 0.110854 loss)
I0122 16:34:55.112486 51935 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:34:55.975752 51935 solver.cpp:266] Iteration 17200 (115.844 iter/s, 0.863233s/100 iter), loss = 0.117022
I0122 16:34:55.975781 51935 solver.cpp:285]     Train net output #0: loss = 0.117022 (* 1 = 0.117022 loss)
I0122 16:34:55.975787 51935 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:34:56.868511 51935 solver.cpp:266] Iteration 17300 (112.021 iter/s, 0.892689s/100 iter), loss = 0.127569
I0122 16:34:56.868541 51935 solver.cpp:285]     Train net output #0: loss = 0.127569 (* 1 = 0.127569 loss)
I0122 16:34:56.868546 51935 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:34:57.844983 51935 solver.cpp:266] Iteration 17400 (102.417 iter/s, 0.976398s/100 iter), loss = 0.134496
I0122 16:34:57.845013 51935 solver.cpp:285]     Train net output #0: loss = 0.134497 (* 1 = 0.134497 loss)
I0122 16:34:57.845060 51935 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:34:58.882660 51935 solver.cpp:266] Iteration 17500 (96.3805 iter/s, 1.03755s/100 iter), loss = 0.0969616
I0122 16:34:58.882691 51935 solver.cpp:285]     Train net output #0: loss = 0.0969618 (* 1 = 0.0969618 loss)
I0122 16:34:58.882741 51935 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:34:59.774183 51935 solver.cpp:266] Iteration 17600 (112.183 iter/s, 0.891401s/100 iter), loss = 0.0614263
I0122 16:34:59.774211 51935 solver.cpp:285]     Train net output #0: loss = 0.0614264 (* 1 = 0.0614264 loss)
I0122 16:34:59.774216 51935 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:35:00.645328 51935 solver.cpp:266] Iteration 17700 (114.8 iter/s, 0.871079s/100 iter), loss = 0.155073
I0122 16:35:00.645359 51935 solver.cpp:285]     Train net output #0: loss = 0.155073 (* 1 = 0.155073 loss)
I0122 16:35:00.645364 51935 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:35:01.517626 51935 solver.cpp:266] Iteration 17800 (114.649 iter/s, 0.872229s/100 iter), loss = 0.124367
I0122 16:35:01.517655 51935 solver.cpp:285]     Train net output #0: loss = 0.124367 (* 1 = 0.124367 loss)
I0122 16:35:01.517662 51935 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:35:02.402467 51935 solver.cpp:266] Iteration 17900 (113.023 iter/s, 0.884772s/100 iter), loss = 0.142769
I0122 16:35:02.402495 51935 solver.cpp:285]     Train net output #0: loss = 0.14277 (* 1 = 0.14277 loss)
I0122 16:35:02.402501 51935 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:35:03.312351 51935 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:35:03.534358 51935 solver.cpp:517]     Test net output #0: loss = 0.451239 (* 1 = 0.451239 loss)
I0122 16:35:03.534374 51935 solver.cpp:517]     Test net output #1: top-1 = 0.863333
I0122 16:35:03.534379 51935 solver.cpp:517]     Test net output #2: top-5 = 0.992445
I0122 16:35:03.542438 51935 solver.cpp:266] Iteration 18000 (87.7273 iter/s, 1.1399s/100 iter), loss = 0.121581
I0122 16:35:03.542455 51935 solver.cpp:285]     Train net output #0: loss = 0.121581 (* 1 = 0.121581 loss)
I0122 16:35:03.542461 51935 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:35:04.409508 51935 solver.cpp:266] Iteration 18100 (115.339 iter/s, 0.867013s/100 iter), loss = 0.155241
I0122 16:35:04.409538 51935 solver.cpp:285]     Train net output #0: loss = 0.155241 (* 1 = 0.155241 loss)
I0122 16:35:04.409543 51935 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:35:05.284263 51935 solver.cpp:266] Iteration 18200 (114.327 iter/s, 0.874686s/100 iter), loss = 0.115923
I0122 16:35:05.284292 51935 solver.cpp:285]     Train net output #0: loss = 0.115923 (* 1 = 0.115923 loss)
I0122 16:35:05.284298 51935 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:35:06.148931 51935 solver.cpp:266] Iteration 18300 (115.66 iter/s, 0.8646s/100 iter), loss = 0.159397
I0122 16:35:06.148960 51935 solver.cpp:285]     Train net output #0: loss = 0.159397 (* 1 = 0.159397 loss)
I0122 16:35:06.148967 51935 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:35:07.013697 51935 solver.cpp:266] Iteration 18400 (115.647 iter/s, 0.864698s/100 iter), loss = 0.114069
I0122 16:35:07.013726 51935 solver.cpp:285]     Train net output #0: loss = 0.114069 (* 1 = 0.114069 loss)
I0122 16:35:07.013731 51935 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:35:07.877887 51935 solver.cpp:266] Iteration 18500 (115.724 iter/s, 0.864122s/100 iter), loss = 0.18448
I0122 16:35:07.877916 51935 solver.cpp:285]     Train net output #0: loss = 0.18448 (* 1 = 0.18448 loss)
I0122 16:35:07.877923 51935 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:35:08.747566 51935 solver.cpp:266] Iteration 18600 (114.994 iter/s, 0.86961s/100 iter), loss = 0.112586
I0122 16:35:08.747594 51935 solver.cpp:285]     Train net output #0: loss = 0.112586 (* 1 = 0.112586 loss)
I0122 16:35:08.747599 51935 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:35:09.612011 51935 solver.cpp:266] Iteration 18700 (115.69 iter/s, 0.864378s/100 iter), loss = 0.125483
I0122 16:35:09.612040 51935 solver.cpp:285]     Train net output #0: loss = 0.125483 (* 1 = 0.125483 loss)
I0122 16:35:09.612046 51935 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:35:10.476801 51935 solver.cpp:266] Iteration 18800 (115.644 iter/s, 0.864721s/100 iter), loss = 0.0663433
I0122 16:35:10.476830 51935 solver.cpp:285]     Train net output #0: loss = 0.0663434 (* 1 = 0.0663434 loss)
I0122 16:35:10.476835 51935 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:35:11.342530 51935 solver.cpp:266] Iteration 18900 (115.519 iter/s, 0.865662s/100 iter), loss = 0.131455
I0122 16:35:11.342556 51935 solver.cpp:285]     Train net output #0: loss = 0.131455 (* 1 = 0.131455 loss)
I0122 16:35:11.342578 51935 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:35:12.202513 51935 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:35:12.423616 51935 solver.cpp:517]     Test net output #0: loss = 0.445244 (* 1 = 0.445244 loss)
I0122 16:35:12.423661 51935 solver.cpp:517]     Test net output #1: top-1 = 0.864444
I0122 16:35:12.423666 51935 solver.cpp:517]     Test net output #2: top-5 = 0.993667
I0122 16:35:12.431757 51935 solver.cpp:266] Iteration 19000 (91.8141 iter/s, 1.08916s/100 iter), loss = 0.18011
I0122 16:35:12.431776 51935 solver.cpp:285]     Train net output #0: loss = 0.180111 (* 1 = 0.180111 loss)
I0122 16:35:12.431782 51935 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:35:13.303073 51935 solver.cpp:266] Iteration 19100 (114.777 iter/s, 0.871258s/100 iter), loss = 0.0972266
I0122 16:35:13.303100 51935 solver.cpp:285]     Train net output #0: loss = 0.0972267 (* 1 = 0.0972267 loss)
I0122 16:35:13.303105 51935 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:35:14.174247 51935 solver.cpp:266] Iteration 19200 (114.796 iter/s, 0.871109s/100 iter), loss = 0.122947
I0122 16:35:14.174274 51935 solver.cpp:285]     Train net output #0: loss = 0.122947 (* 1 = 0.122947 loss)
I0122 16:35:14.174280 51935 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:35:15.038808 51935 solver.cpp:266] Iteration 19300 (115.674 iter/s, 0.864495s/100 iter), loss = 0.1051
I0122 16:35:15.038836 51935 solver.cpp:285]     Train net output #0: loss = 0.1051 (* 1 = 0.1051 loss)
I0122 16:35:15.038842 51935 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:35:15.906890 51935 solver.cpp:266] Iteration 19400 (115.205 iter/s, 0.868015s/100 iter), loss = 0.108801
I0122 16:35:15.906930 51935 solver.cpp:285]     Train net output #0: loss = 0.108801 (* 1 = 0.108801 loss)
I0122 16:35:15.906936 51935 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:35:16.771349 51935 solver.cpp:266] Iteration 19500 (115.69 iter/s, 0.864381s/100 iter), loss = 0.10914
I0122 16:35:16.771376 51935 solver.cpp:285]     Train net output #0: loss = 0.10914 (* 1 = 0.10914 loss)
I0122 16:35:16.771383 51935 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:35:17.640228 51935 solver.cpp:266] Iteration 19600 (115.099 iter/s, 0.868814s/100 iter), loss = 0.133237
I0122 16:35:17.640257 51935 solver.cpp:285]     Train net output #0: loss = 0.133237 (* 1 = 0.133237 loss)
I0122 16:35:17.640264 51935 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:35:18.515241 51935 solver.cpp:266] Iteration 19700 (114.293 iter/s, 0.874945s/100 iter), loss = 0.114567
I0122 16:35:18.515269 51935 solver.cpp:285]     Train net output #0: loss = 0.114567 (* 1 = 0.114567 loss)
I0122 16:35:18.515275 51935 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:35:19.380769 51935 solver.cpp:266] Iteration 19800 (115.545 iter/s, 0.865462s/100 iter), loss = 0.110454
I0122 16:35:19.380797 51935 solver.cpp:285]     Train net output #0: loss = 0.110455 (* 1 = 0.110455 loss)
I0122 16:35:19.380803 51935 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:35:20.249554 51935 solver.cpp:266] Iteration 19900 (115.112 iter/s, 0.868718s/100 iter), loss = 0.109341
I0122 16:35:20.249718 51935 solver.cpp:285]     Train net output #0: loss = 0.109342 (* 1 = 0.109342 loss)
I0122 16:35:20.249727 51935 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:35:21.113692 51935 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/_iter_20000.caffemodel
I0122 16:35:21.189584 51935 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.3/snapshots/_iter_20000.solverstate
I0122 16:35:21.203994 51935 solver.cpp:378] Iteration 20000, loss = 0.0144286
I0122 16:35:21.204016 51935 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:35:21.423290 51935 solver.cpp:517]     Test net output #0: loss = 0.443443 (* 1 = 0.443443 loss)
I0122 16:35:21.423306 51935 solver.cpp:517]     Test net output #1: top-1 = 0.865555
I0122 16:35:21.423310 51935 solver.cpp:517]     Test net output #2: top-5 = 0.992556
I0122 16:35:21.423315 51935 solver.cpp:386] Optimization Done (110.529 iter/s).
I0122 16:35:21.423319 51935 caffe_interface.cpp:530] Optimization Done.

## compression: fourth run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config4.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress4_miniVggNet.txt
I0122 16:35:21.962533 52522 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:35:22.012171 52522 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:35:23.416923 52522 pruning_runner.cpp:264] Compression complete 0.00047847%
I0122 16:35:24.050333 52522 pruning_runner.cpp:264] Compression complete 0.000956935%
I0122 16:35:24.668411 52522 pruning_runner.cpp:264] Compression complete 50.0005%
I0122 16:35:25.289764 52522 pruning_runner.cpp:264] Compression complete 75.0002%
I0122 16:35:25.915493 52522 pruning_runner.cpp:264] Compression complete 85.7144%
I0122 16:35:26.548578 52522 pruning_runner.cpp:264] Compression complete 92.8572%
I0122 16:35:27.180989 52522 pruning_runner.cpp:264] Compression complete 98.1132%
I0122 16:35:27.814322 52522 pruning_runner.cpp:264] Compression complete 99.0566%
I0122 16:35:28.448493 52522 pruning_runner.cpp:264] Compression complete 99.5261%
I0122 16:35:29.079660 52522 pruning_runner.cpp:264] Compression complete 99.763%
I0122 16:35:29.715696 52522 pruning_runner.cpp:264] Compression complete 99.8814%
I0122 16:35:30.354516 52522 pruning_runner.cpp:264] Compression complete 99.9407%
I0122 16:35:30.981983 52522 pruning_runner.cpp:264] Compression complete 99.9703%
I0122 16:35:31.612960 52522 pruning_runner.cpp:264] Compression complete 99.9926%
I0122 16:35:32.235760 52522 pruning_runner.cpp:264] Compression complete 99.9963%
I0122 16:35:32.867239 52522 pruning_runner.cpp:264] Compression complete 99.9981%
I0122 16:35:33.493379 52522 pruning_runner.cpp:264] Compression complete 99.9991%
I0122 16:35:34.106838 52522 pruning_runner.cpp:264] Compression complete 99.9995%
I0122 16:35:34.725234 52522 pruning_runner.cpp:264] Compression complete 99.9998%
I0122 16:35:35.356479 52522 pruning_runner.cpp:264] Compression complete 100%
I0122 16:35:35.991945 52522 pruning_runner.cpp:264] Compression complete 100%
I0122 16:35:36.614621 52522 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:35:36.614967 52522 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:35:36.615407 52522 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:35:36.615645 52522 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:35:36.615792 52522 layer_factory.hpp:77] Creating layer data
I0122 16:35:36.615839 52522 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:36.616255 52522 net.cpp:94] Creating Layer data
I0122 16:35:36.616263 52522 net.cpp:409] data -> data
I0122 16:35:36.616273 52522 net.cpp:409] data -> label
I0122 16:35:36.617218 54549 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:35:36.617249 54549 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:35:36.617323 52522 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:35:36.617425 52522 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:35:36.620630 52522 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:36.620678 52522 net.cpp:144] Setting up data
I0122 16:35:36.620687 52522 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:35:36.620692 52522 net.cpp:151] Top shape: 50 (50)
I0122 16:35:36.620697 52522 net.cpp:159] Memory required for data: 614600
I0122 16:35:36.620700 52522 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:35:36.620709 52522 net.cpp:94] Creating Layer label_data_1_split
I0122 16:35:36.620714 52522 net.cpp:435] label_data_1_split <- label
I0122 16:35:36.620723 52522 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:35:36.620734 52522 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:35:36.620744 52522 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:35:36.620857 52522 net.cpp:144] Setting up label_data_1_split
I0122 16:35:36.620867 52522 net.cpp:151] Top shape: 50 (50)
I0122 16:35:36.620870 52522 net.cpp:151] Top shape: 50 (50)
I0122 16:35:36.620874 52522 net.cpp:151] Top shape: 50 (50)
I0122 16:35:36.620877 52522 net.cpp:159] Memory required for data: 615200
I0122 16:35:36.620882 52522 layer_factory.hpp:77] Creating layer conv1
I0122 16:35:36.620893 52522 net.cpp:94] Creating Layer conv1
I0122 16:35:36.620898 52522 net.cpp:435] conv1 <- data
I0122 16:35:36.620904 52522 net.cpp:409] conv1 -> conv1
I0122 16:35:36.622035 52522 net.cpp:144] Setting up conv1
I0122 16:35:36.622047 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.622051 52522 net.cpp:159] Memory required for data: 7168800
I0122 16:35:36.622063 52522 layer_factory.hpp:77] Creating layer bn1
I0122 16:35:36.622072 52522 net.cpp:94] Creating Layer bn1
I0122 16:35:36.622076 52522 net.cpp:435] bn1 <- conv1
I0122 16:35:36.622083 52522 net.cpp:409] bn1 -> scale1
I0122 16:35:36.622830 52522 net.cpp:144] Setting up bn1
I0122 16:35:36.622839 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.622843 52522 net.cpp:159] Memory required for data: 13722400
I0122 16:35:36.622858 52522 layer_factory.hpp:77] Creating layer relu1
I0122 16:35:36.622865 52522 net.cpp:94] Creating Layer relu1
I0122 16:35:36.622870 52522 net.cpp:435] relu1 <- scale1
I0122 16:35:36.622875 52522 net.cpp:409] relu1 -> relu1
I0122 16:35:36.622898 52522 net.cpp:144] Setting up relu1
I0122 16:35:36.622905 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.622908 52522 net.cpp:159] Memory required for data: 20276000
I0122 16:35:36.622911 52522 layer_factory.hpp:77] Creating layer conv2
I0122 16:35:36.622921 52522 net.cpp:94] Creating Layer conv2
I0122 16:35:36.622925 52522 net.cpp:435] conv2 <- relu1
I0122 16:35:36.622932 52522 net.cpp:409] conv2 -> conv2
I0122 16:35:36.624114 52522 net.cpp:144] Setting up conv2
I0122 16:35:36.624125 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.624126 52522 net.cpp:159] Memory required for data: 26829600
I0122 16:35:36.624135 52522 layer_factory.hpp:77] Creating layer bn2
I0122 16:35:36.624142 52522 net.cpp:94] Creating Layer bn2
I0122 16:35:36.624145 52522 net.cpp:435] bn2 <- conv2
I0122 16:35:36.624150 52522 net.cpp:409] bn2 -> scale2
I0122 16:35:36.624811 52522 net.cpp:144] Setting up bn2
I0122 16:35:36.624817 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.624820 52522 net.cpp:159] Memory required for data: 33383200
I0122 16:35:36.624840 52522 layer_factory.hpp:77] Creating layer relu2
I0122 16:35:36.624845 52522 net.cpp:94] Creating Layer relu2
I0122 16:35:36.624847 52522 net.cpp:435] relu2 <- scale2
I0122 16:35:36.624852 52522 net.cpp:409] relu2 -> relu2
I0122 16:35:36.624889 52522 net.cpp:144] Setting up relu2
I0122 16:35:36.624895 52522 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:36.624898 52522 net.cpp:159] Memory required for data: 39936800
I0122 16:35:36.624900 52522 layer_factory.hpp:77] Creating layer pool1
I0122 16:35:36.624907 52522 net.cpp:94] Creating Layer pool1
I0122 16:35:36.624909 52522 net.cpp:435] pool1 <- relu2
I0122 16:35:36.624913 52522 net.cpp:409] pool1 -> pool1
I0122 16:35:36.624956 52522 net.cpp:144] Setting up pool1
I0122 16:35:36.624963 52522 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:35:36.624965 52522 net.cpp:159] Memory required for data: 41575200
I0122 16:35:36.624969 52522 layer_factory.hpp:77] Creating layer drop1
I0122 16:35:36.624972 52522 net.cpp:94] Creating Layer drop1
I0122 16:35:36.624975 52522 net.cpp:435] drop1 <- pool1
I0122 16:35:36.624980 52522 net.cpp:409] drop1 -> drop1
I0122 16:35:36.625005 52522 net.cpp:144] Setting up drop1
I0122 16:35:36.625010 52522 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:35:36.625012 52522 net.cpp:159] Memory required for data: 43213600
I0122 16:35:36.625015 52522 layer_factory.hpp:77] Creating layer conv3
I0122 16:35:36.625023 52522 net.cpp:94] Creating Layer conv3
I0122 16:35:36.625028 52522 net.cpp:435] conv3 <- drop1
I0122 16:35:36.625033 52522 net.cpp:409] conv3 -> conv3
I0122 16:35:36.626067 52522 net.cpp:144] Setting up conv3
I0122 16:35:36.626080 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.626082 52522 net.cpp:159] Memory required for data: 46490400
I0122 16:35:36.626088 52522 layer_factory.hpp:77] Creating layer bn3
I0122 16:35:36.626097 52522 net.cpp:94] Creating Layer bn3
I0122 16:35:36.626101 52522 net.cpp:435] bn3 <- conv3
I0122 16:35:36.626107 52522 net.cpp:409] bn3 -> scale3
I0122 16:35:36.626761 52522 net.cpp:144] Setting up bn3
I0122 16:35:36.626768 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.626772 52522 net.cpp:159] Memory required for data: 49767200
I0122 16:35:36.626783 52522 layer_factory.hpp:77] Creating layer relu3
I0122 16:35:36.626790 52522 net.cpp:94] Creating Layer relu3
I0122 16:35:36.626792 52522 net.cpp:435] relu3 <- scale3
I0122 16:35:36.626797 52522 net.cpp:409] relu3 -> relu3
I0122 16:35:36.626814 52522 net.cpp:144] Setting up relu3
I0122 16:35:36.626821 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.626824 52522 net.cpp:159] Memory required for data: 53044000
I0122 16:35:36.626827 52522 layer_factory.hpp:77] Creating layer conv4
I0122 16:35:36.626834 52522 net.cpp:94] Creating Layer conv4
I0122 16:35:36.626837 52522 net.cpp:435] conv4 <- relu3
I0122 16:35:36.626842 52522 net.cpp:409] conv4 -> conv4
I0122 16:35:36.627332 52522 net.cpp:144] Setting up conv4
I0122 16:35:36.627341 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.627343 52522 net.cpp:159] Memory required for data: 56320800
I0122 16:35:36.627348 52522 layer_factory.hpp:77] Creating layer bn4
I0122 16:35:36.627354 52522 net.cpp:94] Creating Layer bn4
I0122 16:35:36.627357 52522 net.cpp:435] bn4 <- conv4
I0122 16:35:36.627362 52522 net.cpp:409] bn4 -> scale4
I0122 16:35:36.628021 52522 net.cpp:144] Setting up bn4
I0122 16:35:36.628026 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.628029 52522 net.cpp:159] Memory required for data: 59597600
I0122 16:35:36.628036 52522 layer_factory.hpp:77] Creating layer relu4
I0122 16:35:36.628041 52522 net.cpp:94] Creating Layer relu4
I0122 16:35:36.628044 52522 net.cpp:435] relu4 <- scale4
I0122 16:35:36.628048 52522 net.cpp:409] relu4 -> relu4
I0122 16:35:36.628082 52522 net.cpp:144] Setting up relu4
I0122 16:35:36.628089 52522 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:36.628091 52522 net.cpp:159] Memory required for data: 62874400
I0122 16:35:36.628108 52522 layer_factory.hpp:77] Creating layer pool2
I0122 16:35:36.628113 52522 net.cpp:94] Creating Layer pool2
I0122 16:35:36.628116 52522 net.cpp:435] pool2 <- relu4
I0122 16:35:36.628123 52522 net.cpp:409] pool2 -> pool2
I0122 16:35:36.628211 52522 net.cpp:144] Setting up pool2
I0122 16:35:36.628217 52522 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:35:36.628221 52522 net.cpp:159] Memory required for data: 63693600
I0122 16:35:36.628222 52522 layer_factory.hpp:77] Creating layer drop2
I0122 16:35:36.628228 52522 net.cpp:94] Creating Layer drop2
I0122 16:35:36.628232 52522 net.cpp:435] drop2 <- pool2
I0122 16:35:36.628235 52522 net.cpp:409] drop2 -> drop2
I0122 16:35:36.628262 52522 net.cpp:144] Setting up drop2
I0122 16:35:36.628268 52522 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:35:36.628270 52522 net.cpp:159] Memory required for data: 64512800
I0122 16:35:36.628273 52522 layer_factory.hpp:77] Creating layer fc1
I0122 16:35:36.628279 52522 net.cpp:94] Creating Layer fc1
I0122 16:35:36.628283 52522 net.cpp:435] fc1 <- drop2
I0122 16:35:36.628288 52522 net.cpp:409] fc1 -> fc1
I0122 16:35:36.642514 52522 net.cpp:144] Setting up fc1
I0122 16:35:36.642531 52522 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:36.642534 52522 net.cpp:159] Memory required for data: 64615200
I0122 16:35:36.642542 52522 layer_factory.hpp:77] Creating layer bn5
I0122 16:35:36.642551 52522 net.cpp:94] Creating Layer bn5
I0122 16:35:36.642556 52522 net.cpp:435] bn5 <- fc1
I0122 16:35:36.642562 52522 net.cpp:409] bn5 -> scale5
I0122 16:35:36.643107 52522 net.cpp:144] Setting up bn5
I0122 16:35:36.643115 52522 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:36.643117 52522 net.cpp:159] Memory required for data: 64717600
I0122 16:35:36.643129 52522 layer_factory.hpp:77] Creating layer relu5
I0122 16:35:36.643136 52522 net.cpp:94] Creating Layer relu5
I0122 16:35:36.643139 52522 net.cpp:435] relu5 <- scale5
I0122 16:35:36.643144 52522 net.cpp:409] relu5 -> relu5
I0122 16:35:36.643162 52522 net.cpp:144] Setting up relu5
I0122 16:35:36.643167 52522 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:36.643170 52522 net.cpp:159] Memory required for data: 64820000
I0122 16:35:36.643172 52522 layer_factory.hpp:77] Creating layer drop3
I0122 16:35:36.643177 52522 net.cpp:94] Creating Layer drop3
I0122 16:35:36.643182 52522 net.cpp:435] drop3 <- relu5
I0122 16:35:36.643187 52522 net.cpp:409] drop3 -> drop3
I0122 16:35:36.643213 52522 net.cpp:144] Setting up drop3
I0122 16:35:36.643218 52522 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:36.643220 52522 net.cpp:159] Memory required for data: 64922400
I0122 16:35:36.643224 52522 layer_factory.hpp:77] Creating layer fc2
I0122 16:35:36.643230 52522 net.cpp:94] Creating Layer fc2
I0122 16:35:36.643234 52522 net.cpp:435] fc2 <- drop3
I0122 16:35:36.643239 52522 net.cpp:409] fc2 -> fc2
I0122 16:35:36.643370 52522 net.cpp:144] Setting up fc2
I0122 16:35:36.643376 52522 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:36.643378 52522 net.cpp:159] Memory required for data: 64924400
I0122 16:35:36.643383 52522 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:35:36.643389 52522 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:35:36.643391 52522 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:35:36.643396 52522 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:35:36.643402 52522 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:35:36.643411 52522 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:35:36.643446 52522 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:35:36.643451 52522 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:36.643455 52522 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:36.643458 52522 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:36.643460 52522 net.cpp:159] Memory required for data: 64930400
I0122 16:35:36.643463 52522 layer_factory.hpp:77] Creating layer loss
I0122 16:35:36.643468 52522 net.cpp:94] Creating Layer loss
I0122 16:35:36.643472 52522 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:35:36.643476 52522 net.cpp:435] loss <- label_data_1_split_0
I0122 16:35:36.643494 52522 net.cpp:409] loss -> loss
I0122 16:35:36.643502 52522 layer_factory.hpp:77] Creating layer loss
I0122 16:35:36.643571 52522 net.cpp:144] Setting up loss
I0122 16:35:36.643576 52522 net.cpp:151] Top shape: (1)
I0122 16:35:36.643579 52522 net.cpp:154]     with loss weight 1
I0122 16:35:36.643589 52522 net.cpp:159] Memory required for data: 64930404
I0122 16:35:36.643591 52522 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:35:36.643597 52522 net.cpp:94] Creating Layer accuracy-top1
I0122 16:35:36.643600 52522 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:35:36.643604 52522 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:35:36.643610 52522 net.cpp:409] accuracy-top1 -> top-1
I0122 16:35:36.643617 52522 net.cpp:144] Setting up accuracy-top1
I0122 16:35:36.643622 52522 net.cpp:151] Top shape: (1)
I0122 16:35:36.643625 52522 net.cpp:159] Memory required for data: 64930408
I0122 16:35:36.643627 52522 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:35:36.643632 52522 net.cpp:94] Creating Layer accuracy-top5
I0122 16:35:36.643635 52522 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:35:36.643638 52522 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:35:36.643643 52522 net.cpp:409] accuracy-top5 -> top-5
I0122 16:35:36.643649 52522 net.cpp:144] Setting up accuracy-top5
I0122 16:35:36.643652 52522 net.cpp:151] Top shape: (1)
I0122 16:35:36.643654 52522 net.cpp:159] Memory required for data: 64930412
I0122 16:35:36.643658 52522 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:35:36.643661 52522 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:35:36.643666 52522 net.cpp:220] loss needs backward computation.
I0122 16:35:36.643669 52522 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:35:36.643672 52522 net.cpp:220] fc2 needs backward computation.
I0122 16:35:36.643676 52522 net.cpp:220] drop3 needs backward computation.
I0122 16:35:36.643679 52522 net.cpp:220] relu5 needs backward computation.
I0122 16:35:36.643682 52522 net.cpp:220] bn5 needs backward computation.
I0122 16:35:36.643685 52522 net.cpp:220] fc1 needs backward computation.
I0122 16:35:36.643688 52522 net.cpp:220] drop2 needs backward computation.
I0122 16:35:36.643692 52522 net.cpp:220] pool2 needs backward computation.
I0122 16:35:36.643694 52522 net.cpp:220] relu4 needs backward computation.
I0122 16:35:36.643698 52522 net.cpp:220] bn4 needs backward computation.
I0122 16:35:36.643702 52522 net.cpp:220] conv4 needs backward computation.
I0122 16:35:36.643704 52522 net.cpp:220] relu3 needs backward computation.
I0122 16:35:36.643707 52522 net.cpp:220] bn3 needs backward computation.
I0122 16:35:36.643710 52522 net.cpp:220] conv3 needs backward computation.
I0122 16:35:36.643713 52522 net.cpp:220] drop1 needs backward computation.
I0122 16:35:36.643716 52522 net.cpp:220] pool1 needs backward computation.
I0122 16:35:36.643719 52522 net.cpp:220] relu2 needs backward computation.
I0122 16:35:36.643723 52522 net.cpp:220] bn2 needs backward computation.
I0122 16:35:36.643725 52522 net.cpp:220] conv2 needs backward computation.
I0122 16:35:36.643728 52522 net.cpp:220] relu1 needs backward computation.
I0122 16:35:36.643731 52522 net.cpp:220] bn1 needs backward computation.
I0122 16:35:36.643735 52522 net.cpp:220] conv1 needs backward computation.
I0122 16:35:36.643738 52522 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:35:36.643743 52522 net.cpp:222] data does not need backward computation.
I0122 16:35:36.643744 52522 net.cpp:264] This network produces output loss
I0122 16:35:36.643748 52522 net.cpp:264] This network produces output top-1
I0122 16:35:36.643751 52522 net.cpp:264] This network produces output top-5
I0122 16:35:36.643772 52522 net.cpp:284] Network initialization done.
I0122 16:35:36.646807 52522 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:35:36.653259 52522 caffe_interface.cpp:125] Batch 0, loss = 0.741704
I0122 16:35:36.653278 52522 caffe_interface.cpp:125] Batch 0, top-1 = 0.72
I0122 16:35:36.653296 52522 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:35:36.654592 52522 caffe_interface.cpp:125] Batch 1, loss = 0.688106
I0122 16:35:36.654603 52522 caffe_interface.cpp:125] Batch 1, top-1 = 0.76
I0122 16:35:36.654605 52522 caffe_interface.cpp:125] Batch 1, top-5 = 1
I0122 16:35:36.655901 52522 caffe_interface.cpp:125] Batch 2, loss = 0.602382
I0122 16:35:36.655910 52522 caffe_interface.cpp:125] Batch 2, top-1 = 0.82
I0122 16:35:36.655913 52522 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:35:36.657181 52522 caffe_interface.cpp:125] Batch 3, loss = 1.18797
I0122 16:35:36.657187 52522 caffe_interface.cpp:125] Batch 3, top-1 = 0.78
I0122 16:35:36.657191 52522 caffe_interface.cpp:125] Batch 3, top-5 = 0.98
I0122 16:35:36.658473 52522 caffe_interface.cpp:125] Batch 4, loss = 0.482092
I0122 16:35:36.658480 52522 caffe_interface.cpp:125] Batch 4, top-1 = 0.84
I0122 16:35:36.658484 52522 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0122 16:35:36.659752 52522 caffe_interface.cpp:125] Batch 5, loss = 0.95857
I0122 16:35:36.659760 52522 caffe_interface.cpp:125] Batch 5, top-1 = 0.76
I0122 16:35:36.659763 52522 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0122 16:35:36.661022 52522 caffe_interface.cpp:125] Batch 6, loss = 0.891622
I0122 16:35:36.661029 52522 caffe_interface.cpp:125] Batch 6, top-1 = 0.74
I0122 16:35:36.661034 52522 caffe_interface.cpp:125] Batch 6, top-5 = 1
I0122 16:35:36.662307 52522 caffe_interface.cpp:125] Batch 7, loss = 0.700238
I0122 16:35:36.662314 52522 caffe_interface.cpp:125] Batch 7, top-1 = 0.8
I0122 16:35:36.662317 52522 caffe_interface.cpp:125] Batch 7, top-5 = 1
I0122 16:35:36.663594 52522 caffe_interface.cpp:125] Batch 8, loss = 0.430974
I0122 16:35:36.663600 52522 caffe_interface.cpp:125] Batch 8, top-1 = 0.84
I0122 16:35:36.663604 52522 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:35:36.664878 52522 caffe_interface.cpp:125] Batch 9, loss = 1.01972
I0122 16:35:36.664886 52522 caffe_interface.cpp:125] Batch 9, top-1 = 0.74
I0122 16:35:36.664891 52522 caffe_interface.cpp:125] Batch 9, top-5 = 0.96
I0122 16:35:36.666159 52522 caffe_interface.cpp:125] Batch 10, loss = 0.437392
I0122 16:35:36.666167 52522 caffe_interface.cpp:125] Batch 10, top-1 = 0.86
I0122 16:35:36.666172 52522 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:35:36.667451 52522 caffe_interface.cpp:125] Batch 11, loss = 1.00946
I0122 16:35:36.667459 52522 caffe_interface.cpp:125] Batch 11, top-1 = 0.76
I0122 16:35:36.667462 52522 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:35:36.668750 52522 caffe_interface.cpp:125] Batch 12, loss = 0.88248
I0122 16:35:36.668759 52522 caffe_interface.cpp:125] Batch 12, top-1 = 0.74
I0122 16:35:36.668762 52522 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:35:36.670049 52522 caffe_interface.cpp:125] Batch 13, loss = 1.14056
I0122 16:35:36.670058 52522 caffe_interface.cpp:125] Batch 13, top-1 = 0.76
I0122 16:35:36.670060 52522 caffe_interface.cpp:125] Batch 13, top-5 = 0.94
I0122 16:35:36.671332 52522 caffe_interface.cpp:125] Batch 14, loss = 0.444633
I0122 16:35:36.671339 52522 caffe_interface.cpp:125] Batch 14, top-1 = 0.92
I0122 16:35:36.671344 52522 caffe_interface.cpp:125] Batch 14, top-5 = 1
I0122 16:35:36.672632 52522 caffe_interface.cpp:125] Batch 15, loss = 0.726758
I0122 16:35:36.672639 52522 caffe_interface.cpp:125] Batch 15, top-1 = 0.74
I0122 16:35:36.672643 52522 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:35:36.673947 52522 caffe_interface.cpp:125] Batch 16, loss = 0.798977
I0122 16:35:36.673955 52522 caffe_interface.cpp:125] Batch 16, top-1 = 0.76
I0122 16:35:36.673959 52522 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0122 16:35:36.675232 52522 caffe_interface.cpp:125] Batch 17, loss = 0.697463
I0122 16:35:36.675240 52522 caffe_interface.cpp:125] Batch 17, top-1 = 0.8
I0122 16:35:36.675243 52522 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0122 16:35:36.676508 52522 caffe_interface.cpp:125] Batch 18, loss = 1.33379
I0122 16:35:36.676517 52522 caffe_interface.cpp:125] Batch 18, top-1 = 0.66
I0122 16:35:36.676530 52522 caffe_interface.cpp:125] Batch 18, top-5 = 0.94
I0122 16:35:36.678939 52522 caffe_interface.cpp:125] Batch 19, loss = 0.36036
I0122 16:35:36.678947 52522 caffe_interface.cpp:125] Batch 19, top-1 = 0.9
I0122 16:35:36.678951 52522 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:35:36.680366 52522 caffe_interface.cpp:125] Batch 20, loss = 1.04705
I0122 16:35:36.680374 52522 caffe_interface.cpp:125] Batch 20, top-1 = 0.64
I0122 16:35:36.680378 52522 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0122 16:35:36.681846 52522 caffe_interface.cpp:125] Batch 21, loss = 0.925264
I0122 16:35:36.681859 52522 caffe_interface.cpp:125] Batch 21, top-1 = 0.76
I0122 16:35:36.681862 52522 caffe_interface.cpp:125] Batch 21, top-5 = 1
I0122 16:35:36.683149 52522 caffe_interface.cpp:125] Batch 22, loss = 0.626002
I0122 16:35:36.683157 52522 caffe_interface.cpp:125] Batch 22, top-1 = 0.84
I0122 16:35:36.683161 52522 caffe_interface.cpp:125] Batch 22, top-5 = 1
I0122 16:35:36.684458 52522 caffe_interface.cpp:125] Batch 23, loss = 0.512818
I0122 16:35:36.684464 52522 caffe_interface.cpp:125] Batch 23, top-1 = 0.84
I0122 16:35:36.684468 52522 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:35:36.685752 52522 caffe_interface.cpp:125] Batch 24, loss = 0.952627
I0122 16:35:36.685760 52522 caffe_interface.cpp:125] Batch 24, top-1 = 0.74
I0122 16:35:36.685762 52522 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0122 16:35:36.687032 52522 caffe_interface.cpp:125] Batch 25, loss = 0.93569
I0122 16:35:36.687041 52522 caffe_interface.cpp:125] Batch 25, top-1 = 0.78
I0122 16:35:36.687044 52522 caffe_interface.cpp:125] Batch 25, top-5 = 0.98
I0122 16:35:36.688347 52522 caffe_interface.cpp:125] Batch 26, loss = 0.808998
I0122 16:35:36.688360 52522 caffe_interface.cpp:125] Batch 26, top-1 = 0.76
I0122 16:35:36.688364 52522 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0122 16:35:36.689651 52522 caffe_interface.cpp:125] Batch 27, loss = 0.78703
I0122 16:35:36.689657 52522 caffe_interface.cpp:125] Batch 27, top-1 = 0.78
I0122 16:35:36.689661 52522 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:35:36.690942 52522 caffe_interface.cpp:125] Batch 28, loss = 0.864742
I0122 16:35:36.690949 52522 caffe_interface.cpp:125] Batch 28, top-1 = 0.76
I0122 16:35:36.690953 52522 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:35:36.692220 52522 caffe_interface.cpp:125] Batch 29, loss = 0.776883
I0122 16:35:36.692229 52522 caffe_interface.cpp:125] Batch 29, top-1 = 0.84
I0122 16:35:36.692231 52522 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:35:36.693521 52522 caffe_interface.cpp:125] Batch 30, loss = 0.620929
I0122 16:35:36.693529 52522 caffe_interface.cpp:125] Batch 30, top-1 = 0.76
I0122 16:35:36.693533 52522 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:35:36.694808 52522 caffe_interface.cpp:125] Batch 31, loss = 0.865766
I0122 16:35:36.694815 52522 caffe_interface.cpp:125] Batch 31, top-1 = 0.76
I0122 16:35:36.694818 52522 caffe_interface.cpp:125] Batch 31, top-5 = 0.96
I0122 16:35:36.696089 52522 caffe_interface.cpp:125] Batch 32, loss = 0.730699
I0122 16:35:36.696096 52522 caffe_interface.cpp:125] Batch 32, top-1 = 0.82
I0122 16:35:36.696099 52522 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:35:36.697368 52522 caffe_interface.cpp:125] Batch 33, loss = 1.1202
I0122 16:35:36.697376 52522 caffe_interface.cpp:125] Batch 33, top-1 = 0.72
I0122 16:35:36.697379 52522 caffe_interface.cpp:125] Batch 33, top-5 = 0.98
I0122 16:35:36.698907 52522 caffe_interface.cpp:125] Batch 34, loss = 0.846316
I0122 16:35:36.698915 52522 caffe_interface.cpp:125] Batch 34, top-1 = 0.78
I0122 16:35:36.698920 52522 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0122 16:35:36.700187 52522 caffe_interface.cpp:125] Batch 35, loss = 0.797248
I0122 16:35:36.700196 52522 caffe_interface.cpp:125] Batch 35, top-1 = 0.8
I0122 16:35:36.700197 52522 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0122 16:35:36.701469 52522 caffe_interface.cpp:125] Batch 36, loss = 0.668645
I0122 16:35:36.701488 52522 caffe_interface.cpp:125] Batch 36, top-1 = 0.78
I0122 16:35:36.701491 52522 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:35:36.702769 52522 caffe_interface.cpp:125] Batch 37, loss = 0.967152
I0122 16:35:36.702777 52522 caffe_interface.cpp:125] Batch 37, top-1 = 0.7
I0122 16:35:36.702780 52522 caffe_interface.cpp:125] Batch 37, top-5 = 1
I0122 16:35:36.704075 52522 caffe_interface.cpp:125] Batch 38, loss = 1.11926
I0122 16:35:36.704087 52522 caffe_interface.cpp:125] Batch 38, top-1 = 0.72
I0122 16:35:36.704092 52522 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0122 16:35:36.705374 52522 caffe_interface.cpp:125] Batch 39, loss = 0.653402
I0122 16:35:36.705382 52522 caffe_interface.cpp:125] Batch 39, top-1 = 0.8
I0122 16:35:36.705385 52522 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:35:36.706662 52522 caffe_interface.cpp:125] Batch 40, loss = 0.499109
I0122 16:35:36.706670 52522 caffe_interface.cpp:125] Batch 40, top-1 = 0.82
I0122 16:35:36.706674 52522 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:35:36.707947 52522 caffe_interface.cpp:125] Batch 41, loss = 1.62656
I0122 16:35:36.707953 52522 caffe_interface.cpp:125] Batch 41, top-1 = 0.72
I0122 16:35:36.707957 52522 caffe_interface.cpp:125] Batch 41, top-5 = 0.98
I0122 16:35:36.709224 52522 caffe_interface.cpp:125] Batch 42, loss = 1.24684
I0122 16:35:36.709235 52522 caffe_interface.cpp:125] Batch 42, top-1 = 0.68
I0122 16:35:36.709239 52522 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0122 16:35:36.710507 52522 caffe_interface.cpp:125] Batch 43, loss = 0.904974
I0122 16:35:36.710515 52522 caffe_interface.cpp:125] Batch 43, top-1 = 0.74
I0122 16:35:36.710520 52522 caffe_interface.cpp:125] Batch 43, top-5 = 0.98
I0122 16:35:36.711793 52522 caffe_interface.cpp:125] Batch 44, loss = 0.693742
I0122 16:35:36.711800 52522 caffe_interface.cpp:125] Batch 44, top-1 = 0.76
I0122 16:35:36.711802 52522 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0122 16:35:36.714090 52522 caffe_interface.cpp:125] Batch 45, loss = 0.661503
I0122 16:35:36.714098 52522 caffe_interface.cpp:125] Batch 45, top-1 = 0.82
I0122 16:35:36.714102 52522 caffe_interface.cpp:125] Batch 45, top-5 = 1
I0122 16:35:36.715471 52522 caffe_interface.cpp:125] Batch 46, loss = 0.628309
I0122 16:35:36.715481 52522 caffe_interface.cpp:125] Batch 46, top-1 = 0.82
I0122 16:35:36.715484 52522 caffe_interface.cpp:125] Batch 46, top-5 = 0.98
I0122 16:35:36.716945 52522 caffe_interface.cpp:125] Batch 47, loss = 0.702303
I0122 16:35:36.716962 52522 caffe_interface.cpp:125] Batch 47, top-1 = 0.84
I0122 16:35:36.716965 52522 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0122 16:35:36.718235 52522 caffe_interface.cpp:125] Batch 48, loss = 0.831782
I0122 16:35:36.718243 52522 caffe_interface.cpp:125] Batch 48, top-1 = 0.8
I0122 16:35:36.718245 52522 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:35:36.719516 52522 caffe_interface.cpp:125] Batch 49, loss = 1.21701
I0122 16:35:36.719524 52522 caffe_interface.cpp:125] Batch 49, top-1 = 0.64
I0122 16:35:36.719527 52522 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0122 16:35:36.720804 52522 caffe_interface.cpp:125] Batch 50, loss = 0.704749
I0122 16:35:36.720811 52522 caffe_interface.cpp:125] Batch 50, top-1 = 0.78
I0122 16:35:36.720815 52522 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:35:36.722110 52522 caffe_interface.cpp:125] Batch 51, loss = 0.448838
I0122 16:35:36.722127 52522 caffe_interface.cpp:125] Batch 51, top-1 = 0.84
I0122 16:35:36.722131 52522 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0122 16:35:36.723399 52522 caffe_interface.cpp:125] Batch 52, loss = 0.994381
I0122 16:35:36.723407 52522 caffe_interface.cpp:125] Batch 52, top-1 = 0.72
I0122 16:35:36.723409 52522 caffe_interface.cpp:125] Batch 52, top-5 = 0.94
I0122 16:35:36.724690 52522 caffe_interface.cpp:125] Batch 53, loss = 0.87132
I0122 16:35:36.724706 52522 caffe_interface.cpp:125] Batch 53, top-1 = 0.76
I0122 16:35:36.724709 52522 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0122 16:35:36.725986 52522 caffe_interface.cpp:125] Batch 54, loss = 0.603566
I0122 16:35:36.726003 52522 caffe_interface.cpp:125] Batch 54, top-1 = 0.82
I0122 16:35:36.726007 52522 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0122 16:35:36.727269 52522 caffe_interface.cpp:125] Batch 55, loss = 0.828643
I0122 16:35:36.727277 52522 caffe_interface.cpp:125] Batch 55, top-1 = 0.72
I0122 16:35:36.727279 52522 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:35:36.728540 52522 caffe_interface.cpp:125] Batch 56, loss = 0.724449
I0122 16:35:36.728549 52522 caffe_interface.cpp:125] Batch 56, top-1 = 0.74
I0122 16:35:36.728550 52522 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0122 16:35:36.729822 52522 caffe_interface.cpp:125] Batch 57, loss = 0.615419
I0122 16:35:36.729830 52522 caffe_interface.cpp:125] Batch 57, top-1 = 0.76
I0122 16:35:36.729832 52522 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:35:36.731312 52522 caffe_interface.cpp:125] Batch 58, loss = 0.985772
I0122 16:35:36.731320 52522 caffe_interface.cpp:125] Batch 58, top-1 = 0.68
I0122 16:35:36.731323 52522 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0122 16:35:36.732590 52522 caffe_interface.cpp:125] Batch 59, loss = 0.597058
I0122 16:35:36.732597 52522 caffe_interface.cpp:125] Batch 59, top-1 = 0.8
I0122 16:35:36.732600 52522 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:35:36.733887 52522 caffe_interface.cpp:125] Batch 60, loss = 0.715384
I0122 16:35:36.733894 52522 caffe_interface.cpp:125] Batch 60, top-1 = 0.78
I0122 16:35:36.733897 52522 caffe_interface.cpp:125] Batch 60, top-5 = 0.98
I0122 16:35:36.735199 52522 caffe_interface.cpp:125] Batch 61, loss = 1.50405
I0122 16:35:36.735208 52522 caffe_interface.cpp:125] Batch 61, top-1 = 0.68
I0122 16:35:36.735210 52522 caffe_interface.cpp:125] Batch 61, top-5 = 0.94
I0122 16:35:36.736490 52522 caffe_interface.cpp:125] Batch 62, loss = 0.535276
I0122 16:35:36.736497 52522 caffe_interface.cpp:125] Batch 62, top-1 = 0.78
I0122 16:35:36.736501 52522 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:35:36.737766 52522 caffe_interface.cpp:125] Batch 63, loss = 0.878042
I0122 16:35:36.737773 52522 caffe_interface.cpp:125] Batch 63, top-1 = 0.7
I0122 16:35:36.737776 52522 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0122 16:35:36.739049 52522 caffe_interface.cpp:125] Batch 64, loss = 0.650734
I0122 16:35:36.739056 52522 caffe_interface.cpp:125] Batch 64, top-1 = 0.8
I0122 16:35:36.739060 52522 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:35:36.740329 52522 caffe_interface.cpp:125] Batch 65, loss = 0.851382
I0122 16:35:36.740336 52522 caffe_interface.cpp:125] Batch 65, top-1 = 0.8
I0122 16:35:36.740339 52522 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:35:36.741598 52522 caffe_interface.cpp:125] Batch 66, loss = 0.847648
I0122 16:35:36.741605 52522 caffe_interface.cpp:125] Batch 66, top-1 = 0.78
I0122 16:35:36.741609 52522 caffe_interface.cpp:125] Batch 66, top-5 = 0.98
I0122 16:35:36.742872 52522 caffe_interface.cpp:125] Batch 67, loss = 1.41474
I0122 16:35:36.742879 52522 caffe_interface.cpp:125] Batch 67, top-1 = 0.7
I0122 16:35:36.742883 52522 caffe_interface.cpp:125] Batch 67, top-5 = 0.94
I0122 16:35:36.744149 52522 caffe_interface.cpp:125] Batch 68, loss = 0.468069
I0122 16:35:36.744156 52522 caffe_interface.cpp:125] Batch 68, top-1 = 0.84
I0122 16:35:36.744159 52522 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:35:36.745432 52522 caffe_interface.cpp:125] Batch 69, loss = 0.710708
I0122 16:35:36.745448 52522 caffe_interface.cpp:125] Batch 69, top-1 = 0.78
I0122 16:35:36.745451 52522 caffe_interface.cpp:125] Batch 69, top-5 = 0.98
I0122 16:35:36.747747 52522 caffe_interface.cpp:125] Batch 70, loss = 0.689413
I0122 16:35:36.747754 52522 caffe_interface.cpp:125] Batch 70, top-1 = 0.84
I0122 16:35:36.747757 52522 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0122 16:35:36.749074 52522 caffe_interface.cpp:125] Batch 71, loss = 0.698956
I0122 16:35:36.749083 52522 caffe_interface.cpp:125] Batch 71, top-1 = 0.72
I0122 16:35:36.749085 52522 caffe_interface.cpp:125] Batch 71, top-5 = 0.98
I0122 16:35:36.750519 52522 caffe_interface.cpp:125] Batch 72, loss = 0.796508
I0122 16:35:36.750546 52522 caffe_interface.cpp:125] Batch 72, top-1 = 0.8
I0122 16:35:36.750550 52522 caffe_interface.cpp:125] Batch 72, top-5 = 0.98
I0122 16:35:36.751842 52522 caffe_interface.cpp:125] Batch 73, loss = 0.564491
I0122 16:35:36.751850 52522 caffe_interface.cpp:125] Batch 73, top-1 = 0.82
I0122 16:35:36.751853 52522 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0122 16:35:36.753113 52522 caffe_interface.cpp:125] Batch 74, loss = 1.13583
I0122 16:35:36.753119 52522 caffe_interface.cpp:125] Batch 74, top-1 = 0.74
I0122 16:35:36.753124 52522 caffe_interface.cpp:125] Batch 74, top-5 = 0.94
I0122 16:35:36.754410 52522 caffe_interface.cpp:125] Batch 75, loss = 0.939023
I0122 16:35:36.754418 52522 caffe_interface.cpp:125] Batch 75, top-1 = 0.74
I0122 16:35:36.754421 52522 caffe_interface.cpp:125] Batch 75, top-5 = 0.98
I0122 16:35:36.755713 52522 caffe_interface.cpp:125] Batch 76, loss = 0.84248
I0122 16:35:36.755722 52522 caffe_interface.cpp:125] Batch 76, top-1 = 0.72
I0122 16:35:36.755725 52522 caffe_interface.cpp:125] Batch 76, top-5 = 0.96
I0122 16:35:36.757009 52522 caffe_interface.cpp:125] Batch 77, loss = 0.549709
I0122 16:35:36.757017 52522 caffe_interface.cpp:125] Batch 77, top-1 = 0.84
I0122 16:35:36.757021 52522 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0122 16:35:36.758292 52522 caffe_interface.cpp:125] Batch 78, loss = 0.893529
I0122 16:35:36.758299 52522 caffe_interface.cpp:125] Batch 78, top-1 = 0.8
I0122 16:35:36.758303 52522 caffe_interface.cpp:125] Batch 78, top-5 = 0.98
I0122 16:35:36.759572 52522 caffe_interface.cpp:125] Batch 79, loss = 0.612108
I0122 16:35:36.759579 52522 caffe_interface.cpp:125] Batch 79, top-1 = 0.86
I0122 16:35:36.759582 52522 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0122 16:35:36.760848 52522 caffe_interface.cpp:125] Batch 80, loss = 0.793488
I0122 16:35:36.760856 52522 caffe_interface.cpp:125] Batch 80, top-1 = 0.74
I0122 16:35:36.760859 52522 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:35:36.762142 52522 caffe_interface.cpp:125] Batch 81, loss = 0.971431
I0122 16:35:36.762152 52522 caffe_interface.cpp:125] Batch 81, top-1 = 0.74
I0122 16:35:36.762156 52522 caffe_interface.cpp:125] Batch 81, top-5 = 1
I0122 16:35:36.763432 52522 caffe_interface.cpp:125] Batch 82, loss = 0.589352
I0122 16:35:36.763438 52522 caffe_interface.cpp:125] Batch 82, top-1 = 0.78
I0122 16:35:36.763442 52522 caffe_interface.cpp:125] Batch 82, top-5 = 1
I0122 16:35:36.764991 52522 caffe_interface.cpp:125] Batch 83, loss = 0.78485
I0122 16:35:36.764997 52522 caffe_interface.cpp:125] Batch 83, top-1 = 0.78
I0122 16:35:36.765000 52522 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:35:36.766264 52522 caffe_interface.cpp:125] Batch 84, loss = 1.22217
I0122 16:35:36.766276 52522 caffe_interface.cpp:125] Batch 84, top-1 = 0.7
I0122 16:35:36.766280 52522 caffe_interface.cpp:125] Batch 84, top-5 = 1
I0122 16:35:36.767556 52522 caffe_interface.cpp:125] Batch 85, loss = 0.617533
I0122 16:35:36.767565 52522 caffe_interface.cpp:125] Batch 85, top-1 = 0.8
I0122 16:35:36.767566 52522 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:35:36.768847 52522 caffe_interface.cpp:125] Batch 86, loss = 0.792675
I0122 16:35:36.768856 52522 caffe_interface.cpp:125] Batch 86, top-1 = 0.76
I0122 16:35:36.768859 52522 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:35:36.770129 52522 caffe_interface.cpp:125] Batch 87, loss = 0.912531
I0122 16:35:36.770138 52522 caffe_interface.cpp:125] Batch 87, top-1 = 0.78
I0122 16:35:36.770141 52522 caffe_interface.cpp:125] Batch 87, top-5 = 1
I0122 16:35:36.771417 52522 caffe_interface.cpp:125] Batch 88, loss = 1.3309
I0122 16:35:36.771425 52522 caffe_interface.cpp:125] Batch 88, top-1 = 0.7
I0122 16:35:36.771428 52522 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0122 16:35:36.772706 52522 caffe_interface.cpp:125] Batch 89, loss = 0.851031
I0122 16:35:36.772714 52522 caffe_interface.cpp:125] Batch 89, top-1 = 0.72
I0122 16:35:36.772717 52522 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0122 16:35:36.773999 52522 caffe_interface.cpp:125] Batch 90, loss = 0.75402
I0122 16:35:36.774019 52522 caffe_interface.cpp:125] Batch 90, top-1 = 0.72
I0122 16:35:36.774022 52522 caffe_interface.cpp:125] Batch 90, top-5 = 1
I0122 16:35:36.775279 52522 caffe_interface.cpp:125] Batch 91, loss = 0.687032
I0122 16:35:36.775286 52522 caffe_interface.cpp:125] Batch 91, top-1 = 0.8
I0122 16:35:36.775290 52522 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0122 16:35:36.776552 52522 caffe_interface.cpp:125] Batch 92, loss = 0.840383
I0122 16:35:36.776559 52522 caffe_interface.cpp:125] Batch 92, top-1 = 0.82
I0122 16:35:36.776563 52522 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:35:36.777843 52522 caffe_interface.cpp:125] Batch 93, loss = 0.91547
I0122 16:35:36.777851 52522 caffe_interface.cpp:125] Batch 93, top-1 = 0.74
I0122 16:35:36.777854 52522 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0122 16:35:36.779145 52522 caffe_interface.cpp:125] Batch 94, loss = 1.36487
I0122 16:35:36.779152 52522 caffe_interface.cpp:125] Batch 94, top-1 = 0.7
I0122 16:35:36.779156 52522 caffe_interface.cpp:125] Batch 94, top-5 = 1
I0122 16:35:36.781437 52522 caffe_interface.cpp:125] Batch 95, loss = 0.842604
I0122 16:35:36.781443 52522 caffe_interface.cpp:125] Batch 95, top-1 = 0.82
I0122 16:35:36.781446 52522 caffe_interface.cpp:125] Batch 95, top-5 = 0.98
I0122 16:35:36.782801 52522 caffe_interface.cpp:125] Batch 96, loss = 0.940611
I0122 16:35:36.782811 52522 caffe_interface.cpp:125] Batch 96, top-1 = 0.76
I0122 16:35:36.782814 52522 caffe_interface.cpp:125] Batch 96, top-5 = 1
I0122 16:35:36.786950 52522 caffe_interface.cpp:125] Batch 97, loss = 0.635677
I0122 16:35:36.786978 52522 caffe_interface.cpp:125] Batch 97, top-1 = 0.8
I0122 16:35:36.786985 52522 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:35:36.788384 52522 caffe_interface.cpp:125] Batch 98, loss = 0.762424
I0122 16:35:36.788401 52522 caffe_interface.cpp:125] Batch 98, top-1 = 0.76
I0122 16:35:36.788408 52522 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:35:36.790146 52522 caffe_interface.cpp:125] Batch 99, loss = 0.884203
I0122 16:35:36.790163 52522 caffe_interface.cpp:125] Batch 99, top-1 = 0.8
I0122 16:35:36.790169 52522 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:35:36.791549 52522 caffe_interface.cpp:125] Batch 100, loss = 0.591777
I0122 16:35:36.791574 52522 caffe_interface.cpp:125] Batch 100, top-1 = 0.84
I0122 16:35:36.791580 52522 caffe_interface.cpp:125] Batch 100, top-5 = 1
I0122 16:35:36.792973 52522 caffe_interface.cpp:125] Batch 101, loss = 0.310457
I0122 16:35:36.792989 52522 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0122 16:35:36.792994 52522 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:35:36.794384 52522 caffe_interface.cpp:125] Batch 102, loss = 0.927598
I0122 16:35:36.794399 52522 caffe_interface.cpp:125] Batch 102, top-1 = 0.74
I0122 16:35:36.794405 52522 caffe_interface.cpp:125] Batch 102, top-5 = 0.96
I0122 16:35:36.795742 52522 caffe_interface.cpp:125] Batch 103, loss = 0.913993
I0122 16:35:36.795758 52522 caffe_interface.cpp:125] Batch 103, top-1 = 0.74
I0122 16:35:36.795763 52522 caffe_interface.cpp:125] Batch 103, top-5 = 0.98
I0122 16:35:36.797099 52522 caffe_interface.cpp:125] Batch 104, loss = 0.77033
I0122 16:35:36.797111 52522 caffe_interface.cpp:125] Batch 104, top-1 = 0.8
I0122 16:35:36.797116 52522 caffe_interface.cpp:125] Batch 104, top-5 = 0.98
I0122 16:35:36.798521 52522 caffe_interface.cpp:125] Batch 105, loss = 0.491821
I0122 16:35:36.798539 52522 caffe_interface.cpp:125] Batch 105, top-1 = 0.88
I0122 16:35:36.798544 52522 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:35:36.799991 52522 caffe_interface.cpp:125] Batch 106, loss = 0.681131
I0122 16:35:36.800004 52522 caffe_interface.cpp:125] Batch 106, top-1 = 0.84
I0122 16:35:36.800009 52522 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:35:36.801473 52522 caffe_interface.cpp:125] Batch 107, loss = 1.00739
I0122 16:35:36.801486 52522 caffe_interface.cpp:125] Batch 107, top-1 = 0.72
I0122 16:35:36.801491 52522 caffe_interface.cpp:125] Batch 107, top-5 = 1
I0122 16:35:36.802834 52522 caffe_interface.cpp:125] Batch 108, loss = 0.705824
I0122 16:35:36.802847 52522 caffe_interface.cpp:125] Batch 108, top-1 = 0.78
I0122 16:35:36.802852 52522 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0122 16:35:36.804177 52522 caffe_interface.cpp:125] Batch 109, loss = 0.885896
I0122 16:35:36.804190 52522 caffe_interface.cpp:125] Batch 109, top-1 = 0.72
I0122 16:35:36.804195 52522 caffe_interface.cpp:125] Batch 109, top-5 = 0.98
I0122 16:35:36.805516 52522 caffe_interface.cpp:125] Batch 110, loss = 0.712675
I0122 16:35:36.805528 52522 caffe_interface.cpp:125] Batch 110, top-1 = 0.76
I0122 16:35:36.805533 52522 caffe_interface.cpp:125] Batch 110, top-5 = 1
I0122 16:35:36.806777 52522 caffe_interface.cpp:125] Batch 111, loss = 0.606738
I0122 16:35:36.806787 52522 caffe_interface.cpp:125] Batch 111, top-1 = 0.84
I0122 16:35:36.806789 52522 caffe_interface.cpp:125] Batch 111, top-5 = 1
I0122 16:35:36.808008 52522 caffe_interface.cpp:125] Batch 112, loss = 0.461249
I0122 16:35:36.808017 52522 caffe_interface.cpp:125] Batch 112, top-1 = 0.82
I0122 16:35:36.808020 52522 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0122 16:35:36.809247 52522 caffe_interface.cpp:125] Batch 113, loss = 0.889843
I0122 16:35:36.809263 52522 caffe_interface.cpp:125] Batch 113, top-1 = 0.74
I0122 16:35:36.809267 52522 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:35:36.810482 52522 caffe_interface.cpp:125] Batch 114, loss = 1.19501
I0122 16:35:36.810492 52522 caffe_interface.cpp:125] Batch 114, top-1 = 0.56
I0122 16:35:36.810494 52522 caffe_interface.cpp:125] Batch 114, top-5 = 1
I0122 16:35:36.811707 52522 caffe_interface.cpp:125] Batch 115, loss = 0.338906
I0122 16:35:36.811717 52522 caffe_interface.cpp:125] Batch 115, top-1 = 0.82
I0122 16:35:36.811719 52522 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:35:36.812938 52522 caffe_interface.cpp:125] Batch 116, loss = 1.18447
I0122 16:35:36.812947 52522 caffe_interface.cpp:125] Batch 116, top-1 = 0.74
I0122 16:35:36.812950 52522 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0122 16:35:36.815151 52522 caffe_interface.cpp:125] Batch 117, loss = 0.910868
I0122 16:35:36.815160 52522 caffe_interface.cpp:125] Batch 117, top-1 = 0.76
I0122 16:35:36.815165 52522 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0122 16:35:36.816486 52522 caffe_interface.cpp:125] Batch 118, loss = 0.801586
I0122 16:35:36.816494 52522 caffe_interface.cpp:125] Batch 118, top-1 = 0.76
I0122 16:35:36.816498 52522 caffe_interface.cpp:125] Batch 118, top-5 = 1
I0122 16:35:36.817883 52522 caffe_interface.cpp:125] Batch 119, loss = 1.16816
I0122 16:35:36.817893 52522 caffe_interface.cpp:125] Batch 119, top-1 = 0.78
I0122 16:35:36.817895 52522 caffe_interface.cpp:125] Batch 119, top-5 = 0.96
I0122 16:35:36.819126 52522 caffe_interface.cpp:125] Batch 120, loss = 0.598523
I0122 16:35:36.819135 52522 caffe_interface.cpp:125] Batch 120, top-1 = 0.76
I0122 16:35:36.819139 52522 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0122 16:35:36.820358 52522 caffe_interface.cpp:125] Batch 121, loss = 1.332
I0122 16:35:36.820366 52522 caffe_interface.cpp:125] Batch 121, top-1 = 0.7
I0122 16:35:36.820370 52522 caffe_interface.cpp:125] Batch 121, top-5 = 0.96
I0122 16:35:36.821593 52522 caffe_interface.cpp:125] Batch 122, loss = 1.04318
I0122 16:35:36.821602 52522 caffe_interface.cpp:125] Batch 122, top-1 = 0.8
I0122 16:35:36.821606 52522 caffe_interface.cpp:125] Batch 122, top-5 = 0.98
I0122 16:35:36.822831 52522 caffe_interface.cpp:125] Batch 123, loss = 0.851978
I0122 16:35:36.822840 52522 caffe_interface.cpp:125] Batch 123, top-1 = 0.76
I0122 16:35:36.822844 52522 caffe_interface.cpp:125] Batch 123, top-5 = 0.98
I0122 16:35:36.824056 52522 caffe_interface.cpp:125] Batch 124, loss = 0.744213
I0122 16:35:36.824064 52522 caffe_interface.cpp:125] Batch 124, top-1 = 0.8
I0122 16:35:36.824067 52522 caffe_interface.cpp:125] Batch 124, top-5 = 0.98
I0122 16:35:36.825299 52522 caffe_interface.cpp:125] Batch 125, loss = 0.601366
I0122 16:35:36.825307 52522 caffe_interface.cpp:125] Batch 125, top-1 = 0.82
I0122 16:35:36.825321 52522 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0122 16:35:36.826557 52522 caffe_interface.cpp:125] Batch 126, loss = 1.0137
I0122 16:35:36.826566 52522 caffe_interface.cpp:125] Batch 126, top-1 = 0.74
I0122 16:35:36.826570 52522 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:35:36.827780 52522 caffe_interface.cpp:125] Batch 127, loss = 0.807699
I0122 16:35:36.827787 52522 caffe_interface.cpp:125] Batch 127, top-1 = 0.76
I0122 16:35:36.827791 52522 caffe_interface.cpp:125] Batch 127, top-5 = 1
I0122 16:35:36.828995 52522 caffe_interface.cpp:125] Batch 128, loss = 0.787925
I0122 16:35:36.829002 52522 caffe_interface.cpp:125] Batch 128, top-1 = 0.76
I0122 16:35:36.829005 52522 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0122 16:35:36.830226 52522 caffe_interface.cpp:125] Batch 129, loss = 1.02537
I0122 16:35:36.830236 52522 caffe_interface.cpp:125] Batch 129, top-1 = 0.64
I0122 16:35:36.830240 52522 caffe_interface.cpp:125] Batch 129, top-5 = 0.98
I0122 16:35:36.831702 52522 caffe_interface.cpp:125] Batch 130, loss = 1.15547
I0122 16:35:36.831710 52522 caffe_interface.cpp:125] Batch 130, top-1 = 0.68
I0122 16:35:36.831714 52522 caffe_interface.cpp:125] Batch 130, top-5 = 0.98
I0122 16:35:36.832933 52522 caffe_interface.cpp:125] Batch 131, loss = 1.1521
I0122 16:35:36.832942 52522 caffe_interface.cpp:125] Batch 131, top-1 = 0.64
I0122 16:35:36.832945 52522 caffe_interface.cpp:125] Batch 131, top-5 = 0.98
I0122 16:35:36.834170 52522 caffe_interface.cpp:125] Batch 132, loss = 1.06056
I0122 16:35:36.834179 52522 caffe_interface.cpp:125] Batch 132, top-1 = 0.76
I0122 16:35:36.834183 52522 caffe_interface.cpp:125] Batch 132, top-5 = 0.94
I0122 16:35:36.835402 52522 caffe_interface.cpp:125] Batch 133, loss = 1.16771
I0122 16:35:36.835409 52522 caffe_interface.cpp:125] Batch 133, top-1 = 0.68
I0122 16:35:36.835412 52522 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0122 16:35:36.836624 52522 caffe_interface.cpp:125] Batch 134, loss = 1.14127
I0122 16:35:36.836632 52522 caffe_interface.cpp:125] Batch 134, top-1 = 0.72
I0122 16:35:36.836637 52522 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:35:36.837831 52522 caffe_interface.cpp:125] Batch 135, loss = 1.12055
I0122 16:35:36.837838 52522 caffe_interface.cpp:125] Batch 135, top-1 = 0.76
I0122 16:35:36.837841 52522 caffe_interface.cpp:125] Batch 135, top-5 = 0.96
I0122 16:35:36.839052 52522 caffe_interface.cpp:125] Batch 136, loss = 0.520679
I0122 16:35:36.839061 52522 caffe_interface.cpp:125] Batch 136, top-1 = 0.84
I0122 16:35:36.839066 52522 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0122 16:35:36.840277 52522 caffe_interface.cpp:125] Batch 137, loss = 0.934631
I0122 16:35:36.840286 52522 caffe_interface.cpp:125] Batch 137, top-1 = 0.76
I0122 16:35:36.840289 52522 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:35:36.841501 52522 caffe_interface.cpp:125] Batch 138, loss = 0.782855
I0122 16:35:36.841509 52522 caffe_interface.cpp:125] Batch 138, top-1 = 0.78
I0122 16:35:36.841513 52522 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0122 16:35:36.842726 52522 caffe_interface.cpp:125] Batch 139, loss = 1.15137
I0122 16:35:36.842734 52522 caffe_interface.cpp:125] Batch 139, top-1 = 0.76
I0122 16:35:36.842738 52522 caffe_interface.cpp:125] Batch 139, top-5 = 0.94
I0122 16:35:36.843947 52522 caffe_interface.cpp:125] Batch 140, loss = 0.765796
I0122 16:35:36.843955 52522 caffe_interface.cpp:125] Batch 140, top-1 = 0.76
I0122 16:35:36.843961 52522 caffe_interface.cpp:125] Batch 140, top-5 = 1
I0122 16:35:36.845170 52522 caffe_interface.cpp:125] Batch 141, loss = 0.580615
I0122 16:35:36.845180 52522 caffe_interface.cpp:125] Batch 141, top-1 = 0.86
I0122 16:35:36.845183 52522 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:35:36.846392 52522 caffe_interface.cpp:125] Batch 142, loss = 0.536468
I0122 16:35:36.846400 52522 caffe_interface.cpp:125] Batch 142, top-1 = 0.78
I0122 16:35:36.846405 52522 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:35:36.847631 52522 caffe_interface.cpp:125] Batch 143, loss = 0.768261
I0122 16:35:36.847651 52522 caffe_interface.cpp:125] Batch 143, top-1 = 0.78
I0122 16:35:36.847654 52522 caffe_interface.cpp:125] Batch 143, top-5 = 1
I0122 16:35:36.849887 52522 caffe_interface.cpp:125] Batch 144, loss = 0.900261
I0122 16:35:36.849895 52522 caffe_interface.cpp:125] Batch 144, top-1 = 0.7
I0122 16:35:36.849900 52522 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0122 16:35:36.851426 52522 caffe_interface.cpp:125] Batch 145, loss = 1.19345
I0122 16:35:36.851436 52522 caffe_interface.cpp:125] Batch 145, top-1 = 0.68
I0122 16:35:36.851439 52522 caffe_interface.cpp:125] Batch 145, top-5 = 0.98
I0122 16:35:36.852654 52522 caffe_interface.cpp:125] Batch 146, loss = 0.458235
I0122 16:35:36.852663 52522 caffe_interface.cpp:125] Batch 146, top-1 = 0.84
I0122 16:35:36.852666 52522 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0122 16:35:36.853883 52522 caffe_interface.cpp:125] Batch 147, loss = 1.10177
I0122 16:35:36.853891 52522 caffe_interface.cpp:125] Batch 147, top-1 = 0.74
I0122 16:35:36.853894 52522 caffe_interface.cpp:125] Batch 147, top-5 = 1
I0122 16:35:36.855108 52522 caffe_interface.cpp:125] Batch 148, loss = 0.750072
I0122 16:35:36.855118 52522 caffe_interface.cpp:125] Batch 148, top-1 = 0.76
I0122 16:35:36.855121 52522 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:35:36.856329 52522 caffe_interface.cpp:125] Batch 149, loss = 0.421721
I0122 16:35:36.856338 52522 caffe_interface.cpp:125] Batch 149, top-1 = 0.84
I0122 16:35:36.856341 52522 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:35:36.857565 52522 caffe_interface.cpp:125] Batch 150, loss = 0.269922
I0122 16:35:36.857573 52522 caffe_interface.cpp:125] Batch 150, top-1 = 0.9
I0122 16:35:36.857578 52522 caffe_interface.cpp:125] Batch 150, top-5 = 1
I0122 16:35:36.858786 52522 caffe_interface.cpp:125] Batch 151, loss = 1.19275
I0122 16:35:36.858795 52522 caffe_interface.cpp:125] Batch 151, top-1 = 0.68
I0122 16:35:36.858798 52522 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:35:36.860021 52522 caffe_interface.cpp:125] Batch 152, loss = 0.74476
I0122 16:35:36.860029 52522 caffe_interface.cpp:125] Batch 152, top-1 = 0.78
I0122 16:35:36.860033 52522 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0122 16:35:36.861243 52522 caffe_interface.cpp:125] Batch 153, loss = 0.592691
I0122 16:35:36.861251 52522 caffe_interface.cpp:125] Batch 153, top-1 = 0.84
I0122 16:35:36.861254 52522 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:35:36.862471 52522 caffe_interface.cpp:125] Batch 154, loss = 0.60016
I0122 16:35:36.862479 52522 caffe_interface.cpp:125] Batch 154, top-1 = 0.76
I0122 16:35:36.862483 52522 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:35:36.863704 52522 caffe_interface.cpp:125] Batch 155, loss = 0.364934
I0122 16:35:36.863711 52522 caffe_interface.cpp:125] Batch 155, top-1 = 0.82
I0122 16:35:36.863714 52522 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:35:36.865185 52522 caffe_interface.cpp:125] Batch 156, loss = 0.816913
I0122 16:35:36.865191 52522 caffe_interface.cpp:125] Batch 156, top-1 = 0.68
I0122 16:35:36.865195 52522 caffe_interface.cpp:125] Batch 156, top-5 = 1
I0122 16:35:36.866408 52522 caffe_interface.cpp:125] Batch 157, loss = 0.53253
I0122 16:35:36.866417 52522 caffe_interface.cpp:125] Batch 157, top-1 = 0.82
I0122 16:35:36.866420 52522 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:35:36.867632 52522 caffe_interface.cpp:125] Batch 158, loss = 1.10386
I0122 16:35:36.867640 52522 caffe_interface.cpp:125] Batch 158, top-1 = 0.72
I0122 16:35:36.867643 52522 caffe_interface.cpp:125] Batch 158, top-5 = 1
I0122 16:35:36.868850 52522 caffe_interface.cpp:125] Batch 159, loss = 1.01231
I0122 16:35:36.868858 52522 caffe_interface.cpp:125] Batch 159, top-1 = 0.7
I0122 16:35:36.868862 52522 caffe_interface.cpp:125] Batch 159, top-5 = 1
I0122 16:35:36.870074 52522 caffe_interface.cpp:125] Batch 160, loss = 0.465974
I0122 16:35:36.870082 52522 caffe_interface.cpp:125] Batch 160, top-1 = 0.86
I0122 16:35:36.870085 52522 caffe_interface.cpp:125] Batch 160, top-5 = 0.98
I0122 16:35:36.871315 52522 caffe_interface.cpp:125] Batch 161, loss = 0.743312
I0122 16:35:36.871325 52522 caffe_interface.cpp:125] Batch 161, top-1 = 0.8
I0122 16:35:36.871327 52522 caffe_interface.cpp:125] Batch 161, top-5 = 0.98
I0122 16:35:36.872550 52522 caffe_interface.cpp:125] Batch 162, loss = 0.690319
I0122 16:35:36.872558 52522 caffe_interface.cpp:125] Batch 162, top-1 = 0.82
I0122 16:35:36.872562 52522 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:35:36.873769 52522 caffe_interface.cpp:125] Batch 163, loss = 0.653307
I0122 16:35:36.873776 52522 caffe_interface.cpp:125] Batch 163, top-1 = 0.8
I0122 16:35:36.873780 52522 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:35:36.874994 52522 caffe_interface.cpp:125] Batch 164, loss = 0.871086
I0122 16:35:36.875002 52522 caffe_interface.cpp:125] Batch 164, top-1 = 0.74
I0122 16:35:36.875006 52522 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:35:36.876215 52522 caffe_interface.cpp:125] Batch 165, loss = 0.744699
I0122 16:35:36.876224 52522 caffe_interface.cpp:125] Batch 165, top-1 = 0.88
I0122 16:35:36.876226 52522 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:35:36.877441 52522 caffe_interface.cpp:125] Batch 166, loss = 0.647106
I0122 16:35:36.877449 52522 caffe_interface.cpp:125] Batch 166, top-1 = 0.76
I0122 16:35:36.877454 52522 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:35:36.878656 52522 caffe_interface.cpp:125] Batch 167, loss = 1.1
I0122 16:35:36.878664 52522 caffe_interface.cpp:125] Batch 167, top-1 = 0.76
I0122 16:35:36.878669 52522 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0122 16:35:36.879879 52522 caffe_interface.cpp:125] Batch 168, loss = 0.338623
I0122 16:35:36.879887 52522 caffe_interface.cpp:125] Batch 168, top-1 = 0.94
I0122 16:35:36.879890 52522 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:35:36.881093 52522 caffe_interface.cpp:125] Batch 169, loss = 1.35464
I0122 16:35:36.881103 52522 caffe_interface.cpp:125] Batch 169, top-1 = 0.62
I0122 16:35:36.881105 52522 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0122 16:35:36.882313 52522 caffe_interface.cpp:125] Batch 170, loss = 0.633045
I0122 16:35:36.882320 52522 caffe_interface.cpp:125] Batch 170, top-1 = 0.84
I0122 16:35:36.882324 52522 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0122 16:35:36.884622 52522 caffe_interface.cpp:125] Batch 171, loss = 0.705752
I0122 16:35:36.884630 52522 caffe_interface.cpp:125] Batch 171, top-1 = 0.78
I0122 16:35:36.884634 52522 caffe_interface.cpp:125] Batch 171, top-5 = 0.98
I0122 16:35:36.885975 52522 caffe_interface.cpp:125] Batch 172, loss = 1.14959
I0122 16:35:36.885983 52522 caffe_interface.cpp:125] Batch 172, top-1 = 0.66
I0122 16:35:36.885987 52522 caffe_interface.cpp:125] Batch 172, top-5 = 0.96
I0122 16:35:36.887387 52522 caffe_interface.cpp:125] Batch 173, loss = 0.720614
I0122 16:35:36.887395 52522 caffe_interface.cpp:125] Batch 173, top-1 = 0.84
I0122 16:35:36.887398 52522 caffe_interface.cpp:125] Batch 173, top-5 = 0.98
I0122 16:35:36.888644 52522 caffe_interface.cpp:125] Batch 174, loss = 0.554056
I0122 16:35:36.888653 52522 caffe_interface.cpp:125] Batch 174, top-1 = 0.74
I0122 16:35:36.888656 52522 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:35:36.889875 52522 caffe_interface.cpp:125] Batch 175, loss = 0.885289
I0122 16:35:36.889883 52522 caffe_interface.cpp:125] Batch 175, top-1 = 0.84
I0122 16:35:36.889888 52522 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0122 16:35:36.891090 52522 caffe_interface.cpp:125] Batch 176, loss = 1.35934
I0122 16:35:36.891099 52522 caffe_interface.cpp:125] Batch 176, top-1 = 0.7
I0122 16:35:36.891103 52522 caffe_interface.cpp:125] Batch 176, top-5 = 0.96
I0122 16:35:36.892309 52522 caffe_interface.cpp:125] Batch 177, loss = 1.08186
I0122 16:35:36.892318 52522 caffe_interface.cpp:125] Batch 177, top-1 = 0.72
I0122 16:35:36.892321 52522 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0122 16:35:36.893528 52522 caffe_interface.cpp:125] Batch 178, loss = 0.766927
I0122 16:35:36.893537 52522 caffe_interface.cpp:125] Batch 178, top-1 = 0.76
I0122 16:35:36.893550 52522 caffe_interface.cpp:125] Batch 178, top-5 = 0.96
I0122 16:35:36.894762 52522 caffe_interface.cpp:125] Batch 179, loss = 0.659724
I0122 16:35:36.894771 52522 caffe_interface.cpp:125] Batch 179, top-1 = 0.8
I0122 16:35:36.894775 52522 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:35:36.894778 52522 caffe_interface.cpp:130] Loss: 0.820674
I0122 16:35:36.894784 52522 caffe_interface.cpp:142] loss = 0.820674 (* 1 = 0.820674 loss)
I0122 16:35:36.894789 52522 caffe_interface.cpp:142] top-1 = 0.77
I0122 16:35:36.894793 52522 caffe_interface.cpp:142] top-5 = 0.988333
I0122 16:35:37.044013 52522 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/sparse.caffemodel
I0122 16:35:37.044054 52522 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.4:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.769999862    | -0.0926665664  |
+-------------------------------------------------------------------+
| Weights        | 68389          | 47355          | -30.7564087%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 30338560       | -38.1523476%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config4.prototxt
## fine-tuning: fourth run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config4.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune4_miniVggNet.txt
I0122 16:35:37.285224 54582 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0122 16:35:37.464540 54582 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:35:37.465049 54582 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24900665344, dev_info[0]: total=25620447232 free=24900665344
I0122 16:35:37.465059 54582 caffe_interface.cpp:493] Using GPUs 0
I0122 16:35:37.465313 54582 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:35:38.060784 54582 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt"
type: "SGD"
I0122 16:35:38.060897 54582 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0122 16:35:38.061126 54582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:35:38.061141 54582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:35:38.061142 54582 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:35:38.061280 54582 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:35:38.061342 54582 layer_factory.hpp:77] Creating layer data
I0122 16:35:38.061448 54582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:38.062091 54582 net.cpp:94] Creating Layer data
I0122 16:35:38.062103 54582 net.cpp:409] data -> data
I0122 16:35:38.062124 54582 net.cpp:409] data -> label
I0122 16:35:38.063462 54623 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:35:38.063509 54623 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:35:38.063609 54582 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:35:38.063686 54582 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:35:38.071395 54582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:38.071441 54582 net.cpp:144] Setting up data
I0122 16:35:38.071449 54582 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:35:38.071452 54582 net.cpp:151] Top shape: 128 (128)
I0122 16:35:38.071455 54582 net.cpp:159] Memory required for data: 1573376
I0122 16:35:38.071460 54582 layer_factory.hpp:77] Creating layer conv1
I0122 16:35:38.071471 54582 net.cpp:94] Creating Layer conv1
I0122 16:35:38.071475 54582 net.cpp:435] conv1 <- data
I0122 16:35:38.071492 54582 net.cpp:409] conv1 -> conv1
I0122 16:35:38.072542 54582 net.cpp:144] Setting up conv1
I0122 16:35:38.072553 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.072556 54582 net.cpp:159] Memory required for data: 18350592
I0122 16:35:38.072582 54582 layer_factory.hpp:77] Creating layer bn1
I0122 16:35:38.072590 54582 net.cpp:94] Creating Layer bn1
I0122 16:35:38.072593 54582 net.cpp:435] bn1 <- conv1
I0122 16:35:38.072597 54582 net.cpp:409] bn1 -> scale1
I0122 16:35:38.073165 54582 net.cpp:144] Setting up bn1
I0122 16:35:38.073173 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.073176 54582 net.cpp:159] Memory required for data: 35127808
I0122 16:35:38.073186 54582 layer_factory.hpp:77] Creating layer relu1
I0122 16:35:38.073194 54582 net.cpp:94] Creating Layer relu1
I0122 16:35:38.073197 54582 net.cpp:435] relu1 <- scale1
I0122 16:35:38.073201 54582 net.cpp:409] relu1 -> relu1
I0122 16:35:38.073221 54582 net.cpp:144] Setting up relu1
I0122 16:35:38.073227 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.073231 54582 net.cpp:159] Memory required for data: 51905024
I0122 16:35:38.073233 54582 layer_factory.hpp:77] Creating layer conv2
I0122 16:35:38.073241 54582 net.cpp:94] Creating Layer conv2
I0122 16:35:38.073246 54582 net.cpp:435] conv2 <- relu1
I0122 16:35:38.073251 54582 net.cpp:409] conv2 -> conv2
I0122 16:35:38.074793 54582 net.cpp:144] Setting up conv2
I0122 16:35:38.074805 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.074807 54582 net.cpp:159] Memory required for data: 68682240
I0122 16:35:38.074815 54582 layer_factory.hpp:77] Creating layer bn2
I0122 16:35:38.074822 54582 net.cpp:94] Creating Layer bn2
I0122 16:35:38.074826 54582 net.cpp:435] bn2 <- conv2
I0122 16:35:38.074831 54582 net.cpp:409] bn2 -> scale2
I0122 16:35:38.075655 54582 net.cpp:144] Setting up bn2
I0122 16:35:38.075662 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.075666 54582 net.cpp:159] Memory required for data: 85459456
I0122 16:35:38.075675 54582 layer_factory.hpp:77] Creating layer relu2
I0122 16:35:38.075680 54582 net.cpp:94] Creating Layer relu2
I0122 16:35:38.075683 54582 net.cpp:435] relu2 <- scale2
I0122 16:35:38.075687 54582 net.cpp:409] relu2 -> relu2
I0122 16:35:38.075788 54582 net.cpp:144] Setting up relu2
I0122 16:35:38.075812 54582 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:35:38.075821 54582 net.cpp:159] Memory required for data: 102236672
I0122 16:35:38.075829 54582 layer_factory.hpp:77] Creating layer pool1
I0122 16:35:38.075845 54582 net.cpp:94] Creating Layer pool1
I0122 16:35:38.075853 54582 net.cpp:435] pool1 <- relu2
I0122 16:35:38.075865 54582 net.cpp:409] pool1 -> pool1
I0122 16:35:38.076041 54582 net.cpp:144] Setting up pool1
I0122 16:35:38.076056 54582 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:35:38.076061 54582 net.cpp:159] Memory required for data: 106430976
I0122 16:35:38.076067 54582 layer_factory.hpp:77] Creating layer drop1
I0122 16:35:38.076082 54582 net.cpp:94] Creating Layer drop1
I0122 16:35:38.076089 54582 net.cpp:435] drop1 <- pool1
I0122 16:35:38.076122 54582 net.cpp:409] drop1 -> drop1
I0122 16:35:38.076182 54582 net.cpp:144] Setting up drop1
I0122 16:35:38.076200 54582 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:35:38.076206 54582 net.cpp:159] Memory required for data: 110625280
I0122 16:35:38.076211 54582 layer_factory.hpp:77] Creating layer conv3
I0122 16:35:38.076226 54582 net.cpp:94] Creating Layer conv3
I0122 16:35:38.076234 54582 net.cpp:435] conv3 <- drop1
I0122 16:35:38.076244 54582 net.cpp:409] conv3 -> conv3
I0122 16:35:38.078013 54582 net.cpp:144] Setting up conv3
I0122 16:35:38.078034 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.078040 54582 net.cpp:159] Memory required for data: 119013888
I0122 16:35:38.078055 54582 layer_factory.hpp:77] Creating layer bn3
I0122 16:35:38.078068 54582 net.cpp:94] Creating Layer bn3
I0122 16:35:38.078078 54582 net.cpp:435] bn3 <- conv3
I0122 16:35:38.078088 54582 net.cpp:409] bn3 -> scale3
I0122 16:35:38.079344 54582 net.cpp:144] Setting up bn3
I0122 16:35:38.079356 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.079361 54582 net.cpp:159] Memory required for data: 127402496
I0122 16:35:38.079385 54582 layer_factory.hpp:77] Creating layer relu3
I0122 16:35:38.079396 54582 net.cpp:94] Creating Layer relu3
I0122 16:35:38.079403 54582 net.cpp:435] relu3 <- scale3
I0122 16:35:38.079411 54582 net.cpp:409] relu3 -> relu3
I0122 16:35:38.079445 54582 net.cpp:144] Setting up relu3
I0122 16:35:38.079459 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.079464 54582 net.cpp:159] Memory required for data: 135791104
I0122 16:35:38.079470 54582 layer_factory.hpp:77] Creating layer conv4
I0122 16:35:38.079484 54582 net.cpp:94] Creating Layer conv4
I0122 16:35:38.079493 54582 net.cpp:435] conv4 <- relu3
I0122 16:35:38.079502 54582 net.cpp:409] conv4 -> conv4
I0122 16:35:38.080332 54582 net.cpp:144] Setting up conv4
I0122 16:35:38.080343 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.080350 54582 net.cpp:159] Memory required for data: 144179712
I0122 16:35:38.080359 54582 layer_factory.hpp:77] Creating layer bn4
I0122 16:35:38.080373 54582 net.cpp:94] Creating Layer bn4
I0122 16:35:38.080379 54582 net.cpp:435] bn4 <- conv4
I0122 16:35:38.080390 54582 net.cpp:409] bn4 -> scale4
I0122 16:35:38.081591 54582 net.cpp:144] Setting up bn4
I0122 16:35:38.081604 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.081609 54582 net.cpp:159] Memory required for data: 152568320
I0122 16:35:38.081624 54582 layer_factory.hpp:77] Creating layer relu4
I0122 16:35:38.081634 54582 net.cpp:94] Creating Layer relu4
I0122 16:35:38.081640 54582 net.cpp:435] relu4 <- scale4
I0122 16:35:38.081648 54582 net.cpp:409] relu4 -> relu4
I0122 16:35:38.081681 54582 net.cpp:144] Setting up relu4
I0122 16:35:38.081691 54582 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:35:38.081696 54582 net.cpp:159] Memory required for data: 160956928
I0122 16:35:38.081701 54582 layer_factory.hpp:77] Creating layer pool2
I0122 16:35:38.081712 54582 net.cpp:94] Creating Layer pool2
I0122 16:35:38.081719 54582 net.cpp:435] pool2 <- relu4
I0122 16:35:38.081728 54582 net.cpp:409] pool2 -> pool2
I0122 16:35:38.081786 54582 net.cpp:144] Setting up pool2
I0122 16:35:38.081797 54582 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:35:38.081802 54582 net.cpp:159] Memory required for data: 163054080
I0122 16:35:38.081807 54582 layer_factory.hpp:77] Creating layer drop2
I0122 16:35:38.081817 54582 net.cpp:94] Creating Layer drop2
I0122 16:35:38.081826 54582 net.cpp:435] drop2 <- pool2
I0122 16:35:38.081835 54582 net.cpp:409] drop2 -> drop2
I0122 16:35:38.082007 54582 net.cpp:144] Setting up drop2
I0122 16:35:38.082021 54582 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:35:38.082027 54582 net.cpp:159] Memory required for data: 165151232
I0122 16:35:38.082032 54582 layer_factory.hpp:77] Creating layer fc1
I0122 16:35:38.082044 54582 net.cpp:94] Creating Layer fc1
I0122 16:35:38.082052 54582 net.cpp:435] fc1 <- drop2
I0122 16:35:38.082063 54582 net.cpp:409] fc1 -> fc1
I0122 16:35:38.104075 54582 net.cpp:144] Setting up fc1
I0122 16:35:38.104104 54582 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:35:38.104106 54582 net.cpp:159] Memory required for data: 165413376
I0122 16:35:38.104115 54582 layer_factory.hpp:77] Creating layer bn5
I0122 16:35:38.104123 54582 net.cpp:94] Creating Layer bn5
I0122 16:35:38.104127 54582 net.cpp:435] bn5 <- fc1
I0122 16:35:38.104135 54582 net.cpp:409] bn5 -> scale5
I0122 16:35:38.104707 54582 net.cpp:144] Setting up bn5
I0122 16:35:38.104713 54582 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:35:38.104715 54582 net.cpp:159] Memory required for data: 165675520
I0122 16:35:38.104728 54582 layer_factory.hpp:77] Creating layer relu5
I0122 16:35:38.104737 54582 net.cpp:94] Creating Layer relu5
I0122 16:35:38.104740 54582 net.cpp:435] relu5 <- scale5
I0122 16:35:38.104744 54582 net.cpp:409] relu5 -> relu5
I0122 16:35:38.104764 54582 net.cpp:144] Setting up relu5
I0122 16:35:38.104769 54582 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:35:38.104773 54582 net.cpp:159] Memory required for data: 165937664
I0122 16:35:38.104774 54582 layer_factory.hpp:77] Creating layer drop3
I0122 16:35:38.104779 54582 net.cpp:94] Creating Layer drop3
I0122 16:35:38.104782 54582 net.cpp:435] drop3 <- relu5
I0122 16:35:38.104789 54582 net.cpp:409] drop3 -> drop3
I0122 16:35:38.104815 54582 net.cpp:144] Setting up drop3
I0122 16:35:38.104820 54582 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:35:38.104823 54582 net.cpp:159] Memory required for data: 166199808
I0122 16:35:38.104826 54582 layer_factory.hpp:77] Creating layer fc2
I0122 16:35:38.104833 54582 net.cpp:94] Creating Layer fc2
I0122 16:35:38.104837 54582 net.cpp:435] fc2 <- drop3
I0122 16:35:38.104843 54582 net.cpp:409] fc2 -> fc2
I0122 16:35:38.104979 54582 net.cpp:144] Setting up fc2
I0122 16:35:38.104984 54582 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:35:38.104986 54582 net.cpp:159] Memory required for data: 166204928
I0122 16:35:38.104991 54582 layer_factory.hpp:77] Creating layer loss
I0122 16:35:38.104996 54582 net.cpp:94] Creating Layer loss
I0122 16:35:38.105000 54582 net.cpp:435] loss <- fc2
I0122 16:35:38.105003 54582 net.cpp:435] loss <- label
I0122 16:35:38.105008 54582 net.cpp:409] loss -> loss
I0122 16:35:38.105015 54582 layer_factory.hpp:77] Creating layer loss
I0122 16:35:38.105751 54582 net.cpp:144] Setting up loss
I0122 16:35:38.105762 54582 net.cpp:151] Top shape: (1)
I0122 16:35:38.105765 54582 net.cpp:154]     with loss weight 1
I0122 16:35:38.105774 54582 net.cpp:159] Memory required for data: 166204932
I0122 16:35:38.105777 54582 net.cpp:220] loss needs backward computation.
I0122 16:35:38.105790 54582 net.cpp:220] fc2 needs backward computation.
I0122 16:35:38.105793 54582 net.cpp:220] drop3 needs backward computation.
I0122 16:35:38.105796 54582 net.cpp:220] relu5 needs backward computation.
I0122 16:35:38.105799 54582 net.cpp:220] bn5 needs backward computation.
I0122 16:35:38.105803 54582 net.cpp:220] fc1 needs backward computation.
I0122 16:35:38.105805 54582 net.cpp:220] drop2 needs backward computation.
I0122 16:35:38.105808 54582 net.cpp:220] pool2 needs backward computation.
I0122 16:35:38.105811 54582 net.cpp:220] relu4 needs backward computation.
I0122 16:35:38.105814 54582 net.cpp:220] bn4 needs backward computation.
I0122 16:35:38.105818 54582 net.cpp:220] conv4 needs backward computation.
I0122 16:35:38.105821 54582 net.cpp:220] relu3 needs backward computation.
I0122 16:35:38.105824 54582 net.cpp:220] bn3 needs backward computation.
I0122 16:35:38.105829 54582 net.cpp:220] conv3 needs backward computation.
I0122 16:35:38.105832 54582 net.cpp:220] drop1 needs backward computation.
I0122 16:35:38.105835 54582 net.cpp:220] pool1 needs backward computation.
I0122 16:35:38.105839 54582 net.cpp:220] relu2 needs backward computation.
I0122 16:35:38.105840 54582 net.cpp:220] bn2 needs backward computation.
I0122 16:35:38.105844 54582 net.cpp:220] conv2 needs backward computation.
I0122 16:35:38.105847 54582 net.cpp:220] relu1 needs backward computation.
I0122 16:35:38.105861 54582 net.cpp:220] bn1 needs backward computation.
I0122 16:35:38.105865 54582 net.cpp:220] conv1 needs backward computation.
I0122 16:35:38.105868 54582 net.cpp:222] data does not need backward computation.
I0122 16:35:38.105871 54582 net.cpp:264] This network produces output loss
I0122 16:35:38.105892 54582 net.cpp:284] Network initialization done.
I0122 16:35:38.106206 54582 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/net_finetune.prototxt
I0122 16:35:38.106240 54582 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:35:38.106427 54582 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:35:38.106532 54582 layer_factory.hpp:77] Creating layer data
I0122 16:35:38.106576 54582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:38.107393 54582 net.cpp:94] Creating Layer data
I0122 16:35:38.107409 54582 net.cpp:409] data -> data
I0122 16:35:38.107416 54582 net.cpp:409] data -> label
I0122 16:35:38.108531 54653 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:35:38.108566 54653 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:35:38.108640 54582 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:35:38.108724 54582 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:35:38.112179 54582 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:35:38.112229 54582 net.cpp:144] Setting up data
I0122 16:35:38.112246 54582 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:35:38.112251 54582 net.cpp:151] Top shape: 50 (50)
I0122 16:35:38.112252 54582 net.cpp:159] Memory required for data: 614600
I0122 16:35:38.112257 54582 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:35:38.112263 54582 net.cpp:94] Creating Layer label_data_1_split
I0122 16:35:38.112268 54582 net.cpp:435] label_data_1_split <- label
I0122 16:35:38.112288 54582 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:35:38.112298 54582 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:35:38.112303 54582 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:35:38.112409 54582 net.cpp:144] Setting up label_data_1_split
I0122 16:35:38.112414 54582 net.cpp:151] Top shape: 50 (50)
I0122 16:35:38.112417 54582 net.cpp:151] Top shape: 50 (50)
I0122 16:35:38.112421 54582 net.cpp:151] Top shape: 50 (50)
I0122 16:35:38.112422 54582 net.cpp:159] Memory required for data: 615200
I0122 16:35:38.112426 54582 layer_factory.hpp:77] Creating layer conv1
I0122 16:35:38.112444 54582 net.cpp:94] Creating Layer conv1
I0122 16:35:38.112449 54582 net.cpp:435] conv1 <- data
I0122 16:35:38.112457 54582 net.cpp:409] conv1 -> conv1
I0122 16:35:38.112788 54582 net.cpp:144] Setting up conv1
I0122 16:35:38.112795 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.112797 54582 net.cpp:159] Memory required for data: 7168800
I0122 16:35:38.112807 54582 layer_factory.hpp:77] Creating layer bn1
I0122 16:35:38.112814 54582 net.cpp:94] Creating Layer bn1
I0122 16:35:38.112819 54582 net.cpp:435] bn1 <- conv1
I0122 16:35:38.112825 54582 net.cpp:409] bn1 -> scale1
I0122 16:35:38.113497 54582 net.cpp:144] Setting up bn1
I0122 16:35:38.113504 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.113508 54582 net.cpp:159] Memory required for data: 13722400
I0122 16:35:38.113519 54582 layer_factory.hpp:77] Creating layer relu1
I0122 16:35:38.113526 54582 net.cpp:94] Creating Layer relu1
I0122 16:35:38.113529 54582 net.cpp:435] relu1 <- scale1
I0122 16:35:38.113534 54582 net.cpp:409] relu1 -> relu1
I0122 16:35:38.113554 54582 net.cpp:144] Setting up relu1
I0122 16:35:38.113559 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.113561 54582 net.cpp:159] Memory required for data: 20276000
I0122 16:35:38.113564 54582 layer_factory.hpp:77] Creating layer conv2
I0122 16:35:38.113574 54582 net.cpp:94] Creating Layer conv2
I0122 16:35:38.113579 54582 net.cpp:435] conv2 <- relu1
I0122 16:35:38.113584 54582 net.cpp:409] conv2 -> conv2
I0122 16:35:38.114156 54582 net.cpp:144] Setting up conv2
I0122 16:35:38.114163 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.114166 54582 net.cpp:159] Memory required for data: 26829600
I0122 16:35:38.114174 54582 layer_factory.hpp:77] Creating layer bn2
I0122 16:35:38.114182 54582 net.cpp:94] Creating Layer bn2
I0122 16:35:38.114187 54582 net.cpp:435] bn2 <- conv2
I0122 16:35:38.114193 54582 net.cpp:409] bn2 -> scale2
I0122 16:35:38.115003 54582 net.cpp:144] Setting up bn2
I0122 16:35:38.115011 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.115015 54582 net.cpp:159] Memory required for data: 33383200
I0122 16:35:38.115021 54582 layer_factory.hpp:77] Creating layer relu2
I0122 16:35:38.115027 54582 net.cpp:94] Creating Layer relu2
I0122 16:35:38.115032 54582 net.cpp:435] relu2 <- scale2
I0122 16:35:38.115037 54582 net.cpp:409] relu2 -> relu2
I0122 16:35:38.115075 54582 net.cpp:144] Setting up relu2
I0122 16:35:38.115087 54582 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:35:38.115090 54582 net.cpp:159] Memory required for data: 39936800
I0122 16:35:38.115093 54582 layer_factory.hpp:77] Creating layer pool1
I0122 16:35:38.115100 54582 net.cpp:94] Creating Layer pool1
I0122 16:35:38.115105 54582 net.cpp:435] pool1 <- relu2
I0122 16:35:38.115110 54582 net.cpp:409] pool1 -> pool1
I0122 16:35:38.115162 54582 net.cpp:144] Setting up pool1
I0122 16:35:38.115177 54582 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:35:38.115180 54582 net.cpp:159] Memory required for data: 41575200
I0122 16:35:38.115183 54582 layer_factory.hpp:77] Creating layer drop1
I0122 16:35:38.115188 54582 net.cpp:94] Creating Layer drop1
I0122 16:35:38.115191 54582 net.cpp:435] drop1 <- pool1
I0122 16:35:38.115196 54582 net.cpp:409] drop1 -> drop1
I0122 16:35:38.115263 54582 net.cpp:144] Setting up drop1
I0122 16:35:38.115269 54582 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:35:38.115272 54582 net.cpp:159] Memory required for data: 43213600
I0122 16:35:38.115274 54582 layer_factory.hpp:77] Creating layer conv3
I0122 16:35:38.115283 54582 net.cpp:94] Creating Layer conv3
I0122 16:35:38.115288 54582 net.cpp:435] conv3 <- drop1
I0122 16:35:38.115294 54582 net.cpp:409] conv3 -> conv3
I0122 16:35:38.115715 54582 net.cpp:144] Setting up conv3
I0122 16:35:38.115721 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.115725 54582 net.cpp:159] Memory required for data: 46490400
I0122 16:35:38.115730 54582 layer_factory.hpp:77] Creating layer bn3
I0122 16:35:38.115741 54582 net.cpp:94] Creating Layer bn3
I0122 16:35:38.115747 54582 net.cpp:435] bn3 <- conv3
I0122 16:35:38.115753 54582 net.cpp:409] bn3 -> scale3
I0122 16:35:38.116430 54582 net.cpp:144] Setting up bn3
I0122 16:35:38.116436 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.116439 54582 net.cpp:159] Memory required for data: 49767200
I0122 16:35:38.116451 54582 layer_factory.hpp:77] Creating layer relu3
I0122 16:35:38.116457 54582 net.cpp:94] Creating Layer relu3
I0122 16:35:38.116461 54582 net.cpp:435] relu3 <- scale3
I0122 16:35:38.116467 54582 net.cpp:409] relu3 -> relu3
I0122 16:35:38.116485 54582 net.cpp:144] Setting up relu3
I0122 16:35:38.116492 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.116494 54582 net.cpp:159] Memory required for data: 53044000
I0122 16:35:38.116498 54582 layer_factory.hpp:77] Creating layer conv4
I0122 16:35:38.116508 54582 net.cpp:94] Creating Layer conv4
I0122 16:35:38.116511 54582 net.cpp:435] conv4 <- relu3
I0122 16:35:38.116518 54582 net.cpp:409] conv4 -> conv4
I0122 16:35:38.117038 54582 net.cpp:144] Setting up conv4
I0122 16:35:38.117044 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.117046 54582 net.cpp:159] Memory required for data: 56320800
I0122 16:35:38.117053 54582 layer_factory.hpp:77] Creating layer bn4
I0122 16:35:38.117061 54582 net.cpp:94] Creating Layer bn4
I0122 16:35:38.117064 54582 net.cpp:435] bn4 <- conv4
I0122 16:35:38.117070 54582 net.cpp:409] bn4 -> scale4
I0122 16:35:38.117772 54582 net.cpp:144] Setting up bn4
I0122 16:35:38.117779 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.117784 54582 net.cpp:159] Memory required for data: 59597600
I0122 16:35:38.117790 54582 layer_factory.hpp:77] Creating layer relu4
I0122 16:35:38.117795 54582 net.cpp:94] Creating Layer relu4
I0122 16:35:38.117799 54582 net.cpp:435] relu4 <- scale4
I0122 16:35:38.117805 54582 net.cpp:409] relu4 -> relu4
I0122 16:35:38.117822 54582 net.cpp:144] Setting up relu4
I0122 16:35:38.117830 54582 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:35:38.117832 54582 net.cpp:159] Memory required for data: 62874400
I0122 16:35:38.117835 54582 layer_factory.hpp:77] Creating layer pool2
I0122 16:35:38.117844 54582 net.cpp:94] Creating Layer pool2
I0122 16:35:38.117846 54582 net.cpp:435] pool2 <- relu4
I0122 16:35:38.117851 54582 net.cpp:409] pool2 -> pool2
I0122 16:35:38.117947 54582 net.cpp:144] Setting up pool2
I0122 16:35:38.117954 54582 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:35:38.117955 54582 net.cpp:159] Memory required for data: 63693600
I0122 16:35:38.117959 54582 layer_factory.hpp:77] Creating layer drop2
I0122 16:35:38.117964 54582 net.cpp:94] Creating Layer drop2
I0122 16:35:38.117966 54582 net.cpp:435] drop2 <- pool2
I0122 16:35:38.117971 54582 net.cpp:409] drop2 -> drop2
I0122 16:35:38.118003 54582 net.cpp:144] Setting up drop2
I0122 16:35:38.118010 54582 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:35:38.118021 54582 net.cpp:159] Memory required for data: 64512800
I0122 16:35:38.118024 54582 layer_factory.hpp:77] Creating layer fc1
I0122 16:35:38.118031 54582 net.cpp:94] Creating Layer fc1
I0122 16:35:38.118033 54582 net.cpp:435] fc1 <- drop2
I0122 16:35:38.118039 54582 net.cpp:409] fc1 -> fc1
I0122 16:35:38.132061 54582 net.cpp:144] Setting up fc1
I0122 16:35:38.132078 54582 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:38.132081 54582 net.cpp:159] Memory required for data: 64615200
I0122 16:35:38.132087 54582 layer_factory.hpp:77] Creating layer bn5
I0122 16:35:38.132094 54582 net.cpp:94] Creating Layer bn5
I0122 16:35:38.132098 54582 net.cpp:435] bn5 <- fc1
I0122 16:35:38.132105 54582 net.cpp:409] bn5 -> scale5
I0122 16:35:38.132648 54582 net.cpp:144] Setting up bn5
I0122 16:35:38.132654 54582 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:38.132658 54582 net.cpp:159] Memory required for data: 64717600
I0122 16:35:38.132670 54582 layer_factory.hpp:77] Creating layer relu5
I0122 16:35:38.132676 54582 net.cpp:94] Creating Layer relu5
I0122 16:35:38.132678 54582 net.cpp:435] relu5 <- scale5
I0122 16:35:38.132683 54582 net.cpp:409] relu5 -> relu5
I0122 16:35:38.132701 54582 net.cpp:144] Setting up relu5
I0122 16:35:38.132706 54582 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:38.132709 54582 net.cpp:159] Memory required for data: 64820000
I0122 16:35:38.132711 54582 layer_factory.hpp:77] Creating layer drop3
I0122 16:35:38.132716 54582 net.cpp:94] Creating Layer drop3
I0122 16:35:38.132719 54582 net.cpp:435] drop3 <- relu5
I0122 16:35:38.132722 54582 net.cpp:409] drop3 -> drop3
I0122 16:35:38.132761 54582 net.cpp:144] Setting up drop3
I0122 16:35:38.132766 54582 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:35:38.132769 54582 net.cpp:159] Memory required for data: 64922400
I0122 16:35:38.132771 54582 layer_factory.hpp:77] Creating layer fc2
I0122 16:35:38.132779 54582 net.cpp:94] Creating Layer fc2
I0122 16:35:38.132781 54582 net.cpp:435] fc2 <- drop3
I0122 16:35:38.132786 54582 net.cpp:409] fc2 -> fc2
I0122 16:35:38.132928 54582 net.cpp:144] Setting up fc2
I0122 16:35:38.132935 54582 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:38.132937 54582 net.cpp:159] Memory required for data: 64924400
I0122 16:35:38.132941 54582 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:35:38.132948 54582 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:35:38.132954 54582 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:35:38.132958 54582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:35:38.132964 54582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:35:38.132971 54582 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:35:38.133011 54582 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:35:38.133016 54582 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:38.133020 54582 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:38.133023 54582 net.cpp:151] Top shape: 50 10 (500)
I0122 16:35:38.133025 54582 net.cpp:159] Memory required for data: 64930400
I0122 16:35:38.133028 54582 layer_factory.hpp:77] Creating layer loss
I0122 16:35:38.133033 54582 net.cpp:94] Creating Layer loss
I0122 16:35:38.133036 54582 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:35:38.133040 54582 net.cpp:435] loss <- label_data_1_split_0
I0122 16:35:38.133045 54582 net.cpp:409] loss -> loss
I0122 16:35:38.133052 54582 layer_factory.hpp:77] Creating layer loss
I0122 16:35:38.133128 54582 net.cpp:144] Setting up loss
I0122 16:35:38.133134 54582 net.cpp:151] Top shape: (1)
I0122 16:35:38.133136 54582 net.cpp:154]     with loss weight 1
I0122 16:35:38.133147 54582 net.cpp:159] Memory required for data: 64930404
I0122 16:35:38.133150 54582 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:35:38.133157 54582 net.cpp:94] Creating Layer accuracy-top1
I0122 16:35:38.133159 54582 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:35:38.133162 54582 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:35:38.133168 54582 net.cpp:409] accuracy-top1 -> top-1
I0122 16:35:38.133188 54582 net.cpp:144] Setting up accuracy-top1
I0122 16:35:38.133191 54582 net.cpp:151] Top shape: (1)
I0122 16:35:38.133194 54582 net.cpp:159] Memory required for data: 64930408
I0122 16:35:38.133196 54582 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:35:38.133201 54582 net.cpp:94] Creating Layer accuracy-top5
I0122 16:35:38.133204 54582 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:35:38.133208 54582 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:35:38.133213 54582 net.cpp:409] accuracy-top5 -> top-5
I0122 16:35:38.133219 54582 net.cpp:144] Setting up accuracy-top5
I0122 16:35:38.133221 54582 net.cpp:151] Top shape: (1)
I0122 16:35:38.133224 54582 net.cpp:159] Memory required for data: 64930412
I0122 16:35:38.133227 54582 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:35:38.133231 54582 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:35:38.133234 54582 net.cpp:220] loss needs backward computation.
I0122 16:35:38.133239 54582 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:35:38.133241 54582 net.cpp:220] fc2 needs backward computation.
I0122 16:35:38.133245 54582 net.cpp:220] drop3 needs backward computation.
I0122 16:35:38.133249 54582 net.cpp:220] relu5 needs backward computation.
I0122 16:35:38.133251 54582 net.cpp:220] bn5 needs backward computation.
I0122 16:35:38.133255 54582 net.cpp:220] fc1 needs backward computation.
I0122 16:35:38.133257 54582 net.cpp:220] drop2 needs backward computation.
I0122 16:35:38.133260 54582 net.cpp:220] pool2 needs backward computation.
I0122 16:35:38.133263 54582 net.cpp:220] relu4 needs backward computation.
I0122 16:35:38.133266 54582 net.cpp:220] bn4 needs backward computation.
I0122 16:35:38.133270 54582 net.cpp:220] conv4 needs backward computation.
I0122 16:35:38.133273 54582 net.cpp:220] relu3 needs backward computation.
I0122 16:35:38.133276 54582 net.cpp:220] bn3 needs backward computation.
I0122 16:35:38.133280 54582 net.cpp:220] conv3 needs backward computation.
I0122 16:35:38.133285 54582 net.cpp:220] drop1 needs backward computation.
I0122 16:35:38.133287 54582 net.cpp:220] pool1 needs backward computation.
I0122 16:35:38.133291 54582 net.cpp:220] relu2 needs backward computation.
I0122 16:35:38.133292 54582 net.cpp:220] bn2 needs backward computation.
I0122 16:35:38.133296 54582 net.cpp:220] conv2 needs backward computation.
I0122 16:35:38.133299 54582 net.cpp:220] relu1 needs backward computation.
I0122 16:35:38.133302 54582 net.cpp:220] bn1 needs backward computation.
I0122 16:35:38.133306 54582 net.cpp:220] conv1 needs backward computation.
I0122 16:35:38.133309 54582 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:35:38.133312 54582 net.cpp:222] data does not need backward computation.
I0122 16:35:38.133316 54582 net.cpp:264] This network produces output loss
I0122 16:35:38.133318 54582 net.cpp:264] This network produces output top-1
I0122 16:35:38.133322 54582 net.cpp:264] This network produces output top-5
I0122 16:35:38.133345 54582 net.cpp:284] Network initialization done.
I0122 16:35:38.133445 54582 solver.cpp:63] Solver scaffolding done.
I0122 16:35:38.134622 54582 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/sparse.caffemodel
I0122 16:35:38.191623 54582 caffe_interface.cpp:527] Starting Optimization
I0122 16:35:38.191643 54582 solver.cpp:335] Solving 
I0122 16:35:38.191646 54582 solver.cpp:336] Learning Rate Policy: poly
I0122 16:35:38.192881 54582 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:35:38.418634 54582 solver.cpp:517]     Test net output #0: loss = 0.820674 (* 1 = 0.820674 loss)
I0122 16:35:38.418653 54582 solver.cpp:517]     Test net output #1: top-1 = 0.77
I0122 16:35:38.418658 54582 solver.cpp:517]     Test net output #2: top-5 = 0.988333
I0122 16:35:38.435894 54582 solver.cpp:266] Iteration 0 (0 iter/s, 0.244209s/100 iter), loss = 0.203421
I0122 16:35:38.435928 54582 solver.cpp:285]     Train net output #0: loss = 0.203421 (* 1 = 0.203421 loss)
I0122 16:35:38.435955 54582 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:35:39.299768 54582 solver.cpp:266] Iteration 100 (115.767 iter/s, 0.863801s/100 iter), loss = 0.245202
I0122 16:35:39.299794 54582 solver.cpp:285]     Train net output #0: loss = 0.245202 (* 1 = 0.245202 loss)
I0122 16:35:39.299800 54582 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:35:40.162856 54582 solver.cpp:266] Iteration 200 (115.872 iter/s, 0.863024s/100 iter), loss = 0.21156
I0122 16:35:40.162883 54582 solver.cpp:285]     Train net output #0: loss = 0.21156 (* 1 = 0.21156 loss)
I0122 16:35:40.162889 54582 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:35:41.025204 54582 solver.cpp:266] Iteration 300 (115.971 iter/s, 0.862282s/100 iter), loss = 0.238103
I0122 16:35:41.025231 54582 solver.cpp:285]     Train net output #0: loss = 0.238103 (* 1 = 0.238103 loss)
I0122 16:35:41.025238 54582 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:35:41.891034 54582 solver.cpp:266] Iteration 400 (115.505 iter/s, 0.865763s/100 iter), loss = 0.215545
I0122 16:35:41.891062 54582 solver.cpp:285]     Train net output #0: loss = 0.215545 (* 1 = 0.215545 loss)
I0122 16:35:41.891067 54582 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:35:42.752544 54582 solver.cpp:266] Iteration 500 (116.084 iter/s, 0.861444s/100 iter), loss = 0.304839
I0122 16:35:42.752571 54582 solver.cpp:285]     Train net output #0: loss = 0.304839 (* 1 = 0.304839 loss)
I0122 16:35:42.752578 54582 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:35:43.617437 54582 solver.cpp:266] Iteration 600 (115.63 iter/s, 0.864825s/100 iter), loss = 0.18142
I0122 16:35:43.617463 54582 solver.cpp:285]     Train net output #0: loss = 0.18142 (* 1 = 0.18142 loss)
I0122 16:35:43.617470 54582 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:35:44.483996 54582 solver.cpp:266] Iteration 700 (115.408 iter/s, 0.866491s/100 iter), loss = 0.410873
I0122 16:35:44.484022 54582 solver.cpp:285]     Train net output #0: loss = 0.410873 (* 1 = 0.410873 loss)
I0122 16:35:44.484028 54582 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:35:45.350687 54582 solver.cpp:266] Iteration 800 (115.39 iter/s, 0.866623s/100 iter), loss = 0.299983
I0122 16:35:45.350713 54582 solver.cpp:285]     Train net output #0: loss = 0.299983 (* 1 = 0.299983 loss)
I0122 16:35:45.350719 54582 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:35:46.236369 54582 solver.cpp:266] Iteration 900 (112.916 iter/s, 0.885616s/100 iter), loss = 0.260073
I0122 16:35:46.236397 54582 solver.cpp:285]     Train net output #0: loss = 0.260073 (* 1 = 0.260073 loss)
I0122 16:35:46.236403 54582 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:35:47.101915 54582 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:35:47.324585 54582 solver.cpp:517]     Test net output #0: loss = 1.12341 (* 1 = 1.12341 loss)
I0122 16:35:47.324602 54582 solver.cpp:517]     Test net output #1: top-1 = 0.759445
I0122 16:35:47.324607 54582 solver.cpp:517]     Test net output #2: top-5 = 0.973556
I0122 16:35:47.332675 54582 solver.cpp:266] Iteration 1000 (91.2214 iter/s, 1.09623s/100 iter), loss = 0.312019
I0122 16:35:47.332693 54582 solver.cpp:285]     Train net output #0: loss = 0.312019 (* 1 = 0.312019 loss)
I0122 16:35:47.332700 54582 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:35:48.215200 54582 solver.cpp:266] Iteration 1100 (113.319 iter/s, 0.882466s/100 iter), loss = 0.27364
I0122 16:35:48.215229 54582 solver.cpp:285]     Train net output #0: loss = 0.27364 (* 1 = 0.27364 loss)
I0122 16:35:48.215234 54582 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:35:49.100710 54582 solver.cpp:266] Iteration 1200 (112.938 iter/s, 0.88544s/100 iter), loss = 0.281273
I0122 16:35:49.100740 54582 solver.cpp:285]     Train net output #0: loss = 0.281273 (* 1 = 0.281273 loss)
I0122 16:35:49.100746 54582 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:35:49.969316 54582 solver.cpp:266] Iteration 1300 (115.136 iter/s, 0.868534s/100 iter), loss = 0.276964
I0122 16:35:49.969343 54582 solver.cpp:285]     Train net output #0: loss = 0.276964 (* 1 = 0.276964 loss)
I0122 16:35:49.969375 54582 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:35:50.840427 54582 solver.cpp:266] Iteration 1400 (114.805 iter/s, 0.871043s/100 iter), loss = 0.265172
I0122 16:35:50.840456 54582 solver.cpp:285]     Train net output #0: loss = 0.265172 (* 1 = 0.265172 loss)
I0122 16:35:50.840461 54582 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:35:51.723361 54582 solver.cpp:266] Iteration 1500 (113.268 iter/s, 0.882862s/100 iter), loss = 0.220212
I0122 16:35:51.723388 54582 solver.cpp:285]     Train net output #0: loss = 0.220212 (* 1 = 0.220212 loss)
I0122 16:35:51.723394 54582 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:35:52.597438 54582 solver.cpp:266] Iteration 1600 (114.415 iter/s, 0.874009s/100 iter), loss = 0.250153
I0122 16:35:52.597468 54582 solver.cpp:285]     Train net output #0: loss = 0.250153 (* 1 = 0.250153 loss)
I0122 16:35:52.597474 54582 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:35:53.493177 54582 solver.cpp:266] Iteration 1700 (111.649 iter/s, 0.895667s/100 iter), loss = 0.190651
I0122 16:35:53.493207 54582 solver.cpp:285]     Train net output #0: loss = 0.190651 (* 1 = 0.190651 loss)
I0122 16:35:53.493213 54582 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:35:54.372191 54582 solver.cpp:266] Iteration 1800 (113.773 iter/s, 0.878942s/100 iter), loss = 0.224128
I0122 16:35:54.372221 54582 solver.cpp:285]     Train net output #0: loss = 0.224128 (* 1 = 0.224128 loss)
I0122 16:35:54.372227 54582 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:35:55.233961 54582 solver.cpp:266] Iteration 1900 (116.05 iter/s, 0.861698s/100 iter), loss = 0.234391
I0122 16:35:55.233989 54582 solver.cpp:285]     Train net output #0: loss = 0.234391 (* 1 = 0.234391 loss)
I0122 16:35:55.233995 54582 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:35:56.091627 54582 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:35:56.313932 54582 solver.cpp:517]     Test net output #0: loss = 0.769916 (* 1 = 0.769916 loss)
I0122 16:35:56.313949 54582 solver.cpp:517]     Test net output #1: top-1 = 0.799
I0122 16:35:56.313953 54582 solver.cpp:517]     Test net output #2: top-5 = 0.982333
I0122 16:35:56.322039 54582 solver.cpp:266] Iteration 2000 (91.9115 iter/s, 1.088s/100 iter), loss = 0.200302
I0122 16:35:56.322057 54582 solver.cpp:285]     Train net output #0: loss = 0.200302 (* 1 = 0.200302 loss)
I0122 16:35:56.322063 54582 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:35:57.218502 54582 solver.cpp:266] Iteration 2100 (111.557 iter/s, 0.8964s/100 iter), loss = 0.223822
I0122 16:35:57.218530 54582 solver.cpp:285]     Train net output #0: loss = 0.223822 (* 1 = 0.223822 loss)
I0122 16:35:57.218536 54582 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:35:58.140815 54582 solver.cpp:266] Iteration 2200 (108.432 iter/s, 0.92224s/100 iter), loss = 0.260225
I0122 16:35:58.140843 54582 solver.cpp:285]     Train net output #0: loss = 0.260225 (* 1 = 0.260225 loss)
I0122 16:35:58.140849 54582 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:35:59.017989 54582 solver.cpp:266] Iteration 2300 (114.012 iter/s, 0.877103s/100 iter), loss = 0.17737
I0122 16:35:59.018018 54582 solver.cpp:285]     Train net output #0: loss = 0.17737 (* 1 = 0.17737 loss)
I0122 16:35:59.018023 54582 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:35:59.883846 54582 solver.cpp:266] Iteration 2400 (115.502 iter/s, 0.865787s/100 iter), loss = 0.191074
I0122 16:35:59.883873 54582 solver.cpp:285]     Train net output #0: loss = 0.191074 (* 1 = 0.191074 loss)
I0122 16:35:59.883878 54582 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:36:00.774418 54582 solver.cpp:266] Iteration 2500 (112.296 iter/s, 0.890501s/100 iter), loss = 0.189927
I0122 16:36:00.774446 54582 solver.cpp:285]     Train net output #0: loss = 0.189927 (* 1 = 0.189927 loss)
I0122 16:36:00.774452 54582 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:36:01.670470 54582 solver.cpp:266] Iteration 2600 (111.61 iter/s, 0.895981s/100 iter), loss = 0.253563
I0122 16:36:01.670514 54582 solver.cpp:285]     Train net output #0: loss = 0.253563 (* 1 = 0.253563 loss)
I0122 16:36:01.670521 54582 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:36:02.533919 54582 solver.cpp:266] Iteration 2700 (115.826 iter/s, 0.863365s/100 iter), loss = 0.254284
I0122 16:36:02.533946 54582 solver.cpp:285]     Train net output #0: loss = 0.254284 (* 1 = 0.254284 loss)
I0122 16:36:02.533952 54582 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:36:03.408542 54582 solver.cpp:266] Iteration 2800 (114.344 iter/s, 0.874553s/100 iter), loss = 0.145692
I0122 16:36:03.408572 54582 solver.cpp:285]     Train net output #0: loss = 0.145692 (* 1 = 0.145692 loss)
I0122 16:36:03.408578 54582 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:36:04.280077 54582 solver.cpp:266] Iteration 2900 (114.75 iter/s, 0.871462s/100 iter), loss = 0.231949
I0122 16:36:04.280103 54582 solver.cpp:285]     Train net output #0: loss = 0.231949 (* 1 = 0.231949 loss)
I0122 16:36:04.280109 54582 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:36:05.145535 54582 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:36:05.376030 54582 solver.cpp:517]     Test net output #0: loss = 0.662854 (* 1 = 0.662854 loss)
I0122 16:36:05.376049 54582 solver.cpp:517]     Test net output #1: top-1 = 0.810222
I0122 16:36:05.376054 54582 solver.cpp:517]     Test net output #2: top-5 = 0.985445
I0122 16:36:05.384135 54582 solver.cpp:266] Iteration 3000 (90.5811 iter/s, 1.10398s/100 iter), loss = 0.205079
I0122 16:36:05.384168 54582 solver.cpp:285]     Train net output #0: loss = 0.205079 (* 1 = 0.205079 loss)
I0122 16:36:05.384176 54582 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:36:06.286715 54582 solver.cpp:266] Iteration 3100 (110.803 iter/s, 0.902502s/100 iter), loss = 0.132031
I0122 16:36:06.286744 54582 solver.cpp:285]     Train net output #0: loss = 0.132031 (* 1 = 0.132031 loss)
I0122 16:36:06.286751 54582 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:36:07.252632 54582 solver.cpp:266] Iteration 3200 (103.537 iter/s, 0.965842s/100 iter), loss = 0.230708
I0122 16:36:07.252662 54582 solver.cpp:285]     Train net output #0: loss = 0.230708 (* 1 = 0.230708 loss)
I0122 16:36:07.252667 54582 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:36:08.221038 54582 solver.cpp:266] Iteration 3300 (103.271 iter/s, 0.968328s/100 iter), loss = 0.106607
I0122 16:36:08.221230 54582 solver.cpp:285]     Train net output #0: loss = 0.106607 (* 1 = 0.106607 loss)
I0122 16:36:08.221240 54582 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:36:09.101091 54582 solver.cpp:266] Iteration 3400 (113.66 iter/s, 0.879821s/100 iter), loss = 0.303012
I0122 16:36:09.101120 54582 solver.cpp:285]     Train net output #0: loss = 0.303012 (* 1 = 0.303012 loss)
I0122 16:36:09.101125 54582 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:36:09.963953 54582 solver.cpp:266] Iteration 3500 (115.903 iter/s, 0.862791s/100 iter), loss = 0.202822
I0122 16:36:09.963979 54582 solver.cpp:285]     Train net output #0: loss = 0.202822 (* 1 = 0.202822 loss)
I0122 16:36:09.963985 54582 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:36:10.843253 54582 solver.cpp:266] Iteration 3600 (113.736 iter/s, 0.879231s/100 iter), loss = 0.199398
I0122 16:36:10.843281 54582 solver.cpp:285]     Train net output #0: loss = 0.199398 (* 1 = 0.199398 loss)
I0122 16:36:10.843287 54582 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:36:11.731833 54582 solver.cpp:266] Iteration 3700 (112.548 iter/s, 0.888508s/100 iter), loss = 0.153713
I0122 16:36:11.731860 54582 solver.cpp:285]     Train net output #0: loss = 0.153713 (* 1 = 0.153713 loss)
I0122 16:36:11.731868 54582 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:36:12.619385 54582 solver.cpp:266] Iteration 3800 (112.678 iter/s, 0.887481s/100 iter), loss = 0.160197
I0122 16:36:12.619413 54582 solver.cpp:285]     Train net output #0: loss = 0.160197 (* 1 = 0.160197 loss)
I0122 16:36:12.619419 54582 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:36:13.490499 54582 solver.cpp:266] Iteration 3900 (114.805 iter/s, 0.871042s/100 iter), loss = 0.167277
I0122 16:36:13.490525 54582 solver.cpp:285]     Train net output #0: loss = 0.167277 (* 1 = 0.167277 loss)
I0122 16:36:13.490530 54582 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:36:14.393831 54582 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:36:14.617771 54582 solver.cpp:517]     Test net output #0: loss = 0.547126 (* 1 = 0.547126 loss)
I0122 16:36:14.617794 54582 solver.cpp:517]     Test net output #1: top-1 = 0.828333
I0122 16:36:14.617797 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0122 16:36:14.625891 54582 solver.cpp:266] Iteration 4000 (88.081 iter/s, 1.13532s/100 iter), loss = 0.205577
I0122 16:36:14.625913 54582 solver.cpp:285]     Train net output #0: loss = 0.205577 (* 1 = 0.205577 loss)
I0122 16:36:14.625919 54582 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:36:15.502903 54582 solver.cpp:266] Iteration 4100 (114.032 iter/s, 0.876946s/100 iter), loss = 0.19265
I0122 16:36:15.502931 54582 solver.cpp:285]     Train net output #0: loss = 0.19265 (* 1 = 0.19265 loss)
I0122 16:36:15.502938 54582 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:36:16.369421 54582 solver.cpp:266] Iteration 4200 (115.414 iter/s, 0.866448s/100 iter), loss = 0.190066
I0122 16:36:16.369448 54582 solver.cpp:285]     Train net output #0: loss = 0.190066 (* 1 = 0.190066 loss)
I0122 16:36:16.369454 54582 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:36:17.235991 54582 solver.cpp:266] Iteration 4300 (115.407 iter/s, 0.866502s/100 iter), loss = 0.247337
I0122 16:36:17.236019 54582 solver.cpp:285]     Train net output #0: loss = 0.247337 (* 1 = 0.247337 loss)
I0122 16:36:17.236024 54582 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:36:18.108122 54582 solver.cpp:266] Iteration 4400 (114.671 iter/s, 0.872061s/100 iter), loss = 0.219679
I0122 16:36:18.108153 54582 solver.cpp:285]     Train net output #0: loss = 0.219679 (* 1 = 0.219679 loss)
I0122 16:36:18.108158 54582 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:36:18.971729 54582 solver.cpp:266] Iteration 4500 (115.803 iter/s, 0.863537s/100 iter), loss = 0.213285
I0122 16:36:18.971758 54582 solver.cpp:285]     Train net output #0: loss = 0.213285 (* 1 = 0.213285 loss)
I0122 16:36:18.971765 54582 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:36:19.839733 54582 solver.cpp:266] Iteration 4600 (115.216 iter/s, 0.867935s/100 iter), loss = 0.17052
I0122 16:36:19.839779 54582 solver.cpp:285]     Train net output #0: loss = 0.17052 (* 1 = 0.17052 loss)
I0122 16:36:19.839785 54582 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:36:20.702651 54582 solver.cpp:266] Iteration 4700 (115.897 iter/s, 0.862833s/100 iter), loss = 0.217085
I0122 16:36:20.702679 54582 solver.cpp:285]     Train net output #0: loss = 0.217085 (* 1 = 0.217085 loss)
I0122 16:36:20.702684 54582 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:36:21.565703 54582 solver.cpp:266] Iteration 4800 (115.877 iter/s, 0.862984s/100 iter), loss = 0.225987
I0122 16:36:21.565732 54582 solver.cpp:285]     Train net output #0: loss = 0.225987 (* 1 = 0.225987 loss)
I0122 16:36:21.565737 54582 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:36:22.432562 54582 solver.cpp:266] Iteration 4900 (115.368 iter/s, 0.86679s/100 iter), loss = 0.342412
I0122 16:36:22.432588 54582 solver.cpp:285]     Train net output #0: loss = 0.342412 (* 1 = 0.342412 loss)
I0122 16:36:22.432595 54582 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:36:23.299196 54582 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:36:23.518579 54582 solver.cpp:517]     Test net output #0: loss = 0.57387 (* 1 = 0.57387 loss)
I0122 16:36:23.518594 54582 solver.cpp:517]     Test net output #1: top-1 = 0.831333
I0122 16:36:23.518599 54582 solver.cpp:517]     Test net output #2: top-5 = 0.988111
I0122 16:36:23.526650 54582 solver.cpp:266] Iteration 5000 (91.4063 iter/s, 1.09402s/100 iter), loss = 0.172349
I0122 16:36:23.526669 54582 solver.cpp:285]     Train net output #0: loss = 0.172349 (* 1 = 0.172349 loss)
I0122 16:36:23.526675 54582 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:36:24.392431 54582 solver.cpp:266] Iteration 5100 (115.51 iter/s, 0.865722s/100 iter), loss = 0.215994
I0122 16:36:24.392458 54582 solver.cpp:285]     Train net output #0: loss = 0.215994 (* 1 = 0.215994 loss)
I0122 16:36:24.392463 54582 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:36:25.263207 54582 solver.cpp:266] Iteration 5200 (114.849 iter/s, 0.870708s/100 iter), loss = 0.22846
I0122 16:36:25.263236 54582 solver.cpp:285]     Train net output #0: loss = 0.22846 (* 1 = 0.22846 loss)
I0122 16:36:25.263242 54582 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:36:26.129931 54582 solver.cpp:266] Iteration 5300 (115.386 iter/s, 0.866655s/100 iter), loss = 0.181565
I0122 16:36:26.129958 54582 solver.cpp:285]     Train net output #0: loss = 0.181565 (* 1 = 0.181565 loss)
I0122 16:36:26.129964 54582 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:36:26.993150 54582 solver.cpp:266] Iteration 5400 (115.854 iter/s, 0.863152s/100 iter), loss = 0.21162
I0122 16:36:26.993177 54582 solver.cpp:285]     Train net output #0: loss = 0.21162 (* 1 = 0.21162 loss)
I0122 16:36:26.993183 54582 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:36:27.860131 54582 solver.cpp:266] Iteration 5500 (115.352 iter/s, 0.86691s/100 iter), loss = 0.257359
I0122 16:36:27.860157 54582 solver.cpp:285]     Train net output #0: loss = 0.257359 (* 1 = 0.257359 loss)
I0122 16:36:27.860162 54582 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:36:28.723917 54582 solver.cpp:266] Iteration 5600 (115.778 iter/s, 0.863719s/100 iter), loss = 0.186753
I0122 16:36:28.723945 54582 solver.cpp:285]     Train net output #0: loss = 0.186753 (* 1 = 0.186753 loss)
I0122 16:36:28.723951 54582 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:36:29.601840 54582 solver.cpp:266] Iteration 5700 (113.914 iter/s, 0.877853s/100 iter), loss = 0.232122
I0122 16:36:29.601869 54582 solver.cpp:285]     Train net output #0: loss = 0.232122 (* 1 = 0.232122 loss)
I0122 16:36:29.601876 54582 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:36:30.472229 54582 solver.cpp:266] Iteration 5800 (114.9 iter/s, 0.870319s/100 iter), loss = 0.237308
I0122 16:36:30.472257 54582 solver.cpp:285]     Train net output #0: loss = 0.237308 (* 1 = 0.237308 loss)
I0122 16:36:30.472290 54582 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:36:31.336747 54582 solver.cpp:266] Iteration 5900 (115.681 iter/s, 0.86445s/100 iter), loss = 0.218734
I0122 16:36:31.336776 54582 solver.cpp:285]     Train net output #0: loss = 0.218734 (* 1 = 0.218734 loss)
I0122 16:36:31.336781 54582 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:36:32.193300 54582 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:36:32.414037 54582 solver.cpp:517]     Test net output #0: loss = 0.546847 (* 1 = 0.546847 loss)
I0122 16:36:32.414057 54582 solver.cpp:517]     Test net output #1: top-1 = 0.843111
I0122 16:36:32.414062 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:36:32.422137 54582 solver.cpp:266] Iteration 6000 (92.1391 iter/s, 1.08532s/100 iter), loss = 0.164418
I0122 16:36:32.422156 54582 solver.cpp:285]     Train net output #0: loss = 0.164418 (* 1 = 0.164418 loss)
I0122 16:36:32.422163 54582 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:36:33.289830 54582 solver.cpp:266] Iteration 6100 (115.256 iter/s, 0.867632s/100 iter), loss = 0.149099
I0122 16:36:33.289858 54582 solver.cpp:285]     Train net output #0: loss = 0.149099 (* 1 = 0.149099 loss)
I0122 16:36:33.289865 54582 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:36:34.159875 54582 solver.cpp:266] Iteration 6200 (114.946 iter/s, 0.869975s/100 iter), loss = 0.163226
I0122 16:36:34.159902 54582 solver.cpp:285]     Train net output #0: loss = 0.163226 (* 1 = 0.163226 loss)
I0122 16:36:34.159909 54582 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:36:35.024866 54582 solver.cpp:266] Iteration 6300 (115.617 iter/s, 0.864923s/100 iter), loss = 0.220251
I0122 16:36:35.024894 54582 solver.cpp:285]     Train net output #0: loss = 0.220251 (* 1 = 0.220251 loss)
I0122 16:36:35.024900 54582 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:36:35.892518 54582 solver.cpp:266] Iteration 6400 (115.263 iter/s, 0.867582s/100 iter), loss = 0.118036
I0122 16:36:35.892545 54582 solver.cpp:285]     Train net output #0: loss = 0.118036 (* 1 = 0.118036 loss)
I0122 16:36:35.892551 54582 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:36:36.755159 54582 solver.cpp:266] Iteration 6500 (115.932 iter/s, 0.862573s/100 iter), loss = 0.232045
I0122 16:36:36.755187 54582 solver.cpp:285]     Train net output #0: loss = 0.232045 (* 1 = 0.232045 loss)
I0122 16:36:36.755193 54582 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:36:37.618088 54582 solver.cpp:266] Iteration 6600 (115.894 iter/s, 0.862859s/100 iter), loss = 0.21348
I0122 16:36:37.618113 54582 solver.cpp:285]     Train net output #0: loss = 0.21348 (* 1 = 0.21348 loss)
I0122 16:36:37.618119 54582 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:36:38.487083 54582 solver.cpp:266] Iteration 6700 (115.084 iter/s, 0.868927s/100 iter), loss = 0.183589
I0122 16:36:38.487185 54582 solver.cpp:285]     Train net output #0: loss = 0.183589 (* 1 = 0.183589 loss)
I0122 16:36:38.487192 54582 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:36:39.355736 54582 solver.cpp:266] Iteration 6800 (115.142 iter/s, 0.868493s/100 iter), loss = 0.196705
I0122 16:36:39.355762 54582 solver.cpp:285]     Train net output #0: loss = 0.196705 (* 1 = 0.196705 loss)
I0122 16:36:39.355768 54582 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:36:40.224817 54582 solver.cpp:266] Iteration 6900 (115.073 iter/s, 0.869012s/100 iter), loss = 0.172981
I0122 16:36:40.224844 54582 solver.cpp:285]     Train net output #0: loss = 0.172981 (* 1 = 0.172981 loss)
I0122 16:36:40.224851 54582 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:36:41.084947 54582 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:36:41.304520 54582 solver.cpp:517]     Test net output #0: loss = 0.556574 (* 1 = 0.556574 loss)
I0122 16:36:41.304544 54582 solver.cpp:517]     Test net output #1: top-1 = 0.834222
I0122 16:36:41.304548 54582 solver.cpp:517]     Test net output #2: top-5 = 0.988556
I0122 16:36:41.312620 54582 solver.cpp:266] Iteration 7000 (91.9346 iter/s, 1.08773s/100 iter), loss = 0.176988
I0122 16:36:41.312638 54582 solver.cpp:285]     Train net output #0: loss = 0.176988 (* 1 = 0.176988 loss)
I0122 16:36:41.312644 54582 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:36:42.182901 54582 solver.cpp:266] Iteration 7100 (114.913 iter/s, 0.870222s/100 iter), loss = 0.185422
I0122 16:36:42.182929 54582 solver.cpp:285]     Train net output #0: loss = 0.185422 (* 1 = 0.185422 loss)
I0122 16:36:42.182934 54582 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:36:43.050350 54582 solver.cpp:266] Iteration 7200 (115.29 iter/s, 0.86738s/100 iter), loss = 0.234715
I0122 16:36:43.050379 54582 solver.cpp:285]     Train net output #0: loss = 0.234715 (* 1 = 0.234715 loss)
I0122 16:36:43.050385 54582 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:36:43.915292 54582 solver.cpp:266] Iteration 7300 (115.624 iter/s, 0.864872s/100 iter), loss = 0.304333
I0122 16:36:43.915321 54582 solver.cpp:285]     Train net output #0: loss = 0.304333 (* 1 = 0.304333 loss)
I0122 16:36:43.915328 54582 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:36:44.778924 54582 solver.cpp:266] Iteration 7400 (115.799 iter/s, 0.863564s/100 iter), loss = 0.130873
I0122 16:36:44.778952 54582 solver.cpp:285]     Train net output #0: loss = 0.130873 (* 1 = 0.130873 loss)
I0122 16:36:44.778959 54582 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:36:45.667523 54582 solver.cpp:266] Iteration 7500 (112.546 iter/s, 0.888528s/100 iter), loss = 0.186053
I0122 16:36:45.667554 54582 solver.cpp:285]     Train net output #0: loss = 0.186053 (* 1 = 0.186053 loss)
I0122 16:36:45.667560 54582 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:36:46.533982 54582 solver.cpp:266] Iteration 7600 (115.422 iter/s, 0.866387s/100 iter), loss = 0.138264
I0122 16:36:46.534013 54582 solver.cpp:285]     Train net output #0: loss = 0.138264 (* 1 = 0.138264 loss)
I0122 16:36:46.534019 54582 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:36:47.402719 54582 solver.cpp:266] Iteration 7700 (115.119 iter/s, 0.868665s/100 iter), loss = 0.149932
I0122 16:36:47.402748 54582 solver.cpp:285]     Train net output #0: loss = 0.149932 (* 1 = 0.149932 loss)
I0122 16:36:47.402755 54582 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:36:48.267812 54582 solver.cpp:266] Iteration 7800 (115.604 iter/s, 0.865025s/100 iter), loss = 0.186811
I0122 16:36:48.267840 54582 solver.cpp:285]     Train net output #0: loss = 0.186811 (* 1 = 0.186811 loss)
I0122 16:36:48.267846 54582 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:36:49.131021 54582 solver.cpp:266] Iteration 7900 (115.856 iter/s, 0.863141s/100 iter), loss = 0.120771
I0122 16:36:49.131048 54582 solver.cpp:285]     Train net output #0: loss = 0.120771 (* 1 = 0.120771 loss)
I0122 16:36:49.131054 54582 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:36:49.998726 54582 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:36:50.218971 54582 solver.cpp:517]     Test net output #0: loss = 0.581929 (* 1 = 0.581929 loss)
I0122 16:36:50.218986 54582 solver.cpp:517]     Test net output #1: top-1 = 0.829111
I0122 16:36:50.218991 54582 solver.cpp:517]     Test net output #2: top-5 = 0.990222
I0122 16:36:50.227056 54582 solver.cpp:266] Iteration 8000 (91.244 iter/s, 1.09596s/100 iter), loss = 0.117058
I0122 16:36:50.227073 54582 solver.cpp:285]     Train net output #0: loss = 0.117058 (* 1 = 0.117058 loss)
I0122 16:36:50.227097 54582 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:36:51.092947 54582 solver.cpp:266] Iteration 8100 (115.496 iter/s, 0.865833s/100 iter), loss = 0.267933
I0122 16:36:51.092975 54582 solver.cpp:285]     Train net output #0: loss = 0.267933 (* 1 = 0.267933 loss)
I0122 16:36:51.092981 54582 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:36:51.957434 54582 solver.cpp:266] Iteration 8200 (115.684 iter/s, 0.864421s/100 iter), loss = 0.0986973
I0122 16:36:51.957461 54582 solver.cpp:285]     Train net output #0: loss = 0.0986973 (* 1 = 0.0986973 loss)
I0122 16:36:51.957468 54582 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:36:52.825759 54582 solver.cpp:266] Iteration 8300 (115.173 iter/s, 0.868258s/100 iter), loss = 0.151943
I0122 16:36:52.825786 54582 solver.cpp:285]     Train net output #0: loss = 0.151943 (* 1 = 0.151943 loss)
I0122 16:36:52.825793 54582 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:36:53.695128 54582 solver.cpp:266] Iteration 8400 (115.035 iter/s, 0.869303s/100 iter), loss = 0.151637
I0122 16:36:53.695156 54582 solver.cpp:285]     Train net output #0: loss = 0.151637 (* 1 = 0.151637 loss)
I0122 16:36:53.695163 54582 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:36:54.565449 54582 solver.cpp:266] Iteration 8500 (114.909 iter/s, 0.870255s/100 iter), loss = 0.191296
I0122 16:36:54.565475 54582 solver.cpp:285]     Train net output #0: loss = 0.191296 (* 1 = 0.191296 loss)
I0122 16:36:54.565481 54582 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:36:55.431285 54582 solver.cpp:266] Iteration 8600 (115.504 iter/s, 0.865771s/100 iter), loss = 0.193522
I0122 16:36:55.431311 54582 solver.cpp:285]     Train net output #0: loss = 0.193522 (* 1 = 0.193522 loss)
I0122 16:36:55.431318 54582 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:36:56.295691 54582 solver.cpp:266] Iteration 8700 (115.695 iter/s, 0.86434s/100 iter), loss = 0.185247
I0122 16:36:56.295717 54582 solver.cpp:285]     Train net output #0: loss = 0.185247 (* 1 = 0.185247 loss)
I0122 16:36:56.295723 54582 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:36:57.158780 54582 solver.cpp:266] Iteration 8800 (115.872 iter/s, 0.863024s/100 iter), loss = 0.139992
I0122 16:36:57.158808 54582 solver.cpp:285]     Train net output #0: loss = 0.139992 (* 1 = 0.139992 loss)
I0122 16:36:57.158813 54582 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:36:58.022163 54582 solver.cpp:266] Iteration 8900 (115.832 iter/s, 0.863317s/100 iter), loss = 0.206729
I0122 16:36:58.022189 54582 solver.cpp:285]     Train net output #0: loss = 0.206729 (* 1 = 0.206729 loss)
I0122 16:36:58.022195 54582 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:36:58.877060 54582 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:36:59.095953 54582 solver.cpp:517]     Test net output #0: loss = 0.47734 (* 1 = 0.47734 loss)
I0122 16:36:59.095968 54582 solver.cpp:517]     Test net output #1: top-1 = 0.848444
I0122 16:36:59.095973 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991334
I0122 16:36:59.104027 54582 solver.cpp:266] Iteration 9000 (92.439 iter/s, 1.08179s/100 iter), loss = 0.107139
I0122 16:36:59.104044 54582 solver.cpp:285]     Train net output #0: loss = 0.107139 (* 1 = 0.107139 loss)
I0122 16:36:59.104050 54582 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:36:59.968657 54582 solver.cpp:266] Iteration 9100 (115.664 iter/s, 0.864573s/100 iter), loss = 0.126407
I0122 16:36:59.968683 54582 solver.cpp:285]     Train net output #0: loss = 0.126407 (* 1 = 0.126407 loss)
I0122 16:36:59.968708 54582 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:37:00.833305 54582 solver.cpp:266] Iteration 9200 (115.662 iter/s, 0.864584s/100 iter), loss = 0.213593
I0122 16:37:00.833333 54582 solver.cpp:285]     Train net output #0: loss = 0.213593 (* 1 = 0.213593 loss)
I0122 16:37:00.833338 54582 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:37:01.696899 54582 solver.cpp:266] Iteration 9300 (115.804 iter/s, 0.863529s/100 iter), loss = 0.112899
I0122 16:37:01.696924 54582 solver.cpp:285]     Train net output #0: loss = 0.112899 (* 1 = 0.112899 loss)
I0122 16:37:01.696930 54582 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:37:02.559453 54582 solver.cpp:266] Iteration 9400 (115.944 iter/s, 0.862489s/100 iter), loss = 0.157776
I0122 16:37:02.559479 54582 solver.cpp:285]     Train net output #0: loss = 0.157776 (* 1 = 0.157776 loss)
I0122 16:37:02.559484 54582 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:37:03.422503 54582 solver.cpp:266] Iteration 9500 (115.877 iter/s, 0.862986s/100 iter), loss = 0.119459
I0122 16:37:03.422528 54582 solver.cpp:285]     Train net output #0: loss = 0.119459 (* 1 = 0.119459 loss)
I0122 16:37:03.422533 54582 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:37:04.285837 54582 solver.cpp:266] Iteration 9600 (115.838 iter/s, 0.863272s/100 iter), loss = 0.178144
I0122 16:37:04.285863 54582 solver.cpp:285]     Train net output #0: loss = 0.178144 (* 1 = 0.178144 loss)
I0122 16:37:04.285868 54582 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:37:05.149286 54582 solver.cpp:266] Iteration 9700 (115.823 iter/s, 0.863385s/100 iter), loss = 0.21042
I0122 16:37:05.149312 54582 solver.cpp:285]     Train net output #0: loss = 0.21042 (* 1 = 0.21042 loss)
I0122 16:37:05.149317 54582 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:37:06.012526 54582 solver.cpp:266] Iteration 9800 (115.851 iter/s, 0.863176s/100 iter), loss = 0.146022
I0122 16:37:06.012550 54582 solver.cpp:285]     Train net output #0: loss = 0.146022 (* 1 = 0.146022 loss)
I0122 16:37:06.012557 54582 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:37:06.876348 54582 solver.cpp:266] Iteration 9900 (115.773 iter/s, 0.863759s/100 iter), loss = 0.261924
I0122 16:37:06.876374 54582 solver.cpp:285]     Train net output #0: loss = 0.261924 (* 1 = 0.261924 loss)
I0122 16:37:06.876379 54582 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:37:07.731937 54582 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:37:07.950994 54582 solver.cpp:517]     Test net output #0: loss = 0.488575 (* 1 = 0.488575 loss)
I0122 16:37:07.951007 54582 solver.cpp:517]     Test net output #1: top-1 = 0.844111
I0122 16:37:07.951014 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:37:07.959074 54582 solver.cpp:266] Iteration 10000 (92.3653 iter/s, 1.08266s/100 iter), loss = 0.208131
I0122 16:37:07.959092 54582 solver.cpp:285]     Train net output #0: loss = 0.208131 (* 1 = 0.208131 loss)
I0122 16:37:07.959098 54582 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:37:08.822584 54582 solver.cpp:266] Iteration 10100 (115.814 iter/s, 0.863453s/100 iter), loss = 0.201595
I0122 16:37:08.822695 54582 solver.cpp:285]     Train net output #0: loss = 0.201595 (* 1 = 0.201595 loss)
I0122 16:37:08.822703 54582 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:37:09.686139 54582 solver.cpp:266] Iteration 10200 (115.82 iter/s, 0.863406s/100 iter), loss = 0.187302
I0122 16:37:09.686164 54582 solver.cpp:285]     Train net output #0: loss = 0.187302 (* 1 = 0.187302 loss)
I0122 16:37:09.686169 54582 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:37:10.549970 54582 solver.cpp:266] Iteration 10300 (115.772 iter/s, 0.863767s/100 iter), loss = 0.194602
I0122 16:37:10.549995 54582 solver.cpp:285]     Train net output #0: loss = 0.194602 (* 1 = 0.194602 loss)
I0122 16:37:10.550001 54582 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:37:11.414618 54582 solver.cpp:266] Iteration 10400 (115.663 iter/s, 0.864584s/100 iter), loss = 0.148988
I0122 16:37:11.414644 54582 solver.cpp:285]     Train net output #0: loss = 0.148988 (* 1 = 0.148988 loss)
I0122 16:37:11.414650 54582 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:37:12.276665 54582 solver.cpp:266] Iteration 10500 (116.012 iter/s, 0.861983s/100 iter), loss = 0.124527
I0122 16:37:12.276690 54582 solver.cpp:285]     Train net output #0: loss = 0.124527 (* 1 = 0.124527 loss)
I0122 16:37:12.276696 54582 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:37:13.140647 54582 solver.cpp:266] Iteration 10600 (115.751 iter/s, 0.86392s/100 iter), loss = 0.127045
I0122 16:37:13.140674 54582 solver.cpp:285]     Train net output #0: loss = 0.127045 (* 1 = 0.127045 loss)
I0122 16:37:13.140679 54582 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:37:14.005322 54582 solver.cpp:266] Iteration 10700 (115.659 iter/s, 0.86461s/100 iter), loss = 0.182829
I0122 16:37:14.005348 54582 solver.cpp:285]     Train net output #0: loss = 0.182829 (* 1 = 0.182829 loss)
I0122 16:37:14.005353 54582 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:37:14.868247 54582 solver.cpp:266] Iteration 10800 (115.894 iter/s, 0.86286s/100 iter), loss = 0.136356
I0122 16:37:14.868273 54582 solver.cpp:285]     Train net output #0: loss = 0.136356 (* 1 = 0.136356 loss)
I0122 16:37:14.868278 54582 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:37:15.733387 54582 solver.cpp:266] Iteration 10900 (115.597 iter/s, 0.865076s/100 iter), loss = 0.197893
I0122 16:37:15.733415 54582 solver.cpp:285]     Train net output #0: loss = 0.197893 (* 1 = 0.197893 loss)
I0122 16:37:15.733422 54582 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:37:16.591802 54582 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:37:16.811302 54582 solver.cpp:517]     Test net output #0: loss = 0.486102 (* 1 = 0.486102 loss)
I0122 16:37:16.811326 54582 solver.cpp:517]     Test net output #1: top-1 = 0.848889
I0122 16:37:16.811331 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991222
I0122 16:37:16.819393 54582 solver.cpp:266] Iteration 11000 (92.0864 iter/s, 1.08594s/100 iter), loss = 0.204619
I0122 16:37:16.819411 54582 solver.cpp:285]     Train net output #0: loss = 0.204619 (* 1 = 0.204619 loss)
I0122 16:37:16.819417 54582 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:37:17.682039 54582 solver.cpp:266] Iteration 11100 (115.93 iter/s, 0.862589s/100 iter), loss = 0.123141
I0122 16:37:17.682066 54582 solver.cpp:285]     Train net output #0: loss = 0.123141 (* 1 = 0.123141 loss)
I0122 16:37:17.682072 54582 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:37:18.545238 54582 solver.cpp:266] Iteration 11200 (115.857 iter/s, 0.863134s/100 iter), loss = 0.11958
I0122 16:37:18.545264 54582 solver.cpp:285]     Train net output #0: loss = 0.11958 (* 1 = 0.11958 loss)
I0122 16:37:18.545269 54582 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:37:19.408560 54582 solver.cpp:266] Iteration 11300 (115.84 iter/s, 0.863257s/100 iter), loss = 0.158068
I0122 16:37:19.408586 54582 solver.cpp:285]     Train net output #0: loss = 0.158068 (* 1 = 0.158068 loss)
I0122 16:37:19.408592 54582 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:37:20.271574 54582 solver.cpp:266] Iteration 11400 (115.882 iter/s, 0.862949s/100 iter), loss = 0.20382
I0122 16:37:20.271600 54582 solver.cpp:285]     Train net output #0: loss = 0.20382 (* 1 = 0.20382 loss)
I0122 16:37:20.271605 54582 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:37:21.134155 54582 solver.cpp:266] Iteration 11500 (115.94 iter/s, 0.862517s/100 iter), loss = 0.141699
I0122 16:37:21.134182 54582 solver.cpp:285]     Train net output #0: loss = 0.141699 (* 1 = 0.141699 loss)
I0122 16:37:21.134187 54582 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:37:22.004056 54582 solver.cpp:266] Iteration 11600 (114.964 iter/s, 0.869835s/100 iter), loss = 0.14163
I0122 16:37:22.004084 54582 solver.cpp:285]     Train net output #0: loss = 0.14163 (* 1 = 0.14163 loss)
I0122 16:37:22.004091 54582 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:37:22.868028 54582 solver.cpp:266] Iteration 11700 (115.753 iter/s, 0.863905s/100 iter), loss = 0.147222
I0122 16:37:22.868057 54582 solver.cpp:285]     Train net output #0: loss = 0.147222 (* 1 = 0.147222 loss)
I0122 16:37:22.868062 54582 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:37:23.731418 54582 solver.cpp:266] Iteration 11800 (115.831 iter/s, 0.863323s/100 iter), loss = 0.1478
I0122 16:37:23.731447 54582 solver.cpp:285]     Train net output #0: loss = 0.1478 (* 1 = 0.1478 loss)
I0122 16:37:23.731453 54582 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:37:24.594457 54582 solver.cpp:266] Iteration 11900 (115.879 iter/s, 0.862971s/100 iter), loss = 0.183588
I0122 16:37:24.594486 54582 solver.cpp:285]     Train net output #0: loss = 0.183588 (* 1 = 0.183588 loss)
I0122 16:37:24.594492 54582 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:37:25.448624 54582 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:37:25.667388 54582 solver.cpp:517]     Test net output #0: loss = 0.481484 (* 1 = 0.481484 loss)
I0122 16:37:25.667403 54582 solver.cpp:517]     Test net output #1: top-1 = 0.850222
I0122 16:37:25.667407 54582 solver.cpp:517]     Test net output #2: top-5 = 0.990889
I0122 16:37:25.675508 54582 solver.cpp:266] Iteration 12000 (92.5087 iter/s, 1.08098s/100 iter), loss = 0.156232
I0122 16:37:25.675529 54582 solver.cpp:285]     Train net output #0: loss = 0.156232 (* 1 = 0.156232 loss)
I0122 16:37:25.675534 54582 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:37:26.539868 54582 solver.cpp:266] Iteration 12100 (115.7 iter/s, 0.864301s/100 iter), loss = 0.119435
I0122 16:37:26.539896 54582 solver.cpp:285]     Train net output #0: loss = 0.119435 (* 1 = 0.119435 loss)
I0122 16:37:26.539902 54582 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:37:27.409858 54582 solver.cpp:266] Iteration 12200 (114.953 iter/s, 0.869923s/100 iter), loss = 0.270255
I0122 16:37:27.409886 54582 solver.cpp:285]     Train net output #0: loss = 0.270255 (* 1 = 0.270255 loss)
I0122 16:37:27.409891 54582 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:37:28.277660 54582 solver.cpp:266] Iteration 12300 (115.242 iter/s, 0.867736s/100 iter), loss = 0.146158
I0122 16:37:28.277689 54582 solver.cpp:285]     Train net output #0: loss = 0.146158 (* 1 = 0.146158 loss)
I0122 16:37:28.277694 54582 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:37:29.141084 54582 solver.cpp:266] Iteration 12400 (115.827 iter/s, 0.863356s/100 iter), loss = 0.148348
I0122 16:37:29.141113 54582 solver.cpp:285]     Train net output #0: loss = 0.148348 (* 1 = 0.148348 loss)
I0122 16:37:29.141119 54582 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:37:30.005496 54582 solver.cpp:266] Iteration 12500 (115.695 iter/s, 0.864343s/100 iter), loss = 0.0983777
I0122 16:37:30.005527 54582 solver.cpp:285]     Train net output #0: loss = 0.0983777 (* 1 = 0.0983777 loss)
I0122 16:37:30.005533 54582 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:37:30.868119 54582 solver.cpp:266] Iteration 12600 (115.935 iter/s, 0.862553s/100 iter), loss = 0.200742
I0122 16:37:30.868149 54582 solver.cpp:285]     Train net output #0: loss = 0.200742 (* 1 = 0.200742 loss)
I0122 16:37:30.868172 54582 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:37:31.732733 54582 solver.cpp:266] Iteration 12700 (115.667 iter/s, 0.864548s/100 iter), loss = 0.202135
I0122 16:37:31.732762 54582 solver.cpp:285]     Train net output #0: loss = 0.202135 (* 1 = 0.202135 loss)
I0122 16:37:31.732769 54582 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:37:32.596779 54582 solver.cpp:266] Iteration 12800 (115.744 iter/s, 0.863979s/100 iter), loss = 0.21302
I0122 16:37:32.596809 54582 solver.cpp:285]     Train net output #0: loss = 0.21302 (* 1 = 0.21302 loss)
I0122 16:37:32.596814 54582 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:37:33.462021 54582 solver.cpp:266] Iteration 12900 (115.584 iter/s, 0.865174s/100 iter), loss = 0.216013
I0122 16:37:33.462050 54582 solver.cpp:285]     Train net output #0: loss = 0.216013 (* 1 = 0.216013 loss)
I0122 16:37:33.462056 54582 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:37:34.317337 54582 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:37:34.535686 54582 solver.cpp:517]     Test net output #0: loss = 0.47067 (* 1 = 0.47067 loss)
I0122 16:37:34.535702 54582 solver.cpp:517]     Test net output #1: top-1 = 0.851667
I0122 16:37:34.535706 54582 solver.cpp:517]     Test net output #2: top-5 = 0.990445
I0122 16:37:34.543787 54582 solver.cpp:266] Iteration 13000 (92.4476 iter/s, 1.08169s/100 iter), loss = 0.169622
I0122 16:37:34.543805 54582 solver.cpp:285]     Train net output #0: loss = 0.169622 (* 1 = 0.169622 loss)
I0122 16:37:34.543812 54582 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:37:35.407006 54582 solver.cpp:266] Iteration 13100 (115.853 iter/s, 0.863162s/100 iter), loss = 0.15089
I0122 16:37:35.407033 54582 solver.cpp:285]     Train net output #0: loss = 0.15089 (* 1 = 0.15089 loss)
I0122 16:37:35.407039 54582 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:37:36.270123 54582 solver.cpp:266] Iteration 13200 (115.868 iter/s, 0.863051s/100 iter), loss = 0.255818
I0122 16:37:36.270149 54582 solver.cpp:285]     Train net output #0: loss = 0.255818 (* 1 = 0.255818 loss)
I0122 16:37:36.270154 54582 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:37:37.133761 54582 solver.cpp:266] Iteration 13300 (115.798 iter/s, 0.863576s/100 iter), loss = 0.18814
I0122 16:37:37.133787 54582 solver.cpp:285]     Train net output #0: loss = 0.18814 (* 1 = 0.18814 loss)
I0122 16:37:37.133792 54582 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:37:37.996309 54582 solver.cpp:266] Iteration 13400 (115.944 iter/s, 0.862485s/100 iter), loss = 0.132434
I0122 16:37:37.996335 54582 solver.cpp:285]     Train net output #0: loss = 0.132434 (* 1 = 0.132434 loss)
I0122 16:37:37.996340 54582 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:37:38.860297 54582 solver.cpp:266] Iteration 13500 (115.751 iter/s, 0.863924s/100 iter), loss = 0.112059
I0122 16:37:38.860399 54582 solver.cpp:285]     Train net output #0: loss = 0.112059 (* 1 = 0.112059 loss)
I0122 16:37:38.860407 54582 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:37:39.726527 54582 solver.cpp:266] Iteration 13600 (115.461 iter/s, 0.866093s/100 iter), loss = 0.204329
I0122 16:37:39.726555 54582 solver.cpp:285]     Train net output #0: loss = 0.204329 (* 1 = 0.204329 loss)
I0122 16:37:39.726560 54582 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:37:40.590195 54582 solver.cpp:266] Iteration 13700 (115.794 iter/s, 0.863602s/100 iter), loss = 0.185122
I0122 16:37:40.590222 54582 solver.cpp:285]     Train net output #0: loss = 0.185122 (* 1 = 0.185122 loss)
I0122 16:37:40.590229 54582 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:37:41.454205 54582 solver.cpp:266] Iteration 13800 (115.748 iter/s, 0.863944s/100 iter), loss = 0.125181
I0122 16:37:41.454231 54582 solver.cpp:285]     Train net output #0: loss = 0.125181 (* 1 = 0.125181 loss)
I0122 16:37:41.454236 54582 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:37:42.318516 54582 solver.cpp:266] Iteration 13900 (115.708 iter/s, 0.864247s/100 iter), loss = 0.145097
I0122 16:37:42.318543 54582 solver.cpp:285]     Train net output #0: loss = 0.145097 (* 1 = 0.145097 loss)
I0122 16:37:42.318548 54582 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:37:43.174221 54582 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:37:43.394132 54582 solver.cpp:517]     Test net output #0: loss = 0.466895 (* 1 = 0.466895 loss)
I0122 16:37:43.394148 54582 solver.cpp:517]     Test net output #1: top-1 = 0.853444
I0122 16:37:43.394152 54582 solver.cpp:517]     Test net output #2: top-5 = 0.992778
I0122 16:37:43.402231 54582 solver.cpp:266] Iteration 14000 (92.281 iter/s, 1.08365s/100 iter), loss = 0.155309
I0122 16:37:43.402249 54582 solver.cpp:285]     Train net output #0: loss = 0.155309 (* 1 = 0.155309 loss)
I0122 16:37:43.402256 54582 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:37:44.265756 54582 solver.cpp:266] Iteration 14100 (115.812 iter/s, 0.86347s/100 iter), loss = 0.0803007
I0122 16:37:44.265784 54582 solver.cpp:285]     Train net output #0: loss = 0.0803007 (* 1 = 0.0803007 loss)
I0122 16:37:44.265790 54582 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:37:45.129417 54582 solver.cpp:266] Iteration 14200 (115.795 iter/s, 0.863595s/100 iter), loss = 0.1469
I0122 16:37:45.129443 54582 solver.cpp:285]     Train net output #0: loss = 0.1469 (* 1 = 0.1469 loss)
I0122 16:37:45.129449 54582 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:37:45.991890 54582 solver.cpp:266] Iteration 14300 (115.954 iter/s, 0.862409s/100 iter), loss = 0.0949911
I0122 16:37:45.991917 54582 solver.cpp:285]     Train net output #0: loss = 0.0949911 (* 1 = 0.0949911 loss)
I0122 16:37:45.991924 54582 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:37:46.857863 54582 solver.cpp:266] Iteration 14400 (115.486 iter/s, 0.865907s/100 iter), loss = 0.0851121
I0122 16:37:46.857889 54582 solver.cpp:285]     Train net output #0: loss = 0.0851121 (* 1 = 0.0851121 loss)
I0122 16:37:46.857895 54582 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:37:47.721853 54582 solver.cpp:266] Iteration 14500 (115.751 iter/s, 0.863925s/100 iter), loss = 0.0986315
I0122 16:37:47.721879 54582 solver.cpp:285]     Train net output #0: loss = 0.0986315 (* 1 = 0.0986315 loss)
I0122 16:37:47.721885 54582 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:37:48.586377 54582 solver.cpp:266] Iteration 14600 (115.679 iter/s, 0.864459s/100 iter), loss = 0.131283
I0122 16:37:48.586405 54582 solver.cpp:285]     Train net output #0: loss = 0.131283 (* 1 = 0.131283 loss)
I0122 16:37:48.586410 54582 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:37:49.451702 54582 solver.cpp:266] Iteration 14700 (115.572 iter/s, 0.865259s/100 iter), loss = 0.0828188
I0122 16:37:49.451730 54582 solver.cpp:285]     Train net output #0: loss = 0.0828188 (* 1 = 0.0828188 loss)
I0122 16:37:49.451736 54582 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:37:50.314806 54582 solver.cpp:266] Iteration 14800 (115.87 iter/s, 0.863037s/100 iter), loss = 0.171239
I0122 16:37:50.314833 54582 solver.cpp:285]     Train net output #0: loss = 0.171239 (* 1 = 0.171239 loss)
I0122 16:37:50.314839 54582 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:37:51.178373 54582 solver.cpp:266] Iteration 14900 (115.808 iter/s, 0.863501s/100 iter), loss = 0.0771469
I0122 16:37:51.178401 54582 solver.cpp:285]     Train net output #0: loss = 0.0771469 (* 1 = 0.0771469 loss)
I0122 16:37:51.178407 54582 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:37:52.033042 54582 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:37:52.253132 54582 solver.cpp:517]     Test net output #0: loss = 0.483831 (* 1 = 0.483831 loss)
I0122 16:37:52.253147 54582 solver.cpp:517]     Test net output #1: top-1 = 0.854445
I0122 16:37:52.253152 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991445
I0122 16:37:52.261234 54582 solver.cpp:266] Iteration 15000 (92.3539 iter/s, 1.08279s/100 iter), loss = 0.172715
I0122 16:37:52.261253 54582 solver.cpp:285]     Train net output #0: loss = 0.172715 (* 1 = 0.172715 loss)
I0122 16:37:52.261260 54582 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:37:53.126571 54582 solver.cpp:266] Iteration 15100 (115.57 iter/s, 0.86528s/100 iter), loss = 0.158414
I0122 16:37:53.126597 54582 solver.cpp:285]     Train net output #0: loss = 0.158414 (* 1 = 0.158414 loss)
I0122 16:37:53.126603 54582 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:37:53.990213 54582 solver.cpp:266] Iteration 15200 (115.797 iter/s, 0.863577s/100 iter), loss = 0.182113
I0122 16:37:53.990240 54582 solver.cpp:285]     Train net output #0: loss = 0.182113 (* 1 = 0.182113 loss)
I0122 16:37:53.990245 54582 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:37:54.853750 54582 solver.cpp:266] Iteration 15300 (115.812 iter/s, 0.863472s/100 iter), loss = 0.190778
I0122 16:37:54.853777 54582 solver.cpp:285]     Train net output #0: loss = 0.190778 (* 1 = 0.190778 loss)
I0122 16:37:54.853783 54582 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:37:55.718932 54582 solver.cpp:266] Iteration 15400 (115.592 iter/s, 0.865115s/100 iter), loss = 0.153705
I0122 16:37:55.718960 54582 solver.cpp:285]     Train net output #0: loss = 0.153705 (* 1 = 0.153705 loss)
I0122 16:37:55.718964 54582 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:37:56.582764 54582 solver.cpp:266] Iteration 15500 (115.772 iter/s, 0.863767s/100 iter), loss = 0.136372
I0122 16:37:56.582792 54582 solver.cpp:285]     Train net output #0: loss = 0.136372 (* 1 = 0.136372 loss)
I0122 16:37:56.582798 54582 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:37:57.448823 54582 solver.cpp:266] Iteration 15600 (115.474 iter/s, 0.865994s/100 iter), loss = 0.164333
I0122 16:37:57.448850 54582 solver.cpp:285]     Train net output #0: loss = 0.164333 (* 1 = 0.164333 loss)
I0122 16:37:57.448855 54582 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:37:58.313565 54582 solver.cpp:266] Iteration 15700 (115.65 iter/s, 0.864677s/100 iter), loss = 0.144605
I0122 16:37:58.313591 54582 solver.cpp:285]     Train net output #0: loss = 0.144605 (* 1 = 0.144605 loss)
I0122 16:37:58.313596 54582 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:37:59.178267 54582 solver.cpp:266] Iteration 15800 (115.655 iter/s, 0.864637s/100 iter), loss = 0.0765868
I0122 16:37:59.178292 54582 solver.cpp:285]     Train net output #0: loss = 0.0765868 (* 1 = 0.0765868 loss)
I0122 16:37:59.178298 54582 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:38:00.042771 54582 solver.cpp:266] Iteration 15900 (115.682 iter/s, 0.86444s/100 iter), loss = 0.20282
I0122 16:38:00.042798 54582 solver.cpp:285]     Train net output #0: loss = 0.20282 (* 1 = 0.20282 loss)
I0122 16:38:00.042804 54582 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:38:00.899086 54582 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:38:01.118455 54582 solver.cpp:517]     Test net output #0: loss = 0.466597 (* 1 = 0.466597 loss)
I0122 16:38:01.118487 54582 solver.cpp:517]     Test net output #1: top-1 = 0.853778
I0122 16:38:01.118492 54582 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:38:01.126565 54582 solver.cpp:266] Iteration 16000 (92.2744 iter/s, 1.08372s/100 iter), loss = 0.164173
I0122 16:38:01.126583 54582 solver.cpp:285]     Train net output #0: loss = 0.164173 (* 1 = 0.164173 loss)
I0122 16:38:01.126590 54582 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:38:01.990049 54582 solver.cpp:266] Iteration 16100 (115.817 iter/s, 0.863428s/100 iter), loss = 0.185841
I0122 16:38:01.990074 54582 solver.cpp:285]     Train net output #0: loss = 0.185841 (* 1 = 0.185841 loss)
I0122 16:38:01.990080 54582 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:38:02.853973 54582 solver.cpp:266] Iteration 16200 (115.759 iter/s, 0.863861s/100 iter), loss = 0.10275
I0122 16:38:02.854001 54582 solver.cpp:285]     Train net output #0: loss = 0.10275 (* 1 = 0.10275 loss)
I0122 16:38:02.854007 54582 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:38:03.721110 54582 solver.cpp:266] Iteration 16300 (115.331 iter/s, 0.867069s/100 iter), loss = 0.128787
I0122 16:38:03.721137 54582 solver.cpp:285]     Train net output #0: loss = 0.128787 (* 1 = 0.128787 loss)
I0122 16:38:03.721143 54582 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:38:04.584599 54582 solver.cpp:266] Iteration 16400 (115.818 iter/s, 0.863424s/100 iter), loss = 0.147027
I0122 16:38:04.584626 54582 solver.cpp:285]     Train net output #0: loss = 0.147027 (* 1 = 0.147027 loss)
I0122 16:38:04.584631 54582 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:38:05.447181 54582 solver.cpp:266] Iteration 16500 (115.94 iter/s, 0.862516s/100 iter), loss = 0.13609
I0122 16:38:05.447208 54582 solver.cpp:285]     Train net output #0: loss = 0.13609 (* 1 = 0.13609 loss)
I0122 16:38:05.447214 54582 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:38:06.310371 54582 solver.cpp:266] Iteration 16600 (115.858 iter/s, 0.863124s/100 iter), loss = 0.173462
I0122 16:38:06.310397 54582 solver.cpp:285]     Train net output #0: loss = 0.173462 (* 1 = 0.173462 loss)
I0122 16:38:06.310403 54582 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:38:07.176497 54582 solver.cpp:266] Iteration 16700 (115.465 iter/s, 0.866061s/100 iter), loss = 0.127232
I0122 16:38:07.176523 54582 solver.cpp:285]     Train net output #0: loss = 0.127232 (* 1 = 0.127232 loss)
I0122 16:38:07.176528 54582 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:38:08.039168 54582 solver.cpp:266] Iteration 16800 (115.928 iter/s, 0.862606s/100 iter), loss = 0.0879603
I0122 16:38:08.039196 54582 solver.cpp:285]     Train net output #0: loss = 0.0879603 (* 1 = 0.0879603 loss)
I0122 16:38:08.039201 54582 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:38:08.905320 54582 solver.cpp:266] Iteration 16900 (115.462 iter/s, 0.866085s/100 iter), loss = 0.160727
I0122 16:38:08.905447 54582 solver.cpp:285]     Train net output #0: loss = 0.160727 (* 1 = 0.160727 loss)
I0122 16:38:08.905454 54582 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:38:09.762917 54582 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:38:09.982455 54582 solver.cpp:517]     Test net output #0: loss = 0.472476 (* 1 = 0.472476 loss)
I0122 16:38:09.982472 54582 solver.cpp:517]     Test net output #1: top-1 = 0.855333
I0122 16:38:09.982476 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:38:09.990588 54582 solver.cpp:266] Iteration 17000 (92.1575 iter/s, 1.0851s/100 iter), loss = 0.146568
I0122 16:38:09.990605 54582 solver.cpp:285]     Train net output #0: loss = 0.146568 (* 1 = 0.146568 loss)
I0122 16:38:09.990612 54582 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:38:10.854022 54582 solver.cpp:266] Iteration 17100 (115.824 iter/s, 0.863378s/100 iter), loss = 0.130008
I0122 16:38:10.854048 54582 solver.cpp:285]     Train net output #0: loss = 0.130008 (* 1 = 0.130008 loss)
I0122 16:38:10.854053 54582 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:38:11.717460 54582 solver.cpp:266] Iteration 17200 (115.825 iter/s, 0.863373s/100 iter), loss = 0.123373
I0122 16:38:11.717486 54582 solver.cpp:285]     Train net output #0: loss = 0.123373 (* 1 = 0.123373 loss)
I0122 16:38:11.717491 54582 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:38:12.581917 54582 solver.cpp:266] Iteration 17300 (115.688 iter/s, 0.864392s/100 iter), loss = 0.187068
I0122 16:38:12.581943 54582 solver.cpp:285]     Train net output #0: loss = 0.187068 (* 1 = 0.187068 loss)
I0122 16:38:12.581949 54582 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:38:13.445240 54582 solver.cpp:266] Iteration 17400 (115.84 iter/s, 0.863257s/100 iter), loss = 0.116539
I0122 16:38:13.445263 54582 solver.cpp:285]     Train net output #0: loss = 0.116539 (* 1 = 0.116539 loss)
I0122 16:38:13.445269 54582 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:38:14.310895 54582 solver.cpp:266] Iteration 17500 (115.528 iter/s, 0.865592s/100 iter), loss = 0.117935
I0122 16:38:14.310922 54582 solver.cpp:285]     Train net output #0: loss = 0.117935 (* 1 = 0.117935 loss)
I0122 16:38:14.310927 54582 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:38:15.175693 54582 solver.cpp:266] Iteration 17600 (115.643 iter/s, 0.864731s/100 iter), loss = 0.106491
I0122 16:38:15.175719 54582 solver.cpp:285]     Train net output #0: loss = 0.106491 (* 1 = 0.106491 loss)
I0122 16:38:15.175724 54582 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:38:16.040365 54582 solver.cpp:266] Iteration 17700 (115.659 iter/s, 0.864607s/100 iter), loss = 0.177623
I0122 16:38:16.040391 54582 solver.cpp:285]     Train net output #0: loss = 0.177623 (* 1 = 0.177623 loss)
I0122 16:38:16.040397 54582 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:38:16.903839 54582 solver.cpp:266] Iteration 17800 (115.82 iter/s, 0.86341s/100 iter), loss = 0.196594
I0122 16:38:16.903865 54582 solver.cpp:285]     Train net output #0: loss = 0.196594 (* 1 = 0.196594 loss)
I0122 16:38:16.903870 54582 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:38:17.771436 54582 solver.cpp:266] Iteration 17900 (115.27 iter/s, 0.867531s/100 iter), loss = 0.177114
I0122 16:38:17.771461 54582 solver.cpp:285]     Train net output #0: loss = 0.177114 (* 1 = 0.177114 loss)
I0122 16:38:17.771466 54582 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:38:18.626684 54582 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:38:18.846328 54582 solver.cpp:517]     Test net output #0: loss = 0.450033 (* 1 = 0.450033 loss)
I0122 16:38:18.846344 54582 solver.cpp:517]     Test net output #1: top-1 = 0.859555
I0122 16:38:18.846349 54582 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:38:18.854450 54582 solver.cpp:266] Iteration 18000 (92.3407 iter/s, 1.08295s/100 iter), loss = 0.166589
I0122 16:38:18.854468 54582 solver.cpp:285]     Train net output #0: loss = 0.166589 (* 1 = 0.166589 loss)
I0122 16:38:18.854475 54582 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:38:19.718832 54582 solver.cpp:266] Iteration 18100 (115.697 iter/s, 0.864324s/100 iter), loss = 0.186931
I0122 16:38:19.718858 54582 solver.cpp:285]     Train net output #0: loss = 0.186931 (* 1 = 0.186931 loss)
I0122 16:38:19.718863 54582 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:38:20.592185 54582 solver.cpp:266] Iteration 18200 (114.51 iter/s, 0.87329s/100 iter), loss = 0.154751
I0122 16:38:20.592213 54582 solver.cpp:285]     Train net output #0: loss = 0.154751 (* 1 = 0.154751 loss)
I0122 16:38:20.592219 54582 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:38:21.457159 54582 solver.cpp:266] Iteration 18300 (115.619 iter/s, 0.864906s/100 iter), loss = 0.131017
I0122 16:38:21.457185 54582 solver.cpp:285]     Train net output #0: loss = 0.131017 (* 1 = 0.131017 loss)
I0122 16:38:21.457190 54582 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:38:22.327286 54582 solver.cpp:266] Iteration 18400 (114.934 iter/s, 0.870061s/100 iter), loss = 0.169461
I0122 16:38:22.327311 54582 solver.cpp:285]     Train net output #0: loss = 0.169461 (* 1 = 0.169461 loss)
I0122 16:38:22.327316 54582 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:38:23.207199 54582 solver.cpp:266] Iteration 18500 (113.656 iter/s, 0.879848s/100 iter), loss = 0.165316
I0122 16:38:23.207227 54582 solver.cpp:285]     Train net output #0: loss = 0.165316 (* 1 = 0.165316 loss)
I0122 16:38:23.207233 54582 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:38:24.089920 54582 solver.cpp:266] Iteration 18600 (113.295 iter/s, 0.882653s/100 iter), loss = 0.139129
I0122 16:38:24.089948 54582 solver.cpp:285]     Train net output #0: loss = 0.139129 (* 1 = 0.139129 loss)
I0122 16:38:24.089953 54582 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:38:24.953766 54582 solver.cpp:266] Iteration 18700 (115.77 iter/s, 0.863779s/100 iter), loss = 0.130682
I0122 16:38:24.953794 54582 solver.cpp:285]     Train net output #0: loss = 0.130682 (* 1 = 0.130682 loss)
I0122 16:38:24.953799 54582 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:38:25.816339 54582 solver.cpp:266] Iteration 18800 (115.941 iter/s, 0.862507s/100 iter), loss = 0.0951014
I0122 16:38:25.816367 54582 solver.cpp:285]     Train net output #0: loss = 0.0951014 (* 1 = 0.0951014 loss)
I0122 16:38:25.816373 54582 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:38:26.688503 54582 solver.cpp:266] Iteration 18900 (114.666 iter/s, 0.872097s/100 iter), loss = 0.129696
I0122 16:38:26.688530 54582 solver.cpp:285]     Train net output #0: loss = 0.129696 (* 1 = 0.129696 loss)
I0122 16:38:26.688536 54582 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:38:27.544098 54582 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:38:27.762214 54582 solver.cpp:517]     Test net output #0: loss = 0.448881 (* 1 = 0.448881 loss)
I0122 16:38:27.762229 54582 solver.cpp:517]     Test net output #1: top-1 = 0.863444
I0122 16:38:27.762234 54582 solver.cpp:517]     Test net output #2: top-5 = 0.992445
I0122 16:38:27.770303 54582 solver.cpp:266] Iteration 19000 (92.4445 iter/s, 1.08173s/100 iter), loss = 0.253155
I0122 16:38:27.770320 54582 solver.cpp:285]     Train net output #0: loss = 0.253155 (* 1 = 0.253155 loss)
I0122 16:38:27.770326 54582 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:38:28.633529 54582 solver.cpp:266] Iteration 19100 (115.852 iter/s, 0.863169s/100 iter), loss = 0.11553
I0122 16:38:28.633555 54582 solver.cpp:285]     Train net output #0: loss = 0.11553 (* 1 = 0.11553 loss)
I0122 16:38:28.633560 54582 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:38:29.496348 54582 solver.cpp:266] Iteration 19200 (115.908 iter/s, 0.862755s/100 iter), loss = 0.13547
I0122 16:38:29.496376 54582 solver.cpp:285]     Train net output #0: loss = 0.13547 (* 1 = 0.13547 loss)
I0122 16:38:29.496381 54582 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:38:30.362207 54582 solver.cpp:266] Iteration 19300 (115.501 iter/s, 0.865794s/100 iter), loss = 0.101784
I0122 16:38:30.362251 54582 solver.cpp:285]     Train net output #0: loss = 0.101784 (* 1 = 0.101784 loss)
I0122 16:38:30.362257 54582 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:38:31.228356 54582 solver.cpp:266] Iteration 19400 (115.465 iter/s, 0.866065s/100 iter), loss = 0.171445
I0122 16:38:31.228384 54582 solver.cpp:285]     Train net output #0: loss = 0.171445 (* 1 = 0.171445 loss)
I0122 16:38:31.228389 54582 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:38:32.107344 54582 solver.cpp:266] Iteration 19500 (113.776 iter/s, 0.87892s/100 iter), loss = 0.113872
I0122 16:38:32.107373 54582 solver.cpp:285]     Train net output #0: loss = 0.113872 (* 1 = 0.113872 loss)
I0122 16:38:32.107456 54582 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:38:32.987841 54582 solver.cpp:266] Iteration 19600 (113.592 iter/s, 0.880346s/100 iter), loss = 0.197463
I0122 16:38:32.987869 54582 solver.cpp:285]     Train net output #0: loss = 0.197463 (* 1 = 0.197463 loss)
I0122 16:38:32.987874 54582 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:38:33.851094 54582 solver.cpp:266] Iteration 19700 (115.85 iter/s, 0.863184s/100 iter), loss = 0.129869
I0122 16:38:33.851122 54582 solver.cpp:285]     Train net output #0: loss = 0.129869 (* 1 = 0.129869 loss)
I0122 16:38:33.851128 54582 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:38:34.714378 54582 solver.cpp:266] Iteration 19800 (115.846 iter/s, 0.863218s/100 iter), loss = 0.100233
I0122 16:38:34.714406 54582 solver.cpp:285]     Train net output #0: loss = 0.100233 (* 1 = 0.100233 loss)
I0122 16:38:34.714411 54582 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:38:35.577396 54582 solver.cpp:266] Iteration 19900 (115.881 iter/s, 0.862953s/100 iter), loss = 0.166921
I0122 16:38:35.577423 54582 solver.cpp:285]     Train net output #0: loss = 0.166921 (* 1 = 0.166921 loss)
I0122 16:38:35.577430 54582 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:38:36.435941 54582 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/_iter_20000.caffemodel
I0122 16:38:36.506551 54582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.4/snapshots/_iter_20000.solverstate
I0122 16:38:36.519235 54582 solver.cpp:378] Iteration 20000, loss = 0.0175734
I0122 16:38:36.519259 54582 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:38:36.738054 54582 solver.cpp:517]     Test net output #0: loss = 0.4455 (* 1 = 0.4455 loss)
I0122 16:38:36.738070 54582 solver.cpp:517]     Test net output #1: top-1 = 0.863111
I0122 16:38:36.738075 54582 solver.cpp:517]     Test net output #2: top-5 = 0.992
I0122 16:38:36.738078 54582 solver.cpp:386] Optimization Done (112.758 iter/s).
I0122 16:38:36.738083 54582 caffe_interface.cpp:530] Optimization Done.

## compression: fifth run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config5.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress5_miniVggNet.txt
I0122 16:38:37.277863 54759 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:38:37.327970 54759 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:38:38.749472 54759 pruning_runner.cpp:264] Compression complete 0.00029484%
I0122 16:38:39.381206 54759 pruning_runner.cpp:264] Compression complete 0.000589679%
I0122 16:38:40.016479 54759 pruning_runner.cpp:264] Compression complete 50.0003%
I0122 16:38:40.648313 54759 pruning_runner.cpp:264] Compression complete 75.0002%
I0122 16:38:41.282964 54759 pruning_runner.cpp:264] Compression complete 96.875%
I0122 16:38:41.986680 54759 pruning_runner.cpp:264] Compression complete 99.2%
I0122 16:38:42.610679 54759 pruning_runner.cpp:264] Compression complete 99.8%
I0122 16:38:43.242522 54759 pruning_runner.cpp:264] Compression complete 99.9499%
I0122 16:38:43.879129 54759 pruning_runner.cpp:264] Compression complete 99.975%
I0122 16:38:44.514036 54759 pruning_runner.cpp:264] Compression complete 99.9875%
I0122 16:38:45.140625 54759 pruning_runner.cpp:264] Compression complete 99.9937%
I0122 16:38:45.765483 54759 pruning_runner.cpp:264] Compression complete 99.9969%
I0122 16:38:46.389863 54759 pruning_runner.cpp:264] Compression complete 99.9984%
I0122 16:38:47.020022 54759 pruning_runner.cpp:264] Compression complete 99.9996%
I0122 16:38:47.657100 54759 pruning_runner.cpp:264] Compression complete 99.9998%
I0122 16:38:48.277405 54759 pruning_runner.cpp:264] Compression complete 99.9999%
I0122 16:38:48.901047 54759 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:38:48.901437 54759 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:38:48.901985 54759 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:38:48.902279 54759 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:38:48.902449 54759 layer_factory.hpp:77] Creating layer data
I0122 16:38:48.902508 54759 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:48.903005 54759 net.cpp:94] Creating Layer data
I0122 16:38:48.903017 54759 net.cpp:409] data -> data
I0122 16:38:48.903030 54759 net.cpp:409] data -> label
I0122 16:38:48.903931 56326 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:38:48.903962 56326 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:38:48.904129 54759 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:38:48.904294 54759 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:38:48.908627 54759 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:48.908695 54759 net.cpp:144] Setting up data
I0122 16:38:48.908707 54759 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:38:48.908717 54759 net.cpp:151] Top shape: 50 (50)
I0122 16:38:48.908725 54759 net.cpp:159] Memory required for data: 614600
I0122 16:38:48.908735 54759 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:38:48.908749 54759 net.cpp:94] Creating Layer label_data_1_split
I0122 16:38:48.908757 54759 net.cpp:435] label_data_1_split <- label
I0122 16:38:48.908778 54759 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:38:48.908797 54759 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:38:48.908823 54759 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:38:48.908983 54759 net.cpp:144] Setting up label_data_1_split
I0122 16:38:48.908994 54759 net.cpp:151] Top shape: 50 (50)
I0122 16:38:48.909003 54759 net.cpp:151] Top shape: 50 (50)
I0122 16:38:48.909009 54759 net.cpp:151] Top shape: 50 (50)
I0122 16:38:48.909016 54759 net.cpp:159] Memory required for data: 615200
I0122 16:38:48.909021 54759 layer_factory.hpp:77] Creating layer conv1
I0122 16:38:48.909041 54759 net.cpp:94] Creating Layer conv1
I0122 16:38:48.909049 54759 net.cpp:435] conv1 <- data
I0122 16:38:48.909059 54759 net.cpp:409] conv1 -> conv1
I0122 16:38:48.910629 54759 net.cpp:144] Setting up conv1
I0122 16:38:48.910648 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.910655 54759 net.cpp:159] Memory required for data: 7168800
I0122 16:38:48.910672 54759 layer_factory.hpp:77] Creating layer bn1
I0122 16:38:48.910687 54759 net.cpp:94] Creating Layer bn1
I0122 16:38:48.910696 54759 net.cpp:435] bn1 <- conv1
I0122 16:38:48.910706 54759 net.cpp:409] bn1 -> scale1
I0122 16:38:48.912106 54759 net.cpp:144] Setting up bn1
I0122 16:38:48.912118 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.912127 54759 net.cpp:159] Memory required for data: 13722400
I0122 16:38:48.912149 54759 layer_factory.hpp:77] Creating layer relu1
I0122 16:38:48.912163 54759 net.cpp:94] Creating Layer relu1
I0122 16:38:48.912170 54759 net.cpp:435] relu1 <- scale1
I0122 16:38:48.912180 54759 net.cpp:409] relu1 -> relu1
I0122 16:38:48.912212 54759 net.cpp:144] Setting up relu1
I0122 16:38:48.912222 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.912227 54759 net.cpp:159] Memory required for data: 20276000
I0122 16:38:48.912235 54759 layer_factory.hpp:77] Creating layer conv2
I0122 16:38:48.912250 54759 net.cpp:94] Creating Layer conv2
I0122 16:38:48.912256 54759 net.cpp:435] conv2 <- relu1
I0122 16:38:48.912267 54759 net.cpp:409] conv2 -> conv2
I0122 16:38:48.913882 54759 net.cpp:144] Setting up conv2
I0122 16:38:48.913902 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.913924 54759 net.cpp:159] Memory required for data: 26829600
I0122 16:38:48.913942 54759 layer_factory.hpp:77] Creating layer bn2
I0122 16:38:48.913959 54759 net.cpp:94] Creating Layer bn2
I0122 16:38:48.913965 54759 net.cpp:435] bn2 <- conv2
I0122 16:38:48.913976 54759 net.cpp:409] bn2 -> scale2
I0122 16:38:48.915213 54759 net.cpp:144] Setting up bn2
I0122 16:38:48.915225 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.915227 54759 net.cpp:159] Memory required for data: 33383200
I0122 16:38:48.915238 54759 layer_factory.hpp:77] Creating layer relu2
I0122 16:38:48.915246 54759 net.cpp:94] Creating Layer relu2
I0122 16:38:48.915251 54759 net.cpp:435] relu2 <- scale2
I0122 16:38:48.915256 54759 net.cpp:409] relu2 -> relu2
I0122 16:38:48.915277 54759 net.cpp:144] Setting up relu2
I0122 16:38:48.915284 54759 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:48.915302 54759 net.cpp:159] Memory required for data: 39936800
I0122 16:38:48.915307 54759 layer_factory.hpp:77] Creating layer pool1
I0122 16:38:48.915314 54759 net.cpp:94] Creating Layer pool1
I0122 16:38:48.915319 54759 net.cpp:435] pool1 <- relu2
I0122 16:38:48.915325 54759 net.cpp:409] pool1 -> pool1
I0122 16:38:48.915397 54759 net.cpp:144] Setting up pool1
I0122 16:38:48.915405 54759 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:38:48.915407 54759 net.cpp:159] Memory required for data: 41575200
I0122 16:38:48.915411 54759 layer_factory.hpp:77] Creating layer drop1
I0122 16:38:48.915417 54759 net.cpp:94] Creating Layer drop1
I0122 16:38:48.915421 54759 net.cpp:435] drop1 <- pool1
I0122 16:38:48.915426 54759 net.cpp:409] drop1 -> drop1
I0122 16:38:48.915459 54759 net.cpp:144] Setting up drop1
I0122 16:38:48.915465 54759 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:38:48.915469 54759 net.cpp:159] Memory required for data: 43213600
I0122 16:38:48.915472 54759 layer_factory.hpp:77] Creating layer conv3
I0122 16:38:48.915482 54759 net.cpp:94] Creating Layer conv3
I0122 16:38:48.915486 54759 net.cpp:435] conv3 <- drop1
I0122 16:38:48.915493 54759 net.cpp:409] conv3 -> conv3
I0122 16:38:48.916671 54759 net.cpp:144] Setting up conv3
I0122 16:38:48.916684 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.916688 54759 net.cpp:159] Memory required for data: 46490400
I0122 16:38:48.916697 54759 layer_factory.hpp:77] Creating layer bn3
I0122 16:38:48.916704 54759 net.cpp:94] Creating Layer bn3
I0122 16:38:48.916709 54759 net.cpp:435] bn3 <- conv3
I0122 16:38:48.916718 54759 net.cpp:409] bn3 -> scale3
I0122 16:38:48.917517 54759 net.cpp:144] Setting up bn3
I0122 16:38:48.917526 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.917531 54759 net.cpp:159] Memory required for data: 49767200
I0122 16:38:48.917542 54759 layer_factory.hpp:77] Creating layer relu3
I0122 16:38:48.917551 54759 net.cpp:94] Creating Layer relu3
I0122 16:38:48.917556 54759 net.cpp:435] relu3 <- scale3
I0122 16:38:48.917562 54759 net.cpp:409] relu3 -> relu3
I0122 16:38:48.917583 54759 net.cpp:144] Setting up relu3
I0122 16:38:48.917589 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.917593 54759 net.cpp:159] Memory required for data: 53044000
I0122 16:38:48.917598 54759 layer_factory.hpp:77] Creating layer conv4
I0122 16:38:48.917608 54759 net.cpp:94] Creating Layer conv4
I0122 16:38:48.917613 54759 net.cpp:435] conv4 <- relu3
I0122 16:38:48.917619 54759 net.cpp:409] conv4 -> conv4
I0122 16:38:48.918210 54759 net.cpp:144] Setting up conv4
I0122 16:38:48.918220 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.918224 54759 net.cpp:159] Memory required for data: 56320800
I0122 16:38:48.918231 54759 layer_factory.hpp:77] Creating layer bn4
I0122 16:38:48.918239 54759 net.cpp:94] Creating Layer bn4
I0122 16:38:48.918243 54759 net.cpp:435] bn4 <- conv4
I0122 16:38:48.918251 54759 net.cpp:409] bn4 -> scale4
I0122 16:38:48.919085 54759 net.cpp:144] Setting up bn4
I0122 16:38:48.919093 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.919097 54759 net.cpp:159] Memory required for data: 59597600
I0122 16:38:48.919106 54759 layer_factory.hpp:77] Creating layer relu4
I0122 16:38:48.919113 54759 net.cpp:94] Creating Layer relu4
I0122 16:38:48.919118 54759 net.cpp:435] relu4 <- scale4
I0122 16:38:48.919123 54759 net.cpp:409] relu4 -> relu4
I0122 16:38:48.919147 54759 net.cpp:144] Setting up relu4
I0122 16:38:48.919152 54759 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:48.919158 54759 net.cpp:159] Memory required for data: 62874400
I0122 16:38:48.919162 54759 layer_factory.hpp:77] Creating layer pool2
I0122 16:38:48.919169 54759 net.cpp:94] Creating Layer pool2
I0122 16:38:48.919175 54759 net.cpp:435] pool2 <- relu4
I0122 16:38:48.919183 54759 net.cpp:409] pool2 -> pool2
I0122 16:38:48.919219 54759 net.cpp:144] Setting up pool2
I0122 16:38:48.919225 54759 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:38:48.919229 54759 net.cpp:159] Memory required for data: 63693600
I0122 16:38:48.919246 54759 layer_factory.hpp:77] Creating layer drop2
I0122 16:38:48.919252 54759 net.cpp:94] Creating Layer drop2
I0122 16:38:48.919255 54759 net.cpp:435] drop2 <- pool2
I0122 16:38:48.919261 54759 net.cpp:409] drop2 -> drop2
I0122 16:38:48.919332 54759 net.cpp:144] Setting up drop2
I0122 16:38:48.919338 54759 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:38:48.919342 54759 net.cpp:159] Memory required for data: 64512800
I0122 16:38:48.919345 54759 layer_factory.hpp:77] Creating layer fc1
I0122 16:38:48.919354 54759 net.cpp:94] Creating Layer fc1
I0122 16:38:48.919359 54759 net.cpp:435] fc1 <- drop2
I0122 16:38:48.919365 54759 net.cpp:409] fc1 -> fc1
I0122 16:38:48.934758 54759 net.cpp:144] Setting up fc1
I0122 16:38:48.934778 54759 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:48.934780 54759 net.cpp:159] Memory required for data: 64615200
I0122 16:38:48.934788 54759 layer_factory.hpp:77] Creating layer bn5
I0122 16:38:48.934806 54759 net.cpp:94] Creating Layer bn5
I0122 16:38:48.934813 54759 net.cpp:435] bn5 <- fc1
I0122 16:38:48.934818 54759 net.cpp:409] bn5 -> scale5
I0122 16:38:48.935354 54759 net.cpp:144] Setting up bn5
I0122 16:38:48.935361 54759 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:48.935364 54759 net.cpp:159] Memory required for data: 64717600
I0122 16:38:48.935376 54759 layer_factory.hpp:77] Creating layer relu5
I0122 16:38:48.935382 54759 net.cpp:94] Creating Layer relu5
I0122 16:38:48.935385 54759 net.cpp:435] relu5 <- scale5
I0122 16:38:48.935391 54759 net.cpp:409] relu5 -> relu5
I0122 16:38:48.935408 54759 net.cpp:144] Setting up relu5
I0122 16:38:48.935413 54759 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:48.935416 54759 net.cpp:159] Memory required for data: 64820000
I0122 16:38:48.935420 54759 layer_factory.hpp:77] Creating layer drop3
I0122 16:38:48.935425 54759 net.cpp:94] Creating Layer drop3
I0122 16:38:48.935427 54759 net.cpp:435] drop3 <- relu5
I0122 16:38:48.935431 54759 net.cpp:409] drop3 -> drop3
I0122 16:38:48.935457 54759 net.cpp:144] Setting up drop3
I0122 16:38:48.935461 54759 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:48.935464 54759 net.cpp:159] Memory required for data: 64922400
I0122 16:38:48.935467 54759 layer_factory.hpp:77] Creating layer fc2
I0122 16:38:48.935473 54759 net.cpp:94] Creating Layer fc2
I0122 16:38:48.935477 54759 net.cpp:435] fc2 <- drop3
I0122 16:38:48.935480 54759 net.cpp:409] fc2 -> fc2
I0122 16:38:48.935601 54759 net.cpp:144] Setting up fc2
I0122 16:38:48.935607 54759 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:48.935611 54759 net.cpp:159] Memory required for data: 64924400
I0122 16:38:48.935616 54759 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:38:48.935621 54759 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:38:48.935623 54759 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:38:48.935628 54759 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:38:48.935634 54759 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:38:48.935640 54759 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:38:48.935674 54759 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:38:48.935679 54759 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:48.935683 54759 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:48.935685 54759 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:48.935688 54759 net.cpp:159] Memory required for data: 64930400
I0122 16:38:48.935690 54759 layer_factory.hpp:77] Creating layer loss
I0122 16:38:48.935695 54759 net.cpp:94] Creating Layer loss
I0122 16:38:48.935698 54759 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:38:48.935703 54759 net.cpp:435] loss <- label_data_1_split_0
I0122 16:38:48.935708 54759 net.cpp:409] loss -> loss
I0122 16:38:48.935714 54759 layer_factory.hpp:77] Creating layer loss
I0122 16:38:48.935776 54759 net.cpp:144] Setting up loss
I0122 16:38:48.935782 54759 net.cpp:151] Top shape: (1)
I0122 16:38:48.935786 54759 net.cpp:154]     with loss weight 1
I0122 16:38:48.935796 54759 net.cpp:159] Memory required for data: 64930404
I0122 16:38:48.935811 54759 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:38:48.935817 54759 net.cpp:94] Creating Layer accuracy-top1
I0122 16:38:48.935819 54759 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:38:48.935823 54759 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:38:48.935828 54759 net.cpp:409] accuracy-top1 -> top-1
I0122 16:38:48.935835 54759 net.cpp:144] Setting up accuracy-top1
I0122 16:38:48.935838 54759 net.cpp:151] Top shape: (1)
I0122 16:38:48.935842 54759 net.cpp:159] Memory required for data: 64930408
I0122 16:38:48.935843 54759 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:38:48.935849 54759 net.cpp:94] Creating Layer accuracy-top5
I0122 16:38:48.935853 54759 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:38:48.935855 54759 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:38:48.935859 54759 net.cpp:409] accuracy-top5 -> top-5
I0122 16:38:48.935866 54759 net.cpp:144] Setting up accuracy-top5
I0122 16:38:48.935869 54759 net.cpp:151] Top shape: (1)
I0122 16:38:48.935871 54759 net.cpp:159] Memory required for data: 64930412
I0122 16:38:48.935874 54759 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:38:48.935878 54759 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:38:48.935883 54759 net.cpp:220] loss needs backward computation.
I0122 16:38:48.935886 54759 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:38:48.935889 54759 net.cpp:220] fc2 needs backward computation.
I0122 16:38:48.935894 54759 net.cpp:220] drop3 needs backward computation.
I0122 16:38:48.935896 54759 net.cpp:220] relu5 needs backward computation.
I0122 16:38:48.935899 54759 net.cpp:220] bn5 needs backward computation.
I0122 16:38:48.935902 54759 net.cpp:220] fc1 needs backward computation.
I0122 16:38:48.935906 54759 net.cpp:220] drop2 needs backward computation.
I0122 16:38:48.935909 54759 net.cpp:220] pool2 needs backward computation.
I0122 16:38:48.935911 54759 net.cpp:220] relu4 needs backward computation.
I0122 16:38:48.935915 54759 net.cpp:220] bn4 needs backward computation.
I0122 16:38:48.935919 54759 net.cpp:220] conv4 needs backward computation.
I0122 16:38:48.935921 54759 net.cpp:220] relu3 needs backward computation.
I0122 16:38:48.935925 54759 net.cpp:220] bn3 needs backward computation.
I0122 16:38:48.935927 54759 net.cpp:220] conv3 needs backward computation.
I0122 16:38:48.935930 54759 net.cpp:220] drop1 needs backward computation.
I0122 16:38:48.935935 54759 net.cpp:220] pool1 needs backward computation.
I0122 16:38:48.935937 54759 net.cpp:220] relu2 needs backward computation.
I0122 16:38:48.935940 54759 net.cpp:220] bn2 needs backward computation.
I0122 16:38:48.935943 54759 net.cpp:220] conv2 needs backward computation.
I0122 16:38:48.935946 54759 net.cpp:220] relu1 needs backward computation.
I0122 16:38:48.935948 54759 net.cpp:220] bn1 needs backward computation.
I0122 16:38:48.935951 54759 net.cpp:220] conv1 needs backward computation.
I0122 16:38:48.935956 54759 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:38:48.935959 54759 net.cpp:222] data does not need backward computation.
I0122 16:38:48.935962 54759 net.cpp:264] This network produces output loss
I0122 16:38:48.935966 54759 net.cpp:264] This network produces output top-1
I0122 16:38:48.935968 54759 net.cpp:264] This network produces output top-5
I0122 16:38:48.935989 54759 net.cpp:284] Network initialization done.
I0122 16:38:48.938817 54759 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:38:48.945085 54759 caffe_interface.cpp:125] Batch 0, loss = 0.684837
I0122 16:38:48.945106 54759 caffe_interface.cpp:125] Batch 0, top-1 = 0.74
I0122 16:38:48.945109 54759 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:38:48.946765 54759 caffe_interface.cpp:125] Batch 1, loss = 1.30444
I0122 16:38:48.946775 54759 caffe_interface.cpp:125] Batch 1, top-1 = 0.74
I0122 16:38:48.946779 54759 caffe_interface.cpp:125] Batch 1, top-5 = 0.98
I0122 16:38:48.948076 54759 caffe_interface.cpp:125] Batch 2, loss = 1.34292
I0122 16:38:48.948096 54759 caffe_interface.cpp:125] Batch 2, top-1 = 0.6
I0122 16:38:48.948099 54759 caffe_interface.cpp:125] Batch 2, top-5 = 0.98
I0122 16:38:48.949483 54759 caffe_interface.cpp:125] Batch 3, loss = 2.01648
I0122 16:38:48.949491 54759 caffe_interface.cpp:125] Batch 3, top-1 = 0.56
I0122 16:38:48.949494 54759 caffe_interface.cpp:125] Batch 3, top-5 = 0.96
I0122 16:38:48.950974 54759 caffe_interface.cpp:125] Batch 4, loss = 1.24447
I0122 16:38:48.950983 54759 caffe_interface.cpp:125] Batch 4, top-1 = 0.64
I0122 16:38:48.950985 54759 caffe_interface.cpp:125] Batch 4, top-5 = 0.98
I0122 16:38:48.952260 54759 caffe_interface.cpp:125] Batch 5, loss = 1.68232
I0122 16:38:48.952267 54759 caffe_interface.cpp:125] Batch 5, top-1 = 0.58
I0122 16:38:48.952271 54759 caffe_interface.cpp:125] Batch 5, top-5 = 0.94
I0122 16:38:48.953572 54759 caffe_interface.cpp:125] Batch 6, loss = 1.14822
I0122 16:38:48.953588 54759 caffe_interface.cpp:125] Batch 6, top-1 = 0.7
I0122 16:38:48.953590 54759 caffe_interface.cpp:125] Batch 6, top-5 = 0.96
I0122 16:38:48.954869 54759 caffe_interface.cpp:125] Batch 7, loss = 1.47272
I0122 16:38:48.954877 54759 caffe_interface.cpp:125] Batch 7, top-1 = 0.6
I0122 16:38:48.954881 54759 caffe_interface.cpp:125] Batch 7, top-5 = 0.98
I0122 16:38:48.956171 54759 caffe_interface.cpp:125] Batch 8, loss = 1.1513
I0122 16:38:48.956179 54759 caffe_interface.cpp:125] Batch 8, top-1 = 0.66
I0122 16:38:48.956183 54759 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:38:48.957470 54759 caffe_interface.cpp:125] Batch 9, loss = 1.3191
I0122 16:38:48.957479 54759 caffe_interface.cpp:125] Batch 9, top-1 = 0.72
I0122 16:38:48.957482 54759 caffe_interface.cpp:125] Batch 9, top-5 = 0.92
I0122 16:38:48.958768 54759 caffe_interface.cpp:125] Batch 10, loss = 1.09817
I0122 16:38:48.958776 54759 caffe_interface.cpp:125] Batch 10, top-1 = 0.68
I0122 16:38:48.958781 54759 caffe_interface.cpp:125] Batch 10, top-5 = 0.96
I0122 16:38:48.960062 54759 caffe_interface.cpp:125] Batch 11, loss = 1.45239
I0122 16:38:48.960068 54759 caffe_interface.cpp:125] Batch 11, top-1 = 0.72
I0122 16:38:48.960073 54759 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:38:48.961359 54759 caffe_interface.cpp:125] Batch 12, loss = 1.01037
I0122 16:38:48.961367 54759 caffe_interface.cpp:125] Batch 12, top-1 = 0.72
I0122 16:38:48.961370 54759 caffe_interface.cpp:125] Batch 12, top-5 = 1
I0122 16:38:48.962656 54759 caffe_interface.cpp:125] Batch 13, loss = 0.876178
I0122 16:38:48.962663 54759 caffe_interface.cpp:125] Batch 13, top-1 = 0.64
I0122 16:38:48.962666 54759 caffe_interface.cpp:125] Batch 13, top-5 = 1
I0122 16:38:48.964197 54759 caffe_interface.cpp:125] Batch 14, loss = 0.983464
I0122 16:38:48.964205 54759 caffe_interface.cpp:125] Batch 14, top-1 = 0.76
I0122 16:38:48.964208 54759 caffe_interface.cpp:125] Batch 14, top-5 = 0.96
I0122 16:38:48.966372 54759 caffe_interface.cpp:125] Batch 15, loss = 0.947041
I0122 16:38:48.966378 54759 caffe_interface.cpp:125] Batch 15, top-1 = 0.68
I0122 16:38:48.966382 54759 caffe_interface.cpp:125] Batch 15, top-5 = 1
I0122 16:38:48.967743 54759 caffe_interface.cpp:125] Batch 16, loss = 0.959405
I0122 16:38:48.967757 54759 caffe_interface.cpp:125] Batch 16, top-1 = 0.72
I0122 16:38:48.967759 54759 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0122 16:38:48.969125 54759 caffe_interface.cpp:125] Batch 17, loss = 0.7557
I0122 16:38:48.969132 54759 caffe_interface.cpp:125] Batch 17, top-1 = 0.72
I0122 16:38:48.969136 54759 caffe_interface.cpp:125] Batch 17, top-5 = 0.98
I0122 16:38:48.970425 54759 caffe_interface.cpp:125] Batch 18, loss = 1.12881
I0122 16:38:48.970432 54759 caffe_interface.cpp:125] Batch 18, top-1 = 0.72
I0122 16:38:48.970443 54759 caffe_interface.cpp:125] Batch 18, top-5 = 0.98
I0122 16:38:48.971727 54759 caffe_interface.cpp:125] Batch 19, loss = 0.937849
I0122 16:38:48.971735 54759 caffe_interface.cpp:125] Batch 19, top-1 = 0.68
I0122 16:38:48.971737 54759 caffe_interface.cpp:125] Batch 19, top-5 = 1
I0122 16:38:48.973026 54759 caffe_interface.cpp:125] Batch 20, loss = 1.50379
I0122 16:38:48.973043 54759 caffe_interface.cpp:125] Batch 20, top-1 = 0.64
I0122 16:38:48.973047 54759 caffe_interface.cpp:125] Batch 20, top-5 = 0.96
I0122 16:38:48.974335 54759 caffe_interface.cpp:125] Batch 21, loss = 1.35405
I0122 16:38:48.974342 54759 caffe_interface.cpp:125] Batch 21, top-1 = 0.68
I0122 16:38:48.974346 54759 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0122 16:38:48.975633 54759 caffe_interface.cpp:125] Batch 22, loss = 1.66172
I0122 16:38:48.975641 54759 caffe_interface.cpp:125] Batch 22, top-1 = 0.6
I0122 16:38:48.975644 54759 caffe_interface.cpp:125] Batch 22, top-5 = 0.94
I0122 16:38:48.976932 54759 caffe_interface.cpp:125] Batch 23, loss = 0.878533
I0122 16:38:48.976938 54759 caffe_interface.cpp:125] Batch 23, top-1 = 0.76
I0122 16:38:48.976941 54759 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:38:48.978248 54759 caffe_interface.cpp:125] Batch 24, loss = 1.22125
I0122 16:38:48.978256 54759 caffe_interface.cpp:125] Batch 24, top-1 = 0.7
I0122 16:38:48.978260 54759 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0122 16:38:48.979557 54759 caffe_interface.cpp:125] Batch 25, loss = 1.17865
I0122 16:38:48.979563 54759 caffe_interface.cpp:125] Batch 25, top-1 = 0.68
I0122 16:38:48.979568 54759 caffe_interface.cpp:125] Batch 25, top-5 = 1
I0122 16:38:48.981112 54759 caffe_interface.cpp:125] Batch 26, loss = 1.23353
I0122 16:38:48.981119 54759 caffe_interface.cpp:125] Batch 26, top-1 = 0.66
I0122 16:38:48.981122 54759 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0122 16:38:48.982726 54759 caffe_interface.cpp:125] Batch 27, loss = 1.09601
I0122 16:38:48.982734 54759 caffe_interface.cpp:125] Batch 27, top-1 = 0.72
I0122 16:38:48.982738 54759 caffe_interface.cpp:125] Batch 27, top-5 = 0.96
I0122 16:38:48.984118 54759 caffe_interface.cpp:125] Batch 28, loss = 1.38411
I0122 16:38:48.984133 54759 caffe_interface.cpp:125] Batch 28, top-1 = 0.6
I0122 16:38:48.984135 54759 caffe_interface.cpp:125] Batch 28, top-5 = 0.94
I0122 16:38:48.985621 54759 caffe_interface.cpp:125] Batch 29, loss = 0.8591
I0122 16:38:48.985628 54759 caffe_interface.cpp:125] Batch 29, top-1 = 0.74
I0122 16:38:48.985631 54759 caffe_interface.cpp:125] Batch 29, top-5 = 0.98
I0122 16:38:48.986914 54759 caffe_interface.cpp:125] Batch 30, loss = 0.965066
I0122 16:38:48.986922 54759 caffe_interface.cpp:125] Batch 30, top-1 = 0.7
I0122 16:38:48.986925 54759 caffe_interface.cpp:125] Batch 30, top-5 = 0.98
I0122 16:38:48.988204 54759 caffe_interface.cpp:125] Batch 31, loss = 1.24849
I0122 16:38:48.988210 54759 caffe_interface.cpp:125] Batch 31, top-1 = 0.7
I0122 16:38:48.988214 54759 caffe_interface.cpp:125] Batch 31, top-5 = 0.96
I0122 16:38:48.989512 54759 caffe_interface.cpp:125] Batch 32, loss = 1.00005
I0122 16:38:48.989521 54759 caffe_interface.cpp:125] Batch 32, top-1 = 0.74
I0122 16:38:48.989524 54759 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:38:48.990794 54759 caffe_interface.cpp:125] Batch 33, loss = 1.99335
I0122 16:38:48.990803 54759 caffe_interface.cpp:125] Batch 33, top-1 = 0.6
I0122 16:38:48.990805 54759 caffe_interface.cpp:125] Batch 33, top-5 = 0.96
I0122 16:38:48.992099 54759 caffe_interface.cpp:125] Batch 34, loss = 0.909547
I0122 16:38:48.992106 54759 caffe_interface.cpp:125] Batch 34, top-1 = 0.78
I0122 16:38:48.992110 54759 caffe_interface.cpp:125] Batch 34, top-5 = 0.98
I0122 16:38:48.993412 54759 caffe_interface.cpp:125] Batch 35, loss = 1.37298
I0122 16:38:48.993418 54759 caffe_interface.cpp:125] Batch 35, top-1 = 0.68
I0122 16:38:48.993422 54759 caffe_interface.cpp:125] Batch 35, top-5 = 0.96
I0122 16:38:48.994699 54759 caffe_interface.cpp:125] Batch 36, loss = 1.11409
I0122 16:38:48.994707 54759 caffe_interface.cpp:125] Batch 36, top-1 = 0.62
I0122 16:38:48.994710 54759 caffe_interface.cpp:125] Batch 36, top-5 = 0.96
I0122 16:38:48.996008 54759 caffe_interface.cpp:125] Batch 37, loss = 1.66473
I0122 16:38:48.996016 54759 caffe_interface.cpp:125] Batch 37, top-1 = 0.62
I0122 16:38:48.996021 54759 caffe_interface.cpp:125] Batch 37, top-5 = 0.92
I0122 16:38:48.997570 54759 caffe_interface.cpp:125] Batch 38, loss = 1.57077
I0122 16:38:48.997588 54759 caffe_interface.cpp:125] Batch 38, top-1 = 0.66
I0122 16:38:48.997592 54759 caffe_interface.cpp:125] Batch 38, top-5 = 0.94
I0122 16:38:48.999714 54759 caffe_interface.cpp:125] Batch 39, loss = 1.00522
I0122 16:38:48.999720 54759 caffe_interface.cpp:125] Batch 39, top-1 = 0.72
I0122 16:38:48.999723 54759 caffe_interface.cpp:125] Batch 39, top-5 = 1
I0122 16:38:49.001086 54759 caffe_interface.cpp:125] Batch 40, loss = 1.01127
I0122 16:38:49.001094 54759 caffe_interface.cpp:125] Batch 40, top-1 = 0.72
I0122 16:38:49.001097 54759 caffe_interface.cpp:125] Batch 40, top-5 = 0.96
I0122 16:38:49.002593 54759 caffe_interface.cpp:125] Batch 41, loss = 1.86596
I0122 16:38:49.002600 54759 caffe_interface.cpp:125] Batch 41, top-1 = 0.6
I0122 16:38:49.002605 54759 caffe_interface.cpp:125] Batch 41, top-5 = 0.92
I0122 16:38:49.003887 54759 caffe_interface.cpp:125] Batch 42, loss = 1.47009
I0122 16:38:49.003903 54759 caffe_interface.cpp:125] Batch 42, top-1 = 0.56
I0122 16:38:49.003907 54759 caffe_interface.cpp:125] Batch 42, top-5 = 0.98
I0122 16:38:49.005184 54759 caffe_interface.cpp:125] Batch 43, loss = 1.35772
I0122 16:38:49.005192 54759 caffe_interface.cpp:125] Batch 43, top-1 = 0.68
I0122 16:38:49.005195 54759 caffe_interface.cpp:125] Batch 43, top-5 = 0.96
I0122 16:38:49.006484 54759 caffe_interface.cpp:125] Batch 44, loss = 1.58403
I0122 16:38:49.006494 54759 caffe_interface.cpp:125] Batch 44, top-1 = 0.7
I0122 16:38:49.006496 54759 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0122 16:38:49.007772 54759 caffe_interface.cpp:125] Batch 45, loss = 1.54772
I0122 16:38:49.007781 54759 caffe_interface.cpp:125] Batch 45, top-1 = 0.6
I0122 16:38:49.007783 54759 caffe_interface.cpp:125] Batch 45, top-5 = 0.98
I0122 16:38:49.009069 54759 caffe_interface.cpp:125] Batch 46, loss = 1.0129
I0122 16:38:49.009076 54759 caffe_interface.cpp:125] Batch 46, top-1 = 0.72
I0122 16:38:49.009080 54759 caffe_interface.cpp:125] Batch 46, top-5 = 0.96
I0122 16:38:49.010363 54759 caffe_interface.cpp:125] Batch 47, loss = 1.05079
I0122 16:38:49.010371 54759 caffe_interface.cpp:125] Batch 47, top-1 = 0.7
I0122 16:38:49.010375 54759 caffe_interface.cpp:125] Batch 47, top-5 = 0.98
I0122 16:38:49.011643 54759 caffe_interface.cpp:125] Batch 48, loss = 1.15888
I0122 16:38:49.011651 54759 caffe_interface.cpp:125] Batch 48, top-1 = 0.62
I0122 16:38:49.011656 54759 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:38:49.012943 54759 caffe_interface.cpp:125] Batch 49, loss = 1.27249
I0122 16:38:49.012950 54759 caffe_interface.cpp:125] Batch 49, top-1 = 0.7
I0122 16:38:49.012954 54759 caffe_interface.cpp:125] Batch 49, top-5 = 0.98
I0122 16:38:49.014478 54759 caffe_interface.cpp:125] Batch 50, loss = 1.48073
I0122 16:38:49.014487 54759 caffe_interface.cpp:125] Batch 50, top-1 = 0.64
I0122 16:38:49.014490 54759 caffe_interface.cpp:125] Batch 50, top-5 = 0.98
I0122 16:38:49.015763 54759 caffe_interface.cpp:125] Batch 51, loss = 1.37314
I0122 16:38:49.015770 54759 caffe_interface.cpp:125] Batch 51, top-1 = 0.66
I0122 16:38:49.015774 54759 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0122 16:38:49.017046 54759 caffe_interface.cpp:125] Batch 52, loss = 1.53301
I0122 16:38:49.017062 54759 caffe_interface.cpp:125] Batch 52, top-1 = 0.66
I0122 16:38:49.017066 54759 caffe_interface.cpp:125] Batch 52, top-5 = 0.96
I0122 16:38:49.018340 54759 caffe_interface.cpp:125] Batch 53, loss = 1.43721
I0122 16:38:49.018347 54759 caffe_interface.cpp:125] Batch 53, top-1 = 0.58
I0122 16:38:49.018352 54759 caffe_interface.cpp:125] Batch 53, top-5 = 1
I0122 16:38:49.019620 54759 caffe_interface.cpp:125] Batch 54, loss = 1.09268
I0122 16:38:49.019628 54759 caffe_interface.cpp:125] Batch 54, top-1 = 0.7
I0122 16:38:49.019631 54759 caffe_interface.cpp:125] Batch 54, top-5 = 0.96
I0122 16:38:49.020906 54759 caffe_interface.cpp:125] Batch 55, loss = 0.937115
I0122 16:38:49.020915 54759 caffe_interface.cpp:125] Batch 55, top-1 = 0.72
I0122 16:38:49.020918 54759 caffe_interface.cpp:125] Batch 55, top-5 = 0.98
I0122 16:38:49.022198 54759 caffe_interface.cpp:125] Batch 56, loss = 1.01915
I0122 16:38:49.022207 54759 caffe_interface.cpp:125] Batch 56, top-1 = 0.78
I0122 16:38:49.022210 54759 caffe_interface.cpp:125] Batch 56, top-5 = 1
I0122 16:38:49.023473 54759 caffe_interface.cpp:125] Batch 57, loss = 1.00569
I0122 16:38:49.023479 54759 caffe_interface.cpp:125] Batch 57, top-1 = 0.72
I0122 16:38:49.023483 54759 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:38:49.024747 54759 caffe_interface.cpp:125] Batch 58, loss = 1.39994
I0122 16:38:49.024755 54759 caffe_interface.cpp:125] Batch 58, top-1 = 0.62
I0122 16:38:49.024758 54759 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0122 16:38:49.026039 54759 caffe_interface.cpp:125] Batch 59, loss = 0.795109
I0122 16:38:49.026047 54759 caffe_interface.cpp:125] Batch 59, top-1 = 0.78
I0122 16:38:49.026051 54759 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:38:49.027308 54759 caffe_interface.cpp:125] Batch 60, loss = 1.38057
I0122 16:38:49.027317 54759 caffe_interface.cpp:125] Batch 60, top-1 = 0.72
I0122 16:38:49.027320 54759 caffe_interface.cpp:125] Batch 60, top-5 = 0.94
I0122 16:38:49.028589 54759 caffe_interface.cpp:125] Batch 61, loss = 1.99325
I0122 16:38:49.028596 54759 caffe_interface.cpp:125] Batch 61, top-1 = 0.56
I0122 16:38:49.028600 54759 caffe_interface.cpp:125] Batch 61, top-5 = 0.92
I0122 16:38:49.029865 54759 caffe_interface.cpp:125] Batch 62, loss = 1.79054
I0122 16:38:49.029872 54759 caffe_interface.cpp:125] Batch 62, top-1 = 0.58
I0122 16:38:49.029875 54759 caffe_interface.cpp:125] Batch 62, top-5 = 0.98
I0122 16:38:49.031159 54759 caffe_interface.cpp:125] Batch 63, loss = 1.4732
I0122 16:38:49.031167 54759 caffe_interface.cpp:125] Batch 63, top-1 = 0.56
I0122 16:38:49.031170 54759 caffe_interface.cpp:125] Batch 63, top-5 = 0.96
I0122 16:38:49.033535 54759 caffe_interface.cpp:125] Batch 64, loss = 1.48421
I0122 16:38:49.033542 54759 caffe_interface.cpp:125] Batch 64, top-1 = 0.54
I0122 16:38:49.033545 54759 caffe_interface.cpp:125] Batch 64, top-5 = 0.98
I0122 16:38:49.034863 54759 caffe_interface.cpp:125] Batch 65, loss = 1.16531
I0122 16:38:49.034869 54759 caffe_interface.cpp:125] Batch 65, top-1 = 0.68
I0122 16:38:49.034873 54759 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:38:49.036310 54759 caffe_interface.cpp:125] Batch 66, loss = 1.25726
I0122 16:38:49.036316 54759 caffe_interface.cpp:125] Batch 66, top-1 = 0.68
I0122 16:38:49.036319 54759 caffe_interface.cpp:125] Batch 66, top-5 = 0.94
I0122 16:38:49.037602 54759 caffe_interface.cpp:125] Batch 67, loss = 1.7703
I0122 16:38:49.037608 54759 caffe_interface.cpp:125] Batch 67, top-1 = 0.54
I0122 16:38:49.037611 54759 caffe_interface.cpp:125] Batch 67, top-5 = 0.96
I0122 16:38:49.038902 54759 caffe_interface.cpp:125] Batch 68, loss = 0.661526
I0122 16:38:49.038910 54759 caffe_interface.cpp:125] Batch 68, top-1 = 0.8
I0122 16:38:49.038913 54759 caffe_interface.cpp:125] Batch 68, top-5 = 0.98
I0122 16:38:49.040185 54759 caffe_interface.cpp:125] Batch 69, loss = 1.06183
I0122 16:38:49.040192 54759 caffe_interface.cpp:125] Batch 69, top-1 = 0.7
I0122 16:38:49.040196 54759 caffe_interface.cpp:125] Batch 69, top-5 = 1
I0122 16:38:49.041484 54759 caffe_interface.cpp:125] Batch 70, loss = 1.24904
I0122 16:38:49.041491 54759 caffe_interface.cpp:125] Batch 70, top-1 = 0.68
I0122 16:38:49.041496 54759 caffe_interface.cpp:125] Batch 70, top-5 = 0.98
I0122 16:38:49.042784 54759 caffe_interface.cpp:125] Batch 71, loss = 1.13106
I0122 16:38:49.042791 54759 caffe_interface.cpp:125] Batch 71, top-1 = 0.7
I0122 16:38:49.042795 54759 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:38:49.044075 54759 caffe_interface.cpp:125] Batch 72, loss = 1.37016
I0122 16:38:49.044083 54759 caffe_interface.cpp:125] Batch 72, top-1 = 0.64
I0122 16:38:49.044087 54759 caffe_interface.cpp:125] Batch 72, top-5 = 0.98
I0122 16:38:49.045364 54759 caffe_interface.cpp:125] Batch 73, loss = 1.40144
I0122 16:38:49.045372 54759 caffe_interface.cpp:125] Batch 73, top-1 = 0.62
I0122 16:38:49.045375 54759 caffe_interface.cpp:125] Batch 73, top-5 = 0.92
I0122 16:38:49.046785 54759 caffe_interface.cpp:125] Batch 74, loss = 2.30911
I0122 16:38:49.046794 54759 caffe_interface.cpp:125] Batch 74, top-1 = 0.58
I0122 16:38:49.046797 54759 caffe_interface.cpp:125] Batch 74, top-5 = 0.92
I0122 16:38:49.048079 54759 caffe_interface.cpp:125] Batch 75, loss = 1.26089
I0122 16:38:49.048087 54759 caffe_interface.cpp:125] Batch 75, top-1 = 0.72
I0122 16:38:49.048091 54759 caffe_interface.cpp:125] Batch 75, top-5 = 0.96
I0122 16:38:49.049360 54759 caffe_interface.cpp:125] Batch 76, loss = 1.3288
I0122 16:38:49.049367 54759 caffe_interface.cpp:125] Batch 76, top-1 = 0.6
I0122 16:38:49.049371 54759 caffe_interface.cpp:125] Batch 76, top-5 = 0.96
I0122 16:38:49.050645 54759 caffe_interface.cpp:125] Batch 77, loss = 1.51949
I0122 16:38:49.050653 54759 caffe_interface.cpp:125] Batch 77, top-1 = 0.56
I0122 16:38:49.050657 54759 caffe_interface.cpp:125] Batch 77, top-5 = 0.96
I0122 16:38:49.051924 54759 caffe_interface.cpp:125] Batch 78, loss = 1.41901
I0122 16:38:49.051932 54759 caffe_interface.cpp:125] Batch 78, top-1 = 0.64
I0122 16:38:49.051935 54759 caffe_interface.cpp:125] Batch 78, top-5 = 0.96
I0122 16:38:49.053211 54759 caffe_interface.cpp:125] Batch 79, loss = 2.46111
I0122 16:38:49.053218 54759 caffe_interface.cpp:125] Batch 79, top-1 = 0.6
I0122 16:38:49.053222 54759 caffe_interface.cpp:125] Batch 79, top-5 = 0.9
I0122 16:38:49.054494 54759 caffe_interface.cpp:125] Batch 80, loss = 1.03074
I0122 16:38:49.054502 54759 caffe_interface.cpp:125] Batch 80, top-1 = 0.72
I0122 16:38:49.054505 54759 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:38:49.055771 54759 caffe_interface.cpp:125] Batch 81, loss = 1.19881
I0122 16:38:49.055779 54759 caffe_interface.cpp:125] Batch 81, top-1 = 0.68
I0122 16:38:49.055783 54759 caffe_interface.cpp:125] Batch 81, top-5 = 0.98
I0122 16:38:49.057046 54759 caffe_interface.cpp:125] Batch 82, loss = 1.98503
I0122 16:38:49.057054 54759 caffe_interface.cpp:125] Batch 82, top-1 = 0.5
I0122 16:38:49.057057 54759 caffe_interface.cpp:125] Batch 82, top-5 = 0.92
I0122 16:38:49.058318 54759 caffe_interface.cpp:125] Batch 83, loss = 1.14327
I0122 16:38:49.058333 54759 caffe_interface.cpp:125] Batch 83, top-1 = 0.68
I0122 16:38:49.058336 54759 caffe_interface.cpp:125] Batch 83, top-5 = 0.98
I0122 16:38:49.059609 54759 caffe_interface.cpp:125] Batch 84, loss = 1.30976
I0122 16:38:49.059617 54759 caffe_interface.cpp:125] Batch 84, top-1 = 0.64
I0122 16:38:49.059621 54759 caffe_interface.cpp:125] Batch 84, top-5 = 0.98
I0122 16:38:49.060889 54759 caffe_interface.cpp:125] Batch 85, loss = 1.81852
I0122 16:38:49.060896 54759 caffe_interface.cpp:125] Batch 85, top-1 = 0.58
I0122 16:38:49.060900 54759 caffe_interface.cpp:125] Batch 85, top-5 = 0.96
I0122 16:38:49.062166 54759 caffe_interface.cpp:125] Batch 86, loss = 1.34832
I0122 16:38:49.062173 54759 caffe_interface.cpp:125] Batch 86, top-1 = 0.66
I0122 16:38:49.062177 54759 caffe_interface.cpp:125] Batch 86, top-5 = 0.98
I0122 16:38:49.063438 54759 caffe_interface.cpp:125] Batch 87, loss = 1.09502
I0122 16:38:49.063446 54759 caffe_interface.cpp:125] Batch 87, top-1 = 0.68
I0122 16:38:49.063449 54759 caffe_interface.cpp:125] Batch 87, top-5 = 0.96
I0122 16:38:49.064715 54759 caffe_interface.cpp:125] Batch 88, loss = 1.8126
I0122 16:38:49.064723 54759 caffe_interface.cpp:125] Batch 88, top-1 = 0.56
I0122 16:38:49.064728 54759 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0122 16:38:49.067124 54759 caffe_interface.cpp:125] Batch 89, loss = 1.03953
I0122 16:38:49.067131 54759 caffe_interface.cpp:125] Batch 89, top-1 = 0.76
I0122 16:38:49.067135 54759 caffe_interface.cpp:125] Batch 89, top-5 = 0.92
I0122 16:38:49.068497 54759 caffe_interface.cpp:125] Batch 90, loss = 1.01003
I0122 16:38:49.068503 54759 caffe_interface.cpp:125] Batch 90, top-1 = 0.78
I0122 16:38:49.068506 54759 caffe_interface.cpp:125] Batch 90, top-5 = 0.96
I0122 16:38:49.069977 54759 caffe_interface.cpp:125] Batch 91, loss = 0.954275
I0122 16:38:49.069984 54759 caffe_interface.cpp:125] Batch 91, top-1 = 0.66
I0122 16:38:49.069996 54759 caffe_interface.cpp:125] Batch 91, top-5 = 0.98
I0122 16:38:49.071266 54759 caffe_interface.cpp:125] Batch 92, loss = 1.24972
I0122 16:38:49.071274 54759 caffe_interface.cpp:125] Batch 92, top-1 = 0.72
I0122 16:38:49.071276 54759 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:38:49.072573 54759 caffe_interface.cpp:125] Batch 93, loss = 1.04364
I0122 16:38:49.072582 54759 caffe_interface.cpp:125] Batch 93, top-1 = 0.68
I0122 16:38:49.072585 54759 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0122 16:38:49.073864 54759 caffe_interface.cpp:125] Batch 94, loss = 1.64614
I0122 16:38:49.073871 54759 caffe_interface.cpp:125] Batch 94, top-1 = 0.6
I0122 16:38:49.073875 54759 caffe_interface.cpp:125] Batch 94, top-5 = 0.96
I0122 16:38:49.075156 54759 caffe_interface.cpp:125] Batch 95, loss = 1.36927
I0122 16:38:49.075163 54759 caffe_interface.cpp:125] Batch 95, top-1 = 0.6
I0122 16:38:49.075167 54759 caffe_interface.cpp:125] Batch 95, top-5 = 0.98
I0122 16:38:49.076434 54759 caffe_interface.cpp:125] Batch 96, loss = 1.16765
I0122 16:38:49.076442 54759 caffe_interface.cpp:125] Batch 96, top-1 = 0.72
I0122 16:38:49.076445 54759 caffe_interface.cpp:125] Batch 96, top-5 = 0.96
I0122 16:38:49.077731 54759 caffe_interface.cpp:125] Batch 97, loss = 1.52542
I0122 16:38:49.077747 54759 caffe_interface.cpp:125] Batch 97, top-1 = 0.6
I0122 16:38:49.077751 54759 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:38:49.079038 54759 caffe_interface.cpp:125] Batch 98, loss = 0.743741
I0122 16:38:49.079046 54759 caffe_interface.cpp:125] Batch 98, top-1 = 0.78
I0122 16:38:49.079049 54759 caffe_interface.cpp:125] Batch 98, top-5 = 0.98
I0122 16:38:49.080412 54759 caffe_interface.cpp:125] Batch 99, loss = 1.0173
I0122 16:38:49.080420 54759 caffe_interface.cpp:125] Batch 99, top-1 = 0.66
I0122 16:38:49.080423 54759 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0122 16:38:49.081707 54759 caffe_interface.cpp:125] Batch 100, loss = 1.19505
I0122 16:38:49.081714 54759 caffe_interface.cpp:125] Batch 100, top-1 = 0.76
I0122 16:38:49.081717 54759 caffe_interface.cpp:125] Batch 100, top-5 = 0.98
I0122 16:38:49.082985 54759 caffe_interface.cpp:125] Batch 101, loss = 0.967073
I0122 16:38:49.082993 54759 caffe_interface.cpp:125] Batch 101, top-1 = 0.7
I0122 16:38:49.082996 54759 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:38:49.084277 54759 caffe_interface.cpp:125] Batch 102, loss = 1.84498
I0122 16:38:49.084285 54759 caffe_interface.cpp:125] Batch 102, top-1 = 0.58
I0122 16:38:49.084288 54759 caffe_interface.cpp:125] Batch 102, top-5 = 0.96
I0122 16:38:49.085500 54759 caffe_interface.cpp:125] Batch 103, loss = 1.00841
I0122 16:38:49.085507 54759 caffe_interface.cpp:125] Batch 103, top-1 = 0.72
I0122 16:38:49.085510 54759 caffe_interface.cpp:125] Batch 103, top-5 = 0.98
I0122 16:38:49.086722 54759 caffe_interface.cpp:125] Batch 104, loss = 1.31478
I0122 16:38:49.086732 54759 caffe_interface.cpp:125] Batch 104, top-1 = 0.56
I0122 16:38:49.086736 54759 caffe_interface.cpp:125] Batch 104, top-5 = 1
I0122 16:38:49.087934 54759 caffe_interface.cpp:125] Batch 105, loss = 0.900763
I0122 16:38:49.087941 54759 caffe_interface.cpp:125] Batch 105, top-1 = 0.74
I0122 16:38:49.087945 54759 caffe_interface.cpp:125] Batch 105, top-5 = 0.96
I0122 16:38:49.089164 54759 caffe_interface.cpp:125] Batch 106, loss = 1.17797
I0122 16:38:49.089170 54759 caffe_interface.cpp:125] Batch 106, top-1 = 0.66
I0122 16:38:49.089174 54759 caffe_interface.cpp:125] Batch 106, top-5 = 0.98
I0122 16:38:49.090387 54759 caffe_interface.cpp:125] Batch 107, loss = 1.18361
I0122 16:38:49.090395 54759 caffe_interface.cpp:125] Batch 107, top-1 = 0.74
I0122 16:38:49.090399 54759 caffe_interface.cpp:125] Batch 107, top-5 = 0.96
I0122 16:38:49.091603 54759 caffe_interface.cpp:125] Batch 108, loss = 1.03029
I0122 16:38:49.091619 54759 caffe_interface.cpp:125] Batch 108, top-1 = 0.7
I0122 16:38:49.091624 54759 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0122 16:38:49.092829 54759 caffe_interface.cpp:125] Batch 109, loss = 1.42074
I0122 16:38:49.092838 54759 caffe_interface.cpp:125] Batch 109, top-1 = 0.68
I0122 16:38:49.092850 54759 caffe_interface.cpp:125] Batch 109, top-5 = 0.96
I0122 16:38:49.094072 54759 caffe_interface.cpp:125] Batch 110, loss = 1.83142
I0122 16:38:49.094080 54759 caffe_interface.cpp:125] Batch 110, top-1 = 0.5
I0122 16:38:49.094084 54759 caffe_interface.cpp:125] Batch 110, top-5 = 0.96
I0122 16:38:49.095291 54759 caffe_interface.cpp:125] Batch 111, loss = 1.3892
I0122 16:38:49.095299 54759 caffe_interface.cpp:125] Batch 111, top-1 = 0.64
I0122 16:38:49.095302 54759 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0122 16:38:49.096501 54759 caffe_interface.cpp:125] Batch 112, loss = 0.975557
I0122 16:38:49.096509 54759 caffe_interface.cpp:125] Batch 112, top-1 = 0.76
I0122 16:38:49.096513 54759 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0122 16:38:49.097713 54759 caffe_interface.cpp:125] Batch 113, loss = 1.32267
I0122 16:38:49.097720 54759 caffe_interface.cpp:125] Batch 113, top-1 = 0.72
I0122 16:38:49.097724 54759 caffe_interface.cpp:125] Batch 113, top-5 = 0.94
I0122 16:38:49.098935 54759 caffe_interface.cpp:125] Batch 114, loss = 1.49135
I0122 16:38:49.098942 54759 caffe_interface.cpp:125] Batch 114, top-1 = 0.62
I0122 16:38:49.098947 54759 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0122 16:38:49.101038 54759 caffe_interface.cpp:125] Batch 115, loss = 1.31578
I0122 16:38:49.101045 54759 caffe_interface.cpp:125] Batch 115, top-1 = 0.6
I0122 16:38:49.101048 54759 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:38:49.102383 54759 caffe_interface.cpp:125] Batch 116, loss = 1.71529
I0122 16:38:49.102391 54759 caffe_interface.cpp:125] Batch 116, top-1 = 0.6
I0122 16:38:49.102396 54759 caffe_interface.cpp:125] Batch 116, top-5 = 0.92
I0122 16:38:49.103777 54759 caffe_interface.cpp:125] Batch 117, loss = 1.1281
I0122 16:38:49.103790 54759 caffe_interface.cpp:125] Batch 117, top-1 = 0.64
I0122 16:38:49.103794 54759 caffe_interface.cpp:125] Batch 117, top-5 = 0.98
I0122 16:38:49.105003 54759 caffe_interface.cpp:125] Batch 118, loss = 1.11098
I0122 16:38:49.105010 54759 caffe_interface.cpp:125] Batch 118, top-1 = 0.74
I0122 16:38:49.105015 54759 caffe_interface.cpp:125] Batch 118, top-5 = 0.96
I0122 16:38:49.106235 54759 caffe_interface.cpp:125] Batch 119, loss = 1.72989
I0122 16:38:49.106242 54759 caffe_interface.cpp:125] Batch 119, top-1 = 0.64
I0122 16:38:49.106245 54759 caffe_interface.cpp:125] Batch 119, top-5 = 0.94
I0122 16:38:49.107450 54759 caffe_interface.cpp:125] Batch 120, loss = 1.37311
I0122 16:38:49.107458 54759 caffe_interface.cpp:125] Batch 120, top-1 = 0.6
I0122 16:38:49.107462 54759 caffe_interface.cpp:125] Batch 120, top-5 = 0.96
I0122 16:38:49.108659 54759 caffe_interface.cpp:125] Batch 121, loss = 2.01125
I0122 16:38:49.108666 54759 caffe_interface.cpp:125] Batch 121, top-1 = 0.64
I0122 16:38:49.108670 54759 caffe_interface.cpp:125] Batch 121, top-5 = 0.9
I0122 16:38:49.109881 54759 caffe_interface.cpp:125] Batch 122, loss = 1.57483
I0122 16:38:49.109889 54759 caffe_interface.cpp:125] Batch 122, top-1 = 0.62
I0122 16:38:49.109892 54759 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:38:49.111100 54759 caffe_interface.cpp:125] Batch 123, loss = 1.51859
I0122 16:38:49.111109 54759 caffe_interface.cpp:125] Batch 123, top-1 = 0.66
I0122 16:38:49.111112 54759 caffe_interface.cpp:125] Batch 123, top-5 = 0.94
I0122 16:38:49.112324 54759 caffe_interface.cpp:125] Batch 124, loss = 1.3024
I0122 16:38:49.112332 54759 caffe_interface.cpp:125] Batch 124, top-1 = 0.6
I0122 16:38:49.112340 54759 caffe_interface.cpp:125] Batch 124, top-5 = 0.98
I0122 16:38:49.113744 54759 caffe_interface.cpp:125] Batch 125, loss = 1.68403
I0122 16:38:49.113751 54759 caffe_interface.cpp:125] Batch 125, top-1 = 0.6
I0122 16:38:49.113755 54759 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0122 16:38:49.114966 54759 caffe_interface.cpp:125] Batch 126, loss = 1.0302
I0122 16:38:49.114974 54759 caffe_interface.cpp:125] Batch 126, top-1 = 0.72
I0122 16:38:49.114977 54759 caffe_interface.cpp:125] Batch 126, top-5 = 1
I0122 16:38:49.116184 54759 caffe_interface.cpp:125] Batch 127, loss = 1.40556
I0122 16:38:49.116201 54759 caffe_interface.cpp:125] Batch 127, top-1 = 0.7
I0122 16:38:49.116205 54759 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:38:49.117414 54759 caffe_interface.cpp:125] Batch 128, loss = 0.878731
I0122 16:38:49.117422 54759 caffe_interface.cpp:125] Batch 128, top-1 = 0.74
I0122 16:38:49.117425 54759 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0122 16:38:49.118640 54759 caffe_interface.cpp:125] Batch 129, loss = 1.60025
I0122 16:38:49.118647 54759 caffe_interface.cpp:125] Batch 129, top-1 = 0.62
I0122 16:38:49.118651 54759 caffe_interface.cpp:125] Batch 129, top-5 = 0.94
I0122 16:38:49.119848 54759 caffe_interface.cpp:125] Batch 130, loss = 1.10728
I0122 16:38:49.119854 54759 caffe_interface.cpp:125] Batch 130, top-1 = 0.7
I0122 16:38:49.119858 54759 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:38:49.121057 54759 caffe_interface.cpp:125] Batch 131, loss = 1.33836
I0122 16:38:49.121064 54759 caffe_interface.cpp:125] Batch 131, top-1 = 0.72
I0122 16:38:49.121068 54759 caffe_interface.cpp:125] Batch 131, top-5 = 0.94
I0122 16:38:49.122273 54759 caffe_interface.cpp:125] Batch 132, loss = 1.31954
I0122 16:38:49.122282 54759 caffe_interface.cpp:125] Batch 132, top-1 = 0.64
I0122 16:38:49.122285 54759 caffe_interface.cpp:125] Batch 132, top-5 = 1
I0122 16:38:49.123504 54759 caffe_interface.cpp:125] Batch 133, loss = 1.51373
I0122 16:38:49.123512 54759 caffe_interface.cpp:125] Batch 133, top-1 = 0.64
I0122 16:38:49.123515 54759 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0122 16:38:49.124727 54759 caffe_interface.cpp:125] Batch 134, loss = 1.85982
I0122 16:38:49.124735 54759 caffe_interface.cpp:125] Batch 134, top-1 = 0.5
I0122 16:38:49.124738 54759 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:38:49.125954 54759 caffe_interface.cpp:125] Batch 135, loss = 1.3702
I0122 16:38:49.125962 54759 caffe_interface.cpp:125] Batch 135, top-1 = 0.62
I0122 16:38:49.125965 54759 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0122 16:38:49.127180 54759 caffe_interface.cpp:125] Batch 136, loss = 1.13843
I0122 16:38:49.127188 54759 caffe_interface.cpp:125] Batch 136, top-1 = 0.66
I0122 16:38:49.127192 54759 caffe_interface.cpp:125] Batch 136, top-5 = 1
I0122 16:38:49.128401 54759 caffe_interface.cpp:125] Batch 137, loss = 1.55457
I0122 16:38:49.128408 54759 caffe_interface.cpp:125] Batch 137, top-1 = 0.62
I0122 16:38:49.128412 54759 caffe_interface.cpp:125] Batch 137, top-5 = 0.96
I0122 16:38:49.129634 54759 caffe_interface.cpp:125] Batch 138, loss = 1.21309
I0122 16:38:49.129642 54759 caffe_interface.cpp:125] Batch 138, top-1 = 0.68
I0122 16:38:49.129647 54759 caffe_interface.cpp:125] Batch 138, top-5 = 0.98
I0122 16:38:49.130861 54759 caffe_interface.cpp:125] Batch 139, loss = 1.53314
I0122 16:38:49.130868 54759 caffe_interface.cpp:125] Batch 139, top-1 = 0.62
I0122 16:38:49.130872 54759 caffe_interface.cpp:125] Batch 139, top-5 = 0.98
I0122 16:38:49.132099 54759 caffe_interface.cpp:125] Batch 140, loss = 1.58281
I0122 16:38:49.132107 54759 caffe_interface.cpp:125] Batch 140, top-1 = 0.56
I0122 16:38:49.132110 54759 caffe_interface.cpp:125] Batch 140, top-5 = 0.98
I0122 16:38:49.133322 54759 caffe_interface.cpp:125] Batch 141, loss = 1.2122
I0122 16:38:49.133329 54759 caffe_interface.cpp:125] Batch 141, top-1 = 0.66
I0122 16:38:49.133333 54759 caffe_interface.cpp:125] Batch 141, top-5 = 1
I0122 16:38:49.135591 54759 caffe_interface.cpp:125] Batch 142, loss = 0.700732
I0122 16:38:49.135598 54759 caffe_interface.cpp:125] Batch 142, top-1 = 0.76
I0122 16:38:49.135601 54759 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:38:49.136945 54759 caffe_interface.cpp:125] Batch 143, loss = 1.41792
I0122 16:38:49.136956 54759 caffe_interface.cpp:125] Batch 143, top-1 = 0.66
I0122 16:38:49.136960 54759 caffe_interface.cpp:125] Batch 143, top-5 = 0.96
I0122 16:38:49.138190 54759 caffe_interface.cpp:125] Batch 144, loss = 1.59623
I0122 16:38:49.138197 54759 caffe_interface.cpp:125] Batch 144, top-1 = 0.64
I0122 16:38:49.138200 54759 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0122 16:38:49.139413 54759 caffe_interface.cpp:125] Batch 145, loss = 1.57928
I0122 16:38:49.139421 54759 caffe_interface.cpp:125] Batch 145, top-1 = 0.58
I0122 16:38:49.139425 54759 caffe_interface.cpp:125] Batch 145, top-5 = 0.96
I0122 16:38:49.140648 54759 caffe_interface.cpp:125] Batch 146, loss = 1.19073
I0122 16:38:49.140656 54759 caffe_interface.cpp:125] Batch 146, top-1 = 0.64
I0122 16:38:49.140660 54759 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0122 16:38:49.141865 54759 caffe_interface.cpp:125] Batch 147, loss = 1.91742
I0122 16:38:49.141871 54759 caffe_interface.cpp:125] Batch 147, top-1 = 0.5
I0122 16:38:49.141877 54759 caffe_interface.cpp:125] Batch 147, top-5 = 0.96
I0122 16:38:49.143107 54759 caffe_interface.cpp:125] Batch 148, loss = 1.13022
I0122 16:38:49.143115 54759 caffe_interface.cpp:125] Batch 148, top-1 = 0.72
I0122 16:38:49.143119 54759 caffe_interface.cpp:125] Batch 148, top-5 = 0.96
I0122 16:38:49.144351 54759 caffe_interface.cpp:125] Batch 149, loss = 1.19353
I0122 16:38:49.144359 54759 caffe_interface.cpp:125] Batch 149, top-1 = 0.76
I0122 16:38:49.144368 54759 caffe_interface.cpp:125] Batch 149, top-5 = 0.96
I0122 16:38:49.145589 54759 caffe_interface.cpp:125] Batch 150, loss = 1.29562
I0122 16:38:49.145596 54759 caffe_interface.cpp:125] Batch 150, top-1 = 0.74
I0122 16:38:49.145599 54759 caffe_interface.cpp:125] Batch 150, top-5 = 0.96
I0122 16:38:49.146908 54759 caffe_interface.cpp:125] Batch 151, loss = 0.953641
I0122 16:38:49.146915 54759 caffe_interface.cpp:125] Batch 151, top-1 = 0.64
I0122 16:38:49.146919 54759 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:38:49.148130 54759 caffe_interface.cpp:125] Batch 152, loss = 0.866968
I0122 16:38:49.148138 54759 caffe_interface.cpp:125] Batch 152, top-1 = 0.72
I0122 16:38:49.148140 54759 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0122 16:38:49.149359 54759 caffe_interface.cpp:125] Batch 153, loss = 1.10214
I0122 16:38:49.149368 54759 caffe_interface.cpp:125] Batch 153, top-1 = 0.66
I0122 16:38:49.149371 54759 caffe_interface.cpp:125] Batch 153, top-5 = 1
I0122 16:38:49.150570 54759 caffe_interface.cpp:125] Batch 154, loss = 1.37731
I0122 16:38:49.150578 54759 caffe_interface.cpp:125] Batch 154, top-1 = 0.62
I0122 16:38:49.150581 54759 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:38:49.151780 54759 caffe_interface.cpp:125] Batch 155, loss = 1.15468
I0122 16:38:49.151787 54759 caffe_interface.cpp:125] Batch 155, top-1 = 0.6
I0122 16:38:49.151791 54759 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:38:49.153005 54759 caffe_interface.cpp:125] Batch 156, loss = 1.67502
I0122 16:38:49.153018 54759 caffe_interface.cpp:125] Batch 156, top-1 = 0.48
I0122 16:38:49.153021 54759 caffe_interface.cpp:125] Batch 156, top-5 = 0.98
I0122 16:38:49.154229 54759 caffe_interface.cpp:125] Batch 157, loss = 0.678903
I0122 16:38:49.154237 54759 caffe_interface.cpp:125] Batch 157, top-1 = 0.8
I0122 16:38:49.154240 54759 caffe_interface.cpp:125] Batch 157, top-5 = 1
I0122 16:38:49.155454 54759 caffe_interface.cpp:125] Batch 158, loss = 1.11974
I0122 16:38:49.155462 54759 caffe_interface.cpp:125] Batch 158, top-1 = 0.68
I0122 16:38:49.155465 54759 caffe_interface.cpp:125] Batch 158, top-5 = 1
I0122 16:38:49.156671 54759 caffe_interface.cpp:125] Batch 159, loss = 1.2087
I0122 16:38:49.156677 54759 caffe_interface.cpp:125] Batch 159, top-1 = 0.64
I0122 16:38:49.156682 54759 caffe_interface.cpp:125] Batch 159, top-5 = 0.98
I0122 16:38:49.157886 54759 caffe_interface.cpp:125] Batch 160, loss = 1.28457
I0122 16:38:49.157893 54759 caffe_interface.cpp:125] Batch 160, top-1 = 0.68
I0122 16:38:49.157897 54759 caffe_interface.cpp:125] Batch 160, top-5 = 0.96
I0122 16:38:49.159124 54759 caffe_interface.cpp:125] Batch 161, loss = 0.923711
I0122 16:38:49.159132 54759 caffe_interface.cpp:125] Batch 161, top-1 = 0.76
I0122 16:38:49.159135 54759 caffe_interface.cpp:125] Batch 161, top-5 = 0.98
I0122 16:38:49.160348 54759 caffe_interface.cpp:125] Batch 162, loss = 1.381
I0122 16:38:49.160356 54759 caffe_interface.cpp:125] Batch 162, top-1 = 0.68
I0122 16:38:49.160368 54759 caffe_interface.cpp:125] Batch 162, top-5 = 0.98
I0122 16:38:49.161579 54759 caffe_interface.cpp:125] Batch 163, loss = 0.98795
I0122 16:38:49.161594 54759 caffe_interface.cpp:125] Batch 163, top-1 = 0.7
I0122 16:38:49.161597 54759 caffe_interface.cpp:125] Batch 163, top-5 = 0.96
I0122 16:38:49.162802 54759 caffe_interface.cpp:125] Batch 164, loss = 1.51884
I0122 16:38:49.162811 54759 caffe_interface.cpp:125] Batch 164, top-1 = 0.66
I0122 16:38:49.162814 54759 caffe_interface.cpp:125] Batch 164, top-5 = 0.92
I0122 16:38:49.164032 54759 caffe_interface.cpp:125] Batch 165, loss = 1.09345
I0122 16:38:49.164041 54759 caffe_interface.cpp:125] Batch 165, top-1 = 0.76
I0122 16:38:49.164043 54759 caffe_interface.cpp:125] Batch 165, top-5 = 0.96
I0122 16:38:49.165256 54759 caffe_interface.cpp:125] Batch 166, loss = 1.16383
I0122 16:38:49.165263 54759 caffe_interface.cpp:125] Batch 166, top-1 = 0.72
I0122 16:38:49.165266 54759 caffe_interface.cpp:125] Batch 166, top-5 = 0.98
I0122 16:38:49.166468 54759 caffe_interface.cpp:125] Batch 167, loss = 1.51053
I0122 16:38:49.166476 54759 caffe_interface.cpp:125] Batch 167, top-1 = 0.6
I0122 16:38:49.166479 54759 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0122 16:38:49.168628 54759 caffe_interface.cpp:125] Batch 168, loss = 0.692633
I0122 16:38:49.168637 54759 caffe_interface.cpp:125] Batch 168, top-1 = 0.82
I0122 16:38:49.168640 54759 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:38:49.169976 54759 caffe_interface.cpp:125] Batch 169, loss = 1.07111
I0122 16:38:49.169984 54759 caffe_interface.cpp:125] Batch 169, top-1 = 0.6
I0122 16:38:49.169987 54759 caffe_interface.cpp:125] Batch 169, top-5 = 0.96
I0122 16:38:49.171372 54759 caffe_interface.cpp:125] Batch 170, loss = 1.36897
I0122 16:38:49.171383 54759 caffe_interface.cpp:125] Batch 170, top-1 = 0.66
I0122 16:38:49.171386 54759 caffe_interface.cpp:125] Batch 170, top-5 = 0.94
I0122 16:38:49.172596 54759 caffe_interface.cpp:125] Batch 171, loss = 1.12508
I0122 16:38:49.172603 54759 caffe_interface.cpp:125] Batch 171, top-1 = 0.7
I0122 16:38:49.172607 54759 caffe_interface.cpp:125] Batch 171, top-5 = 0.98
I0122 16:38:49.173818 54759 caffe_interface.cpp:125] Batch 172, loss = 1.24571
I0122 16:38:49.173826 54759 caffe_interface.cpp:125] Batch 172, top-1 = 0.68
I0122 16:38:49.173830 54759 caffe_interface.cpp:125] Batch 172, top-5 = 0.98
I0122 16:38:49.175046 54759 caffe_interface.cpp:125] Batch 173, loss = 1.06152
I0122 16:38:49.175053 54759 caffe_interface.cpp:125] Batch 173, top-1 = 0.72
I0122 16:38:49.175057 54759 caffe_interface.cpp:125] Batch 173, top-5 = 0.94
I0122 16:38:49.176271 54759 caffe_interface.cpp:125] Batch 174, loss = 1.20242
I0122 16:38:49.176277 54759 caffe_interface.cpp:125] Batch 174, top-1 = 0.64
I0122 16:38:49.176281 54759 caffe_interface.cpp:125] Batch 174, top-5 = 0.98
I0122 16:38:49.177491 54759 caffe_interface.cpp:125] Batch 175, loss = 1.30326
I0122 16:38:49.177498 54759 caffe_interface.cpp:125] Batch 175, top-1 = 0.58
I0122 16:38:49.177502 54759 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0122 16:38:49.178709 54759 caffe_interface.cpp:125] Batch 176, loss = 1.64521
I0122 16:38:49.178716 54759 caffe_interface.cpp:125] Batch 176, top-1 = 0.62
I0122 16:38:49.178720 54759 caffe_interface.cpp:125] Batch 176, top-5 = 0.96
I0122 16:38:49.179929 54759 caffe_interface.cpp:125] Batch 177, loss = 1.28923
I0122 16:38:49.179936 54759 caffe_interface.cpp:125] Batch 177, top-1 = 0.62
I0122 16:38:49.179940 54759 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0122 16:38:49.181316 54759 caffe_interface.cpp:125] Batch 178, loss = 1.49408
I0122 16:38:49.181324 54759 caffe_interface.cpp:125] Batch 178, top-1 = 0.68
I0122 16:38:49.181326 54759 caffe_interface.cpp:125] Batch 178, top-5 = 0.94
I0122 16:38:49.182531 54759 caffe_interface.cpp:125] Batch 179, loss = 0.84319
I0122 16:38:49.182538 54759 caffe_interface.cpp:125] Batch 179, top-1 = 0.7
I0122 16:38:49.182543 54759 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:38:49.182545 54759 caffe_interface.cpp:130] Loss: 1.29264
I0122 16:38:49.182562 54759 caffe_interface.cpp:142] loss = 1.29264 (* 1 = 1.29264 loss)
I0122 16:38:49.182567 54759 caffe_interface.cpp:142] top-1 = 0.663556
I0122 16:38:49.182571 54759 caffe_interface.cpp:142] top-5 = 0.972
I0122 16:38:49.328927 54759 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/sparse.caffemodel
I0122 16:38:49.328954 54759 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.5:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.663555801    | -0.199110627   |
+-------------------------------------------------------------------+
| Weights        | 68389          | 39391          | -42.4015579%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 24801792       | -49.4395027%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config5.prototxt
## fine-tuning: fifth run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config5.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune5_miniVggNet.txt
I0122 16:38:49.565065 56358 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0122 16:38:49.746558 56358 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:38:49.747069 56358 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24887558144, dev_info[0]: total=25620447232 free=24887558144
I0122 16:38:49.747081 56358 caffe_interface.cpp:493] Using GPUs 0
I0122 16:38:49.747332 56358 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:38:50.330523 56358 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt"
type: "SGD"
I0122 16:38:50.330972 56358 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0122 16:38:50.331451 56358 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:38:50.331481 56358 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:38:50.331488 56358 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:38:50.331861 56358 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:38:50.331996 56358 layer_factory.hpp:77] Creating layer data
I0122 16:38:50.332164 56358 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:50.332998 56358 net.cpp:94] Creating Layer data
I0122 16:38:50.333016 56358 net.cpp:409] data -> data
I0122 16:38:50.333050 56358 net.cpp:409] data -> label
I0122 16:38:50.334228 56397 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:38:50.334273 56397 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:38:50.334406 56358 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:38:50.334558 56358 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:38:50.346828 56358 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:50.346886 56358 net.cpp:144] Setting up data
I0122 16:38:50.346896 56358 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:38:50.346902 56358 net.cpp:151] Top shape: 128 (128)
I0122 16:38:50.346907 56358 net.cpp:159] Memory required for data: 1573376
I0122 16:38:50.346913 56358 layer_factory.hpp:77] Creating layer conv1
I0122 16:38:50.346931 56358 net.cpp:94] Creating Layer conv1
I0122 16:38:50.346938 56358 net.cpp:435] conv1 <- data
I0122 16:38:50.346957 56358 net.cpp:409] conv1 -> conv1
I0122 16:38:50.348379 56358 net.cpp:144] Setting up conv1
I0122 16:38:50.348394 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.348398 56358 net.cpp:159] Memory required for data: 18350592
I0122 16:38:50.348419 56358 layer_factory.hpp:77] Creating layer bn1
I0122 16:38:50.348431 56358 net.cpp:94] Creating Layer bn1
I0122 16:38:50.348436 56358 net.cpp:435] bn1 <- conv1
I0122 16:38:50.348443 56358 net.cpp:409] bn1 -> scale1
I0122 16:38:50.349334 56358 net.cpp:144] Setting up bn1
I0122 16:38:50.349344 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.349347 56358 net.cpp:159] Memory required for data: 35127808
I0122 16:38:50.349364 56358 layer_factory.hpp:77] Creating layer relu1
I0122 16:38:50.349372 56358 net.cpp:94] Creating Layer relu1
I0122 16:38:50.349381 56358 net.cpp:435] relu1 <- scale1
I0122 16:38:50.349387 56358 net.cpp:409] relu1 -> relu1
I0122 16:38:50.349954 56358 net.cpp:144] Setting up relu1
I0122 16:38:50.349964 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.349968 56358 net.cpp:159] Memory required for data: 51905024
I0122 16:38:50.349972 56358 layer_factory.hpp:77] Creating layer conv2
I0122 16:38:50.349984 56358 net.cpp:94] Creating Layer conv2
I0122 16:38:50.349989 56358 net.cpp:435] conv2 <- relu1
I0122 16:38:50.349997 56358 net.cpp:409] conv2 -> conv2
I0122 16:38:50.351444 56358 net.cpp:144] Setting up conv2
I0122 16:38:50.351459 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.351464 56358 net.cpp:159] Memory required for data: 68682240
I0122 16:38:50.351475 56358 layer_factory.hpp:77] Creating layer bn2
I0122 16:38:50.351486 56358 net.cpp:94] Creating Layer bn2
I0122 16:38:50.351490 56358 net.cpp:435] bn2 <- conv2
I0122 16:38:50.351500 56358 net.cpp:409] bn2 -> scale2
I0122 16:38:50.352522 56358 net.cpp:144] Setting up bn2
I0122 16:38:50.352533 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.352537 56358 net.cpp:159] Memory required for data: 85459456
I0122 16:38:50.352551 56358 layer_factory.hpp:77] Creating layer relu2
I0122 16:38:50.352560 56358 net.cpp:94] Creating Layer relu2
I0122 16:38:50.352566 56358 net.cpp:435] relu2 <- scale2
I0122 16:38:50.352572 56358 net.cpp:409] relu2 -> relu2
I0122 16:38:50.352598 56358 net.cpp:144] Setting up relu2
I0122 16:38:50.352607 56358 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:38:50.352610 56358 net.cpp:159] Memory required for data: 102236672
I0122 16:38:50.352614 56358 layer_factory.hpp:77] Creating layer pool1
I0122 16:38:50.352623 56358 net.cpp:94] Creating Layer pool1
I0122 16:38:50.352628 56358 net.cpp:435] pool1 <- relu2
I0122 16:38:50.352634 56358 net.cpp:409] pool1 -> pool1
I0122 16:38:50.352679 56358 net.cpp:144] Setting up pool1
I0122 16:38:50.352694 56358 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:38:50.352699 56358 net.cpp:159] Memory required for data: 106430976
I0122 16:38:50.352702 56358 layer_factory.hpp:77] Creating layer drop1
I0122 16:38:50.352710 56358 net.cpp:94] Creating Layer drop1
I0122 16:38:50.352715 56358 net.cpp:435] drop1 <- pool1
I0122 16:38:50.352739 56358 net.cpp:409] drop1 -> drop1
I0122 16:38:50.352784 56358 net.cpp:144] Setting up drop1
I0122 16:38:50.352792 56358 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:38:50.352797 56358 net.cpp:159] Memory required for data: 110625280
I0122 16:38:50.352802 56358 layer_factory.hpp:77] Creating layer conv3
I0122 16:38:50.352813 56358 net.cpp:94] Creating Layer conv3
I0122 16:38:50.352819 56358 net.cpp:435] conv3 <- drop1
I0122 16:38:50.352828 56358 net.cpp:409] conv3 -> conv3
I0122 16:38:50.354102 56358 net.cpp:144] Setting up conv3
I0122 16:38:50.354115 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.354117 56358 net.cpp:159] Memory required for data: 119013888
I0122 16:38:50.354125 56358 layer_factory.hpp:77] Creating layer bn3
I0122 16:38:50.354131 56358 net.cpp:94] Creating Layer bn3
I0122 16:38:50.354137 56358 net.cpp:435] bn3 <- conv3
I0122 16:38:50.354143 56358 net.cpp:409] bn3 -> scale3
I0122 16:38:50.354840 56358 net.cpp:144] Setting up bn3
I0122 16:38:50.354847 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.354851 56358 net.cpp:159] Memory required for data: 127402496
I0122 16:38:50.354861 56358 layer_factory.hpp:77] Creating layer relu3
I0122 16:38:50.354868 56358 net.cpp:94] Creating Layer relu3
I0122 16:38:50.354872 56358 net.cpp:435] relu3 <- scale3
I0122 16:38:50.354876 56358 net.cpp:409] relu3 -> relu3
I0122 16:38:50.354894 56358 net.cpp:144] Setting up relu3
I0122 16:38:50.354902 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.354905 56358 net.cpp:159] Memory required for data: 135791104
I0122 16:38:50.354907 56358 layer_factory.hpp:77] Creating layer conv4
I0122 16:38:50.354916 56358 net.cpp:94] Creating Layer conv4
I0122 16:38:50.354921 56358 net.cpp:435] conv4 <- relu3
I0122 16:38:50.354926 56358 net.cpp:409] conv4 -> conv4
I0122 16:38:50.355394 56358 net.cpp:144] Setting up conv4
I0122 16:38:50.355402 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.355406 56358 net.cpp:159] Memory required for data: 144179712
I0122 16:38:50.355412 56358 layer_factory.hpp:77] Creating layer bn4
I0122 16:38:50.355418 56358 net.cpp:94] Creating Layer bn4
I0122 16:38:50.355422 56358 net.cpp:435] bn4 <- conv4
I0122 16:38:50.355427 56358 net.cpp:409] bn4 -> scale4
I0122 16:38:50.356070 56358 net.cpp:144] Setting up bn4
I0122 16:38:50.356076 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.356079 56358 net.cpp:159] Memory required for data: 152568320
I0122 16:38:50.356088 56358 layer_factory.hpp:77] Creating layer relu4
I0122 16:38:50.356093 56358 net.cpp:94] Creating Layer relu4
I0122 16:38:50.356096 56358 net.cpp:435] relu4 <- scale4
I0122 16:38:50.356101 56358 net.cpp:409] relu4 -> relu4
I0122 16:38:50.356119 56358 net.cpp:144] Setting up relu4
I0122 16:38:50.356127 56358 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:38:50.356130 56358 net.cpp:159] Memory required for data: 160956928
I0122 16:38:50.356133 56358 layer_factory.hpp:77] Creating layer pool2
I0122 16:38:50.356139 56358 net.cpp:94] Creating Layer pool2
I0122 16:38:50.356142 56358 net.cpp:435] pool2 <- relu4
I0122 16:38:50.356146 56358 net.cpp:409] pool2 -> pool2
I0122 16:38:50.356178 56358 net.cpp:144] Setting up pool2
I0122 16:38:50.356184 56358 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:38:50.356186 56358 net.cpp:159] Memory required for data: 163054080
I0122 16:38:50.356189 56358 layer_factory.hpp:77] Creating layer drop2
I0122 16:38:50.356195 56358 net.cpp:94] Creating Layer drop2
I0122 16:38:50.356201 56358 net.cpp:435] drop2 <- pool2
I0122 16:38:50.356205 56358 net.cpp:409] drop2 -> drop2
I0122 16:38:50.356232 56358 net.cpp:144] Setting up drop2
I0122 16:38:50.356238 56358 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:38:50.356242 56358 net.cpp:159] Memory required for data: 165151232
I0122 16:38:50.356245 56358 layer_factory.hpp:77] Creating layer fc1
I0122 16:38:50.356251 56358 net.cpp:94] Creating Layer fc1
I0122 16:38:50.356253 56358 net.cpp:435] fc1 <- drop2
I0122 16:38:50.356258 56358 net.cpp:409] fc1 -> fc1
I0122 16:38:50.371639 56358 net.cpp:144] Setting up fc1
I0122 16:38:50.371655 56358 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:38:50.371657 56358 net.cpp:159] Memory required for data: 165413376
I0122 16:38:50.371665 56358 layer_factory.hpp:77] Creating layer bn5
I0122 16:38:50.371675 56358 net.cpp:94] Creating Layer bn5
I0122 16:38:50.371678 56358 net.cpp:435] bn5 <- fc1
I0122 16:38:50.371685 56358 net.cpp:409] bn5 -> scale5
I0122 16:38:50.372210 56358 net.cpp:144] Setting up bn5
I0122 16:38:50.372217 56358 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:38:50.372220 56358 net.cpp:159] Memory required for data: 165675520
I0122 16:38:50.372231 56358 layer_factory.hpp:77] Creating layer relu5
I0122 16:38:50.372238 56358 net.cpp:94] Creating Layer relu5
I0122 16:38:50.372241 56358 net.cpp:435] relu5 <- scale5
I0122 16:38:50.372246 56358 net.cpp:409] relu5 -> relu5
I0122 16:38:50.372263 56358 net.cpp:144] Setting up relu5
I0122 16:38:50.372269 56358 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:38:50.372272 56358 net.cpp:159] Memory required for data: 165937664
I0122 16:38:50.372274 56358 layer_factory.hpp:77] Creating layer drop3
I0122 16:38:50.372279 56358 net.cpp:94] Creating Layer drop3
I0122 16:38:50.372283 56358 net.cpp:435] drop3 <- relu5
I0122 16:38:50.372287 56358 net.cpp:409] drop3 -> drop3
I0122 16:38:50.372313 56358 net.cpp:144] Setting up drop3
I0122 16:38:50.372318 56358 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:38:50.372321 56358 net.cpp:159] Memory required for data: 166199808
I0122 16:38:50.372323 56358 layer_factory.hpp:77] Creating layer fc2
I0122 16:38:50.372330 56358 net.cpp:94] Creating Layer fc2
I0122 16:38:50.372334 56358 net.cpp:435] fc2 <- drop3
I0122 16:38:50.372340 56358 net.cpp:409] fc2 -> fc2
I0122 16:38:50.372470 56358 net.cpp:144] Setting up fc2
I0122 16:38:50.372475 56358 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:38:50.372478 56358 net.cpp:159] Memory required for data: 166204928
I0122 16:38:50.372483 56358 layer_factory.hpp:77] Creating layer loss
I0122 16:38:50.372488 56358 net.cpp:94] Creating Layer loss
I0122 16:38:50.372490 56358 net.cpp:435] loss <- fc2
I0122 16:38:50.372494 56358 net.cpp:435] loss <- label
I0122 16:38:50.372500 56358 net.cpp:409] loss -> loss
I0122 16:38:50.372506 56358 layer_factory.hpp:77] Creating layer loss
I0122 16:38:50.373229 56358 net.cpp:144] Setting up loss
I0122 16:38:50.373239 56358 net.cpp:151] Top shape: (1)
I0122 16:38:50.373242 56358 net.cpp:154]     with loss weight 1
I0122 16:38:50.373251 56358 net.cpp:159] Memory required for data: 166204932
I0122 16:38:50.373255 56358 net.cpp:220] loss needs backward computation.
I0122 16:38:50.373268 56358 net.cpp:220] fc2 needs backward computation.
I0122 16:38:50.373272 56358 net.cpp:220] drop3 needs backward computation.
I0122 16:38:50.373275 56358 net.cpp:220] relu5 needs backward computation.
I0122 16:38:50.373278 56358 net.cpp:220] bn5 needs backward computation.
I0122 16:38:50.373281 56358 net.cpp:220] fc1 needs backward computation.
I0122 16:38:50.373284 56358 net.cpp:220] drop2 needs backward computation.
I0122 16:38:50.373287 56358 net.cpp:220] pool2 needs backward computation.
I0122 16:38:50.373291 56358 net.cpp:220] relu4 needs backward computation.
I0122 16:38:50.373293 56358 net.cpp:220] bn4 needs backward computation.
I0122 16:38:50.373297 56358 net.cpp:220] conv4 needs backward computation.
I0122 16:38:50.373301 56358 net.cpp:220] relu3 needs backward computation.
I0122 16:38:50.373303 56358 net.cpp:220] bn3 needs backward computation.
I0122 16:38:50.373306 56358 net.cpp:220] conv3 needs backward computation.
I0122 16:38:50.373309 56358 net.cpp:220] drop1 needs backward computation.
I0122 16:38:50.373312 56358 net.cpp:220] pool1 needs backward computation.
I0122 16:38:50.373315 56358 net.cpp:220] relu2 needs backward computation.
I0122 16:38:50.373318 56358 net.cpp:220] bn2 needs backward computation.
I0122 16:38:50.373322 56358 net.cpp:220] conv2 needs backward computation.
I0122 16:38:50.373324 56358 net.cpp:220] relu1 needs backward computation.
I0122 16:38:50.373340 56358 net.cpp:220] bn1 needs backward computation.
I0122 16:38:50.373344 56358 net.cpp:220] conv1 needs backward computation.
I0122 16:38:50.373348 56358 net.cpp:222] data does not need backward computation.
I0122 16:38:50.373351 56358 net.cpp:264] This network produces output loss
I0122 16:38:50.373371 56358 net.cpp:284] Network initialization done.
I0122 16:38:50.373677 56358 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/net_finetune.prototxt
I0122 16:38:50.373709 56358 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:38:50.373898 56358 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:38:50.374002 56358 layer_factory.hpp:77] Creating layer data
I0122 16:38:50.374042 56358 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:50.374891 56358 net.cpp:94] Creating Layer data
I0122 16:38:50.374902 56358 net.cpp:409] data -> data
I0122 16:38:50.374910 56358 net.cpp:409] data -> label
I0122 16:38:50.376050 56427 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:38:50.376085 56427 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:38:50.376188 56358 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:38:50.376274 56358 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:38:50.379904 56358 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:38:50.379956 56358 net.cpp:144] Setting up data
I0122 16:38:50.379963 56358 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:38:50.379967 56358 net.cpp:151] Top shape: 50 (50)
I0122 16:38:50.379971 56358 net.cpp:159] Memory required for data: 614600
I0122 16:38:50.379974 56358 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:38:50.379986 56358 net.cpp:94] Creating Layer label_data_1_split
I0122 16:38:50.379990 56358 net.cpp:435] label_data_1_split <- label
I0122 16:38:50.379995 56358 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:38:50.380022 56358 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:38:50.380028 56358 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:38:50.380173 56358 net.cpp:144] Setting up label_data_1_split
I0122 16:38:50.380178 56358 net.cpp:151] Top shape: 50 (50)
I0122 16:38:50.380182 56358 net.cpp:151] Top shape: 50 (50)
I0122 16:38:50.380187 56358 net.cpp:151] Top shape: 50 (50)
I0122 16:38:50.380188 56358 net.cpp:159] Memory required for data: 615200
I0122 16:38:50.380192 56358 layer_factory.hpp:77] Creating layer conv1
I0122 16:38:50.380201 56358 net.cpp:94] Creating Layer conv1
I0122 16:38:50.380205 56358 net.cpp:435] conv1 <- data
I0122 16:38:50.380210 56358 net.cpp:409] conv1 -> conv1
I0122 16:38:50.380525 56358 net.cpp:144] Setting up conv1
I0122 16:38:50.380532 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.380535 56358 net.cpp:159] Memory required for data: 7168800
I0122 16:38:50.380544 56358 layer_factory.hpp:77] Creating layer bn1
I0122 16:38:50.380553 56358 net.cpp:94] Creating Layer bn1
I0122 16:38:50.380556 56358 net.cpp:435] bn1 <- conv1
I0122 16:38:50.380561 56358 net.cpp:409] bn1 -> scale1
I0122 16:38:50.381202 56358 net.cpp:144] Setting up bn1
I0122 16:38:50.381209 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.381212 56358 net.cpp:159] Memory required for data: 13722400
I0122 16:38:50.381223 56358 layer_factory.hpp:77] Creating layer relu1
I0122 16:38:50.381230 56358 net.cpp:94] Creating Layer relu1
I0122 16:38:50.381233 56358 net.cpp:435] relu1 <- scale1
I0122 16:38:50.381238 56358 net.cpp:409] relu1 -> relu1
I0122 16:38:50.381255 56358 net.cpp:144] Setting up relu1
I0122 16:38:50.381261 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.381264 56358 net.cpp:159] Memory required for data: 20276000
I0122 16:38:50.381268 56358 layer_factory.hpp:77] Creating layer conv2
I0122 16:38:50.381275 56358 net.cpp:94] Creating Layer conv2
I0122 16:38:50.381279 56358 net.cpp:435] conv2 <- relu1
I0122 16:38:50.381283 56358 net.cpp:409] conv2 -> conv2
I0122 16:38:50.381862 56358 net.cpp:144] Setting up conv2
I0122 16:38:50.381868 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.381871 56358 net.cpp:159] Memory required for data: 26829600
I0122 16:38:50.381878 56358 layer_factory.hpp:77] Creating layer bn2
I0122 16:38:50.381884 56358 net.cpp:94] Creating Layer bn2
I0122 16:38:50.381888 56358 net.cpp:435] bn2 <- conv2
I0122 16:38:50.381893 56358 net.cpp:409] bn2 -> scale2
I0122 16:38:50.382683 56358 net.cpp:144] Setting up bn2
I0122 16:38:50.382691 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.382694 56358 net.cpp:159] Memory required for data: 33383200
I0122 16:38:50.382702 56358 layer_factory.hpp:77] Creating layer relu2
I0122 16:38:50.382706 56358 net.cpp:94] Creating Layer relu2
I0122 16:38:50.382710 56358 net.cpp:435] relu2 <- scale2
I0122 16:38:50.382714 56358 net.cpp:409] relu2 -> relu2
I0122 16:38:50.382737 56358 net.cpp:144] Setting up relu2
I0122 16:38:50.382742 56358 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:38:50.382745 56358 net.cpp:159] Memory required for data: 39936800
I0122 16:38:50.382748 56358 layer_factory.hpp:77] Creating layer pool1
I0122 16:38:50.382755 56358 net.cpp:94] Creating Layer pool1
I0122 16:38:50.382760 56358 net.cpp:435] pool1 <- relu2
I0122 16:38:50.382764 56358 net.cpp:409] pool1 -> pool1
I0122 16:38:50.382798 56358 net.cpp:144] Setting up pool1
I0122 16:38:50.382814 56358 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:38:50.382817 56358 net.cpp:159] Memory required for data: 41575200
I0122 16:38:50.382820 56358 layer_factory.hpp:77] Creating layer drop1
I0122 16:38:50.382825 56358 net.cpp:94] Creating Layer drop1
I0122 16:38:50.382827 56358 net.cpp:435] drop1 <- pool1
I0122 16:38:50.382836 56358 net.cpp:409] drop1 -> drop1
I0122 16:38:50.382874 56358 net.cpp:144] Setting up drop1
I0122 16:38:50.382879 56358 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:38:50.382881 56358 net.cpp:159] Memory required for data: 43213600
I0122 16:38:50.382884 56358 layer_factory.hpp:77] Creating layer conv3
I0122 16:38:50.382894 56358 net.cpp:94] Creating Layer conv3
I0122 16:38:50.382895 56358 net.cpp:435] conv3 <- drop1
I0122 16:38:50.382900 56358 net.cpp:409] conv3 -> conv3
I0122 16:38:50.383316 56358 net.cpp:144] Setting up conv3
I0122 16:38:50.383323 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.383327 56358 net.cpp:159] Memory required for data: 46490400
I0122 16:38:50.383332 56358 layer_factory.hpp:77] Creating layer bn3
I0122 16:38:50.383344 56358 net.cpp:94] Creating Layer bn3
I0122 16:38:50.383350 56358 net.cpp:435] bn3 <- conv3
I0122 16:38:50.383357 56358 net.cpp:409] bn3 -> scale3
I0122 16:38:50.384027 56358 net.cpp:144] Setting up bn3
I0122 16:38:50.384034 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.384038 56358 net.cpp:159] Memory required for data: 49767200
I0122 16:38:50.384052 56358 layer_factory.hpp:77] Creating layer relu3
I0122 16:38:50.384057 56358 net.cpp:94] Creating Layer relu3
I0122 16:38:50.384061 56358 net.cpp:435] relu3 <- scale3
I0122 16:38:50.384066 56358 net.cpp:409] relu3 -> relu3
I0122 16:38:50.384085 56358 net.cpp:144] Setting up relu3
I0122 16:38:50.384091 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.384094 56358 net.cpp:159] Memory required for data: 53044000
I0122 16:38:50.384097 56358 layer_factory.hpp:77] Creating layer conv4
I0122 16:38:50.384105 56358 net.cpp:94] Creating Layer conv4
I0122 16:38:50.384110 56358 net.cpp:435] conv4 <- relu3
I0122 16:38:50.384116 56358 net.cpp:409] conv4 -> conv4
I0122 16:38:50.384609 56358 net.cpp:144] Setting up conv4
I0122 16:38:50.384616 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.384619 56358 net.cpp:159] Memory required for data: 56320800
I0122 16:38:50.384624 56358 layer_factory.hpp:77] Creating layer bn4
I0122 16:38:50.384632 56358 net.cpp:94] Creating Layer bn4
I0122 16:38:50.384635 56358 net.cpp:435] bn4 <- conv4
I0122 16:38:50.384649 56358 net.cpp:409] bn4 -> scale4
I0122 16:38:50.385354 56358 net.cpp:144] Setting up bn4
I0122 16:38:50.385361 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.385365 56358 net.cpp:159] Memory required for data: 59597600
I0122 16:38:50.385371 56358 layer_factory.hpp:77] Creating layer relu4
I0122 16:38:50.385380 56358 net.cpp:94] Creating Layer relu4
I0122 16:38:50.385383 56358 net.cpp:435] relu4 <- scale4
I0122 16:38:50.385387 56358 net.cpp:409] relu4 -> relu4
I0122 16:38:50.385416 56358 net.cpp:144] Setting up relu4
I0122 16:38:50.385422 56358 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:38:50.385426 56358 net.cpp:159] Memory required for data: 62874400
I0122 16:38:50.385427 56358 layer_factory.hpp:77] Creating layer pool2
I0122 16:38:50.385435 56358 net.cpp:94] Creating Layer pool2
I0122 16:38:50.385439 56358 net.cpp:435] pool2 <- relu4
I0122 16:38:50.385445 56358 net.cpp:409] pool2 -> pool2
I0122 16:38:50.385479 56358 net.cpp:144] Setting up pool2
I0122 16:38:50.385484 56358 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:38:50.385486 56358 net.cpp:159] Memory required for data: 63693600
I0122 16:38:50.385489 56358 layer_factory.hpp:77] Creating layer drop2
I0122 16:38:50.385495 56358 net.cpp:94] Creating Layer drop2
I0122 16:38:50.385498 56358 net.cpp:435] drop2 <- pool2
I0122 16:38:50.385501 56358 net.cpp:409] drop2 -> drop2
I0122 16:38:50.385573 56358 net.cpp:144] Setting up drop2
I0122 16:38:50.385579 56358 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:38:50.385592 56358 net.cpp:159] Memory required for data: 64512800
I0122 16:38:50.385596 56358 layer_factory.hpp:77] Creating layer fc1
I0122 16:38:50.385603 56358 net.cpp:94] Creating Layer fc1
I0122 16:38:50.385607 56358 net.cpp:435] fc1 <- drop2
I0122 16:38:50.385612 56358 net.cpp:409] fc1 -> fc1
I0122 16:38:50.399791 56358 net.cpp:144] Setting up fc1
I0122 16:38:50.399809 56358 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:50.399811 56358 net.cpp:159] Memory required for data: 64615200
I0122 16:38:50.399818 56358 layer_factory.hpp:77] Creating layer bn5
I0122 16:38:50.399827 56358 net.cpp:94] Creating Layer bn5
I0122 16:38:50.399832 56358 net.cpp:435] bn5 <- fc1
I0122 16:38:50.399838 56358 net.cpp:409] bn5 -> scale5
I0122 16:38:50.400457 56358 net.cpp:144] Setting up bn5
I0122 16:38:50.400463 56358 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:50.400466 56358 net.cpp:159] Memory required for data: 64717600
I0122 16:38:50.400480 56358 layer_factory.hpp:77] Creating layer relu5
I0122 16:38:50.400496 56358 net.cpp:94] Creating Layer relu5
I0122 16:38:50.400499 56358 net.cpp:435] relu5 <- scale5
I0122 16:38:50.400506 56358 net.cpp:409] relu5 -> relu5
I0122 16:38:50.400524 56358 net.cpp:144] Setting up relu5
I0122 16:38:50.400530 56358 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:50.400532 56358 net.cpp:159] Memory required for data: 64820000
I0122 16:38:50.400535 56358 layer_factory.hpp:77] Creating layer drop3
I0122 16:38:50.400540 56358 net.cpp:94] Creating Layer drop3
I0122 16:38:50.400543 56358 net.cpp:435] drop3 <- relu5
I0122 16:38:50.400547 56358 net.cpp:409] drop3 -> drop3
I0122 16:38:50.400576 56358 net.cpp:144] Setting up drop3
I0122 16:38:50.400581 56358 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:38:50.400584 56358 net.cpp:159] Memory required for data: 64922400
I0122 16:38:50.400586 56358 layer_factory.hpp:77] Creating layer fc2
I0122 16:38:50.400594 56358 net.cpp:94] Creating Layer fc2
I0122 16:38:50.400599 56358 net.cpp:435] fc2 <- drop3
I0122 16:38:50.400604 56358 net.cpp:409] fc2 -> fc2
I0122 16:38:50.400743 56358 net.cpp:144] Setting up fc2
I0122 16:38:50.400749 56358 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:50.400753 56358 net.cpp:159] Memory required for data: 64924400
I0122 16:38:50.400756 56358 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:38:50.400763 56358 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:38:50.400766 56358 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:38:50.400770 56358 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:38:50.400776 56358 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:38:50.400784 56358 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:38:50.400825 56358 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:38:50.400830 56358 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:50.400833 56358 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:50.400836 56358 net.cpp:151] Top shape: 50 10 (500)
I0122 16:38:50.400840 56358 net.cpp:159] Memory required for data: 64930400
I0122 16:38:50.400842 56358 layer_factory.hpp:77] Creating layer loss
I0122 16:38:50.400846 56358 net.cpp:94] Creating Layer loss
I0122 16:38:50.400849 56358 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:38:50.400853 56358 net.cpp:435] loss <- label_data_1_split_0
I0122 16:38:50.400858 56358 net.cpp:409] loss -> loss
I0122 16:38:50.400866 56358 layer_factory.hpp:77] Creating layer loss
I0122 16:38:50.400938 56358 net.cpp:144] Setting up loss
I0122 16:38:50.400944 56358 net.cpp:151] Top shape: (1)
I0122 16:38:50.400946 56358 net.cpp:154]     with loss weight 1
I0122 16:38:50.400955 56358 net.cpp:159] Memory required for data: 64930404
I0122 16:38:50.400959 56358 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:38:50.400964 56358 net.cpp:94] Creating Layer accuracy-top1
I0122 16:38:50.400967 56358 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:38:50.400971 56358 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:38:50.400976 56358 net.cpp:409] accuracy-top1 -> top-1
I0122 16:38:50.400995 56358 net.cpp:144] Setting up accuracy-top1
I0122 16:38:50.401000 56358 net.cpp:151] Top shape: (1)
I0122 16:38:50.401001 56358 net.cpp:159] Memory required for data: 64930408
I0122 16:38:50.401003 56358 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:38:50.401008 56358 net.cpp:94] Creating Layer accuracy-top5
I0122 16:38:50.401011 56358 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:38:50.401015 56358 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:38:50.401019 56358 net.cpp:409] accuracy-top5 -> top-5
I0122 16:38:50.401026 56358 net.cpp:144] Setting up accuracy-top5
I0122 16:38:50.401031 56358 net.cpp:151] Top shape: (1)
I0122 16:38:50.401032 56358 net.cpp:159] Memory required for data: 64930412
I0122 16:38:50.401036 56358 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:38:50.401039 56358 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:38:50.401043 56358 net.cpp:220] loss needs backward computation.
I0122 16:38:50.401046 56358 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:38:50.401049 56358 net.cpp:220] fc2 needs backward computation.
I0122 16:38:50.401053 56358 net.cpp:220] drop3 needs backward computation.
I0122 16:38:50.401055 56358 net.cpp:220] relu5 needs backward computation.
I0122 16:38:50.401062 56358 net.cpp:220] bn5 needs backward computation.
I0122 16:38:50.401065 56358 net.cpp:220] fc1 needs backward computation.
I0122 16:38:50.401069 56358 net.cpp:220] drop2 needs backward computation.
I0122 16:38:50.401072 56358 net.cpp:220] pool2 needs backward computation.
I0122 16:38:50.401075 56358 net.cpp:220] relu4 needs backward computation.
I0122 16:38:50.401077 56358 net.cpp:220] bn4 needs backward computation.
I0122 16:38:50.401082 56358 net.cpp:220] conv4 needs backward computation.
I0122 16:38:50.401084 56358 net.cpp:220] relu3 needs backward computation.
I0122 16:38:50.401087 56358 net.cpp:220] bn3 needs backward computation.
I0122 16:38:50.401090 56358 net.cpp:220] conv3 needs backward computation.
I0122 16:38:50.401093 56358 net.cpp:220] drop1 needs backward computation.
I0122 16:38:50.401096 56358 net.cpp:220] pool1 needs backward computation.
I0122 16:38:50.401099 56358 net.cpp:220] relu2 needs backward computation.
I0122 16:38:50.401103 56358 net.cpp:220] bn2 needs backward computation.
I0122 16:38:50.401105 56358 net.cpp:220] conv2 needs backward computation.
I0122 16:38:50.401108 56358 net.cpp:220] relu1 needs backward computation.
I0122 16:38:50.401111 56358 net.cpp:220] bn1 needs backward computation.
I0122 16:38:50.401114 56358 net.cpp:220] conv1 needs backward computation.
I0122 16:38:50.401118 56358 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:38:50.401121 56358 net.cpp:222] data does not need backward computation.
I0122 16:38:50.401124 56358 net.cpp:264] This network produces output loss
I0122 16:38:50.401127 56358 net.cpp:264] This network produces output top-1
I0122 16:38:50.401130 56358 net.cpp:264] This network produces output top-5
I0122 16:38:50.401154 56358 net.cpp:284] Network initialization done.
I0122 16:38:50.401257 56358 solver.cpp:63] Solver scaffolding done.
I0122 16:38:50.402413 56358 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/sparse.caffemodel
I0122 16:38:50.461673 56358 caffe_interface.cpp:527] Starting Optimization
I0122 16:38:50.461693 56358 solver.cpp:335] Solving 
I0122 16:38:50.461694 56358 solver.cpp:336] Learning Rate Policy: poly
I0122 16:38:50.462932 56358 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:38:50.689474 56358 solver.cpp:517]     Test net output #0: loss = 1.29264 (* 1 = 1.29264 loss)
I0122 16:38:50.689498 56358 solver.cpp:517]     Test net output #1: top-1 = 0.663556
I0122 16:38:50.689503 56358 solver.cpp:517]     Test net output #2: top-5 = 0.972
I0122 16:38:50.706653 56358 solver.cpp:266] Iteration 0 (0 iter/s, 0.244919s/100 iter), loss = 0.187225
I0122 16:38:50.706683 56358 solver.cpp:285]     Train net output #0: loss = 0.187225 (* 1 = 0.187225 loss)
I0122 16:38:50.706722 56358 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:38:51.572247 56358 solver.cpp:266] Iteration 100 (115.537 iter/s, 0.865526s/100 iter), loss = 0.275855
I0122 16:38:51.572273 56358 solver.cpp:285]     Train net output #0: loss = 0.275855 (* 1 = 0.275855 loss)
I0122 16:38:51.572280 56358 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:38:52.436559 56358 solver.cpp:266] Iteration 200 (115.708 iter/s, 0.864248s/100 iter), loss = 0.236721
I0122 16:38:52.436599 56358 solver.cpp:285]     Train net output #0: loss = 0.236721 (* 1 = 0.236721 loss)
I0122 16:38:52.436604 56358 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:38:53.301424 56358 solver.cpp:266] Iteration 300 (115.635 iter/s, 0.864787s/100 iter), loss = 0.245984
I0122 16:38:53.301463 56358 solver.cpp:285]     Train net output #0: loss = 0.245984 (* 1 = 0.245984 loss)
I0122 16:38:53.301470 56358 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:38:54.163635 56358 solver.cpp:266] Iteration 400 (115.991 iter/s, 0.862133s/100 iter), loss = 0.183398
I0122 16:38:54.163661 56358 solver.cpp:285]     Train net output #0: loss = 0.183398 (* 1 = 0.183398 loss)
I0122 16:38:54.163667 56358 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:38:55.025578 56358 solver.cpp:266] Iteration 500 (116.026 iter/s, 0.861878s/100 iter), loss = 0.308119
I0122 16:38:55.025605 56358 solver.cpp:285]     Train net output #0: loss = 0.308119 (* 1 = 0.308119 loss)
I0122 16:38:55.025611 56358 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:38:55.887598 56358 solver.cpp:266] Iteration 600 (116.016 iter/s, 0.861953s/100 iter), loss = 0.292541
I0122 16:38:55.887624 56358 solver.cpp:285]     Train net output #0: loss = 0.292541 (* 1 = 0.292541 loss)
I0122 16:38:55.887630 56358 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:38:56.755240 56358 solver.cpp:266] Iteration 700 (115.264 iter/s, 0.867577s/100 iter), loss = 0.472931
I0122 16:38:56.755267 56358 solver.cpp:285]     Train net output #0: loss = 0.472931 (* 1 = 0.472931 loss)
I0122 16:38:56.755273 56358 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:38:57.616762 56358 solver.cpp:266] Iteration 800 (116.083 iter/s, 0.861456s/100 iter), loss = 0.394181
I0122 16:38:57.616791 56358 solver.cpp:285]     Train net output #0: loss = 0.394181 (* 1 = 0.394181 loss)
I0122 16:38:57.616796 56358 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:38:58.480615 56358 solver.cpp:266] Iteration 900 (115.769 iter/s, 0.863787s/100 iter), loss = 0.199125
I0122 16:38:58.480643 56358 solver.cpp:285]     Train net output #0: loss = 0.199125 (* 1 = 0.199125 loss)
I0122 16:38:58.480648 56358 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:38:59.341756 56358 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:38:59.563309 56358 solver.cpp:517]     Test net output #0: loss = 0.868949 (* 1 = 0.868949 loss)
I0122 16:38:59.563324 56358 solver.cpp:517]     Test net output #1: top-1 = 0.774889
I0122 16:38:59.563329 56358 solver.cpp:517]     Test net output #2: top-5 = 0.981112
I0122 16:38:59.571394 56358 solver.cpp:266] Iteration 1000 (91.6834 iter/s, 1.09071s/100 iter), loss = 0.244894
I0122 16:38:59.571411 56358 solver.cpp:285]     Train net output #0: loss = 0.244894 (* 1 = 0.244894 loss)
I0122 16:38:59.571418 56358 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:39:00.437387 56358 solver.cpp:266] Iteration 1100 (115.482 iter/s, 0.865937s/100 iter), loss = 0.25375
I0122 16:39:00.437414 56358 solver.cpp:285]     Train net output #0: loss = 0.25375 (* 1 = 0.25375 loss)
I0122 16:39:00.437419 56358 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:39:01.301543 56358 solver.cpp:266] Iteration 1200 (115.729 iter/s, 0.864091s/100 iter), loss = 0.371845
I0122 16:39:01.301570 56358 solver.cpp:285]     Train net output #0: loss = 0.371845 (* 1 = 0.371845 loss)
I0122 16:39:01.301575 56358 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:39:02.165213 56358 solver.cpp:266] Iteration 1300 (115.794 iter/s, 0.863604s/100 iter), loss = 0.334344
I0122 16:39:02.165241 56358 solver.cpp:285]     Train net output #0: loss = 0.334344 (* 1 = 0.334344 loss)
I0122 16:39:02.165264 56358 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:39:03.027678 56358 solver.cpp:266] Iteration 1400 (115.956 iter/s, 0.862397s/100 iter), loss = 0.208302
I0122 16:39:03.027705 56358 solver.cpp:285]     Train net output #0: loss = 0.208302 (* 1 = 0.208302 loss)
I0122 16:39:03.027711 56358 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:39:03.890936 56358 solver.cpp:266] Iteration 1500 (115.849 iter/s, 0.863192s/100 iter), loss = 0.161811
I0122 16:39:03.890964 56358 solver.cpp:285]     Train net output #0: loss = 0.161811 (* 1 = 0.161811 loss)
I0122 16:39:03.890969 56358 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:39:04.753965 56358 solver.cpp:266] Iteration 1600 (115.88 iter/s, 0.862963s/100 iter), loss = 0.337869
I0122 16:39:04.753993 56358 solver.cpp:285]     Train net output #0: loss = 0.337869 (* 1 = 0.337869 loss)
I0122 16:39:04.753998 56358 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:39:05.617162 56358 solver.cpp:266] Iteration 1700 (115.857 iter/s, 0.863132s/100 iter), loss = 0.263484
I0122 16:39:05.617189 56358 solver.cpp:285]     Train net output #0: loss = 0.263484 (* 1 = 0.263484 loss)
I0122 16:39:05.617195 56358 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:39:06.479921 56358 solver.cpp:266] Iteration 1800 (115.916 iter/s, 0.862694s/100 iter), loss = 0.231168
I0122 16:39:06.479948 56358 solver.cpp:285]     Train net output #0: loss = 0.231168 (* 1 = 0.231168 loss)
I0122 16:39:06.479954 56358 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:39:07.343329 56358 solver.cpp:266] Iteration 1900 (115.829 iter/s, 0.863343s/100 iter), loss = 0.19005
I0122 16:39:07.343355 56358 solver.cpp:285]     Train net output #0: loss = 0.19005 (* 1 = 0.19005 loss)
I0122 16:39:07.343360 56358 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:39:08.197926 56358 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:39:08.417404 56358 solver.cpp:517]     Test net output #0: loss = 0.770257 (* 1 = 0.770257 loss)
I0122 16:39:08.417419 56358 solver.cpp:517]     Test net output #1: top-1 = 0.797333
I0122 16:39:08.417423 56358 solver.cpp:517]     Test net output #2: top-5 = 0.983334
I0122 16:39:08.425510 56358 solver.cpp:266] Iteration 2000 (92.4119 iter/s, 1.08211s/100 iter), loss = 0.227372
I0122 16:39:08.425527 56358 solver.cpp:285]     Train net output #0: loss = 0.227372 (* 1 = 0.227372 loss)
I0122 16:39:08.425532 56358 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:39:09.291072 56358 solver.cpp:266] Iteration 2100 (115.539 iter/s, 0.865506s/100 iter), loss = 0.251298
I0122 16:39:09.291097 56358 solver.cpp:285]     Train net output #0: loss = 0.251298 (* 1 = 0.251298 loss)
I0122 16:39:09.291103 56358 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:39:10.152398 56358 solver.cpp:266] Iteration 2200 (116.108 iter/s, 0.861264s/100 iter), loss = 0.239137
I0122 16:39:10.152426 56358 solver.cpp:285]     Train net output #0: loss = 0.239137 (* 1 = 0.239137 loss)
I0122 16:39:10.152431 56358 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:39:11.015380 56358 solver.cpp:266] Iteration 2300 (115.886 iter/s, 0.862918s/100 iter), loss = 0.346102
I0122 16:39:11.015408 56358 solver.cpp:285]     Train net output #0: loss = 0.346102 (* 1 = 0.346102 loss)
I0122 16:39:11.015413 56358 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:39:11.878247 56358 solver.cpp:266] Iteration 2400 (115.901 iter/s, 0.862802s/100 iter), loss = 0.21026
I0122 16:39:11.878285 56358 solver.cpp:285]     Train net output #0: loss = 0.21026 (* 1 = 0.21026 loss)
I0122 16:39:11.878291 56358 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:39:12.741438 56358 solver.cpp:266] Iteration 2500 (115.86 iter/s, 0.863114s/100 iter), loss = 0.20698
I0122 16:39:12.741466 56358 solver.cpp:285]     Train net output #0: loss = 0.20698 (* 1 = 0.20698 loss)
I0122 16:39:12.741472 56358 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:39:13.605206 56358 solver.cpp:266] Iteration 2600 (115.781 iter/s, 0.863701s/100 iter), loss = 0.221458
I0122 16:39:13.605250 56358 solver.cpp:285]     Train net output #0: loss = 0.221458 (* 1 = 0.221458 loss)
I0122 16:39:13.605257 56358 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:39:14.468376 56358 solver.cpp:266] Iteration 2700 (115.863 iter/s, 0.863089s/100 iter), loss = 0.275625
I0122 16:39:14.468405 56358 solver.cpp:285]     Train net output #0: loss = 0.275625 (* 1 = 0.275625 loss)
I0122 16:39:14.468410 56358 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:39:15.332212 56358 solver.cpp:266] Iteration 2800 (115.771 iter/s, 0.863771s/100 iter), loss = 0.244255
I0122 16:39:15.332242 56358 solver.cpp:285]     Train net output #0: loss = 0.244255 (* 1 = 0.244255 loss)
I0122 16:39:15.332247 56358 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:39:16.194941 56358 solver.cpp:266] Iteration 2900 (115.92 iter/s, 0.862661s/100 iter), loss = 0.226373
I0122 16:39:16.194968 56358 solver.cpp:285]     Train net output #0: loss = 0.226373 (* 1 = 0.226373 loss)
I0122 16:39:16.194974 56358 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:39:17.049672 56358 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:39:17.269393 56358 solver.cpp:517]     Test net output #0: loss = 0.668943 (* 1 = 0.668943 loss)
I0122 16:39:17.269407 56358 solver.cpp:517]     Test net output #1: top-1 = 0.799222
I0122 16:39:17.269412 56358 solver.cpp:517]     Test net output #2: top-5 = 0.986334
I0122 16:39:17.277467 56358 solver.cpp:266] Iteration 3000 (92.3826 iter/s, 1.08246s/100 iter), loss = 0.192249
I0122 16:39:17.277493 56358 solver.cpp:285]     Train net output #0: loss = 0.192249 (* 1 = 0.192249 loss)
I0122 16:39:17.277500 56358 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:39:18.144027 56358 solver.cpp:266] Iteration 3100 (115.406 iter/s, 0.866505s/100 iter), loss = 0.19544
I0122 16:39:18.144053 56358 solver.cpp:285]     Train net output #0: loss = 0.19544 (* 1 = 0.19544 loss)
I0122 16:39:18.144058 56358 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:39:19.006961 56358 solver.cpp:266] Iteration 3200 (115.892 iter/s, 0.862871s/100 iter), loss = 0.284749
I0122 16:39:19.006989 56358 solver.cpp:285]     Train net output #0: loss = 0.284749 (* 1 = 0.284749 loss)
I0122 16:39:19.006994 56358 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:39:19.870378 56358 solver.cpp:266] Iteration 3300 (115.828 iter/s, 0.863351s/100 iter), loss = 0.132221
I0122 16:39:19.870522 56358 solver.cpp:285]     Train net output #0: loss = 0.132221 (* 1 = 0.132221 loss)
I0122 16:39:19.870529 56358 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:39:20.736526 56358 solver.cpp:266] Iteration 3400 (115.477 iter/s, 0.86597s/100 iter), loss = 0.253273
I0122 16:39:20.736553 56358 solver.cpp:285]     Train net output #0: loss = 0.253273 (* 1 = 0.253273 loss)
I0122 16:39:20.736559 56358 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:39:21.608160 56358 solver.cpp:266] Iteration 3500 (114.736 iter/s, 0.871567s/100 iter), loss = 0.266053
I0122 16:39:21.608186 56358 solver.cpp:285]     Train net output #0: loss = 0.266053 (* 1 = 0.266053 loss)
I0122 16:39:21.608191 56358 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:39:22.477016 56358 solver.cpp:266] Iteration 3600 (115.102 iter/s, 0.868792s/100 iter), loss = 0.287669
I0122 16:39:22.477042 56358 solver.cpp:285]     Train net output #0: loss = 0.287669 (* 1 = 0.287669 loss)
I0122 16:39:22.477047 56358 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:39:23.342468 56358 solver.cpp:266] Iteration 3700 (115.555 iter/s, 0.865387s/100 iter), loss = 0.193523
I0122 16:39:23.342495 56358 solver.cpp:285]     Train net output #0: loss = 0.193523 (* 1 = 0.193523 loss)
I0122 16:39:23.342502 56358 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:39:24.208081 56358 solver.cpp:266] Iteration 3800 (115.534 iter/s, 0.865546s/100 iter), loss = 0.216373
I0122 16:39:24.208111 56358 solver.cpp:285]     Train net output #0: loss = 0.216373 (* 1 = 0.216373 loss)
I0122 16:39:24.208115 56358 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:39:25.070724 56358 solver.cpp:266] Iteration 3900 (115.932 iter/s, 0.862578s/100 iter), loss = 0.159415
I0122 16:39:25.070751 56358 solver.cpp:285]     Train net output #0: loss = 0.159415 (* 1 = 0.159415 loss)
I0122 16:39:25.070756 56358 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:39:25.923995 56358 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:39:26.144367 56358 solver.cpp:517]     Test net output #0: loss = 0.590049 (* 1 = 0.590049 loss)
I0122 16:39:26.144381 56358 solver.cpp:517]     Test net output #1: top-1 = 0.822556
I0122 16:39:26.144385 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990333
I0122 16:39:26.152503 56358 solver.cpp:266] Iteration 4000 (92.4463 iter/s, 1.08171s/100 iter), loss = 0.219252
I0122 16:39:26.152520 56358 solver.cpp:285]     Train net output #0: loss = 0.219252 (* 1 = 0.219252 loss)
I0122 16:39:26.152525 56358 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:39:27.015589 56358 solver.cpp:266] Iteration 4100 (115.871 iter/s, 0.86303s/100 iter), loss = 0.235417
I0122 16:39:27.015614 56358 solver.cpp:285]     Train net output #0: loss = 0.235417 (* 1 = 0.235417 loss)
I0122 16:39:27.015620 56358 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:39:27.879992 56358 solver.cpp:266] Iteration 4200 (115.695 iter/s, 0.864339s/100 iter), loss = 0.297365
I0122 16:39:27.880017 56358 solver.cpp:285]     Train net output #0: loss = 0.297365 (* 1 = 0.297365 loss)
I0122 16:39:27.880023 56358 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:39:28.743436 56358 solver.cpp:266] Iteration 4300 (115.824 iter/s, 0.863381s/100 iter), loss = 0.246826
I0122 16:39:28.743463 56358 solver.cpp:285]     Train net output #0: loss = 0.246826 (* 1 = 0.246826 loss)
I0122 16:39:28.743468 56358 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:39:29.615756 56358 solver.cpp:266] Iteration 4400 (114.646 iter/s, 0.872253s/100 iter), loss = 0.247167
I0122 16:39:29.615783 56358 solver.cpp:285]     Train net output #0: loss = 0.247168 (* 1 = 0.247168 loss)
I0122 16:39:29.615789 56358 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:39:30.485623 56358 solver.cpp:266] Iteration 4500 (114.969 iter/s, 0.869802s/100 iter), loss = 0.226204
I0122 16:39:30.485651 56358 solver.cpp:285]     Train net output #0: loss = 0.226204 (* 1 = 0.226204 loss)
I0122 16:39:30.485656 56358 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:39:31.350811 56358 solver.cpp:266] Iteration 4600 (115.591 iter/s, 0.865122s/100 iter), loss = 0.169253
I0122 16:39:31.350870 56358 solver.cpp:285]     Train net output #0: loss = 0.169253 (* 1 = 0.169253 loss)
I0122 16:39:31.350878 56358 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:39:32.217686 56358 solver.cpp:266] Iteration 4700 (115.37 iter/s, 0.866777s/100 iter), loss = 0.208356
I0122 16:39:32.217713 56358 solver.cpp:285]     Train net output #0: loss = 0.208356 (* 1 = 0.208356 loss)
I0122 16:39:32.217718 56358 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:39:33.084478 56358 solver.cpp:266] Iteration 4800 (115.377 iter/s, 0.866727s/100 iter), loss = 0.21154
I0122 16:39:33.084506 56358 solver.cpp:285]     Train net output #0: loss = 0.21154 (* 1 = 0.21154 loss)
I0122 16:39:33.084511 56358 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:39:33.950562 56358 solver.cpp:266] Iteration 4900 (115.471 iter/s, 0.866018s/100 iter), loss = 0.355379
I0122 16:39:33.950590 56358 solver.cpp:285]     Train net output #0: loss = 0.355379 (* 1 = 0.355379 loss)
I0122 16:39:33.950597 56358 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:39:34.807580 56358 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:39:35.036131 56358 solver.cpp:517]     Test net output #0: loss = 0.512399 (* 1 = 0.512399 loss)
I0122 16:39:35.036149 56358 solver.cpp:517]     Test net output #1: top-1 = 0.836556
I0122 16:39:35.036154 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990222
I0122 16:39:35.044328 56358 solver.cpp:266] Iteration 5000 (91.4332 iter/s, 1.09369s/100 iter), loss = 0.248553
I0122 16:39:35.044347 56358 solver.cpp:285]     Train net output #0: loss = 0.248553 (* 1 = 0.248553 loss)
I0122 16:39:35.044353 56358 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:39:35.909921 56358 solver.cpp:266] Iteration 5100 (115.536 iter/s, 0.865535s/100 iter), loss = 0.253116
I0122 16:39:35.909947 56358 solver.cpp:285]     Train net output #0: loss = 0.253116 (* 1 = 0.253116 loss)
I0122 16:39:35.909953 56358 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:39:36.777719 56358 solver.cpp:266] Iteration 5200 (115.243 iter/s, 0.867734s/100 iter), loss = 0.233294
I0122 16:39:36.777746 56358 solver.cpp:285]     Train net output #0: loss = 0.233294 (* 1 = 0.233294 loss)
I0122 16:39:36.777751 56358 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:39:37.650826 56358 solver.cpp:266] Iteration 5300 (114.542 iter/s, 0.873041s/100 iter), loss = 0.26883
I0122 16:39:37.650852 56358 solver.cpp:285]     Train net output #0: loss = 0.26883 (* 1 = 0.26883 loss)
I0122 16:39:37.650874 56358 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:39:38.521621 56358 solver.cpp:266] Iteration 5400 (114.846 iter/s, 0.870731s/100 iter), loss = 0.192107
I0122 16:39:38.521648 56358 solver.cpp:285]     Train net output #0: loss = 0.192107 (* 1 = 0.192107 loss)
I0122 16:39:38.521654 56358 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:39:39.384289 56358 solver.cpp:266] Iteration 5500 (115.928 iter/s, 0.862601s/100 iter), loss = 0.173617
I0122 16:39:39.384315 56358 solver.cpp:285]     Train net output #0: loss = 0.173617 (* 1 = 0.173617 loss)
I0122 16:39:39.384320 56358 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:39:40.251325 56358 solver.cpp:266] Iteration 5600 (115.344 iter/s, 0.866972s/100 iter), loss = 0.214834
I0122 16:39:40.251351 56358 solver.cpp:285]     Train net output #0: loss = 0.214834 (* 1 = 0.214834 loss)
I0122 16:39:40.251356 56358 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:39:41.114099 56358 solver.cpp:266] Iteration 5700 (115.914 iter/s, 0.862708s/100 iter), loss = 0.233871
I0122 16:39:41.114126 56358 solver.cpp:285]     Train net output #0: loss = 0.233871 (* 1 = 0.233871 loss)
I0122 16:39:41.114131 56358 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:39:41.990056 56358 solver.cpp:266] Iteration 5800 (114.17 iter/s, 0.875891s/100 iter), loss = 0.227502
I0122 16:39:41.990084 56358 solver.cpp:285]     Train net output #0: loss = 0.227502 (* 1 = 0.227502 loss)
I0122 16:39:41.990108 56358 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:39:42.859736 56358 solver.cpp:266] Iteration 5900 (114.994 iter/s, 0.869613s/100 iter), loss = 0.239008
I0122 16:39:42.859763 56358 solver.cpp:285]     Train net output #0: loss = 0.239008 (* 1 = 0.239008 loss)
I0122 16:39:42.859768 56358 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:39:43.717051 56358 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:39:43.936792 56358 solver.cpp:517]     Test net output #0: loss = 0.557302 (* 1 = 0.557302 loss)
I0122 16:39:43.936807 56358 solver.cpp:517]     Test net output #1: top-1 = 0.837111
I0122 16:39:43.936811 56358 solver.cpp:517]     Test net output #2: top-5 = 0.99
I0122 16:39:43.944916 56358 solver.cpp:266] Iteration 6000 (92.1565 iter/s, 1.08511s/100 iter), loss = 0.248403
I0122 16:39:43.944933 56358 solver.cpp:285]     Train net output #0: loss = 0.248403 (* 1 = 0.248403 loss)
I0122 16:39:43.944938 56358 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:39:44.808531 56358 solver.cpp:266] Iteration 6100 (115.8 iter/s, 0.86356s/100 iter), loss = 0.222723
I0122 16:39:44.808557 56358 solver.cpp:285]     Train net output #0: loss = 0.222723 (* 1 = 0.222723 loss)
I0122 16:39:44.808562 56358 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:39:45.673879 56358 solver.cpp:266] Iteration 6200 (115.569 iter/s, 0.865283s/100 iter), loss = 0.227622
I0122 16:39:45.673907 56358 solver.cpp:285]     Train net output #0: loss = 0.227622 (* 1 = 0.227622 loss)
I0122 16:39:45.673915 56358 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:39:46.541980 56358 solver.cpp:266] Iteration 6300 (115.202 iter/s, 0.868037s/100 iter), loss = 0.146399
I0122 16:39:46.542007 56358 solver.cpp:285]     Train net output #0: loss = 0.146399 (* 1 = 0.146399 loss)
I0122 16:39:46.542012 56358 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:39:47.414607 56358 solver.cpp:266] Iteration 6400 (114.605 iter/s, 0.872559s/100 iter), loss = 0.130591
I0122 16:39:47.414645 56358 solver.cpp:285]     Train net output #0: loss = 0.130591 (* 1 = 0.130591 loss)
I0122 16:39:47.414651 56358 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:39:48.285969 56358 solver.cpp:266] Iteration 6500 (114.773 iter/s, 0.871284s/100 iter), loss = 0.231326
I0122 16:39:48.285996 56358 solver.cpp:285]     Train net output #0: loss = 0.231326 (* 1 = 0.231326 loss)
I0122 16:39:48.286002 56358 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:39:49.149631 56358 solver.cpp:266] Iteration 6600 (115.795 iter/s, 0.863598s/100 iter), loss = 0.245378
I0122 16:39:49.149657 56358 solver.cpp:285]     Train net output #0: loss = 0.245378 (* 1 = 0.245378 loss)
I0122 16:39:49.149663 56358 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:39:50.026733 56358 solver.cpp:266] Iteration 6700 (114.02 iter/s, 0.877036s/100 iter), loss = 0.221438
I0122 16:39:50.026911 56358 solver.cpp:285]     Train net output #0: loss = 0.221438 (* 1 = 0.221438 loss)
I0122 16:39:50.026919 56358 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:39:50.896739 56358 solver.cpp:266] Iteration 6800 (114.97 iter/s, 0.869792s/100 iter), loss = 0.299731
I0122 16:39:50.896766 56358 solver.cpp:285]     Train net output #0: loss = 0.299731 (* 1 = 0.299731 loss)
I0122 16:39:50.896772 56358 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:39:51.760782 56358 solver.cpp:266] Iteration 6900 (115.744 iter/s, 0.863976s/100 iter), loss = 0.207094
I0122 16:39:51.760808 56358 solver.cpp:285]     Train net output #0: loss = 0.207094 (* 1 = 0.207094 loss)
I0122 16:39:51.760814 56358 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:39:52.618552 56358 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:39:52.839859 56358 solver.cpp:517]     Test net output #0: loss = 0.507769 (* 1 = 0.507769 loss)
I0122 16:39:52.839874 56358 solver.cpp:517]     Test net output #1: top-1 = 0.843666
I0122 16:39:52.839879 56358 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0122 16:39:52.847950 56358 solver.cpp:266] Iteration 7000 (91.9879 iter/s, 1.0871s/100 iter), loss = 0.269685
I0122 16:39:52.847968 56358 solver.cpp:285]     Train net output #0: loss = 0.269685 (* 1 = 0.269685 loss)
I0122 16:39:52.847975 56358 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:39:53.710907 56358 solver.cpp:266] Iteration 7100 (115.888 iter/s, 0.8629s/100 iter), loss = 0.238185
I0122 16:39:53.710932 56358 solver.cpp:285]     Train net output #0: loss = 0.238185 (* 1 = 0.238185 loss)
I0122 16:39:53.710938 56358 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:39:54.578660 56358 solver.cpp:266] Iteration 7200 (115.249 iter/s, 0.867687s/100 iter), loss = 0.372838
I0122 16:39:54.578685 56358 solver.cpp:285]     Train net output #0: loss = 0.372838 (* 1 = 0.372838 loss)
I0122 16:39:54.578691 56358 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:39:55.450206 56358 solver.cpp:266] Iteration 7300 (114.747 iter/s, 0.871481s/100 iter), loss = 0.276045
I0122 16:39:55.450232 56358 solver.cpp:285]     Train net output #0: loss = 0.276045 (* 1 = 0.276045 loss)
I0122 16:39:55.450238 56358 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:39:56.316293 56358 solver.cpp:266] Iteration 7400 (115.47 iter/s, 0.866023s/100 iter), loss = 0.208802
I0122 16:39:56.316318 56358 solver.cpp:285]     Train net output #0: loss = 0.208802 (* 1 = 0.208802 loss)
I0122 16:39:56.316324 56358 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:39:57.179309 56358 solver.cpp:266] Iteration 7500 (115.881 iter/s, 0.862951s/100 iter), loss = 0.262486
I0122 16:39:57.179334 56358 solver.cpp:285]     Train net output #0: loss = 0.262486 (* 1 = 0.262486 loss)
I0122 16:39:57.179340 56358 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:39:58.042434 56358 solver.cpp:266] Iteration 7600 (115.867 iter/s, 0.863062s/100 iter), loss = 0.218856
I0122 16:39:58.042460 56358 solver.cpp:285]     Train net output #0: loss = 0.218856 (* 1 = 0.218856 loss)
I0122 16:39:58.042465 56358 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:39:58.905459 56358 solver.cpp:266] Iteration 7700 (115.88 iter/s, 0.862961s/100 iter), loss = 0.151931
I0122 16:39:58.905485 56358 solver.cpp:285]     Train net output #0: loss = 0.151931 (* 1 = 0.151931 loss)
I0122 16:39:58.905491 56358 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:39:59.772233 56358 solver.cpp:266] Iteration 7800 (115.379 iter/s, 0.86671s/100 iter), loss = 0.174728
I0122 16:39:59.772258 56358 solver.cpp:285]     Train net output #0: loss = 0.174728 (* 1 = 0.174728 loss)
I0122 16:39:59.772264 56358 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:40:00.639921 56358 solver.cpp:266] Iteration 7900 (115.257 iter/s, 0.867625s/100 iter), loss = 0.183744
I0122 16:40:00.639948 56358 solver.cpp:285]     Train net output #0: loss = 0.183744 (* 1 = 0.183744 loss)
I0122 16:40:00.639953 56358 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:40:01.496666 56358 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:40:01.716265 56358 solver.cpp:517]     Test net output #0: loss = 0.551618 (* 1 = 0.551618 loss)
I0122 16:40:01.716280 56358 solver.cpp:517]     Test net output #1: top-1 = 0.829222
I0122 16:40:01.716284 56358 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:40:01.724334 56358 solver.cpp:266] Iteration 8000 (92.2218 iter/s, 1.08434s/100 iter), loss = 0.185087
I0122 16:40:01.724350 56358 solver.cpp:285]     Train net output #0: loss = 0.185087 (* 1 = 0.185087 loss)
I0122 16:40:01.724356 56358 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:40:02.587471 56358 solver.cpp:266] Iteration 8100 (115.864 iter/s, 0.863082s/100 iter), loss = 0.224262
I0122 16:40:02.587496 56358 solver.cpp:285]     Train net output #0: loss = 0.224262 (* 1 = 0.224262 loss)
I0122 16:40:02.587502 56358 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:40:03.454341 56358 solver.cpp:266] Iteration 8200 (115.366 iter/s, 0.866807s/100 iter), loss = 0.161389
I0122 16:40:03.454370 56358 solver.cpp:285]     Train net output #0: loss = 0.161389 (* 1 = 0.161389 loss)
I0122 16:40:03.454376 56358 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:40:04.353070 56358 solver.cpp:266] Iteration 8300 (111.277 iter/s, 0.898657s/100 iter), loss = 0.20406
I0122 16:40:04.353097 56358 solver.cpp:285]     Train net output #0: loss = 0.20406 (* 1 = 0.20406 loss)
I0122 16:40:04.353103 56358 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:40:05.245889 56358 solver.cpp:266] Iteration 8400 (112.013 iter/s, 0.892752s/100 iter), loss = 0.134937
I0122 16:40:05.245921 56358 solver.cpp:285]     Train net output #0: loss = 0.134937 (* 1 = 0.134937 loss)
I0122 16:40:05.245926 56358 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:40:06.116338 56358 solver.cpp:266] Iteration 8500 (114.893 iter/s, 0.870378s/100 iter), loss = 0.247626
I0122 16:40:06.116365 56358 solver.cpp:285]     Train net output #0: loss = 0.247626 (* 1 = 0.247626 loss)
I0122 16:40:06.116371 56358 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:40:07.010193 56358 solver.cpp:266] Iteration 8600 (111.883 iter/s, 0.893788s/100 iter), loss = 0.163042
I0122 16:40:07.010221 56358 solver.cpp:285]     Train net output #0: loss = 0.163042 (* 1 = 0.163042 loss)
I0122 16:40:07.010226 56358 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:40:07.873512 56358 solver.cpp:266] Iteration 8700 (115.841 iter/s, 0.863251s/100 iter), loss = 0.167646
I0122 16:40:07.873549 56358 solver.cpp:285]     Train net output #0: loss = 0.167646 (* 1 = 0.167646 loss)
I0122 16:40:07.873555 56358 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:40:08.759862 56358 solver.cpp:266] Iteration 8800 (112.832 iter/s, 0.886274s/100 iter), loss = 0.24476
I0122 16:40:08.759889 56358 solver.cpp:285]     Train net output #0: loss = 0.24476 (* 1 = 0.24476 loss)
I0122 16:40:08.759896 56358 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:40:09.625655 56358 solver.cpp:266] Iteration 8900 (115.51 iter/s, 0.865727s/100 iter), loss = 0.134373
I0122 16:40:09.625694 56358 solver.cpp:285]     Train net output #0: loss = 0.134373 (* 1 = 0.134373 loss)
I0122 16:40:09.625701 56358 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:40:10.481906 56358 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:40:10.704524 56358 solver.cpp:517]     Test net output #0: loss = 0.506887 (* 1 = 0.506887 loss)
I0122 16:40:10.704538 56358 solver.cpp:517]     Test net output #1: top-1 = 0.840444
I0122 16:40:10.704543 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:40:10.712636 56358 solver.cpp:266] Iteration 9000 (92.0048 iter/s, 1.0869s/100 iter), loss = 0.198678
I0122 16:40:10.712652 56358 solver.cpp:285]     Train net output #0: loss = 0.198678 (* 1 = 0.198678 loss)
I0122 16:40:10.712658 56358 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:40:11.575709 56358 solver.cpp:266] Iteration 9100 (115.872 iter/s, 0.863018s/100 iter), loss = 0.115772
I0122 16:40:11.575734 56358 solver.cpp:285]     Train net output #0: loss = 0.115772 (* 1 = 0.115772 loss)
I0122 16:40:11.575768 56358 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:40:12.450681 56358 solver.cpp:266] Iteration 9200 (114.298 iter/s, 0.874908s/100 iter), loss = 0.21927
I0122 16:40:12.450711 56358 solver.cpp:285]     Train net output #0: loss = 0.21927 (* 1 = 0.21927 loss)
I0122 16:40:12.450716 56358 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:40:13.316885 56358 solver.cpp:266] Iteration 9300 (115.455 iter/s, 0.866137s/100 iter), loss = 0.162757
I0122 16:40:13.316911 56358 solver.cpp:285]     Train net output #0: loss = 0.162757 (* 1 = 0.162757 loss)
I0122 16:40:13.316917 56358 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:40:14.184279 56358 solver.cpp:266] Iteration 9400 (115.296 iter/s, 0.867329s/100 iter), loss = 0.204865
I0122 16:40:14.184305 56358 solver.cpp:285]     Train net output #0: loss = 0.204865 (* 1 = 0.204865 loss)
I0122 16:40:14.184310 56358 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:40:15.046594 56358 solver.cpp:266] Iteration 9500 (115.975 iter/s, 0.862251s/100 iter), loss = 0.148123
I0122 16:40:15.046622 56358 solver.cpp:285]     Train net output #0: loss = 0.148123 (* 1 = 0.148123 loss)
I0122 16:40:15.046627 56358 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:40:15.917695 56358 solver.cpp:266] Iteration 9600 (114.806 iter/s, 0.871033s/100 iter), loss = 0.215157
I0122 16:40:15.917721 56358 solver.cpp:285]     Train net output #0: loss = 0.215157 (* 1 = 0.215157 loss)
I0122 16:40:15.917726 56358 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:40:16.781949 56358 solver.cpp:266] Iteration 9700 (115.715 iter/s, 0.86419s/100 iter), loss = 0.154885
I0122 16:40:16.781975 56358 solver.cpp:285]     Train net output #0: loss = 0.154885 (* 1 = 0.154885 loss)
I0122 16:40:16.781980 56358 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:40:17.644497 56358 solver.cpp:266] Iteration 9800 (115.944 iter/s, 0.862486s/100 iter), loss = 0.326336
I0122 16:40:17.644536 56358 solver.cpp:285]     Train net output #0: loss = 0.326336 (* 1 = 0.326336 loss)
I0122 16:40:17.644542 56358 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:40:18.511914 56358 solver.cpp:266] Iteration 9900 (115.295 iter/s, 0.86734s/100 iter), loss = 0.25909
I0122 16:40:18.511940 56358 solver.cpp:285]     Train net output #0: loss = 0.25909 (* 1 = 0.25909 loss)
I0122 16:40:18.511945 56358 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:40:19.373044 56358 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:40:19.593742 56358 solver.cpp:517]     Test net output #0: loss = 0.497428 (* 1 = 0.497428 loss)
I0122 16:40:19.593757 56358 solver.cpp:517]     Test net output #1: top-1 = 0.846333
I0122 16:40:19.593761 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:40:19.602634 56358 solver.cpp:266] Iteration 10000 (91.6883 iter/s, 1.09065s/100 iter), loss = 0.25102
I0122 16:40:19.602653 56358 solver.cpp:285]     Train net output #0: loss = 0.25102 (* 1 = 0.25102 loss)
I0122 16:40:19.602658 56358 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:40:20.466027 56358 solver.cpp:266] Iteration 10100 (115.83 iter/s, 0.863335s/100 iter), loss = 0.144585
I0122 16:40:20.466166 56358 solver.cpp:285]     Train net output #0: loss = 0.144585 (* 1 = 0.144585 loss)
I0122 16:40:20.466173 56358 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:40:21.334406 56358 solver.cpp:266] Iteration 10200 (115.18 iter/s, 0.868205s/100 iter), loss = 0.205947
I0122 16:40:21.334434 56358 solver.cpp:285]     Train net output #0: loss = 0.205947 (* 1 = 0.205947 loss)
I0122 16:40:21.334439 56358 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:40:22.242857 56358 solver.cpp:266] Iteration 10300 (110.086 iter/s, 0.908382s/100 iter), loss = 0.170186
I0122 16:40:22.242887 56358 solver.cpp:285]     Train net output #0: loss = 0.170186 (* 1 = 0.170186 loss)
I0122 16:40:22.242892 56358 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:40:23.127945 56358 solver.cpp:266] Iteration 10400 (112.992 iter/s, 0.88502s/100 iter), loss = 0.191921
I0122 16:40:23.127974 56358 solver.cpp:285]     Train net output #0: loss = 0.191921 (* 1 = 0.191921 loss)
I0122 16:40:23.127979 56358 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:40:23.999495 56358 solver.cpp:266] Iteration 10500 (114.747 iter/s, 0.871482s/100 iter), loss = 0.122154
I0122 16:40:23.999521 56358 solver.cpp:285]     Train net output #0: loss = 0.122154 (* 1 = 0.122154 loss)
I0122 16:40:23.999527 56358 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:40:24.894804 56358 solver.cpp:266] Iteration 10600 (111.701 iter/s, 0.895243s/100 iter), loss = 0.158028
I0122 16:40:24.894831 56358 solver.cpp:285]     Train net output #0: loss = 0.158028 (* 1 = 0.158028 loss)
I0122 16:40:24.894837 56358 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:40:25.759634 56358 solver.cpp:266] Iteration 10700 (115.639 iter/s, 0.864763s/100 iter), loss = 0.203774
I0122 16:40:25.759660 56358 solver.cpp:285]     Train net output #0: loss = 0.203774 (* 1 = 0.203774 loss)
I0122 16:40:25.759666 56358 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:40:26.634130 56358 solver.cpp:266] Iteration 10800 (114.36 iter/s, 0.874433s/100 iter), loss = 0.163711
I0122 16:40:26.634157 56358 solver.cpp:285]     Train net output #0: loss = 0.163711 (* 1 = 0.163711 loss)
I0122 16:40:26.634162 56358 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:40:27.516993 56358 solver.cpp:266] Iteration 10900 (113.276 iter/s, 0.882797s/100 iter), loss = 0.195547
I0122 16:40:27.517020 56358 solver.cpp:285]     Train net output #0: loss = 0.195547 (* 1 = 0.195547 loss)
I0122 16:40:27.517026 56358 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:40:28.372331 56358 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:40:28.592420 56358 solver.cpp:517]     Test net output #0: loss = 0.608123 (* 1 = 0.608123 loss)
I0122 16:40:28.592435 56358 solver.cpp:517]     Test net output #1: top-1 = 0.816
I0122 16:40:28.592439 56358 solver.cpp:517]     Test net output #2: top-5 = 0.986667
I0122 16:40:28.600499 56358 solver.cpp:266] Iteration 11000 (92.2989 iter/s, 1.08344s/100 iter), loss = 0.214699
I0122 16:40:28.600517 56358 solver.cpp:285]     Train net output #0: loss = 0.214699 (* 1 = 0.214699 loss)
I0122 16:40:28.600522 56358 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:40:29.486333 56358 solver.cpp:266] Iteration 11100 (112.895 iter/s, 0.885778s/100 iter), loss = 0.113131
I0122 16:40:29.486359 56358 solver.cpp:285]     Train net output #0: loss = 0.113131 (* 1 = 0.113131 loss)
I0122 16:40:29.486366 56358 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:40:30.364805 56358 solver.cpp:266] Iteration 11200 (113.842 iter/s, 0.878408s/100 iter), loss = 0.155673
I0122 16:40:30.364832 56358 solver.cpp:285]     Train net output #0: loss = 0.155673 (* 1 = 0.155673 loss)
I0122 16:40:30.364837 56358 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:40:31.240711 56358 solver.cpp:266] Iteration 11300 (114.176 iter/s, 0.875839s/100 iter), loss = 0.242291
I0122 16:40:31.240737 56358 solver.cpp:285]     Train net output #0: loss = 0.242291 (* 1 = 0.242291 loss)
I0122 16:40:31.240743 56358 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:40:32.140324 56358 solver.cpp:266] Iteration 11400 (111.167 iter/s, 0.899548s/100 iter), loss = 0.289464
I0122 16:40:32.140352 56358 solver.cpp:285]     Train net output #0: loss = 0.289464 (* 1 = 0.289464 loss)
I0122 16:40:32.140357 56358 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:40:33.010416 56358 solver.cpp:266] Iteration 11500 (114.939 iter/s, 0.870024s/100 iter), loss = 0.17182
I0122 16:40:33.010442 56358 solver.cpp:285]     Train net output #0: loss = 0.17182 (* 1 = 0.17182 loss)
I0122 16:40:33.010448 56358 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:40:33.879307 56358 solver.cpp:266] Iteration 11600 (115.098 iter/s, 0.868824s/100 iter), loss = 0.214879
I0122 16:40:33.879333 56358 solver.cpp:285]     Train net output #0: loss = 0.214879 (* 1 = 0.214879 loss)
I0122 16:40:33.879338 56358 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:40:34.741264 56358 solver.cpp:266] Iteration 11700 (116.024 iter/s, 0.861893s/100 iter), loss = 0.216148
I0122 16:40:34.741291 56358 solver.cpp:285]     Train net output #0: loss = 0.216148 (* 1 = 0.216148 loss)
I0122 16:40:34.741297 56358 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:40:35.604091 56358 solver.cpp:266] Iteration 11800 (115.907 iter/s, 0.86276s/100 iter), loss = 0.149312
I0122 16:40:35.604118 56358 solver.cpp:285]     Train net output #0: loss = 0.149312 (* 1 = 0.149312 loss)
I0122 16:40:35.604123 56358 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:40:36.466786 56358 solver.cpp:266] Iteration 11900 (115.924 iter/s, 0.862631s/100 iter), loss = 0.15686
I0122 16:40:36.466825 56358 solver.cpp:285]     Train net output #0: loss = 0.15686 (* 1 = 0.15686 loss)
I0122 16:40:36.466830 56358 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:40:37.320751 56358 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:40:37.541707 56358 solver.cpp:517]     Test net output #0: loss = 0.499224 (* 1 = 0.499224 loss)
I0122 16:40:37.541720 56358 solver.cpp:517]     Test net output #1: top-1 = 0.848667
I0122 16:40:37.541724 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990333
I0122 16:40:37.549784 56358 solver.cpp:266] Iteration 12000 (92.343 iter/s, 1.08292s/100 iter), loss = 0.207918
I0122 16:40:37.549801 56358 solver.cpp:285]     Train net output #0: loss = 0.207918 (* 1 = 0.207918 loss)
I0122 16:40:37.549806 56358 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:40:38.413285 56358 solver.cpp:266] Iteration 12100 (115.815 iter/s, 0.863446s/100 iter), loss = 0.153007
I0122 16:40:38.413311 56358 solver.cpp:285]     Train net output #0: loss = 0.153007 (* 1 = 0.153007 loss)
I0122 16:40:38.413317 56358 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:40:39.275025 56358 solver.cpp:266] Iteration 12200 (116.053 iter/s, 0.861676s/100 iter), loss = 0.40102
I0122 16:40:39.275049 56358 solver.cpp:285]     Train net output #0: loss = 0.40102 (* 1 = 0.40102 loss)
I0122 16:40:39.275054 56358 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:40:40.138288 56358 solver.cpp:266] Iteration 12300 (115.848 iter/s, 0.8632s/100 iter), loss = 0.187594
I0122 16:40:40.138314 56358 solver.cpp:285]     Train net output #0: loss = 0.187594 (* 1 = 0.187594 loss)
I0122 16:40:40.138319 56358 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:40:41.006512 56358 solver.cpp:266] Iteration 12400 (115.186 iter/s, 0.86816s/100 iter), loss = 0.180739
I0122 16:40:41.006539 56358 solver.cpp:285]     Train net output #0: loss = 0.180739 (* 1 = 0.180739 loss)
I0122 16:40:41.006546 56358 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:40:41.868959 56358 solver.cpp:266] Iteration 12500 (115.958 iter/s, 0.862382s/100 iter), loss = 0.116752
I0122 16:40:41.868988 56358 solver.cpp:285]     Train net output #0: loss = 0.116752 (* 1 = 0.116752 loss)
I0122 16:40:41.868993 56358 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:40:42.732007 56358 solver.cpp:266] Iteration 12600 (115.878 iter/s, 0.86298s/100 iter), loss = 0.175929
I0122 16:40:42.732034 56358 solver.cpp:285]     Train net output #0: loss = 0.175929 (* 1 = 0.175929 loss)
I0122 16:40:42.732058 56358 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:40:43.594584 56358 solver.cpp:266] Iteration 12700 (115.94 iter/s, 0.862511s/100 iter), loss = 0.197073
I0122 16:40:43.594611 56358 solver.cpp:285]     Train net output #0: loss = 0.197073 (* 1 = 0.197073 loss)
I0122 16:40:43.594616 56358 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:40:44.461181 56358 solver.cpp:266] Iteration 12800 (115.403 iter/s, 0.866531s/100 iter), loss = 0.206288
I0122 16:40:44.461210 56358 solver.cpp:285]     Train net output #0: loss = 0.206288 (* 1 = 0.206288 loss)
I0122 16:40:44.461215 56358 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:40:45.329157 56358 solver.cpp:266] Iteration 12900 (115.219 iter/s, 0.867909s/100 iter), loss = 0.138023
I0122 16:40:45.329183 56358 solver.cpp:285]     Train net output #0: loss = 0.138023 (* 1 = 0.138023 loss)
I0122 16:40:45.329190 56358 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:40:46.202891 56358 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:40:46.428483 56358 solver.cpp:517]     Test net output #0: loss = 0.47976 (* 1 = 0.47976 loss)
I0122 16:40:46.428498 56358 solver.cpp:517]     Test net output #1: top-1 = 0.850778
I0122 16:40:46.428503 56358 solver.cpp:517]     Test net output #2: top-5 = 0.992667
I0122 16:40:46.436545 56358 solver.cpp:266] Iteration 13000 (90.3082 iter/s, 1.10732s/100 iter), loss = 0.262727
I0122 16:40:46.436563 56358 solver.cpp:285]     Train net output #0: loss = 0.262727 (* 1 = 0.262727 loss)
I0122 16:40:46.436569 56358 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:40:47.316171 56358 solver.cpp:266] Iteration 13100 (113.692 iter/s, 0.879569s/100 iter), loss = 0.163221
I0122 16:40:47.316198 56358 solver.cpp:285]     Train net output #0: loss = 0.163221 (* 1 = 0.163221 loss)
I0122 16:40:47.316205 56358 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:40:48.177773 56358 solver.cpp:266] Iteration 13200 (116.072 iter/s, 0.861537s/100 iter), loss = 0.329066
I0122 16:40:48.177801 56358 solver.cpp:285]     Train net output #0: loss = 0.329066 (* 1 = 0.329066 loss)
I0122 16:40:48.177806 56358 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:40:49.068063 56358 solver.cpp:266] Iteration 13300 (112.332 iter/s, 0.890222s/100 iter), loss = 0.178849
I0122 16:40:49.068089 56358 solver.cpp:285]     Train net output #0: loss = 0.178849 (* 1 = 0.178849 loss)
I0122 16:40:49.068095 56358 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:40:49.952428 56358 solver.cpp:266] Iteration 13400 (113.084 iter/s, 0.8843s/100 iter), loss = 0.145544
I0122 16:40:49.952455 56358 solver.cpp:285]     Train net output #0: loss = 0.145544 (* 1 = 0.145544 loss)
I0122 16:40:49.952461 56358 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:40:50.819974 56358 solver.cpp:266] Iteration 13500 (115.276 iter/s, 0.867481s/100 iter), loss = 0.125595
I0122 16:40:50.820082 56358 solver.cpp:285]     Train net output #0: loss = 0.125595 (* 1 = 0.125595 loss)
I0122 16:40:50.820088 56358 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:40:51.694248 56358 solver.cpp:266] Iteration 13600 (114.4 iter/s, 0.874129s/100 iter), loss = 0.176674
I0122 16:40:51.694275 56358 solver.cpp:285]     Train net output #0: loss = 0.176674 (* 1 = 0.176674 loss)
I0122 16:40:51.694280 56358 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:40:52.610179 56358 solver.cpp:266] Iteration 13700 (109.187 iter/s, 0.915863s/100 iter), loss = 0.150129
I0122 16:40:52.610218 56358 solver.cpp:285]     Train net output #0: loss = 0.150129 (* 1 = 0.150129 loss)
I0122 16:40:52.610224 56358 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:40:53.476239 56358 solver.cpp:266] Iteration 13800 (115.476 iter/s, 0.865982s/100 iter), loss = 0.174633
I0122 16:40:53.476267 56358 solver.cpp:285]     Train net output #0: loss = 0.174633 (* 1 = 0.174633 loss)
I0122 16:40:53.476274 56358 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:40:54.337461 56358 solver.cpp:266] Iteration 13900 (116.123 iter/s, 0.861156s/100 iter), loss = 0.144366
I0122 16:40:54.337487 56358 solver.cpp:285]     Train net output #0: loss = 0.144366 (* 1 = 0.144366 loss)
I0122 16:40:54.337492 56358 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:40:55.192530 56358 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:40:55.415419 56358 solver.cpp:517]     Test net output #0: loss = 0.478237 (* 1 = 0.478237 loss)
I0122 16:40:55.415433 56358 solver.cpp:517]     Test net output #1: top-1 = 0.851444
I0122 16:40:55.415437 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:40:55.423561 56358 solver.cpp:266] Iteration 14000 (92.0784 iter/s, 1.08603s/100 iter), loss = 0.10762
I0122 16:40:55.423578 56358 solver.cpp:285]     Train net output #0: loss = 0.10762 (* 1 = 0.10762 loss)
I0122 16:40:55.423584 56358 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:40:56.311094 56358 solver.cpp:266] Iteration 14100 (112.679 iter/s, 0.887474s/100 iter), loss = 0.124198
I0122 16:40:56.311120 56358 solver.cpp:285]     Train net output #0: loss = 0.124198 (* 1 = 0.124198 loss)
I0122 16:40:56.311126 56358 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:40:57.186972 56358 solver.cpp:266] Iteration 14200 (114.18 iter/s, 0.875813s/100 iter), loss = 0.12877
I0122 16:40:57.187000 56358 solver.cpp:285]     Train net output #0: loss = 0.12877 (* 1 = 0.12877 loss)
I0122 16:40:57.187005 56358 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:40:58.048964 56358 solver.cpp:266] Iteration 14300 (116.019 iter/s, 0.861925s/100 iter), loss = 0.178183
I0122 16:40:58.048991 56358 solver.cpp:285]     Train net output #0: loss = 0.178183 (* 1 = 0.178183 loss)
I0122 16:40:58.048997 56358 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:40:58.918201 56358 solver.cpp:266] Iteration 14400 (115.052 iter/s, 0.869171s/100 iter), loss = 0.122136
I0122 16:40:58.918229 56358 solver.cpp:285]     Train net output #0: loss = 0.122136 (* 1 = 0.122136 loss)
I0122 16:40:58.918236 56358 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:40:59.797286 56358 solver.cpp:266] Iteration 14500 (113.763 iter/s, 0.879017s/100 iter), loss = 0.0918108
I0122 16:40:59.797313 56358 solver.cpp:285]     Train net output #0: loss = 0.0918108 (* 1 = 0.0918108 loss)
I0122 16:40:59.797319 56358 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:41:00.663060 56358 solver.cpp:266] Iteration 14600 (115.512 iter/s, 0.865708s/100 iter), loss = 0.103151
I0122 16:41:00.663087 56358 solver.cpp:285]     Train net output #0: loss = 0.103151 (* 1 = 0.103151 loss)
I0122 16:41:00.663094 56358 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:41:01.528515 56358 solver.cpp:266] Iteration 14700 (115.555 iter/s, 0.865389s/100 iter), loss = 0.0582767
I0122 16:41:01.528543 56358 solver.cpp:285]     Train net output #0: loss = 0.0582767 (* 1 = 0.0582767 loss)
I0122 16:41:01.528549 56358 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:41:02.393410 56358 solver.cpp:266] Iteration 14800 (115.63 iter/s, 0.864829s/100 iter), loss = 0.142753
I0122 16:41:02.393436 56358 solver.cpp:285]     Train net output #0: loss = 0.142753 (* 1 = 0.142753 loss)
I0122 16:41:02.393442 56358 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:41:03.255591 56358 solver.cpp:266] Iteration 14900 (115.994 iter/s, 0.862116s/100 iter), loss = 0.134496
I0122 16:41:03.255620 56358 solver.cpp:285]     Train net output #0: loss = 0.134496 (* 1 = 0.134496 loss)
I0122 16:41:03.255625 56358 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:41:04.161459 56358 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:41:04.381582 56358 solver.cpp:517]     Test net output #0: loss = 0.490479 (* 1 = 0.490479 loss)
I0122 16:41:04.381603 56358 solver.cpp:517]     Test net output #1: top-1 = 0.851333
I0122 16:41:04.381608 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0122 16:41:04.389672 56358 solver.cpp:266] Iteration 15000 (88.1828 iter/s, 1.13401s/100 iter), loss = 0.20817
I0122 16:41:04.389688 56358 solver.cpp:285]     Train net output #0: loss = 0.20817 (* 1 = 0.20817 loss)
I0122 16:41:04.389694 56358 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:41:05.265817 56358 solver.cpp:266] Iteration 15100 (114.144 iter/s, 0.876089s/100 iter), loss = 0.0869578
I0122 16:41:05.265844 56358 solver.cpp:285]     Train net output #0: loss = 0.0869578 (* 1 = 0.0869578 loss)
I0122 16:41:05.265851 56358 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:41:06.135257 56358 solver.cpp:266] Iteration 15200 (115.025 iter/s, 0.869373s/100 iter), loss = 0.218636
I0122 16:41:06.135285 56358 solver.cpp:285]     Train net output #0: loss = 0.218636 (* 1 = 0.218636 loss)
I0122 16:41:06.135290 56358 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:41:07.010794 56358 solver.cpp:266] Iteration 15300 (114.224 iter/s, 0.875471s/100 iter), loss = 0.193568
I0122 16:41:07.010821 56358 solver.cpp:285]     Train net output #0: loss = 0.193568 (* 1 = 0.193568 loss)
I0122 16:41:07.010828 56358 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:41:07.883733 56358 solver.cpp:266] Iteration 15400 (114.564 iter/s, 0.872871s/100 iter), loss = 0.179632
I0122 16:41:07.883760 56358 solver.cpp:285]     Train net output #0: loss = 0.179632 (* 1 = 0.179632 loss)
I0122 16:41:07.883766 56358 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:41:08.755970 56358 solver.cpp:266] Iteration 15500 (114.657 iter/s, 0.87217s/100 iter), loss = 0.165628
I0122 16:41:08.755997 56358 solver.cpp:285]     Train net output #0: loss = 0.165628 (* 1 = 0.165628 loss)
I0122 16:41:08.756003 56358 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:41:09.620128 56358 solver.cpp:266] Iteration 15600 (115.728 iter/s, 0.864092s/100 iter), loss = 0.161383
I0122 16:41:09.620157 56358 solver.cpp:285]     Train net output #0: loss = 0.161383 (* 1 = 0.161383 loss)
I0122 16:41:09.620162 56358 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:41:10.484211 56358 solver.cpp:266] Iteration 15700 (115.739 iter/s, 0.864016s/100 iter), loss = 0.10945
I0122 16:41:10.484239 56358 solver.cpp:285]     Train net output #0: loss = 0.10945 (* 1 = 0.10945 loss)
I0122 16:41:10.484246 56358 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:41:11.346891 56358 solver.cpp:266] Iteration 15800 (115.927 iter/s, 0.862613s/100 iter), loss = 0.137631
I0122 16:41:11.346918 56358 solver.cpp:285]     Train net output #0: loss = 0.137631 (* 1 = 0.137631 loss)
I0122 16:41:11.346925 56358 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:41:12.208968 56358 solver.cpp:266] Iteration 15900 (116.008 iter/s, 0.862011s/100 iter), loss = 0.232173
I0122 16:41:12.208998 56358 solver.cpp:285]     Train net output #0: loss = 0.232173 (* 1 = 0.232173 loss)
I0122 16:41:12.209003 56358 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:41:13.063845 56358 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:41:13.284840 56358 solver.cpp:517]     Test net output #0: loss = 0.470084 (* 1 = 0.470084 loss)
I0122 16:41:13.284873 56358 solver.cpp:517]     Test net output #1: top-1 = 0.856778
I0122 16:41:13.284878 56358 solver.cpp:517]     Test net output #2: top-5 = 0.990889
I0122 16:41:13.293084 56358 solver.cpp:266] Iteration 16000 (92.2472 iter/s, 1.08404s/100 iter), loss = 0.169693
I0122 16:41:13.293102 56358 solver.cpp:285]     Train net output #0: loss = 0.169693 (* 1 = 0.169693 loss)
I0122 16:41:13.293107 56358 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:41:14.156107 56358 solver.cpp:266] Iteration 16100 (115.879 iter/s, 0.862967s/100 iter), loss = 0.199058
I0122 16:41:14.156144 56358 solver.cpp:285]     Train net output #0: loss = 0.199058 (* 1 = 0.199058 loss)
I0122 16:41:14.156152 56358 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:41:15.019872 56358 solver.cpp:266] Iteration 16200 (115.782 iter/s, 0.86369s/100 iter), loss = 0.147623
I0122 16:41:15.019897 56358 solver.cpp:285]     Train net output #0: loss = 0.147623 (* 1 = 0.147623 loss)
I0122 16:41:15.019903 56358 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:41:15.885440 56358 solver.cpp:266] Iteration 16300 (115.54 iter/s, 0.865504s/100 iter), loss = 0.163395
I0122 16:41:15.885468 56358 solver.cpp:285]     Train net output #0: loss = 0.163395 (* 1 = 0.163395 loss)
I0122 16:41:15.885473 56358 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:41:16.748720 56358 solver.cpp:266] Iteration 16400 (115.846 iter/s, 0.863215s/100 iter), loss = 0.134782
I0122 16:41:16.748747 56358 solver.cpp:285]     Train net output #0: loss = 0.134782 (* 1 = 0.134782 loss)
I0122 16:41:16.748752 56358 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:41:17.614356 56358 solver.cpp:266] Iteration 16500 (115.531 iter/s, 0.86557s/100 iter), loss = 0.161537
I0122 16:41:17.614387 56358 solver.cpp:285]     Train net output #0: loss = 0.161537 (* 1 = 0.161537 loss)
I0122 16:41:17.614392 56358 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:41:18.479460 56358 solver.cpp:266] Iteration 16600 (115.602 iter/s, 0.865035s/100 iter), loss = 0.200338
I0122 16:41:18.479486 56358 solver.cpp:285]     Train net output #0: loss = 0.200338 (* 1 = 0.200338 loss)
I0122 16:41:18.479492 56358 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:41:19.342610 56358 solver.cpp:266] Iteration 16700 (115.863 iter/s, 0.863086s/100 iter), loss = 0.117915
I0122 16:41:19.342638 56358 solver.cpp:285]     Train net output #0: loss = 0.117915 (* 1 = 0.117915 loss)
I0122 16:41:19.342643 56358 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:41:20.280894 56358 solver.cpp:266] Iteration 16800 (106.585 iter/s, 0.938214s/100 iter), loss = 0.157756
I0122 16:41:20.280920 56358 solver.cpp:285]     Train net output #0: loss = 0.157756 (* 1 = 0.157756 loss)
I0122 16:41:20.280925 56358 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:41:21.154065 56358 solver.cpp:266] Iteration 16900 (114.534 iter/s, 0.873106s/100 iter), loss = 0.215282
I0122 16:41:21.154179 56358 solver.cpp:285]     Train net output #0: loss = 0.215282 (* 1 = 0.215282 loss)
I0122 16:41:21.154187 56358 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:41:22.009171 56358 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:41:22.239665 56358 solver.cpp:517]     Test net output #0: loss = 0.468133 (* 1 = 0.468133 loss)
I0122 16:41:22.239682 56358 solver.cpp:517]     Test net output #1: top-1 = 0.855667
I0122 16:41:22.239686 56358 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:41:22.248088 56358 solver.cpp:266] Iteration 17000 (91.4187 iter/s, 1.09387s/100 iter), loss = 0.189364
I0122 16:41:22.248106 56358 solver.cpp:285]     Train net output #0: loss = 0.189364 (* 1 = 0.189364 loss)
I0122 16:41:22.248113 56358 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:41:23.112552 56358 solver.cpp:266] Iteration 17100 (115.686 iter/s, 0.864406s/100 iter), loss = 0.161264
I0122 16:41:23.112579 56358 solver.cpp:285]     Train net output #0: loss = 0.161264 (* 1 = 0.161264 loss)
I0122 16:41:23.112584 56358 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:41:23.999634 56358 solver.cpp:266] Iteration 17200 (112.738 iter/s, 0.887016s/100 iter), loss = 0.206895
I0122 16:41:23.999662 56358 solver.cpp:285]     Train net output #0: loss = 0.206895 (* 1 = 0.206895 loss)
I0122 16:41:23.999668 56358 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:41:24.862287 56358 solver.cpp:266] Iteration 17300 (115.931 iter/s, 0.862586s/100 iter), loss = 0.261465
I0122 16:41:24.862314 56358 solver.cpp:285]     Train net output #0: loss = 0.261465 (* 1 = 0.261465 loss)
I0122 16:41:24.862320 56358 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:41:25.747434 56358 solver.cpp:266] Iteration 17400 (112.984 iter/s, 0.88508s/100 iter), loss = 0.159458
I0122 16:41:25.747463 56358 solver.cpp:285]     Train net output #0: loss = 0.159458 (* 1 = 0.159458 loss)
I0122 16:41:25.747468 56358 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:41:26.613247 56358 solver.cpp:266] Iteration 17500 (115.507 iter/s, 0.865746s/100 iter), loss = 0.148827
I0122 16:41:26.613274 56358 solver.cpp:285]     Train net output #0: loss = 0.148827 (* 1 = 0.148827 loss)
I0122 16:41:26.613281 56358 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:41:27.475894 56358 solver.cpp:266] Iteration 17600 (115.931 iter/s, 0.862581s/100 iter), loss = 0.192074
I0122 16:41:27.475922 56358 solver.cpp:285]     Train net output #0: loss = 0.192074 (* 1 = 0.192074 loss)
I0122 16:41:27.475929 56358 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:41:28.338361 56358 solver.cpp:266] Iteration 17700 (115.955 iter/s, 0.8624s/100 iter), loss = 0.200234
I0122 16:41:28.338388 56358 solver.cpp:285]     Train net output #0: loss = 0.200234 (* 1 = 0.200234 loss)
I0122 16:41:28.338394 56358 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:41:29.202088 56358 solver.cpp:266] Iteration 17800 (115.786 iter/s, 0.863662s/100 iter), loss = 0.193523
I0122 16:41:29.202114 56358 solver.cpp:285]     Train net output #0: loss = 0.193523 (* 1 = 0.193523 loss)
I0122 16:41:29.202121 56358 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:41:30.065737 56358 solver.cpp:266] Iteration 17900 (115.797 iter/s, 0.863583s/100 iter), loss = 0.223768
I0122 16:41:30.065763 56358 solver.cpp:285]     Train net output #0: loss = 0.223768 (* 1 = 0.223768 loss)
I0122 16:41:30.065769 56358 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:41:30.919059 56358 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:41:31.140036 56358 solver.cpp:517]     Test net output #0: loss = 0.445225 (* 1 = 0.445225 loss)
I0122 16:41:31.140051 56358 solver.cpp:517]     Test net output #1: top-1 = 0.860111
I0122 16:41:31.140055 56358 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:41:31.148167 56358 solver.cpp:266] Iteration 18000 (92.3906 iter/s, 1.08236s/100 iter), loss = 0.150569
I0122 16:41:31.148183 56358 solver.cpp:285]     Train net output #0: loss = 0.150569 (* 1 = 0.150569 loss)
I0122 16:41:31.148188 56358 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:41:32.010272 56358 solver.cpp:266] Iteration 18100 (116.003 iter/s, 0.862049s/100 iter), loss = 0.161548
I0122 16:41:32.010298 56358 solver.cpp:285]     Train net output #0: loss = 0.161548 (* 1 = 0.161548 loss)
I0122 16:41:32.010304 56358 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:41:32.872977 56358 solver.cpp:266] Iteration 18200 (115.923 iter/s, 0.862642s/100 iter), loss = 0.178229
I0122 16:41:32.873004 56358 solver.cpp:285]     Train net output #0: loss = 0.178229 (* 1 = 0.178229 loss)
I0122 16:41:32.873009 56358 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:41:33.734499 56358 solver.cpp:266] Iteration 18300 (116.082 iter/s, 0.861457s/100 iter), loss = 0.179794
I0122 16:41:33.734524 56358 solver.cpp:285]     Train net output #0: loss = 0.179794 (* 1 = 0.179794 loss)
I0122 16:41:33.734529 56358 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:41:34.597285 56358 solver.cpp:266] Iteration 18400 (115.912 iter/s, 0.862722s/100 iter), loss = 0.168931
I0122 16:41:34.597311 56358 solver.cpp:285]     Train net output #0: loss = 0.168931 (* 1 = 0.168931 loss)
I0122 16:41:34.597316 56358 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:41:35.461895 56358 solver.cpp:266] Iteration 18500 (115.668 iter/s, 0.864546s/100 iter), loss = 0.241565
I0122 16:41:35.461923 56358 solver.cpp:285]     Train net output #0: loss = 0.241565 (* 1 = 0.241565 loss)
I0122 16:41:35.461930 56358 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:41:36.324481 56358 solver.cpp:266] Iteration 18600 (115.94 iter/s, 0.862518s/100 iter), loss = 0.160278
I0122 16:41:36.324506 56358 solver.cpp:285]     Train net output #0: loss = 0.160278 (* 1 = 0.160278 loss)
I0122 16:41:36.324512 56358 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:41:37.189597 56358 solver.cpp:266] Iteration 18700 (115.6 iter/s, 0.865052s/100 iter), loss = 0.158366
I0122 16:41:37.189620 56358 solver.cpp:285]     Train net output #0: loss = 0.158366 (* 1 = 0.158366 loss)
I0122 16:41:37.189625 56358 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:41:38.051679 56358 solver.cpp:266] Iteration 18800 (116.007 iter/s, 0.86202s/100 iter), loss = 0.103973
I0122 16:41:38.051707 56358 solver.cpp:285]     Train net output #0: loss = 0.103973 (* 1 = 0.103973 loss)
I0122 16:41:38.051712 56358 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:41:38.914188 56358 solver.cpp:266] Iteration 18900 (115.95 iter/s, 0.862442s/100 iter), loss = 0.140709
I0122 16:41:38.914216 56358 solver.cpp:285]     Train net output #0: loss = 0.140709 (* 1 = 0.140709 loss)
I0122 16:41:38.914222 56358 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:41:39.769454 56358 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:41:39.989310 56358 solver.cpp:517]     Test net output #0: loss = 0.447801 (* 1 = 0.447801 loss)
I0122 16:41:39.989323 56358 solver.cpp:517]     Test net output #1: top-1 = 0.860778
I0122 16:41:39.989328 56358 solver.cpp:517]     Test net output #2: top-5 = 0.991667
I0122 16:41:39.997455 56358 solver.cpp:266] Iteration 19000 (92.3194 iter/s, 1.0832s/100 iter), loss = 0.310725
I0122 16:41:39.997473 56358 solver.cpp:285]     Train net output #0: loss = 0.310725 (* 1 = 0.310725 loss)
I0122 16:41:39.997478 56358 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:41:40.859714 56358 solver.cpp:266] Iteration 19100 (115.982 iter/s, 0.862202s/100 iter), loss = 0.131359
I0122 16:41:40.859738 56358 solver.cpp:285]     Train net output #0: loss = 0.131359 (* 1 = 0.131359 loss)
I0122 16:41:40.859743 56358 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:41:41.722190 56358 solver.cpp:266] Iteration 19200 (115.954 iter/s, 0.862414s/100 iter), loss = 0.178168
I0122 16:41:41.722218 56358 solver.cpp:285]     Train net output #0: loss = 0.178168 (* 1 = 0.178168 loss)
I0122 16:41:41.722223 56358 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:41:42.586732 56358 solver.cpp:266] Iteration 19300 (115.677 iter/s, 0.864477s/100 iter), loss = 0.12426
I0122 16:41:42.586777 56358 solver.cpp:285]     Train net output #0: loss = 0.12426 (* 1 = 0.12426 loss)
I0122 16:41:42.586784 56358 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:41:43.450242 56358 solver.cpp:266] Iteration 19400 (115.818 iter/s, 0.863426s/100 iter), loss = 0.167845
I0122 16:41:43.450268 56358 solver.cpp:285]     Train net output #0: loss = 0.167845 (* 1 = 0.167845 loss)
I0122 16:41:43.450273 56358 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:41:44.315016 56358 solver.cpp:266] Iteration 19500 (115.646 iter/s, 0.86471s/100 iter), loss = 0.0847646
I0122 16:41:44.315042 56358 solver.cpp:285]     Train net output #0: loss = 0.0847646 (* 1 = 0.0847646 loss)
I0122 16:41:44.315047 56358 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:41:45.176795 56358 solver.cpp:266] Iteration 19600 (116.048 iter/s, 0.861714s/100 iter), loss = 0.241344
I0122 16:41:45.176821 56358 solver.cpp:285]     Train net output #0: loss = 0.241344 (* 1 = 0.241344 loss)
I0122 16:41:45.176826 56358 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:41:46.039110 56358 solver.cpp:266] Iteration 19700 (115.976 iter/s, 0.86225s/100 iter), loss = 0.147473
I0122 16:41:46.039139 56358 solver.cpp:285]     Train net output #0: loss = 0.147473 (* 1 = 0.147473 loss)
I0122 16:41:46.039144 56358 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:41:46.903443 56358 solver.cpp:266] Iteration 19800 (115.705 iter/s, 0.864267s/100 iter), loss = 0.127007
I0122 16:41:46.903470 56358 solver.cpp:285]     Train net output #0: loss = 0.127007 (* 1 = 0.127007 loss)
I0122 16:41:46.903475 56358 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:41:47.765820 56358 solver.cpp:266] Iteration 19900 (115.967 iter/s, 0.862311s/100 iter), loss = 0.136626
I0122 16:41:47.765846 56358 solver.cpp:285]     Train net output #0: loss = 0.136626 (* 1 = 0.136626 loss)
I0122 16:41:47.765851 56358 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:41:48.621839 56358 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/_iter_20000.caffemodel
I0122 16:41:48.695271 56358 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.5/snapshots/_iter_20000.solverstate
I0122 16:41:48.707917 56358 solver.cpp:378] Iteration 20000, loss = 0.0227214
I0122 16:41:48.707939 56358 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:41:48.928603 56358 solver.cpp:517]     Test net output #0: loss = 0.448445 (* 1 = 0.448445 loss)
I0122 16:41:48.928619 56358 solver.cpp:517]     Test net output #1: top-1 = 0.861222
I0122 16:41:48.928623 56358 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:41:48.928627 56358 solver.cpp:386] Optimization Done (112.809 iter/s).
I0122 16:41:48.928632 56358 caffe_interface.cpp:530] Optimization Done.

## compression: 6-th run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config6.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress6_miniVggNet.txt
I0122 16:41:49.475543 56630 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:41:49.525651 56630 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:41:50.923985 56630 pruning_runner.cpp:264] Compression complete 0.000247015%
I0122 16:41:51.565982 56630 pruning_runner.cpp:264] Compression complete 50.0001%
I0122 16:41:52.197171 56630 pruning_runner.cpp:264] Compression complete 66.6668%
I0122 16:41:52.833164 56630 pruning_runner.cpp:264] Compression complete 80.0001%
I0122 16:41:53.475316 56630 pruning_runner.cpp:264] Compression complete 88.8889%
I0122 16:41:54.110357 56630 pruning_runner.cpp:264] Compression complete 97.2222%
I0122 16:41:54.761303 56630 pruning_runner.cpp:264] Compression complete 98.6111%
I0122 16:41:55.397480 56630 pruning_runner.cpp:264] Compression complete 99.912%
I0122 16:41:56.021700 56630 pruning_runner.cpp:264] Compression complete 99.956%
I0122 16:41:56.661734 56630 pruning_runner.cpp:264] Compression complete 99.9945%
I0122 16:41:57.298319 56630 pruning_runner.cpp:264] Compression complete 99.9993%
I0122 16:41:57.931926 56630 pruning_runner.cpp:264] Compression complete 99.9999%
I0122 16:41:58.550812 56630 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:41:58.551208 56630 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:41:58.551719 56630 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:41:58.551995 56630 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:41:58.552160 56630 layer_factory.hpp:77] Creating layer data
I0122 16:41:58.552217 56630 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:41:58.552680 56630 net.cpp:94] Creating Layer data
I0122 16:41:58.552690 56630 net.cpp:409] data -> data
I0122 16:41:58.552700 56630 net.cpp:409] data -> label
I0122 16:41:58.553588 57843 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:41:58.553619 57843 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:41:58.553748 56630 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:41:58.553881 56630 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:41:58.557157 56630 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:41:58.557201 56630 net.cpp:144] Setting up data
I0122 16:41:58.557209 56630 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:41:58.557214 56630 net.cpp:151] Top shape: 50 (50)
I0122 16:41:58.557216 56630 net.cpp:159] Memory required for data: 614600
I0122 16:41:58.557222 56630 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:41:58.557231 56630 net.cpp:94] Creating Layer label_data_1_split
I0122 16:41:58.557235 56630 net.cpp:435] label_data_1_split <- label
I0122 16:41:58.557241 56630 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:41:58.557255 56630 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:41:58.557265 56630 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:41:58.557324 56630 net.cpp:144] Setting up label_data_1_split
I0122 16:41:58.557330 56630 net.cpp:151] Top shape: 50 (50)
I0122 16:41:58.557335 56630 net.cpp:151] Top shape: 50 (50)
I0122 16:41:58.557338 56630 net.cpp:151] Top shape: 50 (50)
I0122 16:41:58.557341 56630 net.cpp:159] Memory required for data: 615200
I0122 16:41:58.557344 56630 layer_factory.hpp:77] Creating layer conv1
I0122 16:41:58.557356 56630 net.cpp:94] Creating Layer conv1
I0122 16:41:58.557361 56630 net.cpp:435] conv1 <- data
I0122 16:41:58.557368 56630 net.cpp:409] conv1 -> conv1
I0122 16:41:58.558400 56630 net.cpp:144] Setting up conv1
I0122 16:41:58.558413 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.558418 56630 net.cpp:159] Memory required for data: 7168800
I0122 16:41:58.558429 56630 layer_factory.hpp:77] Creating layer bn1
I0122 16:41:58.558439 56630 net.cpp:94] Creating Layer bn1
I0122 16:41:58.558444 56630 net.cpp:435] bn1 <- conv1
I0122 16:41:58.558450 56630 net.cpp:409] bn1 -> scale1
I0122 16:41:58.559124 56630 net.cpp:144] Setting up bn1
I0122 16:41:58.559132 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.559149 56630 net.cpp:159] Memory required for data: 13722400
I0122 16:41:58.559161 56630 layer_factory.hpp:77] Creating layer relu1
I0122 16:41:58.559168 56630 net.cpp:94] Creating Layer relu1
I0122 16:41:58.559172 56630 net.cpp:435] relu1 <- scale1
I0122 16:41:58.559178 56630 net.cpp:409] relu1 -> relu1
I0122 16:41:58.559197 56630 net.cpp:144] Setting up relu1
I0122 16:41:58.559203 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.559206 56630 net.cpp:159] Memory required for data: 20276000
I0122 16:41:58.559211 56630 layer_factory.hpp:77] Creating layer conv2
I0122 16:41:58.559219 56630 net.cpp:94] Creating Layer conv2
I0122 16:41:58.559223 56630 net.cpp:435] conv2 <- relu1
I0122 16:41:58.559228 56630 net.cpp:409] conv2 -> conv2
I0122 16:41:58.560364 56630 net.cpp:144] Setting up conv2
I0122 16:41:58.560377 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.560380 56630 net.cpp:159] Memory required for data: 26829600
I0122 16:41:58.560389 56630 layer_factory.hpp:77] Creating layer bn2
I0122 16:41:58.560400 56630 net.cpp:94] Creating Layer bn2
I0122 16:41:58.560405 56630 net.cpp:435] bn2 <- conv2
I0122 16:41:58.560411 56630 net.cpp:409] bn2 -> scale2
I0122 16:41:58.561211 56630 net.cpp:144] Setting up bn2
I0122 16:41:58.561220 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.561224 56630 net.cpp:159] Memory required for data: 33383200
I0122 16:41:58.561233 56630 layer_factory.hpp:77] Creating layer relu2
I0122 16:41:58.561239 56630 net.cpp:94] Creating Layer relu2
I0122 16:41:58.561242 56630 net.cpp:435] relu2 <- scale2
I0122 16:41:58.561249 56630 net.cpp:409] relu2 -> relu2
I0122 16:41:58.561269 56630 net.cpp:144] Setting up relu2
I0122 16:41:58.561276 56630 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:41:58.561281 56630 net.cpp:159] Memory required for data: 39936800
I0122 16:41:58.561285 56630 layer_factory.hpp:77] Creating layer pool1
I0122 16:41:58.561291 56630 net.cpp:94] Creating Layer pool1
I0122 16:41:58.561295 56630 net.cpp:435] pool1 <- relu2
I0122 16:41:58.561300 56630 net.cpp:409] pool1 -> pool1
I0122 16:41:58.561345 56630 net.cpp:144] Setting up pool1
I0122 16:41:58.561358 56630 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:41:58.561362 56630 net.cpp:159] Memory required for data: 41575200
I0122 16:41:58.561367 56630 layer_factory.hpp:77] Creating layer drop1
I0122 16:41:58.561372 56630 net.cpp:94] Creating Layer drop1
I0122 16:41:58.561374 56630 net.cpp:435] drop1 <- pool1
I0122 16:41:58.561379 56630 net.cpp:409] drop1 -> drop1
I0122 16:41:58.561414 56630 net.cpp:144] Setting up drop1
I0122 16:41:58.561419 56630 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:41:58.561422 56630 net.cpp:159] Memory required for data: 43213600
I0122 16:41:58.561425 56630 layer_factory.hpp:77] Creating layer conv3
I0122 16:41:58.561434 56630 net.cpp:94] Creating Layer conv3
I0122 16:41:58.561439 56630 net.cpp:435] conv3 <- drop1
I0122 16:41:58.561445 56630 net.cpp:409] conv3 -> conv3
I0122 16:41:58.562505 56630 net.cpp:144] Setting up conv3
I0122 16:41:58.562520 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.562523 56630 net.cpp:159] Memory required for data: 46490400
I0122 16:41:58.562536 56630 layer_factory.hpp:77] Creating layer bn3
I0122 16:41:58.562544 56630 net.cpp:94] Creating Layer bn3
I0122 16:41:58.562549 56630 net.cpp:435] bn3 <- conv3
I0122 16:41:58.562556 56630 net.cpp:409] bn3 -> scale3
I0122 16:41:58.563304 56630 net.cpp:144] Setting up bn3
I0122 16:41:58.563311 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.563315 56630 net.cpp:159] Memory required for data: 49767200
I0122 16:41:58.563328 56630 layer_factory.hpp:77] Creating layer relu3
I0122 16:41:58.563335 56630 net.cpp:94] Creating Layer relu3
I0122 16:41:58.563339 56630 net.cpp:435] relu3 <- scale3
I0122 16:41:58.563345 56630 net.cpp:409] relu3 -> relu3
I0122 16:41:58.563366 56630 net.cpp:144] Setting up relu3
I0122 16:41:58.563374 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.563377 56630 net.cpp:159] Memory required for data: 53044000
I0122 16:41:58.563381 56630 layer_factory.hpp:77] Creating layer conv4
I0122 16:41:58.563390 56630 net.cpp:94] Creating Layer conv4
I0122 16:41:58.563393 56630 net.cpp:435] conv4 <- relu3
I0122 16:41:58.563400 56630 net.cpp:409] conv4 -> conv4
I0122 16:41:58.563943 56630 net.cpp:144] Setting up conv4
I0122 16:41:58.563951 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.563954 56630 net.cpp:159] Memory required for data: 56320800
I0122 16:41:58.563959 56630 layer_factory.hpp:77] Creating layer bn4
I0122 16:41:58.563967 56630 net.cpp:94] Creating Layer bn4
I0122 16:41:58.563971 56630 net.cpp:435] bn4 <- conv4
I0122 16:41:58.563977 56630 net.cpp:409] bn4 -> scale4
I0122 16:41:58.564743 56630 net.cpp:144] Setting up bn4
I0122 16:41:58.564750 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.564754 56630 net.cpp:159] Memory required for data: 59597600
I0122 16:41:58.564762 56630 layer_factory.hpp:77] Creating layer relu4
I0122 16:41:58.564767 56630 net.cpp:94] Creating Layer relu4
I0122 16:41:58.564774 56630 net.cpp:435] relu4 <- scale4
I0122 16:41:58.564779 56630 net.cpp:409] relu4 -> relu4
I0122 16:41:58.564801 56630 net.cpp:144] Setting up relu4
I0122 16:41:58.564808 56630 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:41:58.564812 56630 net.cpp:159] Memory required for data: 62874400
I0122 16:41:58.564815 56630 layer_factory.hpp:77] Creating layer pool2
I0122 16:41:58.564821 56630 net.cpp:94] Creating Layer pool2
I0122 16:41:58.564826 56630 net.cpp:435] pool2 <- relu4
I0122 16:41:58.564831 56630 net.cpp:409] pool2 -> pool2
I0122 16:41:58.564920 56630 net.cpp:144] Setting up pool2
I0122 16:41:58.564926 56630 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:41:58.564929 56630 net.cpp:159] Memory required for data: 63693600
I0122 16:41:58.564934 56630 layer_factory.hpp:77] Creating layer drop2
I0122 16:41:58.564940 56630 net.cpp:94] Creating Layer drop2
I0122 16:41:58.564944 56630 net.cpp:435] drop2 <- pool2
I0122 16:41:58.564949 56630 net.cpp:409] drop2 -> drop2
I0122 16:41:58.564980 56630 net.cpp:144] Setting up drop2
I0122 16:41:58.564996 56630 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:41:58.565001 56630 net.cpp:159] Memory required for data: 64512800
I0122 16:41:58.565004 56630 layer_factory.hpp:77] Creating layer fc1
I0122 16:41:58.565012 56630 net.cpp:94] Creating Layer fc1
I0122 16:41:58.565016 56630 net.cpp:435] fc1 <- drop2
I0122 16:41:58.565021 56630 net.cpp:409] fc1 -> fc1
I0122 16:41:58.578801 56630 net.cpp:144] Setting up fc1
I0122 16:41:58.578819 56630 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:41:58.578822 56630 net.cpp:159] Memory required for data: 64615200
I0122 16:41:58.578830 56630 layer_factory.hpp:77] Creating layer bn5
I0122 16:41:58.578837 56630 net.cpp:94] Creating Layer bn5
I0122 16:41:58.578841 56630 net.cpp:435] bn5 <- fc1
I0122 16:41:58.578847 56630 net.cpp:409] bn5 -> scale5
I0122 16:41:58.579377 56630 net.cpp:144] Setting up bn5
I0122 16:41:58.579385 56630 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:41:58.579387 56630 net.cpp:159] Memory required for data: 64717600
I0122 16:41:58.579399 56630 layer_factory.hpp:77] Creating layer relu5
I0122 16:41:58.579406 56630 net.cpp:94] Creating Layer relu5
I0122 16:41:58.579409 56630 net.cpp:435] relu5 <- scale5
I0122 16:41:58.579414 56630 net.cpp:409] relu5 -> relu5
I0122 16:41:58.579432 56630 net.cpp:144] Setting up relu5
I0122 16:41:58.579437 56630 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:41:58.579439 56630 net.cpp:159] Memory required for data: 64820000
I0122 16:41:58.579442 56630 layer_factory.hpp:77] Creating layer drop3
I0122 16:41:58.579448 56630 net.cpp:94] Creating Layer drop3
I0122 16:41:58.579452 56630 net.cpp:435] drop3 <- relu5
I0122 16:41:58.579455 56630 net.cpp:409] drop3 -> drop3
I0122 16:41:58.579483 56630 net.cpp:144] Setting up drop3
I0122 16:41:58.579486 56630 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:41:58.579489 56630 net.cpp:159] Memory required for data: 64922400
I0122 16:41:58.579491 56630 layer_factory.hpp:77] Creating layer fc2
I0122 16:41:58.579499 56630 net.cpp:94] Creating Layer fc2
I0122 16:41:58.579504 56630 net.cpp:435] fc2 <- drop3
I0122 16:41:58.579507 56630 net.cpp:409] fc2 -> fc2
I0122 16:41:58.579636 56630 net.cpp:144] Setting up fc2
I0122 16:41:58.579643 56630 net.cpp:151] Top shape: 50 10 (500)
I0122 16:41:58.579644 56630 net.cpp:159] Memory required for data: 64924400
I0122 16:41:58.579649 56630 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:41:58.579654 56630 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:41:58.579658 56630 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:41:58.579663 56630 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:41:58.579668 56630 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:41:58.579674 56630 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:41:58.579710 56630 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:41:58.579715 56630 net.cpp:151] Top shape: 50 10 (500)
I0122 16:41:58.579720 56630 net.cpp:151] Top shape: 50 10 (500)
I0122 16:41:58.579722 56630 net.cpp:151] Top shape: 50 10 (500)
I0122 16:41:58.579725 56630 net.cpp:159] Memory required for data: 64930400
I0122 16:41:58.579728 56630 layer_factory.hpp:77] Creating layer loss
I0122 16:41:58.579733 56630 net.cpp:94] Creating Layer loss
I0122 16:41:58.579737 56630 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:41:58.579741 56630 net.cpp:435] loss <- label_data_1_split_0
I0122 16:41:58.579746 56630 net.cpp:409] loss -> loss
I0122 16:41:58.579753 56630 layer_factory.hpp:77] Creating layer loss
I0122 16:41:58.579821 56630 net.cpp:144] Setting up loss
I0122 16:41:58.579828 56630 net.cpp:151] Top shape: (1)
I0122 16:41:58.579830 56630 net.cpp:154]     with loss weight 1
I0122 16:41:58.579840 56630 net.cpp:159] Memory required for data: 64930404
I0122 16:41:58.579843 56630 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:41:58.579849 56630 net.cpp:94] Creating Layer accuracy-top1
I0122 16:41:58.579854 56630 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:41:58.579859 56630 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:41:58.579864 56630 net.cpp:409] accuracy-top1 -> top-1
I0122 16:41:58.579882 56630 net.cpp:144] Setting up accuracy-top1
I0122 16:41:58.579887 56630 net.cpp:151] Top shape: (1)
I0122 16:41:58.579890 56630 net.cpp:159] Memory required for data: 64930408
I0122 16:41:58.579892 56630 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:41:58.579897 56630 net.cpp:94] Creating Layer accuracy-top5
I0122 16:41:58.579900 56630 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:41:58.579903 56630 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:41:58.579908 56630 net.cpp:409] accuracy-top5 -> top-5
I0122 16:41:58.579915 56630 net.cpp:144] Setting up accuracy-top5
I0122 16:41:58.579918 56630 net.cpp:151] Top shape: (1)
I0122 16:41:58.579921 56630 net.cpp:159] Memory required for data: 64930412
I0122 16:41:58.579923 56630 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:41:58.579927 56630 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:41:58.579931 56630 net.cpp:220] loss needs backward computation.
I0122 16:41:58.579934 56630 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:41:58.579938 56630 net.cpp:220] fc2 needs backward computation.
I0122 16:41:58.579941 56630 net.cpp:220] drop3 needs backward computation.
I0122 16:41:58.579944 56630 net.cpp:220] relu5 needs backward computation.
I0122 16:41:58.579947 56630 net.cpp:220] bn5 needs backward computation.
I0122 16:41:58.579951 56630 net.cpp:220] fc1 needs backward computation.
I0122 16:41:58.579954 56630 net.cpp:220] drop2 needs backward computation.
I0122 16:41:58.579957 56630 net.cpp:220] pool2 needs backward computation.
I0122 16:41:58.579959 56630 net.cpp:220] relu4 needs backward computation.
I0122 16:41:58.579962 56630 net.cpp:220] bn4 needs backward computation.
I0122 16:41:58.579967 56630 net.cpp:220] conv4 needs backward computation.
I0122 16:41:58.579969 56630 net.cpp:220] relu3 needs backward computation.
I0122 16:41:58.579972 56630 net.cpp:220] bn3 needs backward computation.
I0122 16:41:58.579974 56630 net.cpp:220] conv3 needs backward computation.
I0122 16:41:58.579978 56630 net.cpp:220] drop1 needs backward computation.
I0122 16:41:58.579980 56630 net.cpp:220] pool1 needs backward computation.
I0122 16:41:58.579983 56630 net.cpp:220] relu2 needs backward computation.
I0122 16:41:58.579987 56630 net.cpp:220] bn2 needs backward computation.
I0122 16:41:58.579990 56630 net.cpp:220] conv2 needs backward computation.
I0122 16:41:58.579993 56630 net.cpp:220] relu1 needs backward computation.
I0122 16:41:58.579995 56630 net.cpp:220] bn1 needs backward computation.
I0122 16:41:58.579998 56630 net.cpp:220] conv1 needs backward computation.
I0122 16:41:58.580003 56630 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:41:58.580006 56630 net.cpp:222] data does not need backward computation.
I0122 16:41:58.580009 56630 net.cpp:264] This network produces output loss
I0122 16:41:58.580013 56630 net.cpp:264] This network produces output top-1
I0122 16:41:58.580016 56630 net.cpp:264] This network produces output top-5
I0122 16:41:58.580037 56630 net.cpp:284] Network initialization done.
I0122 16:41:58.582921 56630 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:41:58.589351 56630 caffe_interface.cpp:125] Batch 0, loss = 0.474768
I0122 16:41:58.589371 56630 caffe_interface.cpp:125] Batch 0, top-1 = 0.8
I0122 16:41:58.589376 56630 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:41:58.590699 56630 caffe_interface.cpp:125] Batch 1, loss = 0.658869
I0122 16:41:58.590709 56630 caffe_interface.cpp:125] Batch 1, top-1 = 0.78
I0122 16:41:58.590713 56630 caffe_interface.cpp:125] Batch 1, top-5 = 0.96
I0122 16:41:58.592005 56630 caffe_interface.cpp:125] Batch 2, loss = 0.605272
I0122 16:41:58.592012 56630 caffe_interface.cpp:125] Batch 2, top-1 = 0.82
I0122 16:41:58.592015 56630 caffe_interface.cpp:125] Batch 2, top-5 = 1
I0122 16:41:58.593386 56630 caffe_interface.cpp:125] Batch 3, loss = 0.88103
I0122 16:41:58.593394 56630 caffe_interface.cpp:125] Batch 3, top-1 = 0.72
I0122 16:41:58.593415 56630 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0122 16:41:58.594725 56630 caffe_interface.cpp:125] Batch 4, loss = 0.686303
I0122 16:41:58.594734 56630 caffe_interface.cpp:125] Batch 4, top-1 = 0.82
I0122 16:41:58.594738 56630 caffe_interface.cpp:125] Batch 4, top-5 = 1
I0122 16:41:58.596046 56630 caffe_interface.cpp:125] Batch 5, loss = 0.552915
I0122 16:41:58.596061 56630 caffe_interface.cpp:125] Batch 5, top-1 = 0.82
I0122 16:41:58.596065 56630 caffe_interface.cpp:125] Batch 5, top-5 = 1
I0122 16:41:58.597354 56630 caffe_interface.cpp:125] Batch 6, loss = 1.07113
I0122 16:41:58.597362 56630 caffe_interface.cpp:125] Batch 6, top-1 = 0.74
I0122 16:41:58.597364 56630 caffe_interface.cpp:125] Batch 6, top-5 = 0.98
I0122 16:41:58.598655 56630 caffe_interface.cpp:125] Batch 7, loss = 0.690049
I0122 16:41:58.598664 56630 caffe_interface.cpp:125] Batch 7, top-1 = 0.76
I0122 16:41:58.598667 56630 caffe_interface.cpp:125] Batch 7, top-5 = 0.98
I0122 16:41:58.599951 56630 caffe_interface.cpp:125] Batch 8, loss = 0.915227
I0122 16:41:58.599958 56630 caffe_interface.cpp:125] Batch 8, top-1 = 0.72
I0122 16:41:58.599961 56630 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:41:58.601250 56630 caffe_interface.cpp:125] Batch 9, loss = 1.07899
I0122 16:41:58.601261 56630 caffe_interface.cpp:125] Batch 9, top-1 = 0.72
I0122 16:41:58.601265 56630 caffe_interface.cpp:125] Batch 9, top-5 = 0.94
I0122 16:41:58.602543 56630 caffe_interface.cpp:125] Batch 10, loss = 0.677703
I0122 16:41:58.602550 56630 caffe_interface.cpp:125] Batch 10, top-1 = 0.8
I0122 16:41:58.602553 56630 caffe_interface.cpp:125] Batch 10, top-5 = 0.98
I0122 16:41:58.603837 56630 caffe_interface.cpp:125] Batch 11, loss = 0.976175
I0122 16:41:58.603844 56630 caffe_interface.cpp:125] Batch 11, top-1 = 0.76
I0122 16:41:58.603847 56630 caffe_interface.cpp:125] Batch 11, top-5 = 0.98
I0122 16:41:58.605125 56630 caffe_interface.cpp:125] Batch 12, loss = 1.0046
I0122 16:41:58.605134 56630 caffe_interface.cpp:125] Batch 12, top-1 = 0.78
I0122 16:41:58.605136 56630 caffe_interface.cpp:125] Batch 12, top-5 = 0.98
I0122 16:41:58.606413 56630 caffe_interface.cpp:125] Batch 13, loss = 1.03489
I0122 16:41:58.606421 56630 caffe_interface.cpp:125] Batch 13, top-1 = 0.68
I0122 16:41:58.606426 56630 caffe_interface.cpp:125] Batch 13, top-5 = 0.98
I0122 16:41:58.607717 56630 caffe_interface.cpp:125] Batch 14, loss = 0.845146
I0122 16:41:58.607725 56630 caffe_interface.cpp:125] Batch 14, top-1 = 0.82
I0122 16:41:58.607728 56630 caffe_interface.cpp:125] Batch 14, top-5 = 0.98
I0122 16:41:58.609007 56630 caffe_interface.cpp:125] Batch 15, loss = 0.807712
I0122 16:41:58.609014 56630 caffe_interface.cpp:125] Batch 15, top-1 = 0.74
I0122 16:41:58.609017 56630 caffe_interface.cpp:125] Batch 15, top-5 = 0.98
I0122 16:41:58.610309 56630 caffe_interface.cpp:125] Batch 16, loss = 0.629946
I0122 16:41:58.610318 56630 caffe_interface.cpp:125] Batch 16, top-1 = 0.8
I0122 16:41:58.610321 56630 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0122 16:41:58.611603 56630 caffe_interface.cpp:125] Batch 17, loss = 0.585656
I0122 16:41:58.611610 56630 caffe_interface.cpp:125] Batch 17, top-1 = 0.82
I0122 16:41:58.611613 56630 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0122 16:41:58.612912 56630 caffe_interface.cpp:125] Batch 18, loss = 0.716964
I0122 16:41:58.612920 56630 caffe_interface.cpp:125] Batch 18, top-1 = 0.78
I0122 16:41:58.612921 56630 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0122 16:41:58.615305 56630 caffe_interface.cpp:125] Batch 19, loss = 1.01309
I0122 16:41:58.615312 56630 caffe_interface.cpp:125] Batch 19, top-1 = 0.74
I0122 16:41:58.615315 56630 caffe_interface.cpp:125] Batch 19, top-5 = 0.98
I0122 16:41:58.616701 56630 caffe_interface.cpp:125] Batch 20, loss = 0.745996
I0122 16:41:58.616708 56630 caffe_interface.cpp:125] Batch 20, top-1 = 0.74
I0122 16:41:58.616710 56630 caffe_interface.cpp:125] Batch 20, top-5 = 1
I0122 16:41:58.618175 56630 caffe_interface.cpp:125] Batch 21, loss = 0.847923
I0122 16:41:58.618182 56630 caffe_interface.cpp:125] Batch 21, top-1 = 0.68
I0122 16:41:58.618194 56630 caffe_interface.cpp:125] Batch 21, top-5 = 1
I0122 16:41:58.619494 56630 caffe_interface.cpp:125] Batch 22, loss = 0.714021
I0122 16:41:58.619501 56630 caffe_interface.cpp:125] Batch 22, top-1 = 0.72
I0122 16:41:58.619504 56630 caffe_interface.cpp:125] Batch 22, top-5 = 0.98
I0122 16:41:58.620790 56630 caffe_interface.cpp:125] Batch 23, loss = 0.600999
I0122 16:41:58.620797 56630 caffe_interface.cpp:125] Batch 23, top-1 = 0.78
I0122 16:41:58.620801 56630 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:41:58.622094 56630 caffe_interface.cpp:125] Batch 24, loss = 0.722624
I0122 16:41:58.622102 56630 caffe_interface.cpp:125] Batch 24, top-1 = 0.74
I0122 16:41:58.622105 56630 caffe_interface.cpp:125] Batch 24, top-5 = 0.96
I0122 16:41:58.623409 56630 caffe_interface.cpp:125] Batch 25, loss = 0.491423
I0122 16:41:58.623416 56630 caffe_interface.cpp:125] Batch 25, top-1 = 0.84
I0122 16:41:58.623420 56630 caffe_interface.cpp:125] Batch 25, top-5 = 0.98
I0122 16:41:58.624697 56630 caffe_interface.cpp:125] Batch 26, loss = 0.936057
I0122 16:41:58.624706 56630 caffe_interface.cpp:125] Batch 26, top-1 = 0.76
I0122 16:41:58.624708 56630 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0122 16:41:58.626011 56630 caffe_interface.cpp:125] Batch 27, loss = 0.69577
I0122 16:41:58.626019 56630 caffe_interface.cpp:125] Batch 27, top-1 = 0.78
I0122 16:41:58.626021 56630 caffe_interface.cpp:125] Batch 27, top-5 = 1
I0122 16:41:58.627573 56630 caffe_interface.cpp:125] Batch 28, loss = 0.987139
I0122 16:41:58.627581 56630 caffe_interface.cpp:125] Batch 28, top-1 = 0.7
I0122 16:41:58.627584 56630 caffe_interface.cpp:125] Batch 28, top-5 = 0.98
I0122 16:41:58.628870 56630 caffe_interface.cpp:125] Batch 29, loss = 0.513905
I0122 16:41:58.628877 56630 caffe_interface.cpp:125] Batch 29, top-1 = 0.86
I0122 16:41:58.628881 56630 caffe_interface.cpp:125] Batch 29, top-5 = 1
I0122 16:41:58.630168 56630 caffe_interface.cpp:125] Batch 30, loss = 0.674554
I0122 16:41:58.630175 56630 caffe_interface.cpp:125] Batch 30, top-1 = 0.8
I0122 16:41:58.630179 56630 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:41:58.631462 56630 caffe_interface.cpp:125] Batch 31, loss = 0.91526
I0122 16:41:58.631469 56630 caffe_interface.cpp:125] Batch 31, top-1 = 0.72
I0122 16:41:58.631472 56630 caffe_interface.cpp:125] Batch 31, top-5 = 0.96
I0122 16:41:58.632747 56630 caffe_interface.cpp:125] Batch 32, loss = 0.621708
I0122 16:41:58.632755 56630 caffe_interface.cpp:125] Batch 32, top-1 = 0.82
I0122 16:41:58.632758 56630 caffe_interface.cpp:125] Batch 32, top-5 = 1
I0122 16:41:58.634038 56630 caffe_interface.cpp:125] Batch 33, loss = 0.860836
I0122 16:41:58.634045 56630 caffe_interface.cpp:125] Batch 33, top-1 = 0.68
I0122 16:41:58.634049 56630 caffe_interface.cpp:125] Batch 33, top-5 = 0.98
I0122 16:41:58.635332 56630 caffe_interface.cpp:125] Batch 34, loss = 0.999514
I0122 16:41:58.635339 56630 caffe_interface.cpp:125] Batch 34, top-1 = 0.8
I0122 16:41:58.635342 56630 caffe_interface.cpp:125] Batch 34, top-5 = 0.96
I0122 16:41:58.636622 56630 caffe_interface.cpp:125] Batch 35, loss = 0.631153
I0122 16:41:58.636629 56630 caffe_interface.cpp:125] Batch 35, top-1 = 0.8
I0122 16:41:58.636632 56630 caffe_interface.cpp:125] Batch 35, top-5 = 1
I0122 16:41:58.637933 56630 caffe_interface.cpp:125] Batch 36, loss = 0.717898
I0122 16:41:58.637941 56630 caffe_interface.cpp:125] Batch 36, top-1 = 0.76
I0122 16:41:58.637944 56630 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:41:58.639226 56630 caffe_interface.cpp:125] Batch 37, loss = 1.02582
I0122 16:41:58.639235 56630 caffe_interface.cpp:125] Batch 37, top-1 = 0.72
I0122 16:41:58.639238 56630 caffe_interface.cpp:125] Batch 37, top-5 = 0.98
I0122 16:41:58.640535 56630 caffe_interface.cpp:125] Batch 38, loss = 0.751503
I0122 16:41:58.640543 56630 caffe_interface.cpp:125] Batch 38, top-1 = 0.72
I0122 16:41:58.640545 56630 caffe_interface.cpp:125] Batch 38, top-5 = 0.98
I0122 16:41:58.641818 56630 caffe_interface.cpp:125] Batch 39, loss = 0.516118
I0122 16:41:58.641824 56630 caffe_interface.cpp:125] Batch 39, top-1 = 0.84
I0122 16:41:58.641837 56630 caffe_interface.cpp:125] Batch 39, top-5 = 0.96
I0122 16:41:58.643127 56630 caffe_interface.cpp:125] Batch 40, loss = 0.877668
I0122 16:41:58.643136 56630 caffe_interface.cpp:125] Batch 40, top-1 = 0.76
I0122 16:41:58.643141 56630 caffe_interface.cpp:125] Batch 40, top-5 = 0.98
I0122 16:41:58.644425 56630 caffe_interface.cpp:125] Batch 41, loss = 1.28323
I0122 16:41:58.644433 56630 caffe_interface.cpp:125] Batch 41, top-1 = 0.68
I0122 16:41:58.644438 56630 caffe_interface.cpp:125] Batch 41, top-5 = 0.96
I0122 16:41:58.645711 56630 caffe_interface.cpp:125] Batch 42, loss = 0.607433
I0122 16:41:58.645718 56630 caffe_interface.cpp:125] Batch 42, top-1 = 0.8
I0122 16:41:58.645722 56630 caffe_interface.cpp:125] Batch 42, top-5 = 1
I0122 16:41:58.647006 56630 caffe_interface.cpp:125] Batch 43, loss = 0.983969
I0122 16:41:58.647013 56630 caffe_interface.cpp:125] Batch 43, top-1 = 0.74
I0122 16:41:58.647017 56630 caffe_interface.cpp:125] Batch 43, top-5 = 0.96
I0122 16:41:58.649333 56630 caffe_interface.cpp:125] Batch 44, loss = 0.642594
I0122 16:41:58.649340 56630 caffe_interface.cpp:125] Batch 44, top-1 = 0.72
I0122 16:41:58.649343 56630 caffe_interface.cpp:125] Batch 44, top-5 = 1
I0122 16:41:58.650758 56630 caffe_interface.cpp:125] Batch 45, loss = 0.770725
I0122 16:41:58.650764 56630 caffe_interface.cpp:125] Batch 45, top-1 = 0.76
I0122 16:41:58.650768 56630 caffe_interface.cpp:125] Batch 45, top-5 = 0.98
I0122 16:41:58.652071 56630 caffe_interface.cpp:125] Batch 46, loss = 0.401216
I0122 16:41:58.652078 56630 caffe_interface.cpp:125] Batch 46, top-1 = 0.86
I0122 16:41:58.652081 56630 caffe_interface.cpp:125] Batch 46, top-5 = 1
I0122 16:41:58.653368 56630 caffe_interface.cpp:125] Batch 47, loss = 0.514897
I0122 16:41:58.653375 56630 caffe_interface.cpp:125] Batch 47, top-1 = 0.82
I0122 16:41:58.653379 56630 caffe_interface.cpp:125] Batch 47, top-5 = 1
I0122 16:41:58.654680 56630 caffe_interface.cpp:125] Batch 48, loss = 0.829626
I0122 16:41:58.654687 56630 caffe_interface.cpp:125] Batch 48, top-1 = 0.68
I0122 16:41:58.654690 56630 caffe_interface.cpp:125] Batch 48, top-5 = 1
I0122 16:41:58.655988 56630 caffe_interface.cpp:125] Batch 49, loss = 0.883265
I0122 16:41:58.655995 56630 caffe_interface.cpp:125] Batch 49, top-1 = 0.78
I0122 16:41:58.655999 56630 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0122 16:41:58.657292 56630 caffe_interface.cpp:125] Batch 50, loss = 0.575849
I0122 16:41:58.657299 56630 caffe_interface.cpp:125] Batch 50, top-1 = 0.82
I0122 16:41:58.657302 56630 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:41:58.658582 56630 caffe_interface.cpp:125] Batch 51, loss = 0.730958
I0122 16:41:58.658589 56630 caffe_interface.cpp:125] Batch 51, top-1 = 0.82
I0122 16:41:58.658593 56630 caffe_interface.cpp:125] Batch 51, top-5 = 1
I0122 16:41:58.659888 56630 caffe_interface.cpp:125] Batch 52, loss = 0.871386
I0122 16:41:58.659904 56630 caffe_interface.cpp:125] Batch 52, top-1 = 0.76
I0122 16:41:58.659906 56630 caffe_interface.cpp:125] Batch 52, top-5 = 0.96
I0122 16:41:58.661244 56630 caffe_interface.cpp:125] Batch 53, loss = 1.00236
I0122 16:41:58.661252 56630 caffe_interface.cpp:125] Batch 53, top-1 = 0.7
I0122 16:41:58.661255 56630 caffe_interface.cpp:125] Batch 53, top-5 = 0.98
I0122 16:41:58.662536 56630 caffe_interface.cpp:125] Batch 54, loss = 1.04874
I0122 16:41:58.662544 56630 caffe_interface.cpp:125] Batch 54, top-1 = 0.78
I0122 16:41:58.662547 56630 caffe_interface.cpp:125] Batch 54, top-5 = 0.98
I0122 16:41:58.663830 56630 caffe_interface.cpp:125] Batch 55, loss = 0.543936
I0122 16:41:58.663837 56630 caffe_interface.cpp:125] Batch 55, top-1 = 0.8
I0122 16:41:58.663841 56630 caffe_interface.cpp:125] Batch 55, top-5 = 1
I0122 16:41:58.665128 56630 caffe_interface.cpp:125] Batch 56, loss = 1.00199
I0122 16:41:58.665136 56630 caffe_interface.cpp:125] Batch 56, top-1 = 0.8
I0122 16:41:58.665139 56630 caffe_interface.cpp:125] Batch 56, top-5 = 0.96
I0122 16:41:58.666422 56630 caffe_interface.cpp:125] Batch 57, loss = 0.384546
I0122 16:41:58.666442 56630 caffe_interface.cpp:125] Batch 57, top-1 = 0.86
I0122 16:41:58.666446 56630 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:41:58.667731 56630 caffe_interface.cpp:125] Batch 58, loss = 0.695849
I0122 16:41:58.667737 56630 caffe_interface.cpp:125] Batch 58, top-1 = 0.74
I0122 16:41:58.667740 56630 caffe_interface.cpp:125] Batch 58, top-5 = 1
I0122 16:41:58.669005 56630 caffe_interface.cpp:125] Batch 59, loss = 0.565064
I0122 16:41:58.669013 56630 caffe_interface.cpp:125] Batch 59, top-1 = 0.82
I0122 16:41:58.669015 56630 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:41:58.670279 56630 caffe_interface.cpp:125] Batch 60, loss = 0.808224
I0122 16:41:58.670287 56630 caffe_interface.cpp:125] Batch 60, top-1 = 0.8
I0122 16:41:58.670290 56630 caffe_interface.cpp:125] Batch 60, top-5 = 0.94
I0122 16:41:58.671548 56630 caffe_interface.cpp:125] Batch 61, loss = 1.40246
I0122 16:41:58.671555 56630 caffe_interface.cpp:125] Batch 61, top-1 = 0.7
I0122 16:41:58.671560 56630 caffe_interface.cpp:125] Batch 61, top-5 = 0.96
I0122 16:41:58.672838 56630 caffe_interface.cpp:125] Batch 62, loss = 0.797386
I0122 16:41:58.672845 56630 caffe_interface.cpp:125] Batch 62, top-1 = 0.7
I0122 16:41:58.672848 56630 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:41:58.674129 56630 caffe_interface.cpp:125] Batch 63, loss = 0.90274
I0122 16:41:58.674135 56630 caffe_interface.cpp:125] Batch 63, top-1 = 0.82
I0122 16:41:58.674139 56630 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0122 16:41:58.675424 56630 caffe_interface.cpp:125] Batch 64, loss = 0.744847
I0122 16:41:58.675432 56630 caffe_interface.cpp:125] Batch 64, top-1 = 0.74
I0122 16:41:58.675436 56630 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:41:58.676707 56630 caffe_interface.cpp:125] Batch 65, loss = 0.458631
I0122 16:41:58.676714 56630 caffe_interface.cpp:125] Batch 65, top-1 = 0.84
I0122 16:41:58.676718 56630 caffe_interface.cpp:125] Batch 65, top-5 = 1
I0122 16:41:58.677978 56630 caffe_interface.cpp:125] Batch 66, loss = 0.830178
I0122 16:41:58.677984 56630 caffe_interface.cpp:125] Batch 66, top-1 = 0.76
I0122 16:41:58.677989 56630 caffe_interface.cpp:125] Batch 66, top-5 = 1
I0122 16:41:58.679255 56630 caffe_interface.cpp:125] Batch 67, loss = 1.09166
I0122 16:41:58.679262 56630 caffe_interface.cpp:125] Batch 67, top-1 = 0.72
I0122 16:41:58.679265 56630 caffe_interface.cpp:125] Batch 67, top-5 = 1
I0122 16:41:58.681435 56630 caffe_interface.cpp:125] Batch 68, loss = 0.587118
I0122 16:41:58.681442 56630 caffe_interface.cpp:125] Batch 68, top-1 = 0.8
I0122 16:41:58.681444 56630 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:41:58.682822 56630 caffe_interface.cpp:125] Batch 69, loss = 0.49831
I0122 16:41:58.682832 56630 caffe_interface.cpp:125] Batch 69, top-1 = 0.88
I0122 16:41:58.682835 56630 caffe_interface.cpp:125] Batch 69, top-5 = 0.98
I0122 16:41:58.684307 56630 caffe_interface.cpp:125] Batch 70, loss = 0.704694
I0122 16:41:58.684314 56630 caffe_interface.cpp:125] Batch 70, top-1 = 0.74
I0122 16:41:58.684317 56630 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0122 16:41:58.685582 56630 caffe_interface.cpp:125] Batch 71, loss = 0.7235
I0122 16:41:58.685590 56630 caffe_interface.cpp:125] Batch 71, top-1 = 0.7
I0122 16:41:58.685592 56630 caffe_interface.cpp:125] Batch 71, top-5 = 1
I0122 16:41:58.686866 56630 caffe_interface.cpp:125] Batch 72, loss = 0.804769
I0122 16:41:58.686873 56630 caffe_interface.cpp:125] Batch 72, top-1 = 0.72
I0122 16:41:58.686877 56630 caffe_interface.cpp:125] Batch 72, top-5 = 1
I0122 16:41:58.688145 56630 caffe_interface.cpp:125] Batch 73, loss = 0.920956
I0122 16:41:58.688153 56630 caffe_interface.cpp:125] Batch 73, top-1 = 0.76
I0122 16:41:58.688156 56630 caffe_interface.cpp:125] Batch 73, top-5 = 0.98
I0122 16:41:58.689429 56630 caffe_interface.cpp:125] Batch 74, loss = 1.43216
I0122 16:41:58.689436 56630 caffe_interface.cpp:125] Batch 74, top-1 = 0.7
I0122 16:41:58.689440 56630 caffe_interface.cpp:125] Batch 74, top-5 = 0.92
I0122 16:41:58.690721 56630 caffe_interface.cpp:125] Batch 75, loss = 0.715042
I0122 16:41:58.690742 56630 caffe_interface.cpp:125] Batch 75, top-1 = 0.76
I0122 16:41:58.690745 56630 caffe_interface.cpp:125] Batch 75, top-5 = 0.96
I0122 16:41:58.692030 56630 caffe_interface.cpp:125] Batch 76, loss = 0.909533
I0122 16:41:58.692039 56630 caffe_interface.cpp:125] Batch 76, top-1 = 0.78
I0122 16:41:58.692041 56630 caffe_interface.cpp:125] Batch 76, top-5 = 0.96
I0122 16:41:58.693416 56630 caffe_interface.cpp:125] Batch 77, loss = 0.582451
I0122 16:41:58.693423 56630 caffe_interface.cpp:125] Batch 77, top-1 = 0.8
I0122 16:41:58.693428 56630 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0122 16:41:58.694706 56630 caffe_interface.cpp:125] Batch 78, loss = 0.873191
I0122 16:41:58.694713 56630 caffe_interface.cpp:125] Batch 78, top-1 = 0.78
I0122 16:41:58.694717 56630 caffe_interface.cpp:125] Batch 78, top-5 = 1
I0122 16:41:58.695991 56630 caffe_interface.cpp:125] Batch 79, loss = 0.847006
I0122 16:41:58.695997 56630 caffe_interface.cpp:125] Batch 79, top-1 = 0.72
I0122 16:41:58.696000 56630 caffe_interface.cpp:125] Batch 79, top-5 = 0.94
I0122 16:41:58.697202 56630 caffe_interface.cpp:125] Batch 80, loss = 0.652182
I0122 16:41:58.697211 56630 caffe_interface.cpp:125] Batch 80, top-1 = 0.78
I0122 16:41:58.697213 56630 caffe_interface.cpp:125] Batch 80, top-5 = 1
I0122 16:41:58.698422 56630 caffe_interface.cpp:125] Batch 81, loss = 1.24986
I0122 16:41:58.698429 56630 caffe_interface.cpp:125] Batch 81, top-1 = 0.72
I0122 16:41:58.698433 56630 caffe_interface.cpp:125] Batch 81, top-5 = 1
I0122 16:41:58.699640 56630 caffe_interface.cpp:125] Batch 82, loss = 0.850479
I0122 16:41:58.699646 56630 caffe_interface.cpp:125] Batch 82, top-1 = 0.76
I0122 16:41:58.699649 56630 caffe_interface.cpp:125] Batch 82, top-5 = 0.98
I0122 16:41:58.700861 56630 caffe_interface.cpp:125] Batch 83, loss = 0.684216
I0122 16:41:58.700877 56630 caffe_interface.cpp:125] Batch 83, top-1 = 0.82
I0122 16:41:58.700879 56630 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:41:58.702092 56630 caffe_interface.cpp:125] Batch 84, loss = 0.665863
I0122 16:41:58.702100 56630 caffe_interface.cpp:125] Batch 84, top-1 = 0.74
I0122 16:41:58.702103 56630 caffe_interface.cpp:125] Batch 84, top-5 = 0.96
I0122 16:41:58.703325 56630 caffe_interface.cpp:125] Batch 85, loss = 0.608356
I0122 16:41:58.703332 56630 caffe_interface.cpp:125] Batch 85, top-1 = 0.78
I0122 16:41:58.703336 56630 caffe_interface.cpp:125] Batch 85, top-5 = 1
I0122 16:41:58.704541 56630 caffe_interface.cpp:125] Batch 86, loss = 0.690421
I0122 16:41:58.704548 56630 caffe_interface.cpp:125] Batch 86, top-1 = 0.72
I0122 16:41:58.704551 56630 caffe_interface.cpp:125] Batch 86, top-5 = 1
I0122 16:41:58.705765 56630 caffe_interface.cpp:125] Batch 87, loss = 0.96931
I0122 16:41:58.705771 56630 caffe_interface.cpp:125] Batch 87, top-1 = 0.74
I0122 16:41:58.705775 56630 caffe_interface.cpp:125] Batch 87, top-5 = 0.94
I0122 16:41:58.706981 56630 caffe_interface.cpp:125] Batch 88, loss = 0.662285
I0122 16:41:58.706990 56630 caffe_interface.cpp:125] Batch 88, top-1 = 0.78
I0122 16:41:58.706992 56630 caffe_interface.cpp:125] Batch 88, top-5 = 0.98
I0122 16:41:58.708202 56630 caffe_interface.cpp:125] Batch 89, loss = 0.565065
I0122 16:41:58.708209 56630 caffe_interface.cpp:125] Batch 89, top-1 = 0.8
I0122 16:41:58.708214 56630 caffe_interface.cpp:125] Batch 89, top-5 = 0.98
I0122 16:41:58.709421 56630 caffe_interface.cpp:125] Batch 90, loss = 0.831177
I0122 16:41:58.709429 56630 caffe_interface.cpp:125] Batch 90, top-1 = 0.74
I0122 16:41:58.709431 56630 caffe_interface.cpp:125] Batch 90, top-5 = 0.98
I0122 16:41:58.710647 56630 caffe_interface.cpp:125] Batch 91, loss = 0.64933
I0122 16:41:58.710655 56630 caffe_interface.cpp:125] Batch 91, top-1 = 0.78
I0122 16:41:58.710659 56630 caffe_interface.cpp:125] Batch 91, top-5 = 1
I0122 16:41:58.711877 56630 caffe_interface.cpp:125] Batch 92, loss = 0.685822
I0122 16:41:58.711885 56630 caffe_interface.cpp:125] Batch 92, top-1 = 0.78
I0122 16:41:58.711889 56630 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:41:58.713104 56630 caffe_interface.cpp:125] Batch 93, loss = 0.91185
I0122 16:41:58.713119 56630 caffe_interface.cpp:125] Batch 93, top-1 = 0.74
I0122 16:41:58.713124 56630 caffe_interface.cpp:125] Batch 93, top-5 = 0.96
I0122 16:41:58.715368 56630 caffe_interface.cpp:125] Batch 94, loss = 1.27645
I0122 16:41:58.715374 56630 caffe_interface.cpp:125] Batch 94, top-1 = 0.7
I0122 16:41:58.715378 56630 caffe_interface.cpp:125] Batch 94, top-5 = 0.94
I0122 16:41:58.716747 56630 caffe_interface.cpp:125] Batch 95, loss = 0.940938
I0122 16:41:58.716761 56630 caffe_interface.cpp:125] Batch 95, top-1 = 0.76
I0122 16:41:58.716763 56630 caffe_interface.cpp:125] Batch 95, top-5 = 0.98
I0122 16:41:58.718066 56630 caffe_interface.cpp:125] Batch 96, loss = 0.815657
I0122 16:41:58.718073 56630 caffe_interface.cpp:125] Batch 96, top-1 = 0.86
I0122 16:41:58.718076 56630 caffe_interface.cpp:125] Batch 96, top-5 = 0.96
I0122 16:41:58.719285 56630 caffe_interface.cpp:125] Batch 97, loss = 0.639135
I0122 16:41:58.719292 56630 caffe_interface.cpp:125] Batch 97, top-1 = 0.78
I0122 16:41:58.719296 56630 caffe_interface.cpp:125] Batch 97, top-5 = 1
I0122 16:41:58.720516 56630 caffe_interface.cpp:125] Batch 98, loss = 0.747693
I0122 16:41:58.720523 56630 caffe_interface.cpp:125] Batch 98, top-1 = 0.84
I0122 16:41:58.720526 56630 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:41:58.721742 56630 caffe_interface.cpp:125] Batch 99, loss = 0.9005
I0122 16:41:58.721750 56630 caffe_interface.cpp:125] Batch 99, top-1 = 0.7
I0122 16:41:58.721751 56630 caffe_interface.cpp:125] Batch 99, top-5 = 0.98
I0122 16:41:58.722965 56630 caffe_interface.cpp:125] Batch 100, loss = 0.658674
I0122 16:41:58.722973 56630 caffe_interface.cpp:125] Batch 100, top-1 = 0.84
I0122 16:41:58.722977 56630 caffe_interface.cpp:125] Batch 100, top-5 = 0.98
I0122 16:41:58.724195 56630 caffe_interface.cpp:125] Batch 101, loss = 0.414114
I0122 16:41:58.724201 56630 caffe_interface.cpp:125] Batch 101, top-1 = 0.88
I0122 16:41:58.724205 56630 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:41:58.725412 56630 caffe_interface.cpp:125] Batch 102, loss = 0.816381
I0122 16:41:58.725419 56630 caffe_interface.cpp:125] Batch 102, top-1 = 0.68
I0122 16:41:58.725422 56630 caffe_interface.cpp:125] Batch 102, top-5 = 0.98
I0122 16:41:58.726629 56630 caffe_interface.cpp:125] Batch 103, loss = 0.534744
I0122 16:41:58.726637 56630 caffe_interface.cpp:125] Batch 103, top-1 = 0.76
I0122 16:41:58.726640 56630 caffe_interface.cpp:125] Batch 103, top-5 = 1
I0122 16:41:58.727903 56630 caffe_interface.cpp:125] Batch 104, loss = 0.554746
I0122 16:41:58.727912 56630 caffe_interface.cpp:125] Batch 104, top-1 = 0.84
I0122 16:41:58.727916 56630 caffe_interface.cpp:125] Batch 104, top-5 = 0.98
I0122 16:41:58.729113 56630 caffe_interface.cpp:125] Batch 105, loss = 0.854799
I0122 16:41:58.729121 56630 caffe_interface.cpp:125] Batch 105, top-1 = 0.76
I0122 16:41:58.729125 56630 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:41:58.730334 56630 caffe_interface.cpp:125] Batch 106, loss = 0.822129
I0122 16:41:58.730341 56630 caffe_interface.cpp:125] Batch 106, top-1 = 0.72
I0122 16:41:58.730345 56630 caffe_interface.cpp:125] Batch 106, top-5 = 1
I0122 16:41:58.731568 56630 caffe_interface.cpp:125] Batch 107, loss = 1.12995
I0122 16:41:58.731576 56630 caffe_interface.cpp:125] Batch 107, top-1 = 0.66
I0122 16:41:58.731580 56630 caffe_interface.cpp:125] Batch 107, top-5 = 0.96
I0122 16:41:58.732777 56630 caffe_interface.cpp:125] Batch 108, loss = 0.80278
I0122 16:41:58.732784 56630 caffe_interface.cpp:125] Batch 108, top-1 = 0.76
I0122 16:41:58.732787 56630 caffe_interface.cpp:125] Batch 108, top-5 = 0.98
I0122 16:41:58.734009 56630 caffe_interface.cpp:125] Batch 109, loss = 0.690553
I0122 16:41:58.734015 56630 caffe_interface.cpp:125] Batch 109, top-1 = 0.76
I0122 16:41:58.734019 56630 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:41:58.735224 56630 caffe_interface.cpp:125] Batch 110, loss = 1.20881
I0122 16:41:58.735230 56630 caffe_interface.cpp:125] Batch 110, top-1 = 0.68
I0122 16:41:58.735234 56630 caffe_interface.cpp:125] Batch 110, top-5 = 0.96
I0122 16:41:58.736449 56630 caffe_interface.cpp:125] Batch 111, loss = 1.07828
I0122 16:41:58.736456 56630 caffe_interface.cpp:125] Batch 111, top-1 = 0.68
I0122 16:41:58.736459 56630 caffe_interface.cpp:125] Batch 111, top-5 = 0.96
I0122 16:41:58.737669 56630 caffe_interface.cpp:125] Batch 112, loss = 0.333964
I0122 16:41:58.737677 56630 caffe_interface.cpp:125] Batch 112, top-1 = 0.92
I0122 16:41:58.737680 56630 caffe_interface.cpp:125] Batch 112, top-5 = 1
I0122 16:41:58.738890 56630 caffe_interface.cpp:125] Batch 113, loss = 0.593689
I0122 16:41:58.738898 56630 caffe_interface.cpp:125] Batch 113, top-1 = 0.8
I0122 16:41:58.738901 56630 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:41:58.740109 56630 caffe_interface.cpp:125] Batch 114, loss = 1.07427
I0122 16:41:58.740116 56630 caffe_interface.cpp:125] Batch 114, top-1 = 0.64
I0122 16:41:58.740119 56630 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0122 16:41:58.741330 56630 caffe_interface.cpp:125] Batch 115, loss = 0.414345
I0122 16:41:58.741338 56630 caffe_interface.cpp:125] Batch 115, top-1 = 0.8
I0122 16:41:58.741340 56630 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:41:58.742553 56630 caffe_interface.cpp:125] Batch 116, loss = 1.01641
I0122 16:41:58.742561 56630 caffe_interface.cpp:125] Batch 116, top-1 = 0.62
I0122 16:41:58.742565 56630 caffe_interface.cpp:125] Batch 116, top-5 = 0.96
I0122 16:41:58.743778 56630 caffe_interface.cpp:125] Batch 117, loss = 0.903907
I0122 16:41:58.743786 56630 caffe_interface.cpp:125] Batch 117, top-1 = 0.78
I0122 16:41:58.743789 56630 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0122 16:41:58.744998 56630 caffe_interface.cpp:125] Batch 118, loss = 0.475207
I0122 16:41:58.745007 56630 caffe_interface.cpp:125] Batch 118, top-1 = 0.88
I0122 16:41:58.745008 56630 caffe_interface.cpp:125] Batch 118, top-5 = 0.98
I0122 16:41:58.746217 56630 caffe_interface.cpp:125] Batch 119, loss = 0.917094
I0122 16:41:58.746224 56630 caffe_interface.cpp:125] Batch 119, top-1 = 0.66
I0122 16:41:58.746227 56630 caffe_interface.cpp:125] Batch 119, top-5 = 1
I0122 16:41:58.747443 56630 caffe_interface.cpp:125] Batch 120, loss = 0.777952
I0122 16:41:58.747452 56630 caffe_interface.cpp:125] Batch 120, top-1 = 0.7
I0122 16:41:58.747454 56630 caffe_interface.cpp:125] Batch 120, top-5 = 0.98
I0122 16:41:58.749683 56630 caffe_interface.cpp:125] Batch 121, loss = 1.3366
I0122 16:41:58.749689 56630 caffe_interface.cpp:125] Batch 121, top-1 = 0.68
I0122 16:41:58.749693 56630 caffe_interface.cpp:125] Batch 121, top-5 = 0.96
I0122 16:41:58.751000 56630 caffe_interface.cpp:125] Batch 122, loss = 0.764489
I0122 16:41:58.751008 56630 caffe_interface.cpp:125] Batch 122, top-1 = 0.78
I0122 16:41:58.751009 56630 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:41:58.752374 56630 caffe_interface.cpp:125] Batch 123, loss = 0.387735
I0122 16:41:58.752387 56630 caffe_interface.cpp:125] Batch 123, top-1 = 0.84
I0122 16:41:58.752388 56630 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0122 16:41:58.753600 56630 caffe_interface.cpp:125] Batch 124, loss = 0.83036
I0122 16:41:58.753607 56630 caffe_interface.cpp:125] Batch 124, top-1 = 0.74
I0122 16:41:58.753609 56630 caffe_interface.cpp:125] Batch 124, top-5 = 0.98
I0122 16:41:58.754812 56630 caffe_interface.cpp:125] Batch 125, loss = 1.03038
I0122 16:41:58.754817 56630 caffe_interface.cpp:125] Batch 125, top-1 = 0.7
I0122 16:41:58.754822 56630 caffe_interface.cpp:125] Batch 125, top-5 = 0.96
I0122 16:41:58.756026 56630 caffe_interface.cpp:125] Batch 126, loss = 0.751266
I0122 16:41:58.756033 56630 caffe_interface.cpp:125] Batch 126, top-1 = 0.78
I0122 16:41:58.756036 56630 caffe_interface.cpp:125] Batch 126, top-5 = 0.98
I0122 16:41:58.757253 56630 caffe_interface.cpp:125] Batch 127, loss = 0.938758
I0122 16:41:58.757269 56630 caffe_interface.cpp:125] Batch 127, top-1 = 0.66
I0122 16:41:58.757272 56630 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:41:58.758477 56630 caffe_interface.cpp:125] Batch 128, loss = 0.583821
I0122 16:41:58.758486 56630 caffe_interface.cpp:125] Batch 128, top-1 = 0.8
I0122 16:41:58.758498 56630 caffe_interface.cpp:125] Batch 128, top-5 = 1
I0122 16:41:58.759708 56630 caffe_interface.cpp:125] Batch 129, loss = 0.912923
I0122 16:41:58.759716 56630 caffe_interface.cpp:125] Batch 129, top-1 = 0.72
I0122 16:41:58.759721 56630 caffe_interface.cpp:125] Batch 129, top-5 = 0.96
I0122 16:41:58.761173 56630 caffe_interface.cpp:125] Batch 130, loss = 1.08894
I0122 16:41:58.761180 56630 caffe_interface.cpp:125] Batch 130, top-1 = 0.68
I0122 16:41:58.761183 56630 caffe_interface.cpp:125] Batch 130, top-5 = 1
I0122 16:41:58.762396 56630 caffe_interface.cpp:125] Batch 131, loss = 1.1128
I0122 16:41:58.762403 56630 caffe_interface.cpp:125] Batch 131, top-1 = 0.68
I0122 16:41:58.762406 56630 caffe_interface.cpp:125] Batch 131, top-5 = 0.96
I0122 16:41:58.763620 56630 caffe_interface.cpp:125] Batch 132, loss = 1.05005
I0122 16:41:58.763628 56630 caffe_interface.cpp:125] Batch 132, top-1 = 0.76
I0122 16:41:58.763630 56630 caffe_interface.cpp:125] Batch 132, top-5 = 0.94
I0122 16:41:58.764832 56630 caffe_interface.cpp:125] Batch 133, loss = 0.74273
I0122 16:41:58.764838 56630 caffe_interface.cpp:125] Batch 133, top-1 = 0.82
I0122 16:41:58.764843 56630 caffe_interface.cpp:125] Batch 133, top-5 = 1
I0122 16:41:58.766063 56630 caffe_interface.cpp:125] Batch 134, loss = 0.858627
I0122 16:41:58.766072 56630 caffe_interface.cpp:125] Batch 134, top-1 = 0.74
I0122 16:41:58.766074 56630 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:41:58.767293 56630 caffe_interface.cpp:125] Batch 135, loss = 0.997471
I0122 16:41:58.767300 56630 caffe_interface.cpp:125] Batch 135, top-1 = 0.64
I0122 16:41:58.767303 56630 caffe_interface.cpp:125] Batch 135, top-5 = 0.96
I0122 16:41:58.768524 56630 caffe_interface.cpp:125] Batch 136, loss = 0.837175
I0122 16:41:58.768532 56630 caffe_interface.cpp:125] Batch 136, top-1 = 0.78
I0122 16:41:58.768534 56630 caffe_interface.cpp:125] Batch 136, top-5 = 0.98
I0122 16:41:58.769739 56630 caffe_interface.cpp:125] Batch 137, loss = 0.487919
I0122 16:41:58.769747 56630 caffe_interface.cpp:125] Batch 137, top-1 = 0.84
I0122 16:41:58.769750 56630 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:41:58.770956 56630 caffe_interface.cpp:125] Batch 138, loss = 1.21832
I0122 16:41:58.770963 56630 caffe_interface.cpp:125] Batch 138, top-1 = 0.66
I0122 16:41:58.770967 56630 caffe_interface.cpp:125] Batch 138, top-5 = 0.96
I0122 16:41:58.772178 56630 caffe_interface.cpp:125] Batch 139, loss = 0.902213
I0122 16:41:58.772186 56630 caffe_interface.cpp:125] Batch 139, top-1 = 0.74
I0122 16:41:58.772188 56630 caffe_interface.cpp:125] Batch 139, top-5 = 0.94
I0122 16:41:58.773393 56630 caffe_interface.cpp:125] Batch 140, loss = 0.933317
I0122 16:41:58.773401 56630 caffe_interface.cpp:125] Batch 140, top-1 = 0.78
I0122 16:41:58.773404 56630 caffe_interface.cpp:125] Batch 140, top-5 = 0.96
I0122 16:41:58.774607 56630 caffe_interface.cpp:125] Batch 141, loss = 0.781394
I0122 16:41:58.774614 56630 caffe_interface.cpp:125] Batch 141, top-1 = 0.82
I0122 16:41:58.774618 56630 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:41:58.775823 56630 caffe_interface.cpp:125] Batch 142, loss = 0.375486
I0122 16:41:58.775830 56630 caffe_interface.cpp:125] Batch 142, top-1 = 0.8
I0122 16:41:58.775833 56630 caffe_interface.cpp:125] Batch 142, top-5 = 1
I0122 16:41:58.777060 56630 caffe_interface.cpp:125] Batch 143, loss = 0.834314
I0122 16:41:58.777066 56630 caffe_interface.cpp:125] Batch 143, top-1 = 0.76
I0122 16:41:58.777070 56630 caffe_interface.cpp:125] Batch 143, top-5 = 0.96
I0122 16:41:58.778293 56630 caffe_interface.cpp:125] Batch 144, loss = 0.545845
I0122 16:41:58.778301 56630 caffe_interface.cpp:125] Batch 144, top-1 = 0.8
I0122 16:41:58.778304 56630 caffe_interface.cpp:125] Batch 144, top-5 = 0.98
I0122 16:41:58.779500 56630 caffe_interface.cpp:125] Batch 145, loss = 1.0571
I0122 16:41:58.779507 56630 caffe_interface.cpp:125] Batch 145, top-1 = 0.76
I0122 16:41:58.779512 56630 caffe_interface.cpp:125] Batch 145, top-5 = 0.94
I0122 16:41:58.780721 56630 caffe_interface.cpp:125] Batch 146, loss = 0.72291
I0122 16:41:58.780737 56630 caffe_interface.cpp:125] Batch 146, top-1 = 0.72
I0122 16:41:58.780740 56630 caffe_interface.cpp:125] Batch 146, top-5 = 1
I0122 16:41:58.781966 56630 caffe_interface.cpp:125] Batch 147, loss = 1.16816
I0122 16:41:58.781975 56630 caffe_interface.cpp:125] Batch 147, top-1 = 0.78
I0122 16:41:58.781977 56630 caffe_interface.cpp:125] Batch 147, top-5 = 0.98
I0122 16:41:58.784248 56630 caffe_interface.cpp:125] Batch 148, loss = 0.256151
I0122 16:41:58.784256 56630 caffe_interface.cpp:125] Batch 148, top-1 = 0.92
I0122 16:41:58.784260 56630 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:41:58.785629 56630 caffe_interface.cpp:125] Batch 149, loss = 0.805992
I0122 16:41:58.785641 56630 caffe_interface.cpp:125] Batch 149, top-1 = 0.76
I0122 16:41:58.785645 56630 caffe_interface.cpp:125] Batch 149, top-5 = 1
I0122 16:41:58.786876 56630 caffe_interface.cpp:125] Batch 150, loss = 0.425216
I0122 16:41:58.786885 56630 caffe_interface.cpp:125] Batch 150, top-1 = 0.86
I0122 16:41:58.786887 56630 caffe_interface.cpp:125] Batch 150, top-5 = 0.98
I0122 16:41:58.788089 56630 caffe_interface.cpp:125] Batch 151, loss = 0.687772
I0122 16:41:58.788101 56630 caffe_interface.cpp:125] Batch 151, top-1 = 0.78
I0122 16:41:58.788105 56630 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:41:58.789341 56630 caffe_interface.cpp:125] Batch 152, loss = 0.390491
I0122 16:41:58.789350 56630 caffe_interface.cpp:125] Batch 152, top-1 = 0.92
I0122 16:41:58.789352 56630 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0122 16:41:58.790568 56630 caffe_interface.cpp:125] Batch 153, loss = 0.576906
I0122 16:41:58.790576 56630 caffe_interface.cpp:125] Batch 153, top-1 = 0.82
I0122 16:41:58.790580 56630 caffe_interface.cpp:125] Batch 153, top-5 = 0.98
I0122 16:41:58.791798 56630 caffe_interface.cpp:125] Batch 154, loss = 0.632433
I0122 16:41:58.791805 56630 caffe_interface.cpp:125] Batch 154, top-1 = 0.8
I0122 16:41:58.791808 56630 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:41:58.793021 56630 caffe_interface.cpp:125] Batch 155, loss = 0.847576
I0122 16:41:58.793028 56630 caffe_interface.cpp:125] Batch 155, top-1 = 0.7
I0122 16:41:58.793031 56630 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:41:58.794493 56630 caffe_interface.cpp:125] Batch 156, loss = 0.983993
I0122 16:41:58.794502 56630 caffe_interface.cpp:125] Batch 156, top-1 = 0.7
I0122 16:41:58.794505 56630 caffe_interface.cpp:125] Batch 156, top-5 = 0.96
I0122 16:41:58.795725 56630 caffe_interface.cpp:125] Batch 157, loss = 0.69162
I0122 16:41:58.795733 56630 caffe_interface.cpp:125] Batch 157, top-1 = 0.8
I0122 16:41:58.795737 56630 caffe_interface.cpp:125] Batch 157, top-5 = 0.96
I0122 16:41:58.796957 56630 caffe_interface.cpp:125] Batch 158, loss = 0.30206
I0122 16:41:58.796965 56630 caffe_interface.cpp:125] Batch 158, top-1 = 0.9
I0122 16:41:58.796968 56630 caffe_interface.cpp:125] Batch 158, top-5 = 1
I0122 16:41:58.798182 56630 caffe_interface.cpp:125] Batch 159, loss = 1.05068
I0122 16:41:58.798197 56630 caffe_interface.cpp:125] Batch 159, top-1 = 0.64
I0122 16:41:58.798199 56630 caffe_interface.cpp:125] Batch 159, top-5 = 0.98
I0122 16:41:58.799414 56630 caffe_interface.cpp:125] Batch 160, loss = 0.704892
I0122 16:41:58.799422 56630 caffe_interface.cpp:125] Batch 160, top-1 = 0.74
I0122 16:41:58.799425 56630 caffe_interface.cpp:125] Batch 160, top-5 = 1
I0122 16:41:58.800628 56630 caffe_interface.cpp:125] Batch 161, loss = 0.910862
I0122 16:41:58.800635 56630 caffe_interface.cpp:125] Batch 161, top-1 = 0.76
I0122 16:41:58.800639 56630 caffe_interface.cpp:125] Batch 161, top-5 = 1
I0122 16:41:58.801843 56630 caffe_interface.cpp:125] Batch 162, loss = 0.70239
I0122 16:41:58.801851 56630 caffe_interface.cpp:125] Batch 162, top-1 = 0.86
I0122 16:41:58.801853 56630 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:41:58.803066 56630 caffe_interface.cpp:125] Batch 163, loss = 0.572294
I0122 16:41:58.803073 56630 caffe_interface.cpp:125] Batch 163, top-1 = 0.82
I0122 16:41:58.803076 56630 caffe_interface.cpp:125] Batch 163, top-5 = 1
I0122 16:41:58.804307 56630 caffe_interface.cpp:125] Batch 164, loss = 1.2258
I0122 16:41:58.804316 56630 caffe_interface.cpp:125] Batch 164, top-1 = 0.66
I0122 16:41:58.804319 56630 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:41:58.805526 56630 caffe_interface.cpp:125] Batch 165, loss = 0.558308
I0122 16:41:58.805534 56630 caffe_interface.cpp:125] Batch 165, top-1 = 0.82
I0122 16:41:58.805538 56630 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:41:58.806735 56630 caffe_interface.cpp:125] Batch 166, loss = 0.511176
I0122 16:41:58.806742 56630 caffe_interface.cpp:125] Batch 166, top-1 = 0.78
I0122 16:41:58.806751 56630 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:41:58.807976 56630 caffe_interface.cpp:125] Batch 167, loss = 1.09966
I0122 16:41:58.807982 56630 caffe_interface.cpp:125] Batch 167, top-1 = 0.68
I0122 16:41:58.807986 56630 caffe_interface.cpp:125] Batch 167, top-5 = 0.98
I0122 16:41:58.809190 56630 caffe_interface.cpp:125] Batch 168, loss = 0.680315
I0122 16:41:58.809198 56630 caffe_interface.cpp:125] Batch 168, top-1 = 0.82
I0122 16:41:58.809201 56630 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:41:58.810405 56630 caffe_interface.cpp:125] Batch 169, loss = 1.07608
I0122 16:41:58.810413 56630 caffe_interface.cpp:125] Batch 169, top-1 = 0.68
I0122 16:41:58.810417 56630 caffe_interface.cpp:125] Batch 169, top-5 = 0.96
I0122 16:41:58.811635 56630 caffe_interface.cpp:125] Batch 170, loss = 0.757275
I0122 16:41:58.811648 56630 caffe_interface.cpp:125] Batch 170, top-1 = 0.78
I0122 16:41:58.811651 56630 caffe_interface.cpp:125] Batch 170, top-5 = 0.96
I0122 16:41:58.812860 56630 caffe_interface.cpp:125] Batch 171, loss = 0.677582
I0122 16:41:58.812868 56630 caffe_interface.cpp:125] Batch 171, top-1 = 0.8
I0122 16:41:58.812871 56630 caffe_interface.cpp:125] Batch 171, top-5 = 0.98
I0122 16:41:58.814076 56630 caffe_interface.cpp:125] Batch 172, loss = 1.39852
I0122 16:41:58.814085 56630 caffe_interface.cpp:125] Batch 172, top-1 = 0.68
I0122 16:41:58.814087 56630 caffe_interface.cpp:125] Batch 172, top-5 = 0.94
I0122 16:41:58.815294 56630 caffe_interface.cpp:125] Batch 173, loss = 0.715361
I0122 16:41:58.815301 56630 caffe_interface.cpp:125] Batch 173, top-1 = 0.78
I0122 16:41:58.815305 56630 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0122 16:41:58.817523 56630 caffe_interface.cpp:125] Batch 174, loss = 0.597995
I0122 16:41:58.817531 56630 caffe_interface.cpp:125] Batch 174, top-1 = 0.74
I0122 16:41:58.817533 56630 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:41:58.819032 56630 caffe_interface.cpp:125] Batch 175, loss = 0.78244
I0122 16:41:58.819041 56630 caffe_interface.cpp:125] Batch 175, top-1 = 0.72
I0122 16:41:58.819043 56630 caffe_interface.cpp:125] Batch 175, top-5 = 1
I0122 16:41:58.820264 56630 caffe_interface.cpp:125] Batch 176, loss = 1.24657
I0122 16:41:58.820272 56630 caffe_interface.cpp:125] Batch 176, top-1 = 0.68
I0122 16:41:58.820276 56630 caffe_interface.cpp:125] Batch 176, top-5 = 0.96
I0122 16:41:58.821487 56630 caffe_interface.cpp:125] Batch 177, loss = 0.666318
I0122 16:41:58.821494 56630 caffe_interface.cpp:125] Batch 177, top-1 = 0.76
I0122 16:41:58.821498 56630 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0122 16:41:58.822729 56630 caffe_interface.cpp:125] Batch 178, loss = 0.796042
I0122 16:41:58.822737 56630 caffe_interface.cpp:125] Batch 178, top-1 = 0.74
I0122 16:41:58.822741 56630 caffe_interface.cpp:125] Batch 178, top-5 = 0.94
I0122 16:41:58.823962 56630 caffe_interface.cpp:125] Batch 179, loss = 0.731407
I0122 16:41:58.823969 56630 caffe_interface.cpp:125] Batch 179, top-1 = 0.72
I0122 16:41:58.823979 56630 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:41:58.823982 56630 caffe_interface.cpp:130] Loss: 0.789497
I0122 16:41:58.823989 56630 caffe_interface.cpp:142] loss = 0.789497 (* 1 = 0.789497 loss)
I0122 16:41:58.823994 56630 caffe_interface.cpp:142] top-1 = 0.763444
I0122 16:41:58.823998 56630 caffe_interface.cpp:142] top-5 = 0.982778
I0122 16:41:58.969804 56630 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/sparse.caffemodel
I0122 16:41:58.969844 56630 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.6:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.763444364    | -0.099222064   |
+-------------------------------------------------------------------+
| Weights        | 68389          | 32571          | -52.3739204%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 20594176       | -58.0170746%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config6.prototxt
## fine-tuning: 6-th run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config6.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune6_miniVggNet.txt
I0122 16:41:59.205444 57874 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0122 16:41:59.385984 57874 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:41:59.386494 57874 gpu_memory.cpp:55] Total memory: 25620447232, Free: 24900272128, dev_info[0]: total=25620447232 free=24900272128
I0122 16:41:59.386507 57874 caffe_interface.cpp:493] Using GPUs 0
I0122 16:41:59.386757 57874 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:41:59.971225 57874 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt"
type: "SGD"
I0122 16:41:59.971369 57874 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0122 16:41:59.971611 57874 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:41:59.971626 57874 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:41:59.971629 57874 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:41:59.971789 57874 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:41:59.971856 57874 layer_factory.hpp:77] Creating layer data
I0122 16:41:59.971947 57874 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:41:59.972347 57874 net.cpp:94] Creating Layer data
I0122 16:41:59.972363 57874 net.cpp:409] data -> data
I0122 16:41:59.972389 57874 net.cpp:409] data -> label
I0122 16:41:59.973819 57913 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:41:59.973866 57913 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:41:59.973971 57874 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:41:59.974123 57874 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:41:59.982205 57874 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:41:59.982254 57874 net.cpp:144] Setting up data
I0122 16:41:59.982264 57874 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:41:59.982269 57874 net.cpp:151] Top shape: 128 (128)
I0122 16:41:59.982271 57874 net.cpp:159] Memory required for data: 1573376
I0122 16:41:59.982275 57874 layer_factory.hpp:77] Creating layer conv1
I0122 16:41:59.982290 57874 net.cpp:94] Creating Layer conv1
I0122 16:41:59.982295 57874 net.cpp:435] conv1 <- data
I0122 16:41:59.982313 57874 net.cpp:409] conv1 -> conv1
I0122 16:41:59.983382 57874 net.cpp:144] Setting up conv1
I0122 16:41:59.983392 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.983394 57874 net.cpp:159] Memory required for data: 18350592
I0122 16:41:59.983408 57874 layer_factory.hpp:77] Creating layer bn1
I0122 16:41:59.983415 57874 net.cpp:94] Creating Layer bn1
I0122 16:41:59.983418 57874 net.cpp:435] bn1 <- conv1
I0122 16:41:59.983423 57874 net.cpp:409] bn1 -> scale1
I0122 16:41:59.984004 57874 net.cpp:144] Setting up bn1
I0122 16:41:59.984012 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.984014 57874 net.cpp:159] Memory required for data: 35127808
I0122 16:41:59.984025 57874 layer_factory.hpp:77] Creating layer relu1
I0122 16:41:59.984032 57874 net.cpp:94] Creating Layer relu1
I0122 16:41:59.984035 57874 net.cpp:435] relu1 <- scale1
I0122 16:41:59.984040 57874 net.cpp:409] relu1 -> relu1
I0122 16:41:59.984061 57874 net.cpp:144] Setting up relu1
I0122 16:41:59.984066 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.984069 57874 net.cpp:159] Memory required for data: 51905024
I0122 16:41:59.984071 57874 layer_factory.hpp:77] Creating layer conv2
I0122 16:41:59.984079 57874 net.cpp:94] Creating Layer conv2
I0122 16:41:59.984084 57874 net.cpp:435] conv2 <- relu1
I0122 16:41:59.984091 57874 net.cpp:409] conv2 -> conv2
I0122 16:41:59.985594 57874 net.cpp:144] Setting up conv2
I0122 16:41:59.985602 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.985605 57874 net.cpp:159] Memory required for data: 68682240
I0122 16:41:59.985612 57874 layer_factory.hpp:77] Creating layer bn2
I0122 16:41:59.985636 57874 net.cpp:94] Creating Layer bn2
I0122 16:41:59.985638 57874 net.cpp:435] bn2 <- conv2
I0122 16:41:59.985646 57874 net.cpp:409] bn2 -> scale2
I0122 16:41:59.986376 57874 net.cpp:144] Setting up bn2
I0122 16:41:59.986383 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.986387 57874 net.cpp:159] Memory required for data: 85459456
I0122 16:41:59.986395 57874 layer_factory.hpp:77] Creating layer relu2
I0122 16:41:59.986400 57874 net.cpp:94] Creating Layer relu2
I0122 16:41:59.986403 57874 net.cpp:435] relu2 <- scale2
I0122 16:41:59.986408 57874 net.cpp:409] relu2 -> relu2
I0122 16:41:59.986472 57874 net.cpp:144] Setting up relu2
I0122 16:41:59.986477 57874 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:41:59.986480 57874 net.cpp:159] Memory required for data: 102236672
I0122 16:41:59.986483 57874 layer_factory.hpp:77] Creating layer pool1
I0122 16:41:59.986490 57874 net.cpp:94] Creating Layer pool1
I0122 16:41:59.986491 57874 net.cpp:435] pool1 <- relu2
I0122 16:41:59.986497 57874 net.cpp:409] pool1 -> pool1
I0122 16:41:59.986539 57874 net.cpp:144] Setting up pool1
I0122 16:41:59.986546 57874 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:41:59.986548 57874 net.cpp:159] Memory required for data: 106430976
I0122 16:41:59.986552 57874 layer_factory.hpp:77] Creating layer drop1
I0122 16:41:59.986557 57874 net.cpp:94] Creating Layer drop1
I0122 16:41:59.986560 57874 net.cpp:435] drop1 <- pool1
I0122 16:41:59.986575 57874 net.cpp:409] drop1 -> drop1
I0122 16:41:59.986744 57874 net.cpp:144] Setting up drop1
I0122 16:41:59.986750 57874 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:41:59.986753 57874 net.cpp:159] Memory required for data: 110625280
I0122 16:41:59.986755 57874 layer_factory.hpp:77] Creating layer conv3
I0122 16:41:59.986765 57874 net.cpp:94] Creating Layer conv3
I0122 16:41:59.986769 57874 net.cpp:435] conv3 <- drop1
I0122 16:41:59.986776 57874 net.cpp:409] conv3 -> conv3
I0122 16:41:59.987800 57874 net.cpp:144] Setting up conv3
I0122 16:41:59.987812 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.987814 57874 net.cpp:159] Memory required for data: 119013888
I0122 16:41:59.987821 57874 layer_factory.hpp:77] Creating layer bn3
I0122 16:41:59.987829 57874 net.cpp:94] Creating Layer bn3
I0122 16:41:59.987834 57874 net.cpp:435] bn3 <- conv3
I0122 16:41:59.987841 57874 net.cpp:409] bn3 -> scale3
I0122 16:41:59.988466 57874 net.cpp:144] Setting up bn3
I0122 16:41:59.988472 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.988477 57874 net.cpp:159] Memory required for data: 127402496
I0122 16:41:59.988490 57874 layer_factory.hpp:77] Creating layer relu3
I0122 16:41:59.988497 57874 net.cpp:94] Creating Layer relu3
I0122 16:41:59.988499 57874 net.cpp:435] relu3 <- scale3
I0122 16:41:59.988503 57874 net.cpp:409] relu3 -> relu3
I0122 16:41:59.988536 57874 net.cpp:144] Setting up relu3
I0122 16:41:59.988543 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.988545 57874 net.cpp:159] Memory required for data: 135791104
I0122 16:41:59.988548 57874 layer_factory.hpp:77] Creating layer conv4
I0122 16:41:59.988556 57874 net.cpp:94] Creating Layer conv4
I0122 16:41:59.988561 57874 net.cpp:435] conv4 <- relu3
I0122 16:41:59.988567 57874 net.cpp:409] conv4 -> conv4
I0122 16:41:59.988989 57874 net.cpp:144] Setting up conv4
I0122 16:41:59.988996 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.989001 57874 net.cpp:159] Memory required for data: 144179712
I0122 16:41:59.989006 57874 layer_factory.hpp:77] Creating layer bn4
I0122 16:41:59.989013 57874 net.cpp:94] Creating Layer bn4
I0122 16:41:59.989017 57874 net.cpp:435] bn4 <- conv4
I0122 16:41:59.989022 57874 net.cpp:409] bn4 -> scale4
I0122 16:41:59.989706 57874 net.cpp:144] Setting up bn4
I0122 16:41:59.989712 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.989715 57874 net.cpp:159] Memory required for data: 152568320
I0122 16:41:59.989723 57874 layer_factory.hpp:77] Creating layer relu4
I0122 16:41:59.989729 57874 net.cpp:94] Creating Layer relu4
I0122 16:41:59.989732 57874 net.cpp:435] relu4 <- scale4
I0122 16:41:59.989737 57874 net.cpp:409] relu4 -> relu4
I0122 16:41:59.989755 57874 net.cpp:144] Setting up relu4
I0122 16:41:59.989761 57874 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:41:59.989763 57874 net.cpp:159] Memory required for data: 160956928
I0122 16:41:59.989766 57874 layer_factory.hpp:77] Creating layer pool2
I0122 16:41:59.989771 57874 net.cpp:94] Creating Layer pool2
I0122 16:41:59.989775 57874 net.cpp:435] pool2 <- relu4
I0122 16:41:59.989780 57874 net.cpp:409] pool2 -> pool2
I0122 16:41:59.989811 57874 net.cpp:144] Setting up pool2
I0122 16:41:59.989817 57874 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:41:59.989820 57874 net.cpp:159] Memory required for data: 163054080
I0122 16:41:59.989823 57874 layer_factory.hpp:77] Creating layer drop2
I0122 16:41:59.989830 57874 net.cpp:94] Creating Layer drop2
I0122 16:41:59.989835 57874 net.cpp:435] drop2 <- pool2
I0122 16:41:59.989840 57874 net.cpp:409] drop2 -> drop2
I0122 16:41:59.989868 57874 net.cpp:144] Setting up drop2
I0122 16:41:59.989873 57874 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:41:59.989877 57874 net.cpp:159] Memory required for data: 165151232
I0122 16:41:59.989879 57874 layer_factory.hpp:77] Creating layer fc1
I0122 16:41:59.989886 57874 net.cpp:94] Creating Layer fc1
I0122 16:41:59.989889 57874 net.cpp:435] fc1 <- drop2
I0122 16:41:59.989895 57874 net.cpp:409] fc1 -> fc1
I0122 16:42:00.004231 57874 net.cpp:144] Setting up fc1
I0122 16:42:00.004247 57874 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:42:00.004251 57874 net.cpp:159] Memory required for data: 165413376
I0122 16:42:00.004258 57874 layer_factory.hpp:77] Creating layer bn5
I0122 16:42:00.004267 57874 net.cpp:94] Creating Layer bn5
I0122 16:42:00.004271 57874 net.cpp:435] bn5 <- fc1
I0122 16:42:00.004277 57874 net.cpp:409] bn5 -> scale5
I0122 16:42:00.004835 57874 net.cpp:144] Setting up bn5
I0122 16:42:00.004842 57874 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:42:00.004844 57874 net.cpp:159] Memory required for data: 165675520
I0122 16:42:00.004859 57874 layer_factory.hpp:77] Creating layer relu5
I0122 16:42:00.004866 57874 net.cpp:94] Creating Layer relu5
I0122 16:42:00.004869 57874 net.cpp:435] relu5 <- scale5
I0122 16:42:00.004874 57874 net.cpp:409] relu5 -> relu5
I0122 16:42:00.004894 57874 net.cpp:144] Setting up relu5
I0122 16:42:00.004899 57874 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:42:00.004902 57874 net.cpp:159] Memory required for data: 165937664
I0122 16:42:00.004905 57874 layer_factory.hpp:77] Creating layer drop3
I0122 16:42:00.004909 57874 net.cpp:94] Creating Layer drop3
I0122 16:42:00.004915 57874 net.cpp:435] drop3 <- relu5
I0122 16:42:00.004921 57874 net.cpp:409] drop3 -> drop3
I0122 16:42:00.004951 57874 net.cpp:144] Setting up drop3
I0122 16:42:00.004956 57874 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:42:00.004959 57874 net.cpp:159] Memory required for data: 166199808
I0122 16:42:00.004961 57874 layer_factory.hpp:77] Creating layer fc2
I0122 16:42:00.004967 57874 net.cpp:94] Creating Layer fc2
I0122 16:42:00.004972 57874 net.cpp:435] fc2 <- drop3
I0122 16:42:00.004978 57874 net.cpp:409] fc2 -> fc2
I0122 16:42:00.005146 57874 net.cpp:144] Setting up fc2
I0122 16:42:00.005152 57874 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:42:00.005156 57874 net.cpp:159] Memory required for data: 166204928
I0122 16:42:00.005161 57874 layer_factory.hpp:77] Creating layer loss
I0122 16:42:00.005167 57874 net.cpp:94] Creating Layer loss
I0122 16:42:00.005169 57874 net.cpp:435] loss <- fc2
I0122 16:42:00.005173 57874 net.cpp:435] loss <- label
I0122 16:42:00.005178 57874 net.cpp:409] loss -> loss
I0122 16:42:00.005185 57874 layer_factory.hpp:77] Creating layer loss
I0122 16:42:00.005937 57874 net.cpp:144] Setting up loss
I0122 16:42:00.005949 57874 net.cpp:151] Top shape: (1)
I0122 16:42:00.005951 57874 net.cpp:154]     with loss weight 1
I0122 16:42:00.005960 57874 net.cpp:159] Memory required for data: 166204932
I0122 16:42:00.005964 57874 net.cpp:220] loss needs backward computation.
I0122 16:42:00.005976 57874 net.cpp:220] fc2 needs backward computation.
I0122 16:42:00.005980 57874 net.cpp:220] drop3 needs backward computation.
I0122 16:42:00.005983 57874 net.cpp:220] relu5 needs backward computation.
I0122 16:42:00.005986 57874 net.cpp:220] bn5 needs backward computation.
I0122 16:42:00.005990 57874 net.cpp:220] fc1 needs backward computation.
I0122 16:42:00.005992 57874 net.cpp:220] drop2 needs backward computation.
I0122 16:42:00.005995 57874 net.cpp:220] pool2 needs backward computation.
I0122 16:42:00.005998 57874 net.cpp:220] relu4 needs backward computation.
I0122 16:42:00.006001 57874 net.cpp:220] bn4 needs backward computation.
I0122 16:42:00.006006 57874 net.cpp:220] conv4 needs backward computation.
I0122 16:42:00.006008 57874 net.cpp:220] relu3 needs backward computation.
I0122 16:42:00.006011 57874 net.cpp:220] bn3 needs backward computation.
I0122 16:42:00.006014 57874 net.cpp:220] conv3 needs backward computation.
I0122 16:42:00.006018 57874 net.cpp:220] drop1 needs backward computation.
I0122 16:42:00.006021 57874 net.cpp:220] pool1 needs backward computation.
I0122 16:42:00.006026 57874 net.cpp:220] relu2 needs backward computation.
I0122 16:42:00.006029 57874 net.cpp:220] bn2 needs backward computation.
I0122 16:42:00.006033 57874 net.cpp:220] conv2 needs backward computation.
I0122 16:42:00.006036 57874 net.cpp:220] relu1 needs backward computation.
I0122 16:42:00.006050 57874 net.cpp:220] bn1 needs backward computation.
I0122 16:42:00.006053 57874 net.cpp:220] conv1 needs backward computation.
I0122 16:42:00.006058 57874 net.cpp:222] data does not need backward computation.
I0122 16:42:00.006062 57874 net.cpp:264] This network produces output loss
I0122 16:42:00.006084 57874 net.cpp:284] Network initialization done.
I0122 16:42:00.006391 57874 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/net_finetune.prototxt
I0122 16:42:00.006423 57874 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:42:00.006610 57874 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:42:00.006709 57874 layer_factory.hpp:77] Creating layer data
I0122 16:42:00.006752 57874 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:42:00.007163 57874 net.cpp:94] Creating Layer data
I0122 16:42:00.007171 57874 net.cpp:409] data -> data
I0122 16:42:00.007179 57874 net.cpp:409] data -> label
I0122 16:42:00.008355 57943 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:42:00.008388 57943 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:42:00.008471 57874 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:42:00.008566 57874 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:42:00.012707 57874 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:42:00.012756 57874 net.cpp:144] Setting up data
I0122 16:42:00.012766 57874 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:42:00.012771 57874 net.cpp:151] Top shape: 50 (50)
I0122 16:42:00.012774 57874 net.cpp:159] Memory required for data: 614600
I0122 16:42:00.012778 57874 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:42:00.012789 57874 net.cpp:94] Creating Layer label_data_1_split
I0122 16:42:00.012794 57874 net.cpp:435] label_data_1_split <- label
I0122 16:42:00.012801 57874 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:42:00.012811 57874 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:42:00.012817 57874 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:42:00.012912 57874 net.cpp:144] Setting up label_data_1_split
I0122 16:42:00.012917 57874 net.cpp:151] Top shape: 50 (50)
I0122 16:42:00.012922 57874 net.cpp:151] Top shape: 50 (50)
I0122 16:42:00.012924 57874 net.cpp:151] Top shape: 50 (50)
I0122 16:42:00.012928 57874 net.cpp:159] Memory required for data: 615200
I0122 16:42:00.012929 57874 layer_factory.hpp:77] Creating layer conv1
I0122 16:42:00.012941 57874 net.cpp:94] Creating Layer conv1
I0122 16:42:00.012945 57874 net.cpp:435] conv1 <- data
I0122 16:42:00.012951 57874 net.cpp:409] conv1 -> conv1
I0122 16:42:00.013296 57874 net.cpp:144] Setting up conv1
I0122 16:42:00.013303 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.013306 57874 net.cpp:159] Memory required for data: 7168800
I0122 16:42:00.013314 57874 layer_factory.hpp:77] Creating layer bn1
I0122 16:42:00.013324 57874 net.cpp:94] Creating Layer bn1
I0122 16:42:00.013330 57874 net.cpp:435] bn1 <- conv1
I0122 16:42:00.013336 57874 net.cpp:409] bn1 -> scale1
I0122 16:42:00.013979 57874 net.cpp:144] Setting up bn1
I0122 16:42:00.013986 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.013990 57874 net.cpp:159] Memory required for data: 13722400
I0122 16:42:00.014003 57874 layer_factory.hpp:77] Creating layer relu1
I0122 16:42:00.014009 57874 net.cpp:94] Creating Layer relu1
I0122 16:42:00.014012 57874 net.cpp:435] relu1 <- scale1
I0122 16:42:00.014019 57874 net.cpp:409] relu1 -> relu1
I0122 16:42:00.014036 57874 net.cpp:144] Setting up relu1
I0122 16:42:00.014041 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.014045 57874 net.cpp:159] Memory required for data: 20276000
I0122 16:42:00.014048 57874 layer_factory.hpp:77] Creating layer conv2
I0122 16:42:00.014056 57874 net.cpp:94] Creating Layer conv2
I0122 16:42:00.014061 57874 net.cpp:435] conv2 <- relu1
I0122 16:42:00.014067 57874 net.cpp:409] conv2 -> conv2
I0122 16:42:00.014343 57874 net.cpp:144] Setting up conv2
I0122 16:42:00.014350 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.014353 57874 net.cpp:159] Memory required for data: 26829600
I0122 16:42:00.014359 57874 layer_factory.hpp:77] Creating layer bn2
I0122 16:42:00.014369 57874 net.cpp:94] Creating Layer bn2
I0122 16:42:00.014372 57874 net.cpp:435] bn2 <- conv2
I0122 16:42:00.014379 57874 net.cpp:409] bn2 -> scale2
I0122 16:42:00.015393 57874 net.cpp:144] Setting up bn2
I0122 16:42:00.015400 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.015403 57874 net.cpp:159] Memory required for data: 33383200
I0122 16:42:00.015411 57874 layer_factory.hpp:77] Creating layer relu2
I0122 16:42:00.015416 57874 net.cpp:94] Creating Layer relu2
I0122 16:42:00.015419 57874 net.cpp:435] relu2 <- scale2
I0122 16:42:00.015424 57874 net.cpp:409] relu2 -> relu2
I0122 16:42:00.015477 57874 net.cpp:144] Setting up relu2
I0122 16:42:00.015482 57874 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:42:00.015486 57874 net.cpp:159] Memory required for data: 39936800
I0122 16:42:00.015488 57874 layer_factory.hpp:77] Creating layer pool1
I0122 16:42:00.015496 57874 net.cpp:94] Creating Layer pool1
I0122 16:42:00.015501 57874 net.cpp:435] pool1 <- relu2
I0122 16:42:00.015506 57874 net.cpp:409] pool1 -> pool1
I0122 16:42:00.015549 57874 net.cpp:144] Setting up pool1
I0122 16:42:00.015563 57874 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:42:00.015566 57874 net.cpp:159] Memory required for data: 41575200
I0122 16:42:00.015569 57874 layer_factory.hpp:77] Creating layer drop1
I0122 16:42:00.015575 57874 net.cpp:94] Creating Layer drop1
I0122 16:42:00.015578 57874 net.cpp:435] drop1 <- pool1
I0122 16:42:00.015583 57874 net.cpp:409] drop1 -> drop1
I0122 16:42:00.015637 57874 net.cpp:144] Setting up drop1
I0122 16:42:00.015643 57874 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:42:00.015645 57874 net.cpp:159] Memory required for data: 43213600
I0122 16:42:00.015647 57874 layer_factory.hpp:77] Creating layer conv3
I0122 16:42:00.015656 57874 net.cpp:94] Creating Layer conv3
I0122 16:42:00.015661 57874 net.cpp:435] conv3 <- drop1
I0122 16:42:00.015669 57874 net.cpp:409] conv3 -> conv3
I0122 16:42:00.016130 57874 net.cpp:144] Setting up conv3
I0122 16:42:00.016139 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.016142 57874 net.cpp:159] Memory required for data: 46490400
I0122 16:42:00.016147 57874 layer_factory.hpp:77] Creating layer bn3
I0122 16:42:00.016160 57874 net.cpp:94] Creating Layer bn3
I0122 16:42:00.016166 57874 net.cpp:435] bn3 <- conv3
I0122 16:42:00.016171 57874 net.cpp:409] bn3 -> scale3
I0122 16:42:00.016885 57874 net.cpp:144] Setting up bn3
I0122 16:42:00.016893 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.016896 57874 net.cpp:159] Memory required for data: 49767200
I0122 16:42:00.016907 57874 layer_factory.hpp:77] Creating layer relu3
I0122 16:42:00.016914 57874 net.cpp:94] Creating Layer relu3
I0122 16:42:00.016917 57874 net.cpp:435] relu3 <- scale3
I0122 16:42:00.016923 57874 net.cpp:409] relu3 -> relu3
I0122 16:42:00.016942 57874 net.cpp:144] Setting up relu3
I0122 16:42:00.016948 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.016952 57874 net.cpp:159] Memory required for data: 53044000
I0122 16:42:00.016954 57874 layer_factory.hpp:77] Creating layer conv4
I0122 16:42:00.016963 57874 net.cpp:94] Creating Layer conv4
I0122 16:42:00.016968 57874 net.cpp:435] conv4 <- relu3
I0122 16:42:00.016974 57874 net.cpp:409] conv4 -> conv4
I0122 16:42:00.017436 57874 net.cpp:144] Setting up conv4
I0122 16:42:00.017443 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.017446 57874 net.cpp:159] Memory required for data: 56320800
I0122 16:42:00.017452 57874 layer_factory.hpp:77] Creating layer bn4
I0122 16:42:00.017462 57874 net.cpp:94] Creating Layer bn4
I0122 16:42:00.017467 57874 net.cpp:435] bn4 <- conv4
I0122 16:42:00.017472 57874 net.cpp:409] bn4 -> scale4
I0122 16:42:00.018216 57874 net.cpp:144] Setting up bn4
I0122 16:42:00.018225 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.018229 57874 net.cpp:159] Memory required for data: 59597600
I0122 16:42:00.018236 57874 layer_factory.hpp:77] Creating layer relu4
I0122 16:42:00.018240 57874 net.cpp:94] Creating Layer relu4
I0122 16:42:00.018244 57874 net.cpp:435] relu4 <- scale4
I0122 16:42:00.018250 57874 net.cpp:409] relu4 -> relu4
I0122 16:42:00.018270 57874 net.cpp:144] Setting up relu4
I0122 16:42:00.018275 57874 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:42:00.018280 57874 net.cpp:159] Memory required for data: 62874400
I0122 16:42:00.018282 57874 layer_factory.hpp:77] Creating layer pool2
I0122 16:42:00.018290 57874 net.cpp:94] Creating Layer pool2
I0122 16:42:00.018292 57874 net.cpp:435] pool2 <- relu4
I0122 16:42:00.018298 57874 net.cpp:409] pool2 -> pool2
I0122 16:42:00.018333 57874 net.cpp:144] Setting up pool2
I0122 16:42:00.018338 57874 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:42:00.018342 57874 net.cpp:159] Memory required for data: 63693600
I0122 16:42:00.018344 57874 layer_factory.hpp:77] Creating layer drop2
I0122 16:42:00.018350 57874 net.cpp:94] Creating Layer drop2
I0122 16:42:00.018352 57874 net.cpp:435] drop2 <- pool2
I0122 16:42:00.018357 57874 net.cpp:409] drop2 -> drop2
I0122 16:42:00.018425 57874 net.cpp:144] Setting up drop2
I0122 16:42:00.018431 57874 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:42:00.018443 57874 net.cpp:159] Memory required for data: 64512800
I0122 16:42:00.018446 57874 layer_factory.hpp:77] Creating layer fc1
I0122 16:42:00.018453 57874 net.cpp:94] Creating Layer fc1
I0122 16:42:00.018457 57874 net.cpp:435] fc1 <- drop2
I0122 16:42:00.018463 57874 net.cpp:409] fc1 -> fc1
I0122 16:42:00.033149 57874 net.cpp:144] Setting up fc1
I0122 16:42:00.033167 57874 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:42:00.033170 57874 net.cpp:159] Memory required for data: 64615200
I0122 16:42:00.033176 57874 layer_factory.hpp:77] Creating layer bn5
I0122 16:42:00.033185 57874 net.cpp:94] Creating Layer bn5
I0122 16:42:00.033190 57874 net.cpp:435] bn5 <- fc1
I0122 16:42:00.033195 57874 net.cpp:409] bn5 -> scale5
I0122 16:42:00.033761 57874 net.cpp:144] Setting up bn5
I0122 16:42:00.033767 57874 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:42:00.033771 57874 net.cpp:159] Memory required for data: 64717600
I0122 16:42:00.033784 57874 layer_factory.hpp:77] Creating layer relu5
I0122 16:42:00.033792 57874 net.cpp:94] Creating Layer relu5
I0122 16:42:00.033794 57874 net.cpp:435] relu5 <- scale5
I0122 16:42:00.033799 57874 net.cpp:409] relu5 -> relu5
I0122 16:42:00.033818 57874 net.cpp:144] Setting up relu5
I0122 16:42:00.033823 57874 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:42:00.033825 57874 net.cpp:159] Memory required for data: 64820000
I0122 16:42:00.033829 57874 layer_factory.hpp:77] Creating layer drop3
I0122 16:42:00.033834 57874 net.cpp:94] Creating Layer drop3
I0122 16:42:00.033838 57874 net.cpp:435] drop3 <- relu5
I0122 16:42:00.033843 57874 net.cpp:409] drop3 -> drop3
I0122 16:42:00.033874 57874 net.cpp:144] Setting up drop3
I0122 16:42:00.033881 57874 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:42:00.033884 57874 net.cpp:159] Memory required for data: 64922400
I0122 16:42:00.033886 57874 layer_factory.hpp:77] Creating layer fc2
I0122 16:42:00.033892 57874 net.cpp:94] Creating Layer fc2
I0122 16:42:00.033897 57874 net.cpp:435] fc2 <- drop3
I0122 16:42:00.033910 57874 net.cpp:409] fc2 -> fc2
I0122 16:42:00.034054 57874 net.cpp:144] Setting up fc2
I0122 16:42:00.034060 57874 net.cpp:151] Top shape: 50 10 (500)
I0122 16:42:00.034061 57874 net.cpp:159] Memory required for data: 64924400
I0122 16:42:00.034066 57874 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:42:00.034071 57874 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:42:00.034075 57874 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:42:00.034080 57874 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:42:00.034087 57874 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:42:00.034092 57874 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:42:00.034134 57874 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:42:00.034139 57874 net.cpp:151] Top shape: 50 10 (500)
I0122 16:42:00.034143 57874 net.cpp:151] Top shape: 50 10 (500)
I0122 16:42:00.034145 57874 net.cpp:151] Top shape: 50 10 (500)
I0122 16:42:00.034148 57874 net.cpp:159] Memory required for data: 64930400
I0122 16:42:00.034152 57874 layer_factory.hpp:77] Creating layer loss
I0122 16:42:00.034158 57874 net.cpp:94] Creating Layer loss
I0122 16:42:00.034162 57874 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:42:00.034165 57874 net.cpp:435] loss <- label_data_1_split_0
I0122 16:42:00.034170 57874 net.cpp:409] loss -> loss
I0122 16:42:00.034178 57874 layer_factory.hpp:77] Creating layer loss
I0122 16:42:00.034253 57874 net.cpp:144] Setting up loss
I0122 16:42:00.034260 57874 net.cpp:151] Top shape: (1)
I0122 16:42:00.034261 57874 net.cpp:154]     with loss weight 1
I0122 16:42:00.034272 57874 net.cpp:159] Memory required for data: 64930404
I0122 16:42:00.034276 57874 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:42:00.034282 57874 net.cpp:94] Creating Layer accuracy-top1
I0122 16:42:00.034287 57874 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:42:00.034291 57874 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:42:00.034296 57874 net.cpp:409] accuracy-top1 -> top-1
I0122 16:42:00.034315 57874 net.cpp:144] Setting up accuracy-top1
I0122 16:42:00.034318 57874 net.cpp:151] Top shape: (1)
I0122 16:42:00.034322 57874 net.cpp:159] Memory required for data: 64930408
I0122 16:42:00.034323 57874 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:42:00.034330 57874 net.cpp:94] Creating Layer accuracy-top5
I0122 16:42:00.034333 57874 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:42:00.034337 57874 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:42:00.034341 57874 net.cpp:409] accuracy-top5 -> top-5
I0122 16:42:00.034350 57874 net.cpp:144] Setting up accuracy-top5
I0122 16:42:00.034353 57874 net.cpp:151] Top shape: (1)
I0122 16:42:00.034355 57874 net.cpp:159] Memory required for data: 64930412
I0122 16:42:00.034358 57874 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:42:00.034363 57874 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:42:00.034365 57874 net.cpp:220] loss needs backward computation.
I0122 16:42:00.034369 57874 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:42:00.034373 57874 net.cpp:220] fc2 needs backward computation.
I0122 16:42:00.034376 57874 net.cpp:220] drop3 needs backward computation.
I0122 16:42:00.034379 57874 net.cpp:220] relu5 needs backward computation.
I0122 16:42:00.034382 57874 net.cpp:220] bn5 needs backward computation.
I0122 16:42:00.034385 57874 net.cpp:220] fc1 needs backward computation.
I0122 16:42:00.034389 57874 net.cpp:220] drop2 needs backward computation.
I0122 16:42:00.034391 57874 net.cpp:220] pool2 needs backward computation.
I0122 16:42:00.034395 57874 net.cpp:220] relu4 needs backward computation.
I0122 16:42:00.034399 57874 net.cpp:220] bn4 needs backward computation.
I0122 16:42:00.034401 57874 net.cpp:220] conv4 needs backward computation.
I0122 16:42:00.034405 57874 net.cpp:220] relu3 needs backward computation.
I0122 16:42:00.034407 57874 net.cpp:220] bn3 needs backward computation.
I0122 16:42:00.034410 57874 net.cpp:220] conv3 needs backward computation.
I0122 16:42:00.034415 57874 net.cpp:220] drop1 needs backward computation.
I0122 16:42:00.034417 57874 net.cpp:220] pool1 needs backward computation.
I0122 16:42:00.034420 57874 net.cpp:220] relu2 needs backward computation.
I0122 16:42:00.034422 57874 net.cpp:220] bn2 needs backward computation.
I0122 16:42:00.034426 57874 net.cpp:220] conv2 needs backward computation.
I0122 16:42:00.034430 57874 net.cpp:220] relu1 needs backward computation.
I0122 16:42:00.034431 57874 net.cpp:220] bn1 needs backward computation.
I0122 16:42:00.034435 57874 net.cpp:220] conv1 needs backward computation.
I0122 16:42:00.034440 57874 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:42:00.034445 57874 net.cpp:222] data does not need backward computation.
I0122 16:42:00.034447 57874 net.cpp:264] This network produces output loss
I0122 16:42:00.034451 57874 net.cpp:264] This network produces output top-1
I0122 16:42:00.034454 57874 net.cpp:264] This network produces output top-5
I0122 16:42:00.034476 57874 net.cpp:284] Network initialization done.
I0122 16:42:00.034582 57874 solver.cpp:63] Solver scaffolding done.
I0122 16:42:00.035763 57874 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/sparse.caffemodel
I0122 16:42:00.095844 57874 caffe_interface.cpp:527] Starting Optimization
I0122 16:42:00.095862 57874 solver.cpp:335] Solving 
I0122 16:42:00.095865 57874 solver.cpp:336] Learning Rate Policy: poly
I0122 16:42:00.097093 57874 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:42:00.327097 57874 solver.cpp:517]     Test net output #0: loss = 0.789497 (* 1 = 0.789497 loss)
I0122 16:42:00.327116 57874 solver.cpp:517]     Test net output #1: top-1 = 0.763444
I0122 16:42:00.327121 57874 solver.cpp:517]     Test net output #2: top-5 = 0.982778
I0122 16:42:00.344452 57874 solver.cpp:266] Iteration 0 (0 iter/s, 0.248524s/100 iter), loss = 0.276002
I0122 16:42:00.344485 57874 solver.cpp:285]     Train net output #0: loss = 0.276002 (* 1 = 0.276002 loss)
I0122 16:42:00.344511 57874 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:42:01.210520 57874 solver.cpp:266] Iteration 100 (115.474 iter/s, 0.865996s/100 iter), loss = 0.375994
I0122 16:42:01.210546 57874 solver.cpp:285]     Train net output #0: loss = 0.375994 (* 1 = 0.375994 loss)
I0122 16:42:01.210553 57874 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:42:02.075150 57874 solver.cpp:266] Iteration 200 (115.665 iter/s, 0.864566s/100 iter), loss = 0.337961
I0122 16:42:02.075176 57874 solver.cpp:285]     Train net output #0: loss = 0.337961 (* 1 = 0.337961 loss)
I0122 16:42:02.075181 57874 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:42:02.938602 57874 solver.cpp:266] Iteration 300 (115.823 iter/s, 0.863388s/100 iter), loss = 0.15057
I0122 16:42:02.938628 57874 solver.cpp:285]     Train net output #0: loss = 0.15057 (* 1 = 0.15057 loss)
I0122 16:42:02.938633 57874 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:42:03.802310 57874 solver.cpp:266] Iteration 400 (115.789 iter/s, 0.863643s/100 iter), loss = 0.281082
I0122 16:42:03.802336 57874 solver.cpp:285]     Train net output #0: loss = 0.281082 (* 1 = 0.281082 loss)
I0122 16:42:03.802342 57874 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:42:04.666074 57874 solver.cpp:266] Iteration 500 (115.781 iter/s, 0.863699s/100 iter), loss = 0.248346
I0122 16:42:04.666100 57874 solver.cpp:285]     Train net output #0: loss = 0.248346 (* 1 = 0.248346 loss)
I0122 16:42:04.666105 57874 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:42:05.530020 57874 solver.cpp:266] Iteration 600 (115.757 iter/s, 0.863882s/100 iter), loss = 0.23491
I0122 16:42:05.530045 57874 solver.cpp:285]     Train net output #0: loss = 0.23491 (* 1 = 0.23491 loss)
I0122 16:42:05.530050 57874 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:42:06.394217 57874 solver.cpp:266] Iteration 700 (115.723 iter/s, 0.864134s/100 iter), loss = 0.436264
I0122 16:42:06.394243 57874 solver.cpp:285]     Train net output #0: loss = 0.436264 (* 1 = 0.436264 loss)
I0122 16:42:06.394248 57874 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:42:07.256894 57874 solver.cpp:266] Iteration 800 (115.927 iter/s, 0.862613s/100 iter), loss = 0.279827
I0122 16:42:07.256919 57874 solver.cpp:285]     Train net output #0: loss = 0.279827 (* 1 = 0.279827 loss)
I0122 16:42:07.256925 57874 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:42:08.121361 57874 solver.cpp:266] Iteration 900 (115.687 iter/s, 0.864403s/100 iter), loss = 0.304121
I0122 16:42:08.121384 57874 solver.cpp:285]     Train net output #0: loss = 0.304121 (* 1 = 0.304121 loss)
I0122 16:42:08.121392 57874 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:42:08.977627 57874 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:42:09.197939 57874 solver.cpp:517]     Test net output #0: loss = 1.00773 (* 1 = 1.00773 loss)
I0122 16:42:09.197958 57874 solver.cpp:517]     Test net output #1: top-1 = 0.752889
I0122 16:42:09.197963 57874 solver.cpp:517]     Test net output #2: top-5 = 0.983112
I0122 16:42:09.206013 57874 solver.cpp:266] Iteration 1000 (92.2012 iter/s, 1.08458s/100 iter), loss = 0.257002
I0122 16:42:09.206029 57874 solver.cpp:285]     Train net output #0: loss = 0.257002 (* 1 = 0.257002 loss)
I0122 16:42:09.206035 57874 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:42:10.068348 57874 solver.cpp:266] Iteration 1100 (115.972 iter/s, 0.862281s/100 iter), loss = 0.338492
I0122 16:42:10.068373 57874 solver.cpp:285]     Train net output #0: loss = 0.338492 (* 1 = 0.338492 loss)
I0122 16:42:10.068378 57874 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:42:10.930969 57874 solver.cpp:266] Iteration 1200 (115.934 iter/s, 0.862559s/100 iter), loss = 0.283078
I0122 16:42:10.931005 57874 solver.cpp:285]     Train net output #0: loss = 0.283078 (* 1 = 0.283078 loss)
I0122 16:42:10.931028 57874 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:42:11.793860 57874 solver.cpp:266] Iteration 1300 (115.899 iter/s, 0.862817s/100 iter), loss = 0.339905
I0122 16:42:11.793884 57874 solver.cpp:285]     Train net output #0: loss = 0.339905 (* 1 = 0.339905 loss)
I0122 16:42:11.793922 57874 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:42:12.657171 57874 solver.cpp:266] Iteration 1400 (115.841 iter/s, 0.863249s/100 iter), loss = 0.239724
I0122 16:42:12.657196 57874 solver.cpp:285]     Train net output #0: loss = 0.239724 (* 1 = 0.239724 loss)
I0122 16:42:12.657202 57874 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:42:13.520956 57874 solver.cpp:266] Iteration 1500 (115.778 iter/s, 0.863721s/100 iter), loss = 0.266785
I0122 16:42:13.520980 57874 solver.cpp:285]     Train net output #0: loss = 0.266785 (* 1 = 0.266785 loss)
I0122 16:42:13.520987 57874 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:42:14.385175 57874 solver.cpp:266] Iteration 1600 (115.72 iter/s, 0.864157s/100 iter), loss = 0.345328
I0122 16:42:14.385201 57874 solver.cpp:285]     Train net output #0: loss = 0.345328 (* 1 = 0.345328 loss)
I0122 16:42:14.385207 57874 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:42:15.252800 57874 solver.cpp:266] Iteration 1700 (115.266 iter/s, 0.867561s/100 iter), loss = 0.290098
I0122 16:42:15.252826 57874 solver.cpp:285]     Train net output #0: loss = 0.290098 (* 1 = 0.290098 loss)
I0122 16:42:15.252833 57874 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:42:16.118759 57874 solver.cpp:266] Iteration 1800 (115.487 iter/s, 0.865895s/100 iter), loss = 0.30044
I0122 16:42:16.118785 57874 solver.cpp:285]     Train net output #0: loss = 0.30044 (* 1 = 0.30044 loss)
I0122 16:42:16.118790 57874 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:42:16.983278 57874 solver.cpp:266] Iteration 1900 (115.68 iter/s, 0.864453s/100 iter), loss = 0.183299
I0122 16:42:16.983302 57874 solver.cpp:285]     Train net output #0: loss = 0.183299 (* 1 = 0.183299 loss)
I0122 16:42:16.983309 57874 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:42:17.839601 57874 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:42:18.059362 57874 solver.cpp:517]     Test net output #0: loss = 0.711648 (* 1 = 0.711648 loss)
I0122 16:42:18.059377 57874 solver.cpp:517]     Test net output #1: top-1 = 0.803333
I0122 16:42:18.059381 57874 solver.cpp:517]     Test net output #2: top-5 = 0.984334
I0122 16:42:18.067502 57874 solver.cpp:266] Iteration 2000 (92.2375 iter/s, 1.08416s/100 iter), loss = 0.233886
I0122 16:42:18.067523 57874 solver.cpp:285]     Train net output #0: loss = 0.233886 (* 1 = 0.233886 loss)
I0122 16:42:18.067529 57874 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:42:18.930608 57874 solver.cpp:266] Iteration 2100 (115.868 iter/s, 0.863048s/100 iter), loss = 0.234246
I0122 16:42:18.930632 57874 solver.cpp:285]     Train net output #0: loss = 0.234246 (* 1 = 0.234246 loss)
I0122 16:42:18.930639 57874 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:42:19.795969 57874 solver.cpp:266] Iteration 2200 (115.567 iter/s, 0.865296s/100 iter), loss = 0.200136
I0122 16:42:19.795994 57874 solver.cpp:285]     Train net output #0: loss = 0.200136 (* 1 = 0.200136 loss)
I0122 16:42:19.795998 57874 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:42:20.660615 57874 solver.cpp:266] Iteration 2300 (115.663 iter/s, 0.864584s/100 iter), loss = 0.36518
I0122 16:42:20.660640 57874 solver.cpp:285]     Train net output #0: loss = 0.36518 (* 1 = 0.36518 loss)
I0122 16:42:20.660645 57874 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:42:21.523713 57874 solver.cpp:266] Iteration 2400 (115.87 iter/s, 0.863035s/100 iter), loss = 0.317608
I0122 16:42:21.523738 57874 solver.cpp:285]     Train net output #0: loss = 0.317608 (* 1 = 0.317608 loss)
I0122 16:42:21.523743 57874 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:42:22.386940 57874 solver.cpp:266] Iteration 2500 (115.853 iter/s, 0.863164s/100 iter), loss = 0.30621
I0122 16:42:22.386965 57874 solver.cpp:285]     Train net output #0: loss = 0.30621 (* 1 = 0.30621 loss)
I0122 16:42:22.386970 57874 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:42:23.257138 57874 solver.cpp:266] Iteration 2600 (114.925 iter/s, 0.870134s/100 iter), loss = 0.227911
I0122 16:42:23.257177 57874 solver.cpp:285]     Train net output #0: loss = 0.227911 (* 1 = 0.227911 loss)
I0122 16:42:23.257184 57874 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:42:24.272734 57874 solver.cpp:266] Iteration 2700 (98.4725 iter/s, 1.01551s/100 iter), loss = 0.361492
I0122 16:42:24.272768 57874 solver.cpp:285]     Train net output #0: loss = 0.361492 (* 1 = 0.361492 loss)
I0122 16:42:24.272775 57874 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:42:25.257289 57874 solver.cpp:266] Iteration 2800 (101.577 iter/s, 0.984476s/100 iter), loss = 0.19995
I0122 16:42:25.257323 57874 solver.cpp:285]     Train net output #0: loss = 0.19995 (* 1 = 0.19995 loss)
I0122 16:42:25.257329 57874 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:42:26.247833 57874 solver.cpp:266] Iteration 2900 (100.963 iter/s, 0.990466s/100 iter), loss = 0.265581
I0122 16:42:26.247864 57874 solver.cpp:285]     Train net output #0: loss = 0.265581 (* 1 = 0.265581 loss)
I0122 16:42:26.247870 57874 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:42:27.432679 57874 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:42:27.744577 57874 solver.cpp:517]     Test net output #0: loss = 0.554016 (* 1 = 0.554016 loss)
I0122 16:42:27.744596 57874 solver.cpp:517]     Test net output #1: top-1 = 0.832444
I0122 16:42:27.744601 57874 solver.cpp:517]     Test net output #2: top-5 = 0.988778
I0122 16:42:27.759680 57874 solver.cpp:266] Iteration 3000 (66.1482 iter/s, 1.51176s/100 iter), loss = 0.251635
I0122 16:42:27.759704 57874 solver.cpp:285]     Train net output #0: loss = 0.251635 (* 1 = 0.251635 loss)
I0122 16:42:27.759711 57874 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:42:28.974957 57874 solver.cpp:266] Iteration 3100 (82.291 iter/s, 1.2152s/100 iter), loss = 0.199793
I0122 16:42:28.974992 57874 solver.cpp:285]     Train net output #0: loss = 0.199793 (* 1 = 0.199793 loss)
I0122 16:42:28.974997 57874 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:42:30.208642 57874 solver.cpp:266] Iteration 3200 (81.0636 iter/s, 1.2336s/100 iter), loss = 0.366983
I0122 16:42:30.208824 57874 solver.cpp:285]     Train net output #0: loss = 0.366983 (* 1 = 0.366983 loss)
I0122 16:42:30.208833 57874 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:42:31.376449 57874 solver.cpp:266] Iteration 3300 (85.6475 iter/s, 1.16758s/100 iter), loss = 0.170683
I0122 16:42:31.376502 57874 solver.cpp:285]     Train net output #0: loss = 0.170683 (* 1 = 0.170683 loss)
I0122 16:42:31.378695 57874 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:42:32.584012 57874 solver.cpp:266] Iteration 3400 (82.9692 iter/s, 1.20527s/100 iter), loss = 0.397574
I0122 16:42:32.584062 57874 solver.cpp:285]     Train net output #0: loss = 0.397574 (* 1 = 0.397574 loss)
I0122 16:42:32.584069 57874 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:42:33.770553 57874 solver.cpp:266] Iteration 3500 (84.2857 iter/s, 1.18644s/100 iter), loss = 0.293329
I0122 16:42:33.770596 57874 solver.cpp:285]     Train net output #0: loss = 0.293329 (* 1 = 0.293329 loss)
I0122 16:42:33.770604 57874 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:42:34.968096 57874 solver.cpp:266] Iteration 3600 (83.5109 iter/s, 1.19745s/100 iter), loss = 0.33977
I0122 16:42:34.968139 57874 solver.cpp:285]     Train net output #0: loss = 0.33977 (* 1 = 0.33977 loss)
I0122 16:42:34.968145 57874 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:42:36.153198 57874 solver.cpp:266] Iteration 3700 (84.3876 iter/s, 1.18501s/100 iter), loss = 0.224751
I0122 16:42:36.153241 57874 solver.cpp:285]     Train net output #0: loss = 0.224751 (* 1 = 0.224751 loss)
I0122 16:42:36.153249 57874 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:42:37.349978 57874 solver.cpp:266] Iteration 3800 (83.5642 iter/s, 1.19669s/100 iter), loss = 0.242638
I0122 16:42:37.350020 57874 solver.cpp:285]     Train net output #0: loss = 0.242638 (* 1 = 0.242638 loss)
I0122 16:42:37.350028 57874 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:42:38.803186 57874 solver.cpp:266] Iteration 3900 (68.8181 iter/s, 1.45311s/100 iter), loss = 0.235071
I0122 16:42:38.803228 57874 solver.cpp:285]     Train net output #0: loss = 0.235072 (* 1 = 0.235072 loss)
I0122 16:42:38.803234 57874 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:42:40.225556 57874 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:42:40.625288 57874 solver.cpp:517]     Test net output #0: loss = 0.526889 (* 1 = 0.526889 loss)
I0122 16:42:40.625304 57874 solver.cpp:517]     Test net output #1: top-1 = 0.833334
I0122 16:42:40.625308 57874 solver.cpp:517]     Test net output #2: top-5 = 0.989445
I0122 16:42:40.640898 57874 solver.cpp:266] Iteration 4000 (54.4188 iter/s, 1.8376s/100 iter), loss = 0.234443
I0122 16:42:40.640920 57874 solver.cpp:285]     Train net output #0: loss = 0.234443 (* 1 = 0.234443 loss)
I0122 16:42:40.640928 57874 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:42:42.091497 57874 solver.cpp:266] Iteration 4100 (68.941 iter/s, 1.45052s/100 iter), loss = 0.263465
I0122 16:42:42.091537 57874 solver.cpp:285]     Train net output #0: loss = 0.263465 (* 1 = 0.263465 loss)
I0122 16:42:42.091545 57874 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:42:43.583541 57874 solver.cpp:266] Iteration 4200 (67.0267 iter/s, 1.49194s/100 iter), loss = 0.207384
I0122 16:42:43.583583 57874 solver.cpp:285]     Train net output #0: loss = 0.207384 (* 1 = 0.207384 loss)
I0122 16:42:43.583590 57874 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:42:45.070564 57874 solver.cpp:266] Iteration 4300 (67.2531 iter/s, 1.48692s/100 iter), loss = 0.267472
I0122 16:42:45.070602 57874 solver.cpp:285]     Train net output #0: loss = 0.267472 (* 1 = 0.267472 loss)
I0122 16:42:45.070626 57874 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:42:46.541132 57874 solver.cpp:266] Iteration 4400 (68.0057 iter/s, 1.47046s/100 iter), loss = 0.287119
I0122 16:42:46.541174 57874 solver.cpp:285]     Train net output #0: loss = 0.287119 (* 1 = 0.287119 loss)
I0122 16:42:46.541182 57874 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:42:48.003718 57874 solver.cpp:266] Iteration 4500 (68.3769 iter/s, 1.46248s/100 iter), loss = 0.327167
I0122 16:42:48.003780 57874 solver.cpp:285]     Train net output #0: loss = 0.327167 (* 1 = 0.327167 loss)
I0122 16:42:48.003788 57874 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:42:49.475953 57874 solver.cpp:266] Iteration 4600 (67.9296 iter/s, 1.47211s/100 iter), loss = 0.200019
I0122 16:42:49.475993 57874 solver.cpp:285]     Train net output #0: loss = 0.200019 (* 1 = 0.200019 loss)
I0122 16:42:49.475999 57874 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:42:50.959527 57874 solver.cpp:266] Iteration 4700 (67.4094 iter/s, 1.48347s/100 iter), loss = 0.293756
I0122 16:42:50.959566 57874 solver.cpp:285]     Train net output #0: loss = 0.293756 (* 1 = 0.293756 loss)
I0122 16:42:50.959573 57874 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:42:52.418335 57874 solver.cpp:266] Iteration 4800 (68.5537 iter/s, 1.45871s/100 iter), loss = 0.246348
I0122 16:42:52.418377 57874 solver.cpp:285]     Train net output #0: loss = 0.246348 (* 1 = 0.246348 loss)
I0122 16:42:52.418383 57874 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:42:53.871351 57874 solver.cpp:266] Iteration 4900 (68.8272 iter/s, 1.45291s/100 iter), loss = 0.452768
I0122 16:42:53.871392 57874 solver.cpp:285]     Train net output #0: loss = 0.452769 (* 1 = 0.452769 loss)
I0122 16:42:53.871435 57874 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:42:55.322398 57874 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:42:55.722846 57874 solver.cpp:517]     Test net output #0: loss = 0.531827 (* 1 = 0.531827 loss)
I0122 16:42:55.722864 57874 solver.cpp:517]     Test net output #1: top-1 = 0.827444
I0122 16:42:55.722868 57874 solver.cpp:517]     Test net output #2: top-5 = 0.990111
I0122 16:42:55.731616 57874 solver.cpp:266] Iteration 5000 (53.7603 iter/s, 1.86011s/100 iter), loss = 0.295503
I0122 16:42:55.731647 57874 solver.cpp:285]     Train net output #0: loss = 0.295503 (* 1 = 0.295503 loss)
I0122 16:42:55.731654 57874 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:42:57.213299 57874 solver.cpp:266] Iteration 5100 (67.4952 iter/s, 1.48159s/100 iter), loss = 0.341347
I0122 16:42:57.213330 57874 solver.cpp:285]     Train net output #0: loss = 0.341347 (* 1 = 0.341347 loss)
I0122 16:42:57.213335 57874 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:42:58.685997 57874 solver.cpp:266] Iteration 5200 (67.9068 iter/s, 1.47261s/100 iter), loss = 0.247125
I0122 16:42:58.686028 57874 solver.cpp:285]     Train net output #0: loss = 0.247125 (* 1 = 0.247125 loss)
I0122 16:42:58.686033 57874 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:43:00.150429 57874 solver.cpp:266] Iteration 5300 (68.2902 iter/s, 1.46434s/100 iter), loss = 0.371922
I0122 16:43:00.150458 57874 solver.cpp:285]     Train net output #0: loss = 0.371922 (* 1 = 0.371922 loss)
I0122 16:43:00.150465 57874 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:43:01.630225 57874 solver.cpp:266] Iteration 5400 (67.5809 iter/s, 1.47971s/100 iter), loss = 0.221572
I0122 16:43:01.630390 57874 solver.cpp:285]     Train net output #0: loss = 0.221572 (* 1 = 0.221572 loss)
I0122 16:43:01.630398 57874 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:43:03.092815 57874 solver.cpp:266] Iteration 5500 (68.3822 iter/s, 1.46237s/100 iter), loss = 0.283207
I0122 16:43:03.092844 57874 solver.cpp:285]     Train net output #0: loss = 0.283207 (* 1 = 0.283207 loss)
I0122 16:43:03.092849 57874 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:43:04.562320 57874 solver.cpp:266] Iteration 5600 (68.0542 iter/s, 1.46942s/100 iter), loss = 0.205206
I0122 16:43:04.562362 57874 solver.cpp:285]     Train net output #0: loss = 0.205206 (* 1 = 0.205206 loss)
I0122 16:43:04.562369 57874 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:43:06.004587 57874 solver.cpp:266] Iteration 5700 (69.3402 iter/s, 1.44217s/100 iter), loss = 0.254429
I0122 16:43:06.004626 57874 solver.cpp:285]     Train net output #0: loss = 0.254429 (* 1 = 0.254429 loss)
I0122 16:43:06.005349 57874 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:43:07.445780 57874 solver.cpp:266] Iteration 5800 (69.4265 iter/s, 1.44037s/100 iter), loss = 0.18997
I0122 16:43:07.445811 57874 solver.cpp:285]     Train net output #0: loss = 0.18997 (* 1 = 0.18997 loss)
I0122 16:43:07.445816 57874 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:43:08.905122 57874 solver.cpp:266] Iteration 5900 (68.5284 iter/s, 1.45925s/100 iter), loss = 0.2389
I0122 16:43:08.905149 57874 solver.cpp:285]     Train net output #0: loss = 0.2389 (* 1 = 0.2389 loss)
I0122 16:43:08.905155 57874 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:43:10.341887 57874 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:43:10.763343 57874 solver.cpp:517]     Test net output #0: loss = 0.567746 (* 1 = 0.567746 loss)
I0122 16:43:10.763361 57874 solver.cpp:517]     Test net output #1: top-1 = 0.826333
I0122 16:43:10.763365 57874 solver.cpp:517]     Test net output #2: top-5 = 0.988445
I0122 16:43:10.771483 57874 solver.cpp:266] Iteration 6000 (53.583 iter/s, 1.86626s/100 iter), loss = 0.312716
I0122 16:43:10.771502 57874 solver.cpp:285]     Train net output #0: loss = 0.312716 (* 1 = 0.312716 loss)
I0122 16:43:10.771509 57874 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:43:12.241470 57874 solver.cpp:266] Iteration 6100 (68.0316 iter/s, 1.46991s/100 iter), loss = 0.204413
I0122 16:43:12.241500 57874 solver.cpp:285]     Train net output #0: loss = 0.204413 (* 1 = 0.204413 loss)
I0122 16:43:12.241506 57874 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:43:13.727118 57874 solver.cpp:266] Iteration 6200 (67.3148 iter/s, 1.48556s/100 iter), loss = 0.287769
I0122 16:43:13.727149 57874 solver.cpp:285]     Train net output #0: loss = 0.287769 (* 1 = 0.287769 loss)
I0122 16:43:13.727155 57874 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:43:15.183249 57874 solver.cpp:266] Iteration 6300 (68.6795 iter/s, 1.45604s/100 iter), loss = 0.17961
I0122 16:43:15.183279 57874 solver.cpp:285]     Train net output #0: loss = 0.17961 (* 1 = 0.17961 loss)
I0122 16:43:15.185482 57874 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:43:16.637042 57874 solver.cpp:266] Iteration 6400 (68.8947 iter/s, 1.45149s/100 iter), loss = 0.211205
I0122 16:43:16.637076 57874 solver.cpp:285]     Train net output #0: loss = 0.211205 (* 1 = 0.211205 loss)
I0122 16:43:16.637082 57874 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:43:18.083854 57874 solver.cpp:266] Iteration 6500 (69.1219 iter/s, 1.44672s/100 iter), loss = 0.285707
I0122 16:43:18.083884 57874 solver.cpp:285]     Train net output #0: loss = 0.285707 (* 1 = 0.285707 loss)
I0122 16:43:18.083889 57874 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:43:19.538592 57874 solver.cpp:266] Iteration 6600 (68.745 iter/s, 1.45465s/100 iter), loss = 0.334157
I0122 16:43:19.538620 57874 solver.cpp:285]     Train net output #0: loss = 0.334157 (* 1 = 0.334157 loss)
I0122 16:43:19.538626 57874 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:43:20.952158 57874 solver.cpp:266] Iteration 6700 (70.7474 iter/s, 1.41348s/100 iter), loss = 0.199526
I0122 16:43:20.952209 57874 solver.cpp:285]     Train net output #0: loss = 0.199526 (* 1 = 0.199526 loss)
I0122 16:43:20.952215 57874 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:43:22.403949 57874 solver.cpp:266] Iteration 6800 (68.8857 iter/s, 1.45168s/100 iter), loss = 0.23373
I0122 16:43:22.403980 57874 solver.cpp:285]     Train net output #0: loss = 0.23373 (* 1 = 0.23373 loss)
I0122 16:43:22.403985 57874 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:43:23.824218 57874 solver.cpp:266] Iteration 6900 (70.4137 iter/s, 1.42018s/100 iter), loss = 0.171372
I0122 16:43:23.824247 57874 solver.cpp:285]     Train net output #0: loss = 0.171372 (* 1 = 0.171372 loss)
I0122 16:43:23.824254 57874 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:43:25.270905 57874 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:43:25.668617 57874 solver.cpp:517]     Test net output #0: loss = 0.533462 (* 1 = 0.533462 loss)
I0122 16:43:25.668634 57874 solver.cpp:517]     Test net output #1: top-1 = 0.834889
I0122 16:43:25.668639 57874 solver.cpp:517]     Test net output #2: top-5 = 0.988778
I0122 16:43:25.676734 57874 solver.cpp:266] Iteration 7000 (53.9837 iter/s, 1.85241s/100 iter), loss = 0.288933
I0122 16:43:25.676754 57874 solver.cpp:285]     Train net output #0: loss = 0.288933 (* 1 = 0.288933 loss)
I0122 16:43:25.676760 57874 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:43:27.098876 57874 solver.cpp:266] Iteration 7100 (70.3204 iter/s, 1.42206s/100 iter), loss = 0.261021
I0122 16:43:27.098906 57874 solver.cpp:285]     Train net output #0: loss = 0.261021 (* 1 = 0.261021 loss)
I0122 16:43:27.098951 57874 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:43:28.539008 57874 solver.cpp:266] Iteration 7200 (69.4445 iter/s, 1.44s/100 iter), loss = 0.307863
I0122 16:43:28.539039 57874 solver.cpp:285]     Train net output #0: loss = 0.307863 (* 1 = 0.307863 loss)
I0122 16:43:28.539044 57874 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:43:29.997676 57874 solver.cpp:266] Iteration 7300 (68.5599 iter/s, 1.45858s/100 iter), loss = 0.328252
I0122 16:43:29.997707 57874 solver.cpp:285]     Train net output #0: loss = 0.328252 (* 1 = 0.328252 loss)
I0122 16:43:29.997714 57874 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:43:31.437198 57874 solver.cpp:266] Iteration 7400 (69.4719 iter/s, 1.43943s/100 iter), loss = 0.206738
I0122 16:43:31.437228 57874 solver.cpp:285]     Train net output #0: loss = 0.206738 (* 1 = 0.206738 loss)
I0122 16:43:31.437270 57874 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:43:32.880174 57874 solver.cpp:266] Iteration 7500 (69.3075 iter/s, 1.44285s/100 iter), loss = 0.292944
I0122 16:43:32.880270 57874 solver.cpp:285]     Train net output #0: loss = 0.292944 (* 1 = 0.292944 loss)
I0122 16:43:32.880276 57874 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:43:34.329948 57874 solver.cpp:266] Iteration 7600 (68.9835 iter/s, 1.44962s/100 iter), loss = 0.254851
I0122 16:43:34.329979 57874 solver.cpp:285]     Train net output #0: loss = 0.254851 (* 1 = 0.254851 loss)
I0122 16:43:34.329985 57874 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:43:35.751806 57874 solver.cpp:266] Iteration 7700 (70.335 iter/s, 1.42177s/100 iter), loss = 0.155405
I0122 16:43:35.751837 57874 solver.cpp:285]     Train net output #0: loss = 0.155405 (* 1 = 0.155405 loss)
I0122 16:43:35.751842 57874 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:43:37.201355 57874 solver.cpp:266] Iteration 7800 (68.9913 iter/s, 1.44946s/100 iter), loss = 0.221192
I0122 16:43:37.201385 57874 solver.cpp:285]     Train net output #0: loss = 0.221192 (* 1 = 0.221192 loss)
I0122 16:43:37.201391 57874 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:43:38.634686 57874 solver.cpp:266] Iteration 7900 (69.7719 iter/s, 1.43324s/100 iter), loss = 0.198714
I0122 16:43:38.634716 57874 solver.cpp:285]     Train net output #0: loss = 0.198714 (* 1 = 0.198714 loss)
I0122 16:43:38.634763 57874 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:43:40.074535 57874 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:43:40.473175 57874 solver.cpp:517]     Test net output #0: loss = 0.536685 (* 1 = 0.536685 loss)
I0122 16:43:40.473191 57874 solver.cpp:517]     Test net output #1: top-1 = 0.829111
I0122 16:43:40.473196 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0122 16:43:40.487426 57874 solver.cpp:266] Iteration 8000 (53.9784 iter/s, 1.85259s/100 iter), loss = 0.187333
I0122 16:43:40.487448 57874 solver.cpp:285]     Train net output #0: loss = 0.187333 (* 1 = 0.187333 loss)
I0122 16:43:40.487646 57874 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:43:41.939563 57874 solver.cpp:266] Iteration 8100 (68.8771 iter/s, 1.45186s/100 iter), loss = 0.24474
I0122 16:43:41.939594 57874 solver.cpp:285]     Train net output #0: loss = 0.24474 (* 1 = 0.24474 loss)
I0122 16:43:41.939600 57874 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:43:43.393090 57874 solver.cpp:266] Iteration 8200 (68.8025 iter/s, 1.45344s/100 iter), loss = 0.186147
I0122 16:43:43.393121 57874 solver.cpp:285]     Train net output #0: loss = 0.186147 (* 1 = 0.186147 loss)
I0122 16:43:43.393126 57874 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:43:44.809249 57874 solver.cpp:266] Iteration 8300 (70.618 iter/s, 1.41607s/100 iter), loss = 0.189115
I0122 16:43:44.809280 57874 solver.cpp:285]     Train net output #0: loss = 0.189115 (* 1 = 0.189115 loss)
I0122 16:43:44.809286 57874 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:43:46.261355 57874 solver.cpp:266] Iteration 8400 (68.8698 iter/s, 1.45202s/100 iter), loss = 0.187214
I0122 16:43:46.261387 57874 solver.cpp:285]     Train net output #0: loss = 0.187214 (* 1 = 0.187214 loss)
I0122 16:43:46.261394 57874 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:43:47.692322 57874 solver.cpp:266] Iteration 8500 (69.8874 iter/s, 1.43087s/100 iter), loss = 0.257778
I0122 16:43:47.692353 57874 solver.cpp:285]     Train net output #0: loss = 0.257778 (* 1 = 0.257778 loss)
I0122 16:43:47.692358 57874 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:43:49.140058 57874 solver.cpp:266] Iteration 8600 (69.0777 iter/s, 1.44765s/100 iter), loss = 0.233005
I0122 16:43:49.140087 57874 solver.cpp:285]     Train net output #0: loss = 0.233005 (* 1 = 0.233005 loss)
I0122 16:43:49.140094 57874 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:43:50.571004 57874 solver.cpp:266] Iteration 8700 (69.8882 iter/s, 1.43086s/100 iter), loss = 0.233373
I0122 16:43:50.571033 57874 solver.cpp:285]     Train net output #0: loss = 0.233373 (* 1 = 0.233373 loss)
I0122 16:43:50.571087 57874 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:43:52.003672 57874 solver.cpp:266] Iteration 8800 (69.8067 iter/s, 1.43253s/100 iter), loss = 0.264754
I0122 16:43:52.003721 57874 solver.cpp:285]     Train net output #0: loss = 0.264754 (* 1 = 0.264754 loss)
I0122 16:43:52.003727 57874 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:43:53.418015 57874 solver.cpp:266] Iteration 8900 (70.7096 iter/s, 1.41424s/100 iter), loss = 0.246436
I0122 16:43:53.418045 57874 solver.cpp:285]     Train net output #0: loss = 0.246436 (* 1 = 0.246436 loss)
I0122 16:43:53.418051 57874 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:43:54.866590 57874 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:43:55.265568 57874 solver.cpp:517]     Test net output #0: loss = 0.484355 (* 1 = 0.484355 loss)
I0122 16:43:55.265584 57874 solver.cpp:517]     Test net output #1: top-1 = 0.843444
I0122 16:43:55.265589 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0122 16:43:55.273720 57874 solver.cpp:266] Iteration 9000 (53.8909 iter/s, 1.8556s/100 iter), loss = 0.259713
I0122 16:43:55.273737 57874 solver.cpp:285]     Train net output #0: loss = 0.259713 (* 1 = 0.259713 loss)
I0122 16:43:55.273746 57874 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:43:56.724215 57874 solver.cpp:266] Iteration 9100 (68.9459 iter/s, 1.45041s/100 iter), loss = 0.18733
I0122 16:43:56.724256 57874 solver.cpp:285]     Train net output #0: loss = 0.18733 (* 1 = 0.18733 loss)
I0122 16:43:56.724262 57874 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:43:58.140928 57874 solver.cpp:266] Iteration 9200 (70.5909 iter/s, 1.41661s/100 iter), loss = 0.217279
I0122 16:43:58.140959 57874 solver.cpp:285]     Train net output #0: loss = 0.217279 (* 1 = 0.217279 loss)
I0122 16:43:58.140964 57874 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:43:59.599901 57874 solver.cpp:266] Iteration 9300 (68.5457 iter/s, 1.45888s/100 iter), loss = 0.16709
I0122 16:43:59.599931 57874 solver.cpp:285]     Train net output #0: loss = 0.16709 (* 1 = 0.16709 loss)
I0122 16:43:59.599937 57874 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:44:01.022105 57874 solver.cpp:266] Iteration 9400 (70.3184 iter/s, 1.4221s/100 iter), loss = 0.200872
I0122 16:44:01.022136 57874 solver.cpp:285]     Train net output #0: loss = 0.200872 (* 1 = 0.200872 loss)
I0122 16:44:01.022142 57874 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:44:02.470667 57874 solver.cpp:266] Iteration 9500 (69.0383 iter/s, 1.44847s/100 iter), loss = 0.134348
I0122 16:44:02.470695 57874 solver.cpp:285]     Train net output #0: loss = 0.134348 (* 1 = 0.134348 loss)
I0122 16:44:02.470700 57874 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:44:03.930366 57874 solver.cpp:266] Iteration 9600 (68.5113 iter/s, 1.45961s/100 iter), loss = 0.238934
I0122 16:44:03.930454 57874 solver.cpp:285]     Train net output #0: loss = 0.238934 (* 1 = 0.238934 loss)
I0122 16:44:03.930461 57874 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:44:05.339654 57874 solver.cpp:266] Iteration 9700 (70.9652 iter/s, 1.40914s/100 iter), loss = 0.146586
I0122 16:44:05.339684 57874 solver.cpp:285]     Train net output #0: loss = 0.146586 (* 1 = 0.146586 loss)
I0122 16:44:05.339689 57874 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:44:06.767168 57874 solver.cpp:266] Iteration 9800 (70.0562 iter/s, 1.42743s/100 iter), loss = 0.249075
I0122 16:44:06.767197 57874 solver.cpp:285]     Train net output #0: loss = 0.249075 (* 1 = 0.249075 loss)
I0122 16:44:06.767241 57874 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:44:08.210392 57874 solver.cpp:266] Iteration 9900 (69.2956 iter/s, 1.44309s/100 iter), loss = 0.276257
I0122 16:44:08.210423 57874 solver.cpp:285]     Train net output #0: loss = 0.276257 (* 1 = 0.276257 loss)
I0122 16:44:08.210429 57874 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:44:09.418239 57874 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:44:09.713821 57874 solver.cpp:517]     Test net output #0: loss = 0.50802 (* 1 = 0.50802 loss)
I0122 16:44:09.713840 57874 solver.cpp:517]     Test net output #1: top-1 = 0.836333
I0122 16:44:09.713845 57874 solver.cpp:517]     Test net output #2: top-5 = 0.990667
I0122 16:44:09.721966 57874 solver.cpp:266] Iteration 10000 (66.1601 iter/s, 1.51149s/100 iter), loss = 0.212954
I0122 16:44:09.721983 57874 solver.cpp:285]     Train net output #0: loss = 0.212954 (* 1 = 0.212954 loss)
I0122 16:44:09.721990 57874 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:44:11.084058 57874 solver.cpp:266] Iteration 10100 (73.4206 iter/s, 1.36202s/100 iter), loss = 0.176579
I0122 16:44:11.084087 57874 solver.cpp:285]     Train net output #0: loss = 0.176579 (* 1 = 0.176579 loss)
I0122 16:44:11.086304 57874 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:44:12.523072 57874 solver.cpp:266] Iteration 10200 (69.6035 iter/s, 1.43671s/100 iter), loss = 0.25862
I0122 16:44:12.523100 57874 solver.cpp:285]     Train net output #0: loss = 0.25862 (* 1 = 0.25862 loss)
I0122 16:44:12.523106 57874 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:44:13.983279 57874 solver.cpp:266] Iteration 10300 (68.4876 iter/s, 1.46012s/100 iter), loss = 0.236586
I0122 16:44:13.983321 57874 solver.cpp:285]     Train net output #0: loss = 0.236586 (* 1 = 0.236586 loss)
I0122 16:44:13.983328 57874 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:44:15.422305 57874 solver.cpp:266] Iteration 10400 (69.4964 iter/s, 1.43892s/100 iter), loss = 0.207055
I0122 16:44:15.422334 57874 solver.cpp:285]     Train net output #0: loss = 0.207055 (* 1 = 0.207055 loss)
I0122 16:44:15.424554 57874 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:44:16.855396 57874 solver.cpp:266] Iteration 10500 (69.8917 iter/s, 1.43078s/100 iter), loss = 0.177938
I0122 16:44:16.855427 57874 solver.cpp:285]     Train net output #0: loss = 0.177938 (* 1 = 0.177938 loss)
I0122 16:44:16.855432 57874 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:44:18.308565 57874 solver.cpp:266] Iteration 10600 (68.8194 iter/s, 1.45308s/100 iter), loss = 0.181957
I0122 16:44:18.308595 57874 solver.cpp:285]     Train net output #0: loss = 0.181957 (* 1 = 0.181957 loss)
I0122 16:44:18.308601 57874 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:44:19.734971 57874 solver.cpp:266] Iteration 10700 (70.1107 iter/s, 1.42632s/100 iter), loss = 0.274791
I0122 16:44:19.735002 57874 solver.cpp:285]     Train net output #0: loss = 0.274791 (* 1 = 0.274791 loss)
I0122 16:44:19.735007 57874 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:44:21.182467 57874 solver.cpp:266] Iteration 10800 (69.0891 iter/s, 1.44741s/100 iter), loss = 0.162955
I0122 16:44:21.182498 57874 solver.cpp:285]     Train net output #0: loss = 0.162955 (* 1 = 0.162955 loss)
I0122 16:44:21.182504 57874 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:44:22.646116 57874 solver.cpp:266] Iteration 10900 (68.3267 iter/s, 1.46356s/100 iter), loss = 0.215609
I0122 16:44:22.646167 57874 solver.cpp:285]     Train net output #0: loss = 0.215609 (* 1 = 0.215609 loss)
I0122 16:44:22.646174 57874 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:44:24.054723 57874 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:44:24.452003 57874 solver.cpp:517]     Test net output #0: loss = 0.555862 (* 1 = 0.555862 loss)
I0122 16:44:24.452020 57874 solver.cpp:517]     Test net output #1: top-1 = 0.823667
I0122 16:44:24.452024 57874 solver.cpp:517]     Test net output #2: top-5 = 0.989222
I0122 16:44:24.462250 57874 solver.cpp:266] Iteration 11000 (55.0657 iter/s, 1.81601s/100 iter), loss = 0.257137
I0122 16:44:24.462270 57874 solver.cpp:285]     Train net output #0: loss = 0.257137 (* 1 = 0.257137 loss)
I0122 16:44:24.462278 57874 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:44:25.922186 57874 solver.cpp:266] Iteration 11100 (68.5002 iter/s, 1.45985s/100 iter), loss = 0.158859
I0122 16:44:25.922215 57874 solver.cpp:285]     Train net output #0: loss = 0.158859 (* 1 = 0.158859 loss)
I0122 16:44:25.922222 57874 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:44:27.354264 57874 solver.cpp:266] Iteration 11200 (69.833 iter/s, 1.43199s/100 iter), loss = 0.221254
I0122 16:44:27.354297 57874 solver.cpp:285]     Train net output #0: loss = 0.221254 (* 1 = 0.221254 loss)
I0122 16:44:27.354339 57874 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:44:28.798882 57874 solver.cpp:266] Iteration 11300 (69.2289 iter/s, 1.44448s/100 iter), loss = 0.182286
I0122 16:44:28.798913 57874 solver.cpp:285]     Train net output #0: loss = 0.182286 (* 1 = 0.182286 loss)
I0122 16:44:28.798918 57874 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:44:30.263721 57874 solver.cpp:266] Iteration 11400 (68.2711 iter/s, 1.46475s/100 iter), loss = 0.299406
I0122 16:44:30.263752 57874 solver.cpp:285]     Train net output #0: loss = 0.299406 (* 1 = 0.299406 loss)
I0122 16:44:30.263758 57874 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:44:31.694851 57874 solver.cpp:266] Iteration 11500 (69.8793 iter/s, 1.43104s/100 iter), loss = 0.200026
I0122 16:44:31.694882 57874 solver.cpp:285]     Train net output #0: loss = 0.200026 (* 1 = 0.200026 loss)
I0122 16:44:31.694926 57874 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:44:33.127454 57874 solver.cpp:266] Iteration 11600 (69.8096 iter/s, 1.43247s/100 iter), loss = 0.278357
I0122 16:44:33.127485 57874 solver.cpp:285]     Train net output #0: loss = 0.278357 (* 1 = 0.278357 loss)
I0122 16:44:33.127490 57874 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:44:34.589006 57874 solver.cpp:266] Iteration 11700 (68.4246 iter/s, 1.46146s/100 iter), loss = 0.229382
I0122 16:44:34.589093 57874 solver.cpp:285]     Train net output #0: loss = 0.229382 (* 1 = 0.229382 loss)
I0122 16:44:34.589102 57874 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:44:36.032671 57874 solver.cpp:266] Iteration 11800 (69.2751 iter/s, 1.44352s/100 iter), loss = 0.192509
I0122 16:44:36.032701 57874 solver.cpp:285]     Train net output #0: loss = 0.192509 (* 1 = 0.192509 loss)
I0122 16:44:36.032747 57874 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:44:37.478149 57874 solver.cpp:266] Iteration 11900 (69.1877 iter/s, 1.44534s/100 iter), loss = 0.219322
I0122 16:44:37.478180 57874 solver.cpp:285]     Train net output #0: loss = 0.219322 (* 1 = 0.219322 loss)
I0122 16:44:37.478185 57874 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:44:38.924270 57874 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:44:39.324111 57874 solver.cpp:517]     Test net output #0: loss = 0.486425 (* 1 = 0.486425 loss)
I0122 16:44:39.324129 57874 solver.cpp:517]     Test net output #1: top-1 = 0.845555
I0122 16:44:39.324133 57874 solver.cpp:517]     Test net output #2: top-5 = 0.989778
I0122 16:44:39.332247 57874 solver.cpp:266] Iteration 12000 (53.9375 iter/s, 1.854s/100 iter), loss = 0.156983
I0122 16:44:39.332264 57874 solver.cpp:285]     Train net output #0: loss = 0.156983 (* 1 = 0.156983 loss)
I0122 16:44:39.332270 57874 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:44:40.765712 57874 solver.cpp:266] Iteration 12100 (69.7648 iter/s, 1.43339s/100 iter), loss = 0.147726
I0122 16:44:40.765741 57874 solver.cpp:285]     Train net output #0: loss = 0.147725 (* 1 = 0.147725 loss)
I0122 16:44:40.765787 57874 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:44:42.198348 57874 solver.cpp:266] Iteration 12200 (69.808 iter/s, 1.4325s/100 iter), loss = 0.399809
I0122 16:44:42.198376 57874 solver.cpp:285]     Train net output #0: loss = 0.399809 (* 1 = 0.399809 loss)
I0122 16:44:42.198382 57874 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:44:43.611366 57874 solver.cpp:266] Iteration 12300 (70.7749 iter/s, 1.41293s/100 iter), loss = 0.260291
I0122 16:44:43.611397 57874 solver.cpp:285]     Train net output #0: loss = 0.260291 (* 1 = 0.260291 loss)
I0122 16:44:43.611402 57874 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:44:45.064088 57874 solver.cpp:266] Iteration 12400 (68.8407 iter/s, 1.45263s/100 iter), loss = 0.168047
I0122 16:44:45.064118 57874 solver.cpp:285]     Train net output #0: loss = 0.168047 (* 1 = 0.168047 loss)
I0122 16:44:45.064124 57874 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:44:46.516297 57874 solver.cpp:266] Iteration 12500 (68.8649 iter/s, 1.45212s/100 iter), loss = 0.187772
I0122 16:44:46.516327 57874 solver.cpp:285]     Train net output #0: loss = 0.187772 (* 1 = 0.187772 loss)
I0122 16:44:46.516333 57874 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:44:47.988093 57874 solver.cpp:266] Iteration 12600 (67.9484 iter/s, 1.47171s/100 iter), loss = 0.170965
I0122 16:44:47.988123 57874 solver.cpp:285]     Train net output #0: loss = 0.170965 (* 1 = 0.170965 loss)
I0122 16:44:47.988128 57874 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:44:49.403441 57874 solver.cpp:266] Iteration 12700 (70.6584 iter/s, 1.41526s/100 iter), loss = 0.246063
I0122 16:44:49.403471 57874 solver.cpp:285]     Train net output #0: loss = 0.246063 (* 1 = 0.246063 loss)
I0122 16:44:49.403476 57874 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:44:50.869460 57874 solver.cpp:266] Iteration 12800 (68.2161 iter/s, 1.46593s/100 iter), loss = 0.262123
I0122 16:44:50.869489 57874 solver.cpp:285]     Train net output #0: loss = 0.262123 (* 1 = 0.262123 loss)
I0122 16:44:50.869495 57874 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:44:52.324723 57874 solver.cpp:266] Iteration 12900 (68.7204 iter/s, 1.45517s/100 iter), loss = 0.177631
I0122 16:44:52.324753 57874 solver.cpp:285]     Train net output #0: loss = 0.177631 (* 1 = 0.177631 loss)
I0122 16:44:52.324759 57874 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:44:53.734555 57874 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:44:54.159253 57874 solver.cpp:517]     Test net output #0: loss = 0.482529 (* 1 = 0.482529 loss)
I0122 16:44:54.159270 57874 solver.cpp:517]     Test net output #1: top-1 = 0.847333
I0122 16:44:54.159274 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991222
I0122 16:44:54.189188 57874 solver.cpp:266] Iteration 13000 (53.6376 iter/s, 1.86436s/100 iter), loss = 0.281179
I0122 16:44:54.189209 57874 solver.cpp:285]     Train net output #0: loss = 0.281179 (* 1 = 0.281179 loss)
I0122 16:44:54.189215 57874 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:44:55.605374 57874 solver.cpp:266] Iteration 13100 (70.6163 iter/s, 1.4161s/100 iter), loss = 0.209141
I0122 16:44:55.605414 57874 solver.cpp:285]     Train net output #0: loss = 0.209141 (* 1 = 0.209141 loss)
I0122 16:44:55.605422 57874 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:44:57.053207 57874 solver.cpp:266] Iteration 13200 (69.0737 iter/s, 1.44773s/100 iter), loss = 0.366221
I0122 16:44:57.053238 57874 solver.cpp:285]     Train net output #0: loss = 0.366221 (* 1 = 0.366221 loss)
I0122 16:44:57.053244 57874 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:44:58.490309 57874 solver.cpp:266] Iteration 13300 (69.5888 iter/s, 1.43701s/100 iter), loss = 0.176222
I0122 16:44:58.490340 57874 solver.cpp:285]     Train net output #0: loss = 0.176222 (* 1 = 0.176222 loss)
I0122 16:44:58.490386 57874 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:44:59.948756 57874 solver.cpp:266] Iteration 13400 (68.5725 iter/s, 1.45831s/100 iter), loss = 0.166617
I0122 16:44:59.948787 57874 solver.cpp:285]     Train net output #0: loss = 0.166617 (* 1 = 0.166617 loss)
I0122 16:44:59.948793 57874 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:45:01.394928 57874 solver.cpp:266] Iteration 13500 (69.1525 iter/s, 1.44608s/100 iter), loss = 0.180711
I0122 16:45:01.394958 57874 solver.cpp:285]     Train net output #0: loss = 0.180711 (* 1 = 0.180711 loss)
I0122 16:45:01.394964 57874 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:45:02.855573 57874 solver.cpp:266] Iteration 13600 (68.4672 iter/s, 1.46055s/100 iter), loss = 0.201094
I0122 16:45:02.855603 57874 solver.cpp:285]     Train net output #0: loss = 0.201094 (* 1 = 0.201094 loss)
I0122 16:45:02.855609 57874 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:45:04.276652 57874 solver.cpp:266] Iteration 13700 (70.3735 iter/s, 1.42099s/100 iter), loss = 0.151809
I0122 16:45:04.276679 57874 solver.cpp:285]     Train net output #0: loss = 0.151809 (* 1 = 0.151809 loss)
I0122 16:45:04.276684 57874 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:45:05.738842 57874 solver.cpp:266] Iteration 13800 (68.3947 iter/s, 1.4621s/100 iter), loss = 0.186503
I0122 16:45:05.738927 57874 solver.cpp:285]     Train net output #0: loss = 0.186503 (* 1 = 0.186503 loss)
I0122 16:45:05.738934 57874 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:45:07.155652 57874 solver.cpp:266] Iteration 13900 (70.5882 iter/s, 1.41667s/100 iter), loss = 0.160946
I0122 16:45:07.155683 57874 solver.cpp:285]     Train net output #0: loss = 0.160946 (* 1 = 0.160946 loss)
I0122 16:45:07.155689 57874 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:45:08.595428 57874 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:45:08.995057 57874 solver.cpp:517]     Test net output #0: loss = 0.488765 (* 1 = 0.488765 loss)
I0122 16:45:08.995075 57874 solver.cpp:517]     Test net output #1: top-1 = 0.848444
I0122 16:45:08.995079 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991889
I0122 16:45:09.003181 57874 solver.cpp:266] Iteration 14000 (54.1294 iter/s, 1.84743s/100 iter), loss = 0.19454
I0122 16:45:09.003201 57874 solver.cpp:285]     Train net output #0: loss = 0.19454 (* 1 = 0.19454 loss)
I0122 16:45:09.003207 57874 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:45:10.457065 57874 solver.cpp:266] Iteration 14100 (68.785 iter/s, 1.4538s/100 iter), loss = 0.145693
I0122 16:45:10.457095 57874 solver.cpp:285]     Train net output #0: loss = 0.145693 (* 1 = 0.145693 loss)
I0122 16:45:10.457101 57874 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:45:11.873610 57874 solver.cpp:266] Iteration 14200 (70.5992 iter/s, 1.41645s/100 iter), loss = 0.173887
I0122 16:45:11.873642 57874 solver.cpp:285]     Train net output #0: loss = 0.173887 (* 1 = 0.173887 loss)
I0122 16:45:11.873647 57874 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:45:13.339351 57874 solver.cpp:266] Iteration 14300 (68.2292 iter/s, 1.46565s/100 iter), loss = 0.240245
I0122 16:45:13.339380 57874 solver.cpp:285]     Train net output #0: loss = 0.240245 (* 1 = 0.240245 loss)
I0122 16:45:13.339386 57874 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:45:14.797618 57874 solver.cpp:266] Iteration 14400 (68.5788 iter/s, 1.45818s/100 iter), loss = 0.151456
I0122 16:45:14.797658 57874 solver.cpp:285]     Train net output #0: loss = 0.151455 (* 1 = 0.151455 loss)
I0122 16:45:14.797664 57874 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:45:16.213321 57874 solver.cpp:266] Iteration 14500 (70.6412 iter/s, 1.41561s/100 iter), loss = 0.137593
I0122 16:45:16.213351 57874 solver.cpp:285]     Train net output #0: loss = 0.137593 (* 1 = 0.137593 loss)
I0122 16:45:16.213357 57874 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:45:17.670392 57874 solver.cpp:266] Iteration 14600 (68.6351 iter/s, 1.45698s/100 iter), loss = 0.118594
I0122 16:45:17.670423 57874 solver.cpp:285]     Train net output #0: loss = 0.118594 (* 1 = 0.118594 loss)
I0122 16:45:17.670429 57874 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:45:19.091264 57874 solver.cpp:266] Iteration 14700 (70.3839 iter/s, 1.42078s/100 iter), loss = 0.130331
I0122 16:45:19.091293 57874 solver.cpp:285]     Train net output #0: loss = 0.130331 (* 1 = 0.130331 loss)
I0122 16:45:19.091298 57874 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:45:20.539433 57874 solver.cpp:266] Iteration 14800 (69.0569 iter/s, 1.44808s/100 iter), loss = 0.221297
I0122 16:45:20.539464 57874 solver.cpp:285]     Train net output #0: loss = 0.221297 (* 1 = 0.221297 loss)
I0122 16:45:20.539470 57874 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:45:21.998176 57874 solver.cpp:266] Iteration 14900 (68.5565 iter/s, 1.45865s/100 iter), loss = 0.129344
I0122 16:45:21.998204 57874 solver.cpp:285]     Train net output #0: loss = 0.129344 (* 1 = 0.129344 loss)
I0122 16:45:21.998210 57874 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:45:23.403434 57874 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:45:23.802085 57874 solver.cpp:517]     Test net output #0: loss = 0.48957 (* 1 = 0.48957 loss)
I0122 16:45:23.802103 57874 solver.cpp:517]     Test net output #1: top-1 = 0.841778
I0122 16:45:23.802109 57874 solver.cpp:517]     Test net output #2: top-5 = 0.990222
I0122 16:45:23.814596 57874 solver.cpp:266] Iteration 15000 (55.0563 iter/s, 1.81632s/100 iter), loss = 0.25057
I0122 16:45:23.814617 57874 solver.cpp:285]     Train net output #0: loss = 0.25057 (* 1 = 0.25057 loss)
I0122 16:45:23.814625 57874 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:45:25.271400 57874 solver.cpp:266] Iteration 15100 (68.6473 iter/s, 1.45672s/100 iter), loss = 0.136309
I0122 16:45:25.271430 57874 solver.cpp:285]     Train net output #0: loss = 0.136309 (* 1 = 0.136309 loss)
I0122 16:45:25.271435 57874 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:45:26.732718 57874 solver.cpp:266] Iteration 15200 (68.4355 iter/s, 1.46123s/100 iter), loss = 0.31299
I0122 16:45:26.732748 57874 solver.cpp:285]     Train net output #0: loss = 0.31299 (* 1 = 0.31299 loss)
I0122 16:45:26.732753 57874 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:45:28.153154 57874 solver.cpp:266] Iteration 15300 (70.4054 iter/s, 1.42035s/100 iter), loss = 0.233312
I0122 16:45:28.153187 57874 solver.cpp:285]     Train net output #0: loss = 0.233312 (* 1 = 0.233312 loss)
I0122 16:45:28.153193 57874 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:45:29.603462 57874 solver.cpp:266] Iteration 15400 (68.9552 iter/s, 1.45022s/100 iter), loss = 0.191509
I0122 16:45:29.603493 57874 solver.cpp:285]     Train net output #0: loss = 0.191509 (* 1 = 0.191509 loss)
I0122 16:45:29.603499 57874 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:45:31.066341 57874 solver.cpp:266] Iteration 15500 (68.3626 iter/s, 1.46279s/100 iter), loss = 0.235616
I0122 16:45:31.066370 57874 solver.cpp:285]     Train net output #0: loss = 0.235616 (* 1 = 0.235616 loss)
I0122 16:45:31.066376 57874 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:45:32.509774 57874 solver.cpp:266] Iteration 15600 (69.2836 iter/s, 1.44334s/100 iter), loss = 0.118623
I0122 16:45:32.509806 57874 solver.cpp:285]     Train net output #0: loss = 0.118623 (* 1 = 0.118623 loss)
I0122 16:45:32.509850 57874 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:45:33.967257 57874 solver.cpp:266] Iteration 15700 (68.6178 iter/s, 1.45735s/100 iter), loss = 0.122396
I0122 16:45:33.967288 57874 solver.cpp:285]     Train net output #0: loss = 0.122395 (* 1 = 0.122395 loss)
I0122 16:45:33.967294 57874 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:45:35.415948 57874 solver.cpp:266] Iteration 15800 (69.0321 iter/s, 1.4486s/100 iter), loss = 0.149235
I0122 16:45:35.415978 57874 solver.cpp:285]     Train net output #0: loss = 0.149235 (* 1 = 0.149235 loss)
I0122 16:45:35.415983 57874 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:45:36.876513 57874 solver.cpp:266] Iteration 15900 (68.4709 iter/s, 1.46048s/100 iter), loss = 0.265299
I0122 16:45:36.876600 57874 solver.cpp:285]     Train net output #0: loss = 0.265299 (* 1 = 0.265299 loss)
I0122 16:45:36.876606 57874 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:45:38.281188 57874 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:45:38.680959 57874 solver.cpp:517]     Test net output #0: loss = 0.473294 (* 1 = 0.473294 loss)
I0122 16:45:38.680977 57874 solver.cpp:517]     Test net output #1: top-1 = 0.848555
I0122 16:45:38.680982 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0122 16:45:38.696254 57874 solver.cpp:266] Iteration 16000 (54.9575 iter/s, 1.81959s/100 iter), loss = 0.222271
I0122 16:45:38.696276 57874 solver.cpp:285]     Train net output #0: loss = 0.222271 (* 1 = 0.222271 loss)
I0122 16:45:38.696282 57874 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:45:40.139720 57874 solver.cpp:266] Iteration 16100 (69.2818 iter/s, 1.44338s/100 iter), loss = 0.236144
I0122 16:45:40.139750 57874 solver.cpp:285]     Train net output #0: loss = 0.236144 (* 1 = 0.236144 loss)
I0122 16:45:40.139755 57874 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:45:41.600090 57874 solver.cpp:266] Iteration 16200 (68.48 iter/s, 1.46028s/100 iter), loss = 0.164706
I0122 16:45:41.600121 57874 solver.cpp:285]     Train net output #0: loss = 0.164706 (* 1 = 0.164706 loss)
I0122 16:45:41.600126 57874 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:45:43.010283 57874 solver.cpp:266] Iteration 16300 (70.9168 iter/s, 1.4101s/100 iter), loss = 0.187219
I0122 16:45:43.010314 57874 solver.cpp:285]     Train net output #0: loss = 0.187219 (* 1 = 0.187219 loss)
I0122 16:45:43.010320 57874 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:45:44.476176 57874 solver.cpp:266] Iteration 16400 (68.2221 iter/s, 1.4658s/100 iter), loss = 0.165911
I0122 16:45:44.476205 57874 solver.cpp:285]     Train net output #0: loss = 0.165911 (* 1 = 0.165911 loss)
I0122 16:45:44.476212 57874 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:45:45.912333 57874 solver.cpp:266] Iteration 16500 (69.6345 iter/s, 1.43607s/100 iter), loss = 0.186561
I0122 16:45:45.912364 57874 solver.cpp:285]     Train net output #0: loss = 0.186561 (* 1 = 0.186561 loss)
I0122 16:45:45.912410 57874 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:45:47.355406 57874 solver.cpp:266] Iteration 16600 (69.303 iter/s, 1.44294s/100 iter), loss = 0.213776
I0122 16:45:47.355435 57874 solver.cpp:285]     Train net output #0: loss = 0.213776 (* 1 = 0.213776 loss)
I0122 16:45:47.355442 57874 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:45:48.774965 57874 solver.cpp:266] Iteration 16700 (70.4489 iter/s, 1.41947s/100 iter), loss = 0.152301
I0122 16:45:48.774996 57874 solver.cpp:285]     Train net output #0: loss = 0.152301 (* 1 = 0.152301 loss)
I0122 16:45:48.775001 57874 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:45:49.809701 57874 solver.cpp:266] Iteration 16800 (96.65 iter/s, 1.03466s/100 iter), loss = 0.128245
I0122 16:45:49.809736 57874 solver.cpp:285]     Train net output #0: loss = 0.128245 (* 1 = 0.128245 loss)
I0122 16:45:49.809741 57874 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:45:51.277565 57874 solver.cpp:266] Iteration 16900 (68.1306 iter/s, 1.46777s/100 iter), loss = 0.261944
I0122 16:45:51.277595 57874 solver.cpp:285]     Train net output #0: loss = 0.261944 (* 1 = 0.261944 loss)
I0122 16:45:51.277601 57874 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:45:52.725512 57874 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:45:53.126320 57874 solver.cpp:517]     Test net output #0: loss = 0.488551 (* 1 = 0.488551 loss)
I0122 16:45:53.126336 57874 solver.cpp:517]     Test net output #1: top-1 = 0.846667
I0122 16:45:53.126341 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991778
I0122 16:45:53.134428 57874 solver.cpp:266] Iteration 17000 (53.8572 iter/s, 1.85676s/100 iter), loss = 0.187077
I0122 16:45:53.134447 57874 solver.cpp:285]     Train net output #0: loss = 0.187077 (* 1 = 0.187077 loss)
I0122 16:45:53.134454 57874 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:45:54.595261 57874 solver.cpp:266] Iteration 17100 (68.4579 iter/s, 1.46075s/100 iter), loss = 0.217445
I0122 16:45:54.595290 57874 solver.cpp:285]     Train net output #0: loss = 0.217445 (* 1 = 0.217445 loss)
I0122 16:45:54.595296 57874 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:45:56.012044 57874 solver.cpp:266] Iteration 17200 (70.5868 iter/s, 1.41669s/100 iter), loss = 0.230701
I0122 16:45:56.012073 57874 solver.cpp:285]     Train net output #0: loss = 0.230701 (* 1 = 0.230701 loss)
I0122 16:45:56.012079 57874 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:45:57.465631 57874 solver.cpp:266] Iteration 17300 (68.7996 iter/s, 1.4535s/100 iter), loss = 0.328771
I0122 16:45:57.465661 57874 solver.cpp:285]     Train net output #0: loss = 0.328771 (* 1 = 0.328771 loss)
I0122 16:45:57.465667 57874 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:45:58.889235 57874 solver.cpp:266] Iteration 17400 (70.2486 iter/s, 1.42352s/100 iter), loss = 0.18148
I0122 16:45:58.889267 57874 solver.cpp:285]     Train net output #0: loss = 0.18148 (* 1 = 0.18148 loss)
I0122 16:45:58.889273 57874 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:46:00.339625 57874 solver.cpp:266] Iteration 17500 (68.9514 iter/s, 1.4503s/100 iter), loss = 0.145133
I0122 16:46:00.339666 57874 solver.cpp:285]     Train net output #0: loss = 0.145133 (* 1 = 0.145133 loss)
I0122 16:46:00.339673 57874 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:46:01.778015 57874 solver.cpp:266] Iteration 17600 (69.5271 iter/s, 1.43829s/100 iter), loss = 0.215429
I0122 16:46:01.778059 57874 solver.cpp:285]     Train net output #0: loss = 0.215429 (* 1 = 0.215429 loss)
I0122 16:46:01.780264 57874 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:46:03.211503 57874 solver.cpp:266] Iteration 17700 (69.8724 iter/s, 1.43118s/100 iter), loss = 0.255071
I0122 16:46:03.211532 57874 solver.cpp:285]     Train net output #0: loss = 0.255071 (* 1 = 0.255071 loss)
I0122 16:46:03.211539 57874 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:46:04.662000 57874 solver.cpp:266] Iteration 17800 (68.9461 iter/s, 1.45041s/100 iter), loss = 0.189635
I0122 16:46:04.662039 57874 solver.cpp:285]     Train net output #0: loss = 0.189635 (* 1 = 0.189635 loss)
I0122 16:46:04.662045 57874 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:46:06.089548 57874 solver.cpp:266] Iteration 17900 (70.055 iter/s, 1.42745s/100 iter), loss = 0.196892
I0122 16:46:06.089579 57874 solver.cpp:285]     Train net output #0: loss = 0.196892 (* 1 = 0.196892 loss)
I0122 16:46:06.089586 57874 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:46:07.536200 57874 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:46:07.932319 57874 solver.cpp:517]     Test net output #0: loss = 0.4571 (* 1 = 0.4571 loss)
I0122 16:46:07.932337 57874 solver.cpp:517]     Test net output #1: top-1 = 0.855333
I0122 16:46:07.932340 57874 solver.cpp:517]     Test net output #2: top-5 = 0.992222
I0122 16:46:07.941063 57874 solver.cpp:266] Iteration 18000 (54.0128 iter/s, 1.85141s/100 iter), loss = 0.172655
I0122 16:46:07.941083 57874 solver.cpp:285]     Train net output #0: loss = 0.172655 (* 1 = 0.172655 loss)
I0122 16:46:07.941092 57874 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:46:09.394770 57874 solver.cpp:266] Iteration 18100 (68.7938 iter/s, 1.45362s/100 iter), loss = 0.192032
I0122 16:46:09.394800 57874 solver.cpp:285]     Train net output #0: loss = 0.192032 (* 1 = 0.192032 loss)
I0122 16:46:09.394806 57874 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:46:10.813766 57874 solver.cpp:266] Iteration 18200 (70.4769 iter/s, 1.41891s/100 iter), loss = 0.13272
I0122 16:46:10.813794 57874 solver.cpp:285]     Train net output #0: loss = 0.13272 (* 1 = 0.13272 loss)
I0122 16:46:10.813800 57874 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:46:12.256994 57874 solver.cpp:266] Iteration 18300 (69.2935 iter/s, 1.44314s/100 iter), loss = 0.243819
I0122 16:46:12.257025 57874 solver.cpp:285]     Train net output #0: loss = 0.243819 (* 1 = 0.243819 loss)
I0122 16:46:12.257030 57874 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:46:13.673162 57874 solver.cpp:266] Iteration 18400 (70.6175 iter/s, 1.41608s/100 iter), loss = 0.16114
I0122 16:46:13.673194 57874 solver.cpp:285]     Train net output #0: loss = 0.16114 (* 1 = 0.16114 loss)
I0122 16:46:13.673200 57874 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:46:15.127364 57874 solver.cpp:266] Iteration 18500 (68.7707 iter/s, 1.45411s/100 iter), loss = 0.177071
I0122 16:46:15.127393 57874 solver.cpp:285]     Train net output #0: loss = 0.177071 (* 1 = 0.177071 loss)
I0122 16:46:15.127399 57874 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:46:16.588336 57874 solver.cpp:266] Iteration 18600 (68.4518 iter/s, 1.46088s/100 iter), loss = 0.150364
I0122 16:46:16.588367 57874 solver.cpp:285]     Train net output #0: loss = 0.150364 (* 1 = 0.150364 loss)
I0122 16:46:16.588373 57874 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:46:18.011289 57874 solver.cpp:266] Iteration 18700 (70.2809 iter/s, 1.42286s/100 iter), loss = 0.144319
I0122 16:46:18.011322 57874 solver.cpp:285]     Train net output #0: loss = 0.144319 (* 1 = 0.144319 loss)
I0122 16:46:18.011327 57874 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:46:19.465399 57874 solver.cpp:266] Iteration 18800 (68.775 iter/s, 1.45402s/100 iter), loss = 0.142487
I0122 16:46:19.465425 57874 solver.cpp:285]     Train net output #0: loss = 0.142487 (* 1 = 0.142487 loss)
I0122 16:46:19.465430 57874 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:46:20.893378 57874 solver.cpp:266] Iteration 18900 (70.0333 iter/s, 1.42789s/100 iter), loss = 0.217061
I0122 16:46:20.893409 57874 solver.cpp:285]     Train net output #0: loss = 0.217061 (* 1 = 0.217061 loss)
I0122 16:46:20.893452 57874 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:46:22.324230 57874 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:46:22.722390 57874 solver.cpp:517]     Test net output #0: loss = 0.45583 (* 1 = 0.45583 loss)
I0122 16:46:22.722407 57874 solver.cpp:517]     Test net output #1: top-1 = 0.857444
I0122 16:46:22.722412 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0122 16:46:22.730526 57874 solver.cpp:266] Iteration 19000 (54.4364 iter/s, 1.837s/100 iter), loss = 0.28032
I0122 16:46:22.730545 57874 solver.cpp:285]     Train net output #0: loss = 0.28032 (* 1 = 0.28032 loss)
I0122 16:46:22.730551 57874 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:46:24.188860 57874 solver.cpp:266] Iteration 19100 (68.5751 iter/s, 1.45825s/100 iter), loss = 0.204124
I0122 16:46:24.188889 57874 solver.cpp:285]     Train net output #0: loss = 0.204124 (* 1 = 0.204124 loss)
I0122 16:46:24.188935 57874 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:46:25.622706 57874 solver.cpp:266] Iteration 19200 (69.7468 iter/s, 1.43376s/100 iter), loss = 0.223464
I0122 16:46:25.622738 57874 solver.cpp:285]     Train net output #0: loss = 0.223464 (* 1 = 0.223464 loss)
I0122 16:46:25.622782 57874 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:46:27.063697 57874 solver.cpp:266] Iteration 19300 (69.4032 iter/s, 1.44086s/100 iter), loss = 0.133696
I0122 16:46:27.063729 57874 solver.cpp:285]     Train net output #0: loss = 0.133696 (* 1 = 0.133696 loss)
I0122 16:46:27.063735 57874 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:46:28.524468 57874 solver.cpp:266] Iteration 19400 (68.4618 iter/s, 1.46067s/100 iter), loss = 0.193832
I0122 16:46:28.524498 57874 solver.cpp:285]     Train net output #0: loss = 0.193832 (* 1 = 0.193832 loss)
I0122 16:46:28.524504 57874 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:46:29.969525 57874 solver.cpp:266] Iteration 19500 (69.2057 iter/s, 1.44497s/100 iter), loss = 0.0823969
I0122 16:46:29.969557 57874 solver.cpp:285]     Train net output #0: loss = 0.0823968 (* 1 = 0.0823968 loss)
I0122 16:46:29.969601 57874 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:46:31.419085 57874 solver.cpp:266] Iteration 19600 (68.993 iter/s, 1.44942s/100 iter), loss = 0.267389
I0122 16:46:31.419116 57874 solver.cpp:285]     Train net output #0: loss = 0.267389 (* 1 = 0.267389 loss)
I0122 16:46:31.419121 57874 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:46:32.883430 57874 solver.cpp:266] Iteration 19700 (68.2944 iter/s, 1.46425s/100 iter), loss = 0.186648
I0122 16:46:32.883472 57874 solver.cpp:285]     Train net output #0: loss = 0.186648 (* 1 = 0.186648 loss)
I0122 16:46:32.883479 57874 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:46:34.346019 57874 solver.cpp:266] Iteration 19800 (68.3767 iter/s, 1.46249s/100 iter), loss = 0.15316
I0122 16:46:34.346061 57874 solver.cpp:285]     Train net output #0: loss = 0.15316 (* 1 = 0.15316 loss)
I0122 16:46:34.346067 57874 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:46:35.753762 57874 solver.cpp:266] Iteration 19900 (71.0408 iter/s, 1.40764s/100 iter), loss = 0.17348
I0122 16:46:35.753792 57874 solver.cpp:285]     Train net output #0: loss = 0.173479 (* 1 = 0.173479 loss)
I0122 16:46:35.753796 57874 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:46:37.200654 57874 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/_iter_20000.caffemodel
I0122 16:46:37.273257 57874 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.6/snapshots/_iter_20000.solverstate
I0122 16:46:37.285861 57874 solver.cpp:378] Iteration 20000, loss = 0.0391697
I0122 16:46:37.285885 57874 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:46:37.686759 57874 solver.cpp:517]     Test net output #0: loss = 0.459677 (* 1 = 0.459677 loss)
I0122 16:46:37.686841 57874 solver.cpp:517]     Test net output #1: top-1 = 0.855222
I0122 16:46:37.686847 57874 solver.cpp:517]     Test net output #2: top-5 = 0.991444
I0122 16:46:37.686851 57874 solver.cpp:386] Optimization Done (72.5618 iter/s).
I0122 16:46:37.686856 57874 caffe_interface.cpp:530] Optimization Done.

## compression: 7-th run
${PRUNE_ROOT}/deephi_compress compress -config ${WORK_DIR}/config7.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_compress7_miniVggNet.txt
I0122 16:46:38.231001 58065 pruning_runner.cpp:190] Sens info found, use it.
I0122 16:46:38.279316 58065 pruning_runner.cpp:217] Start compressing, please wait...
I0122 16:46:39.869123 58065 pruning_runner.cpp:264] Compression complete 0.000181452%
I0122 16:46:40.543977 58065 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:46:40.544320 58065 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:46:40.544752 58065 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:46:40.544991 58065 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:46:40.545130 58065 layer_factory.hpp:77] Creating layer data
I0122 16:46:40.545179 58065 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:40.545579 58065 net.cpp:94] Creating Layer data
I0122 16:46:40.545588 58065 net.cpp:409] data -> data
I0122 16:46:40.545598 58065 net.cpp:409] data -> label
I0122 16:46:40.546564 58281 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:46:40.546602 58281 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:46:40.546677 58065 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:46:40.546775 58065 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:46:40.549935 58065 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:40.549978 58065 net.cpp:144] Setting up data
I0122 16:46:40.549988 58065 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:46:40.549993 58065 net.cpp:151] Top shape: 50 (50)
I0122 16:46:40.549998 58065 net.cpp:159] Memory required for data: 614600
I0122 16:46:40.550002 58065 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:46:40.550011 58065 net.cpp:94] Creating Layer label_data_1_split
I0122 16:46:40.550015 58065 net.cpp:435] label_data_1_split <- label
I0122 16:46:40.550024 58065 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:46:40.550037 58065 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:46:40.550045 58065 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:46:40.550107 58065 net.cpp:144] Setting up label_data_1_split
I0122 16:46:40.550127 58065 net.cpp:151] Top shape: 50 (50)
I0122 16:46:40.550132 58065 net.cpp:151] Top shape: 50 (50)
I0122 16:46:40.550135 58065 net.cpp:151] Top shape: 50 (50)
I0122 16:46:40.550138 58065 net.cpp:159] Memory required for data: 615200
I0122 16:46:40.550143 58065 layer_factory.hpp:77] Creating layer conv1
I0122 16:46:40.550154 58065 net.cpp:94] Creating Layer conv1
I0122 16:46:40.550160 58065 net.cpp:435] conv1 <- data
I0122 16:46:40.550168 58065 net.cpp:409] conv1 -> conv1
I0122 16:46:40.551226 58065 net.cpp:144] Setting up conv1
I0122 16:46:40.551239 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.551242 58065 net.cpp:159] Memory required for data: 7168800
I0122 16:46:40.551255 58065 layer_factory.hpp:77] Creating layer bn1
I0122 16:46:40.551265 58065 net.cpp:94] Creating Layer bn1
I0122 16:46:40.551268 58065 net.cpp:435] bn1 <- conv1
I0122 16:46:40.551275 58065 net.cpp:409] bn1 -> scale1
I0122 16:46:40.551992 58065 net.cpp:144] Setting up bn1
I0122 16:46:40.552001 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.552006 58065 net.cpp:159] Memory required for data: 13722400
I0122 16:46:40.552019 58065 layer_factory.hpp:77] Creating layer relu1
I0122 16:46:40.552027 58065 net.cpp:94] Creating Layer relu1
I0122 16:46:40.552032 58065 net.cpp:435] relu1 <- scale1
I0122 16:46:40.552038 58065 net.cpp:409] relu1 -> relu1
I0122 16:46:40.552078 58065 net.cpp:144] Setting up relu1
I0122 16:46:40.552085 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.552090 58065 net.cpp:159] Memory required for data: 20276000
I0122 16:46:40.552093 58065 layer_factory.hpp:77] Creating layer conv2
I0122 16:46:40.552103 58065 net.cpp:94] Creating Layer conv2
I0122 16:46:40.552109 58065 net.cpp:435] conv2 <- relu1
I0122 16:46:40.552116 58065 net.cpp:409] conv2 -> conv2
I0122 16:46:40.553301 58065 net.cpp:144] Setting up conv2
I0122 16:46:40.553311 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.553314 58065 net.cpp:159] Memory required for data: 26829600
I0122 16:46:40.553323 58065 layer_factory.hpp:77] Creating layer bn2
I0122 16:46:40.553330 58065 net.cpp:94] Creating Layer bn2
I0122 16:46:40.553333 58065 net.cpp:435] bn2 <- conv2
I0122 16:46:40.553339 58065 net.cpp:409] bn2 -> scale2
I0122 16:46:40.554095 58065 net.cpp:144] Setting up bn2
I0122 16:46:40.554102 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.554106 58065 net.cpp:159] Memory required for data: 33383200
I0122 16:46:40.554114 58065 layer_factory.hpp:77] Creating layer relu2
I0122 16:46:40.554119 58065 net.cpp:94] Creating Layer relu2
I0122 16:46:40.554123 58065 net.cpp:435] relu2 <- scale2
I0122 16:46:40.554127 58065 net.cpp:409] relu2 -> relu2
I0122 16:46:40.554144 58065 net.cpp:144] Setting up relu2
I0122 16:46:40.554150 58065 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:40.554153 58065 net.cpp:159] Memory required for data: 39936800
I0122 16:46:40.554157 58065 layer_factory.hpp:77] Creating layer pool1
I0122 16:46:40.554178 58065 net.cpp:94] Creating Layer pool1
I0122 16:46:40.554183 58065 net.cpp:435] pool1 <- relu2
I0122 16:46:40.554189 58065 net.cpp:409] pool1 -> pool1
I0122 16:46:40.554217 58065 net.cpp:144] Setting up pool1
I0122 16:46:40.554224 58065 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:46:40.554226 58065 net.cpp:159] Memory required for data: 41575200
I0122 16:46:40.554229 58065 layer_factory.hpp:77] Creating layer drop1
I0122 16:46:40.554234 58065 net.cpp:94] Creating Layer drop1
I0122 16:46:40.554236 58065 net.cpp:435] drop1 <- pool1
I0122 16:46:40.554240 58065 net.cpp:409] drop1 -> drop1
I0122 16:46:40.554265 58065 net.cpp:144] Setting up drop1
I0122 16:46:40.554270 58065 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:46:40.554273 58065 net.cpp:159] Memory required for data: 43213600
I0122 16:46:40.554276 58065 layer_factory.hpp:77] Creating layer conv3
I0122 16:46:40.554284 58065 net.cpp:94] Creating Layer conv3
I0122 16:46:40.554296 58065 net.cpp:435] conv3 <- drop1
I0122 16:46:40.554301 58065 net.cpp:409] conv3 -> conv3
I0122 16:46:40.555367 58065 net.cpp:144] Setting up conv3
I0122 16:46:40.555378 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.555382 58065 net.cpp:159] Memory required for data: 46490400
I0122 16:46:40.555388 58065 layer_factory.hpp:77] Creating layer bn3
I0122 16:46:40.555397 58065 net.cpp:94] Creating Layer bn3
I0122 16:46:40.555399 58065 net.cpp:435] bn3 <- conv3
I0122 16:46:40.555405 58065 net.cpp:409] bn3 -> scale3
I0122 16:46:40.556031 58065 net.cpp:144] Setting up bn3
I0122 16:46:40.556040 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.556042 58065 net.cpp:159] Memory required for data: 49767200
I0122 16:46:40.556053 58065 layer_factory.hpp:77] Creating layer relu3
I0122 16:46:40.556061 58065 net.cpp:94] Creating Layer relu3
I0122 16:46:40.556064 58065 net.cpp:435] relu3 <- scale3
I0122 16:46:40.556069 58065 net.cpp:409] relu3 -> relu3
I0122 16:46:40.556088 58065 net.cpp:144] Setting up relu3
I0122 16:46:40.556094 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.556098 58065 net.cpp:159] Memory required for data: 53044000
I0122 16:46:40.556100 58065 layer_factory.hpp:77] Creating layer conv4
I0122 16:46:40.556109 58065 net.cpp:94] Creating Layer conv4
I0122 16:46:40.556113 58065 net.cpp:435] conv4 <- relu3
I0122 16:46:40.556119 58065 net.cpp:409] conv4 -> conv4
I0122 16:46:40.556591 58065 net.cpp:144] Setting up conv4
I0122 16:46:40.556599 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.556602 58065 net.cpp:159] Memory required for data: 56320800
I0122 16:46:40.556607 58065 layer_factory.hpp:77] Creating layer bn4
I0122 16:46:40.556614 58065 net.cpp:94] Creating Layer bn4
I0122 16:46:40.556617 58065 net.cpp:435] bn4 <- conv4
I0122 16:46:40.556622 58065 net.cpp:409] bn4 -> scale4
I0122 16:46:40.557267 58065 net.cpp:144] Setting up bn4
I0122 16:46:40.557273 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.557277 58065 net.cpp:159] Memory required for data: 59597600
I0122 16:46:40.557284 58065 layer_factory.hpp:77] Creating layer relu4
I0122 16:46:40.557289 58065 net.cpp:94] Creating Layer relu4
I0122 16:46:40.557292 58065 net.cpp:435] relu4 <- scale4
I0122 16:46:40.557297 58065 net.cpp:409] relu4 -> relu4
I0122 16:46:40.557314 58065 net.cpp:144] Setting up relu4
I0122 16:46:40.557320 58065 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:40.557323 58065 net.cpp:159] Memory required for data: 62874400
I0122 16:46:40.557327 58065 layer_factory.hpp:77] Creating layer pool2
I0122 16:46:40.557332 58065 net.cpp:94] Creating Layer pool2
I0122 16:46:40.557337 58065 net.cpp:435] pool2 <- relu4
I0122 16:46:40.557340 58065 net.cpp:409] pool2 -> pool2
I0122 16:46:40.557369 58065 net.cpp:144] Setting up pool2
I0122 16:46:40.557375 58065 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:46:40.557379 58065 net.cpp:159] Memory required for data: 63693600
I0122 16:46:40.557380 58065 layer_factory.hpp:77] Creating layer drop2
I0122 16:46:40.557385 58065 net.cpp:94] Creating Layer drop2
I0122 16:46:40.557389 58065 net.cpp:435] drop2 <- pool2
I0122 16:46:40.557392 58065 net.cpp:409] drop2 -> drop2
I0122 16:46:40.557466 58065 net.cpp:144] Setting up drop2
I0122 16:46:40.557471 58065 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:46:40.557473 58065 net.cpp:159] Memory required for data: 64512800
I0122 16:46:40.557476 58065 layer_factory.hpp:77] Creating layer fc1
I0122 16:46:40.557483 58065 net.cpp:94] Creating Layer fc1
I0122 16:46:40.557487 58065 net.cpp:435] fc1 <- drop2
I0122 16:46:40.557492 58065 net.cpp:409] fc1 -> fc1
I0122 16:46:40.574487 58065 net.cpp:144] Setting up fc1
I0122 16:46:40.574504 58065 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:40.574506 58065 net.cpp:159] Memory required for data: 64615200
I0122 16:46:40.574513 58065 layer_factory.hpp:77] Creating layer bn5
I0122 16:46:40.574522 58065 net.cpp:94] Creating Layer bn5
I0122 16:46:40.574524 58065 net.cpp:435] bn5 <- fc1
I0122 16:46:40.574530 58065 net.cpp:409] bn5 -> scale5
I0122 16:46:40.575065 58065 net.cpp:144] Setting up bn5
I0122 16:46:40.575073 58065 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:40.575088 58065 net.cpp:159] Memory required for data: 64717600
I0122 16:46:40.575100 58065 layer_factory.hpp:77] Creating layer relu5
I0122 16:46:40.575109 58065 net.cpp:94] Creating Layer relu5
I0122 16:46:40.575112 58065 net.cpp:435] relu5 <- scale5
I0122 16:46:40.575117 58065 net.cpp:409] relu5 -> relu5
I0122 16:46:40.575135 58065 net.cpp:144] Setting up relu5
I0122 16:46:40.575140 58065 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:40.575143 58065 net.cpp:159] Memory required for data: 64820000
I0122 16:46:40.575146 58065 layer_factory.hpp:77] Creating layer drop3
I0122 16:46:40.575151 58065 net.cpp:94] Creating Layer drop3
I0122 16:46:40.575155 58065 net.cpp:435] drop3 <- relu5
I0122 16:46:40.575160 58065 net.cpp:409] drop3 -> drop3
I0122 16:46:40.575184 58065 net.cpp:144] Setting up drop3
I0122 16:46:40.575189 58065 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:40.575191 58065 net.cpp:159] Memory required for data: 64922400
I0122 16:46:40.575196 58065 layer_factory.hpp:77] Creating layer fc2
I0122 16:46:40.575201 58065 net.cpp:94] Creating Layer fc2
I0122 16:46:40.575204 58065 net.cpp:435] fc2 <- drop3
I0122 16:46:40.575209 58065 net.cpp:409] fc2 -> fc2
I0122 16:46:40.575337 58065 net.cpp:144] Setting up fc2
I0122 16:46:40.575342 58065 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:40.575345 58065 net.cpp:159] Memory required for data: 64924400
I0122 16:46:40.575350 58065 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:46:40.575356 58065 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:46:40.575358 58065 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:46:40.575362 58065 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:46:40.575368 58065 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:46:40.575374 58065 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:46:40.575409 58065 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:46:40.575413 58065 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:40.575417 58065 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:40.575420 58065 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:40.575423 58065 net.cpp:159] Memory required for data: 64930400
I0122 16:46:40.575425 58065 layer_factory.hpp:77] Creating layer loss
I0122 16:46:40.575430 58065 net.cpp:94] Creating Layer loss
I0122 16:46:40.575436 58065 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:46:40.575440 58065 net.cpp:435] loss <- label_data_1_split_0
I0122 16:46:40.575445 58065 net.cpp:409] loss -> loss
I0122 16:46:40.575453 58065 layer_factory.hpp:77] Creating layer loss
I0122 16:46:40.575518 58065 net.cpp:144] Setting up loss
I0122 16:46:40.575523 58065 net.cpp:151] Top shape: (1)
I0122 16:46:40.575526 58065 net.cpp:154]     with loss weight 1
I0122 16:46:40.575536 58065 net.cpp:159] Memory required for data: 64930404
I0122 16:46:40.575539 58065 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:46:40.575546 58065 net.cpp:94] Creating Layer accuracy-top1
I0122 16:46:40.575549 58065 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:46:40.575553 58065 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:46:40.575559 58065 net.cpp:409] accuracy-top1 -> top-1
I0122 16:46:40.575567 58065 net.cpp:144] Setting up accuracy-top1
I0122 16:46:40.575569 58065 net.cpp:151] Top shape: (1)
I0122 16:46:40.575572 58065 net.cpp:159] Memory required for data: 64930408
I0122 16:46:40.575575 58065 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:46:40.575579 58065 net.cpp:94] Creating Layer accuracy-top5
I0122 16:46:40.575582 58065 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:46:40.575585 58065 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:46:40.575592 58065 net.cpp:409] accuracy-top5 -> top-5
I0122 16:46:40.575598 58065 net.cpp:144] Setting up accuracy-top5
I0122 16:46:40.575600 58065 net.cpp:151] Top shape: (1)
I0122 16:46:40.575603 58065 net.cpp:159] Memory required for data: 64930412
I0122 16:46:40.575606 58065 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:46:40.575616 58065 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:46:40.575620 58065 net.cpp:220] loss needs backward computation.
I0122 16:46:40.575624 58065 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:46:40.575628 58065 net.cpp:220] fc2 needs backward computation.
I0122 16:46:40.575631 58065 net.cpp:220] drop3 needs backward computation.
I0122 16:46:40.575634 58065 net.cpp:220] relu5 needs backward computation.
I0122 16:46:40.575637 58065 net.cpp:220] bn5 needs backward computation.
I0122 16:46:40.575641 58065 net.cpp:220] fc1 needs backward computation.
I0122 16:46:40.575644 58065 net.cpp:220] drop2 needs backward computation.
I0122 16:46:40.575647 58065 net.cpp:220] pool2 needs backward computation.
I0122 16:46:40.575650 58065 net.cpp:220] relu4 needs backward computation.
I0122 16:46:40.575654 58065 net.cpp:220] bn4 needs backward computation.
I0122 16:46:40.575657 58065 net.cpp:220] conv4 needs backward computation.
I0122 16:46:40.575660 58065 net.cpp:220] relu3 needs backward computation.
I0122 16:46:40.575664 58065 net.cpp:220] bn3 needs backward computation.
I0122 16:46:40.575667 58065 net.cpp:220] conv3 needs backward computation.
I0122 16:46:40.575670 58065 net.cpp:220] drop1 needs backward computation.
I0122 16:46:40.575672 58065 net.cpp:220] pool1 needs backward computation.
I0122 16:46:40.575675 58065 net.cpp:220] relu2 needs backward computation.
I0122 16:46:40.575680 58065 net.cpp:220] bn2 needs backward computation.
I0122 16:46:40.575682 58065 net.cpp:220] conv2 needs backward computation.
I0122 16:46:40.575685 58065 net.cpp:220] relu1 needs backward computation.
I0122 16:46:40.575688 58065 net.cpp:220] bn1 needs backward computation.
I0122 16:46:40.575691 58065 net.cpp:220] conv1 needs backward computation.
I0122 16:46:40.575696 58065 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:46:40.575700 58065 net.cpp:222] data does not need backward computation.
I0122 16:46:40.575702 58065 net.cpp:264] This network produces output loss
I0122 16:46:40.575706 58065 net.cpp:264] This network produces output top-1
I0122 16:46:40.575708 58065 net.cpp:264] This network produces output top-5
I0122 16:46:40.575731 58065 net.cpp:284] Network initialization done.
I0122 16:46:40.578721 58065 caffe_interface.cpp:363] Running for 180 iterations.
I0122 16:46:40.602558 58065 caffe_interface.cpp:125] Batch 0, loss = 1.04532
I0122 16:46:40.602579 58065 caffe_interface.cpp:125] Batch 0, top-1 = 0.74
I0122 16:46:40.602584 58065 caffe_interface.cpp:125] Batch 0, top-5 = 1
I0122 16:46:40.618160 58065 caffe_interface.cpp:125] Batch 1, loss = 1.24545
I0122 16:46:40.618168 58065 caffe_interface.cpp:125] Batch 1, top-1 = 0.66
I0122 16:46:40.618172 58065 caffe_interface.cpp:125] Batch 1, top-5 = 0.94
I0122 16:46:40.619478 58065 caffe_interface.cpp:125] Batch 2, loss = 0.806117
I0122 16:46:40.619487 58065 caffe_interface.cpp:125] Batch 2, top-1 = 0.8
I0122 16:46:40.619491 58065 caffe_interface.cpp:125] Batch 2, top-5 = 0.98
I0122 16:46:40.620707 58065 caffe_interface.cpp:125] Batch 3, loss = 0.658854
I0122 16:46:40.620714 58065 caffe_interface.cpp:125] Batch 3, top-1 = 0.72
I0122 16:46:40.620718 58065 caffe_interface.cpp:125] Batch 3, top-5 = 1
I0122 16:46:40.621935 58065 caffe_interface.cpp:125] Batch 4, loss = 0.831529
I0122 16:46:40.621943 58065 caffe_interface.cpp:125] Batch 4, top-1 = 0.76
I0122 16:46:40.621946 58065 caffe_interface.cpp:125] Batch 4, top-5 = 0.98
I0122 16:46:40.623154 58065 caffe_interface.cpp:125] Batch 5, loss = 1.00251
I0122 16:46:40.623162 58065 caffe_interface.cpp:125] Batch 5, top-1 = 0.84
I0122 16:46:40.623167 58065 caffe_interface.cpp:125] Batch 5, top-5 = 0.94
I0122 16:46:40.624377 58065 caffe_interface.cpp:125] Batch 6, loss = 0.675337
I0122 16:46:40.624385 58065 caffe_interface.cpp:125] Batch 6, top-1 = 0.82
I0122 16:46:40.624389 58065 caffe_interface.cpp:125] Batch 6, top-5 = 1
I0122 16:46:40.625597 58065 caffe_interface.cpp:125] Batch 7, loss = 0.848711
I0122 16:46:40.625604 58065 caffe_interface.cpp:125] Batch 7, top-1 = 0.78
I0122 16:46:40.625619 58065 caffe_interface.cpp:125] Batch 7, top-5 = 0.98
I0122 16:46:40.626842 58065 caffe_interface.cpp:125] Batch 8, loss = 0.736623
I0122 16:46:40.626849 58065 caffe_interface.cpp:125] Batch 8, top-1 = 0.8
I0122 16:46:40.626853 58065 caffe_interface.cpp:125] Batch 8, top-5 = 1
I0122 16:46:40.628067 58065 caffe_interface.cpp:125] Batch 9, loss = 1.02799
I0122 16:46:40.628075 58065 caffe_interface.cpp:125] Batch 9, top-1 = 0.76
I0122 16:46:40.628079 58065 caffe_interface.cpp:125] Batch 9, top-5 = 0.94
I0122 16:46:40.629294 58065 caffe_interface.cpp:125] Batch 10, loss = 1.23556
I0122 16:46:40.629302 58065 caffe_interface.cpp:125] Batch 10, top-1 = 0.68
I0122 16:46:40.629307 58065 caffe_interface.cpp:125] Batch 10, top-5 = 0.96
I0122 16:46:40.630533 58065 caffe_interface.cpp:125] Batch 11, loss = 1.12191
I0122 16:46:40.630542 58065 caffe_interface.cpp:125] Batch 11, top-1 = 0.68
I0122 16:46:40.630545 58065 caffe_interface.cpp:125] Batch 11, top-5 = 1
I0122 16:46:40.631757 58065 caffe_interface.cpp:125] Batch 12, loss = 1.02458
I0122 16:46:40.631764 58065 caffe_interface.cpp:125] Batch 12, top-1 = 0.76
I0122 16:46:40.631768 58065 caffe_interface.cpp:125] Batch 12, top-5 = 0.98
I0122 16:46:40.632977 58065 caffe_interface.cpp:125] Batch 13, loss = 1.12988
I0122 16:46:40.632984 58065 caffe_interface.cpp:125] Batch 13, top-1 = 0.72
I0122 16:46:40.632987 58065 caffe_interface.cpp:125] Batch 13, top-5 = 0.96
I0122 16:46:40.634193 58065 caffe_interface.cpp:125] Batch 14, loss = 0.960066
I0122 16:46:40.634202 58065 caffe_interface.cpp:125] Batch 14, top-1 = 0.74
I0122 16:46:40.634205 58065 caffe_interface.cpp:125] Batch 14, top-5 = 0.98
I0122 16:46:40.635422 58065 caffe_interface.cpp:125] Batch 15, loss = 1.42009
I0122 16:46:40.635430 58065 caffe_interface.cpp:125] Batch 15, top-1 = 0.64
I0122 16:46:40.635434 58065 caffe_interface.cpp:125] Batch 15, top-5 = 0.96
I0122 16:46:40.636634 58065 caffe_interface.cpp:125] Batch 16, loss = 0.84664
I0122 16:46:40.636641 58065 caffe_interface.cpp:125] Batch 16, top-1 = 0.8
I0122 16:46:40.636644 58065 caffe_interface.cpp:125] Batch 16, top-5 = 0.98
I0122 16:46:40.637846 58065 caffe_interface.cpp:125] Batch 17, loss = 0.682798
I0122 16:46:40.637854 58065 caffe_interface.cpp:125] Batch 17, top-1 = 0.8
I0122 16:46:40.637857 58065 caffe_interface.cpp:125] Batch 17, top-5 = 1
I0122 16:46:40.639073 58065 caffe_interface.cpp:125] Batch 18, loss = 1.14815
I0122 16:46:40.639081 58065 caffe_interface.cpp:125] Batch 18, top-1 = 0.76
I0122 16:46:40.639086 58065 caffe_interface.cpp:125] Batch 18, top-5 = 1
I0122 16:46:40.640286 58065 caffe_interface.cpp:125] Batch 19, loss = 1.09283
I0122 16:46:40.640295 58065 caffe_interface.cpp:125] Batch 19, top-1 = 0.72
I0122 16:46:40.640297 58065 caffe_interface.cpp:125] Batch 19, top-5 = 0.96
I0122 16:46:40.641503 58065 caffe_interface.cpp:125] Batch 20, loss = 1.1016
I0122 16:46:40.641510 58065 caffe_interface.cpp:125] Batch 20, top-1 = 0.68
I0122 16:46:40.641513 58065 caffe_interface.cpp:125] Batch 20, top-5 = 0.92
I0122 16:46:40.642721 58065 caffe_interface.cpp:125] Batch 21, loss = 1.49977
I0122 16:46:40.642729 58065 caffe_interface.cpp:125] Batch 21, top-1 = 0.68
I0122 16:46:40.642733 58065 caffe_interface.cpp:125] Batch 21, top-5 = 0.98
I0122 16:46:40.645025 58065 caffe_interface.cpp:125] Batch 22, loss = 1.48869
I0122 16:46:40.645032 58065 caffe_interface.cpp:125] Batch 22, top-1 = 0.68
I0122 16:46:40.645037 58065 caffe_interface.cpp:125] Batch 22, top-5 = 0.96
I0122 16:46:40.646420 58065 caffe_interface.cpp:125] Batch 23, loss = 0.607507
I0122 16:46:40.646427 58065 caffe_interface.cpp:125] Batch 23, top-1 = 0.8
I0122 16:46:40.646430 58065 caffe_interface.cpp:125] Batch 23, top-5 = 1
I0122 16:46:40.647656 58065 caffe_interface.cpp:125] Batch 24, loss = 1.1475
I0122 16:46:40.647665 58065 caffe_interface.cpp:125] Batch 24, top-1 = 0.74
I0122 16:46:40.647667 58065 caffe_interface.cpp:125] Batch 24, top-5 = 0.98
I0122 16:46:40.648875 58065 caffe_interface.cpp:125] Batch 25, loss = 0.883799
I0122 16:46:40.648883 58065 caffe_interface.cpp:125] Batch 25, top-1 = 0.76
I0122 16:46:40.648897 58065 caffe_interface.cpp:125] Batch 25, top-5 = 0.98
I0122 16:46:40.650111 58065 caffe_interface.cpp:125] Batch 26, loss = 0.88188
I0122 16:46:40.650120 58065 caffe_interface.cpp:125] Batch 26, top-1 = 0.74
I0122 16:46:40.650123 58065 caffe_interface.cpp:125] Batch 26, top-5 = 0.96
I0122 16:46:40.651327 58065 caffe_interface.cpp:125] Batch 27, loss = 0.912609
I0122 16:46:40.651335 58065 caffe_interface.cpp:125] Batch 27, top-1 = 0.76
I0122 16:46:40.651337 58065 caffe_interface.cpp:125] Batch 27, top-5 = 0.96
I0122 16:46:40.652544 58065 caffe_interface.cpp:125] Batch 28, loss = 1.33968
I0122 16:46:40.652552 58065 caffe_interface.cpp:125] Batch 28, top-1 = 0.64
I0122 16:46:40.652555 58065 caffe_interface.cpp:125] Batch 28, top-5 = 0.96
I0122 16:46:40.653996 58065 caffe_interface.cpp:125] Batch 29, loss = 1.03608
I0122 16:46:40.654004 58065 caffe_interface.cpp:125] Batch 29, top-1 = 0.7
I0122 16:46:40.654007 58065 caffe_interface.cpp:125] Batch 29, top-5 = 0.98
I0122 16:46:40.655220 58065 caffe_interface.cpp:125] Batch 30, loss = 0.872359
I0122 16:46:40.655228 58065 caffe_interface.cpp:125] Batch 30, top-1 = 0.72
I0122 16:46:40.655232 58065 caffe_interface.cpp:125] Batch 30, top-5 = 1
I0122 16:46:40.656431 58065 caffe_interface.cpp:125] Batch 31, loss = 0.814167
I0122 16:46:40.656438 58065 caffe_interface.cpp:125] Batch 31, top-1 = 0.78
I0122 16:46:40.656441 58065 caffe_interface.cpp:125] Batch 31, top-5 = 0.98
I0122 16:46:40.657651 58065 caffe_interface.cpp:125] Batch 32, loss = 1.38205
I0122 16:46:40.657658 58065 caffe_interface.cpp:125] Batch 32, top-1 = 0.7
I0122 16:46:40.657661 58065 caffe_interface.cpp:125] Batch 32, top-5 = 0.94
I0122 16:46:40.658876 58065 caffe_interface.cpp:125] Batch 33, loss = 1.3079
I0122 16:46:40.658885 58065 caffe_interface.cpp:125] Batch 33, top-1 = 0.68
I0122 16:46:40.658888 58065 caffe_interface.cpp:125] Batch 33, top-5 = 0.96
I0122 16:46:40.660089 58065 caffe_interface.cpp:125] Batch 34, loss = 1.37852
I0122 16:46:40.660097 58065 caffe_interface.cpp:125] Batch 34, top-1 = 0.68
I0122 16:46:40.660101 58065 caffe_interface.cpp:125] Batch 34, top-5 = 0.92
I0122 16:46:40.661312 58065 caffe_interface.cpp:125] Batch 35, loss = 0.996785
I0122 16:46:40.661320 58065 caffe_interface.cpp:125] Batch 35, top-1 = 0.72
I0122 16:46:40.661324 58065 caffe_interface.cpp:125] Batch 35, top-5 = 0.98
I0122 16:46:40.662523 58065 caffe_interface.cpp:125] Batch 36, loss = 0.836439
I0122 16:46:40.662531 58065 caffe_interface.cpp:125] Batch 36, top-1 = 0.7
I0122 16:46:40.662535 58065 caffe_interface.cpp:125] Batch 36, top-5 = 1
I0122 16:46:40.663731 58065 caffe_interface.cpp:125] Batch 37, loss = 0.920321
I0122 16:46:40.663739 58065 caffe_interface.cpp:125] Batch 37, top-1 = 0.74
I0122 16:46:40.663743 58065 caffe_interface.cpp:125] Batch 37, top-5 = 0.96
I0122 16:46:40.667973 58065 caffe_interface.cpp:125] Batch 38, loss = 0.858109
I0122 16:46:40.667979 58065 caffe_interface.cpp:125] Batch 38, top-1 = 0.8
I0122 16:46:40.667984 58065 caffe_interface.cpp:125] Batch 38, top-5 = 0.96
I0122 16:46:40.673305 58065 caffe_interface.cpp:125] Batch 39, loss = 0.981076
I0122 16:46:40.673311 58065 caffe_interface.cpp:125] Batch 39, top-1 = 0.72
I0122 16:46:40.673315 58065 caffe_interface.cpp:125] Batch 39, top-5 = 0.94
I0122 16:46:40.682653 58065 caffe_interface.cpp:125] Batch 40, loss = 0.698693
I0122 16:46:40.682662 58065 caffe_interface.cpp:125] Batch 40, top-1 = 0.8
I0122 16:46:40.682664 58065 caffe_interface.cpp:125] Batch 40, top-5 = 1
I0122 16:46:40.694211 58065 caffe_interface.cpp:125] Batch 41, loss = 1.33475
I0122 16:46:40.694217 58065 caffe_interface.cpp:125] Batch 41, top-1 = 0.68
I0122 16:46:40.694221 58065 caffe_interface.cpp:125] Batch 41, top-5 = 0.98
I0122 16:46:40.716658 58065 caffe_interface.cpp:125] Batch 42, loss = 1.1628
I0122 16:46:40.716665 58065 caffe_interface.cpp:125] Batch 42, top-1 = 0.68
I0122 16:46:40.716668 58065 caffe_interface.cpp:125] Batch 42, top-5 = 0.96
I0122 16:46:40.717885 58065 caffe_interface.cpp:125] Batch 43, loss = 1.4617
I0122 16:46:40.717903 58065 caffe_interface.cpp:125] Batch 43, top-1 = 0.6
I0122 16:46:40.717911 58065 caffe_interface.cpp:125] Batch 43, top-5 = 0.92
I0122 16:46:40.719126 58065 caffe_interface.cpp:125] Batch 44, loss = 1.28083
I0122 16:46:40.719133 58065 caffe_interface.cpp:125] Batch 44, top-1 = 0.6
I0122 16:46:40.719136 58065 caffe_interface.cpp:125] Batch 44, top-5 = 0.98
I0122 16:46:40.720616 58065 caffe_interface.cpp:125] Batch 45, loss = 1.0779
I0122 16:46:40.720624 58065 caffe_interface.cpp:125] Batch 45, top-1 = 0.7
I0122 16:46:40.720628 58065 caffe_interface.cpp:125] Batch 45, top-5 = 0.98
I0122 16:46:40.721824 58065 caffe_interface.cpp:125] Batch 46, loss = 0.539751
I0122 16:46:40.721832 58065 caffe_interface.cpp:125] Batch 46, top-1 = 0.9
I0122 16:46:40.721835 58065 caffe_interface.cpp:125] Batch 46, top-5 = 0.96
I0122 16:46:40.723047 58065 caffe_interface.cpp:125] Batch 47, loss = 0.754436
I0122 16:46:40.723053 58065 caffe_interface.cpp:125] Batch 47, top-1 = 0.78
I0122 16:46:40.723057 58065 caffe_interface.cpp:125] Batch 47, top-5 = 0.96
I0122 16:46:40.724261 58065 caffe_interface.cpp:125] Batch 48, loss = 0.931204
I0122 16:46:40.724269 58065 caffe_interface.cpp:125] Batch 48, top-1 = 0.76
I0122 16:46:40.724272 58065 caffe_interface.cpp:125] Batch 48, top-5 = 0.98
I0122 16:46:40.725486 58065 caffe_interface.cpp:125] Batch 49, loss = 0.907087
I0122 16:46:40.725497 58065 caffe_interface.cpp:125] Batch 49, top-1 = 0.74
I0122 16:46:40.725499 58065 caffe_interface.cpp:125] Batch 49, top-5 = 1
I0122 16:46:40.726702 58065 caffe_interface.cpp:125] Batch 50, loss = 0.917156
I0122 16:46:40.726711 58065 caffe_interface.cpp:125] Batch 50, top-1 = 0.74
I0122 16:46:40.726714 58065 caffe_interface.cpp:125] Batch 50, top-5 = 1
I0122 16:46:40.727911 58065 caffe_interface.cpp:125] Batch 51, loss = 1.47179
I0122 16:46:40.727918 58065 caffe_interface.cpp:125] Batch 51, top-1 = 0.58
I0122 16:46:40.727921 58065 caffe_interface.cpp:125] Batch 51, top-5 = 0.98
I0122 16:46:40.729130 58065 caffe_interface.cpp:125] Batch 52, loss = 1.24609
I0122 16:46:40.729137 58065 caffe_interface.cpp:125] Batch 52, top-1 = 0.66
I0122 16:46:40.729140 58065 caffe_interface.cpp:125] Batch 52, top-5 = 0.96
I0122 16:46:40.730347 58065 caffe_interface.cpp:125] Batch 53, loss = 1.31066
I0122 16:46:40.730356 58065 caffe_interface.cpp:125] Batch 53, top-1 = 0.62
I0122 16:46:40.730360 58065 caffe_interface.cpp:125] Batch 53, top-5 = 0.94
I0122 16:46:40.731571 58065 caffe_interface.cpp:125] Batch 54, loss = 0.871637
I0122 16:46:40.731578 58065 caffe_interface.cpp:125] Batch 54, top-1 = 0.72
I0122 16:46:40.731582 58065 caffe_interface.cpp:125] Batch 54, top-5 = 1
I0122 16:46:40.732781 58065 caffe_interface.cpp:125] Batch 55, loss = 0.818047
I0122 16:46:40.732789 58065 caffe_interface.cpp:125] Batch 55, top-1 = 0.68
I0122 16:46:40.732792 58065 caffe_interface.cpp:125] Batch 55, top-5 = 0.98
I0122 16:46:40.734007 58065 caffe_interface.cpp:125] Batch 56, loss = 1.29862
I0122 16:46:40.734015 58065 caffe_interface.cpp:125] Batch 56, top-1 = 0.7
I0122 16:46:40.734019 58065 caffe_interface.cpp:125] Batch 56, top-5 = 0.94
I0122 16:46:40.735229 58065 caffe_interface.cpp:125] Batch 57, loss = 0.466239
I0122 16:46:40.735237 58065 caffe_interface.cpp:125] Batch 57, top-1 = 0.82
I0122 16:46:40.735240 58065 caffe_interface.cpp:125] Batch 57, top-5 = 1
I0122 16:46:40.736464 58065 caffe_interface.cpp:125] Batch 58, loss = 1.07272
I0122 16:46:40.736470 58065 caffe_interface.cpp:125] Batch 58, top-1 = 0.7
I0122 16:46:40.736474 58065 caffe_interface.cpp:125] Batch 58, top-5 = 0.98
I0122 16:46:40.737689 58065 caffe_interface.cpp:125] Batch 59, loss = 0.77794
I0122 16:46:40.737697 58065 caffe_interface.cpp:125] Batch 59, top-1 = 0.8
I0122 16:46:40.737700 58065 caffe_interface.cpp:125] Batch 59, top-5 = 1
I0122 16:46:40.738921 58065 caffe_interface.cpp:125] Batch 60, loss = 1.57578
I0122 16:46:40.738929 58065 caffe_interface.cpp:125] Batch 60, top-1 = 0.62
I0122 16:46:40.738934 58065 caffe_interface.cpp:125] Batch 60, top-5 = 0.96
I0122 16:46:40.740133 58065 caffe_interface.cpp:125] Batch 61, loss = 1.13816
I0122 16:46:40.740150 58065 caffe_interface.cpp:125] Batch 61, top-1 = 0.7
I0122 16:46:40.740154 58065 caffe_interface.cpp:125] Batch 61, top-5 = 0.94
I0122 16:46:40.741358 58065 caffe_interface.cpp:125] Batch 62, loss = 0.971992
I0122 16:46:40.741366 58065 caffe_interface.cpp:125] Batch 62, top-1 = 0.72
I0122 16:46:40.741370 58065 caffe_interface.cpp:125] Batch 62, top-5 = 1
I0122 16:46:40.742574 58065 caffe_interface.cpp:125] Batch 63, loss = 1.10715
I0122 16:46:40.742583 58065 caffe_interface.cpp:125] Batch 63, top-1 = 0.76
I0122 16:46:40.742586 58065 caffe_interface.cpp:125] Batch 63, top-5 = 0.98
I0122 16:46:40.743804 58065 caffe_interface.cpp:125] Batch 64, loss = 0.667739
I0122 16:46:40.743811 58065 caffe_interface.cpp:125] Batch 64, top-1 = 0.76
I0122 16:46:40.743814 58065 caffe_interface.cpp:125] Batch 64, top-5 = 1
I0122 16:46:40.745026 58065 caffe_interface.cpp:125] Batch 65, loss = 0.664507
I0122 16:46:40.745034 58065 caffe_interface.cpp:125] Batch 65, top-1 = 0.84
I0122 16:46:40.745038 58065 caffe_interface.cpp:125] Batch 65, top-5 = 0.98
I0122 16:46:40.746244 58065 caffe_interface.cpp:125] Batch 66, loss = 1.68517
I0122 16:46:40.746253 58065 caffe_interface.cpp:125] Batch 66, top-1 = 0.68
I0122 16:46:40.746256 58065 caffe_interface.cpp:125] Batch 66, top-5 = 0.94
I0122 16:46:40.747468 58065 caffe_interface.cpp:125] Batch 67, loss = 1.14459
I0122 16:46:40.747475 58065 caffe_interface.cpp:125] Batch 67, top-1 = 0.7
I0122 16:46:40.747479 58065 caffe_interface.cpp:125] Batch 67, top-5 = 0.96
I0122 16:46:40.749670 58065 caffe_interface.cpp:125] Batch 68, loss = 0.440255
I0122 16:46:40.749677 58065 caffe_interface.cpp:125] Batch 68, top-1 = 0.84
I0122 16:46:40.749680 58065 caffe_interface.cpp:125] Batch 68, top-5 = 1
I0122 16:46:40.751054 58065 caffe_interface.cpp:125] Batch 69, loss = 0.826125
I0122 16:46:40.751067 58065 caffe_interface.cpp:125] Batch 69, top-1 = 0.82
I0122 16:46:40.751070 58065 caffe_interface.cpp:125] Batch 69, top-5 = 0.98
I0122 16:46:40.752311 58065 caffe_interface.cpp:125] Batch 70, loss = 0.758426
I0122 16:46:40.752322 58065 caffe_interface.cpp:125] Batch 70, top-1 = 0.82
I0122 16:46:40.752327 58065 caffe_interface.cpp:125] Batch 70, top-5 = 1
I0122 16:46:40.753615 58065 caffe_interface.cpp:125] Batch 71, loss = 1.01897
I0122 16:46:40.753623 58065 caffe_interface.cpp:125] Batch 71, top-1 = 0.72
I0122 16:46:40.753625 58065 caffe_interface.cpp:125] Batch 71, top-5 = 0.98
I0122 16:46:40.754830 58065 caffe_interface.cpp:125] Batch 72, loss = 0.988205
I0122 16:46:40.754838 58065 caffe_interface.cpp:125] Batch 72, top-1 = 0.68
I0122 16:46:40.754842 58065 caffe_interface.cpp:125] Batch 72, top-5 = 0.98
I0122 16:46:40.756048 58065 caffe_interface.cpp:125] Batch 73, loss = 1.05152
I0122 16:46:40.756057 58065 caffe_interface.cpp:125] Batch 73, top-1 = 0.7
I0122 16:46:40.756059 58065 caffe_interface.cpp:125] Batch 73, top-5 = 0.96
I0122 16:46:40.757268 58065 caffe_interface.cpp:125] Batch 74, loss = 1.12002
I0122 16:46:40.757275 58065 caffe_interface.cpp:125] Batch 74, top-1 = 0.68
I0122 16:46:40.757279 58065 caffe_interface.cpp:125] Batch 74, top-5 = 0.94
I0122 16:46:40.758493 58065 caffe_interface.cpp:125] Batch 75, loss = 1.53975
I0122 16:46:40.758502 58065 caffe_interface.cpp:125] Batch 75, top-1 = 0.68
I0122 16:46:40.758505 58065 caffe_interface.cpp:125] Batch 75, top-5 = 0.94
I0122 16:46:40.759703 58065 caffe_interface.cpp:125] Batch 76, loss = 1.14159
I0122 16:46:40.759711 58065 caffe_interface.cpp:125] Batch 76, top-1 = 0.76
I0122 16:46:40.759713 58065 caffe_interface.cpp:125] Batch 76, top-5 = 0.94
I0122 16:46:40.760921 58065 caffe_interface.cpp:125] Batch 77, loss = 0.794041
I0122 16:46:40.760928 58065 caffe_interface.cpp:125] Batch 77, top-1 = 0.78
I0122 16:46:40.760931 58065 caffe_interface.cpp:125] Batch 77, top-5 = 0.98
I0122 16:46:40.762141 58065 caffe_interface.cpp:125] Batch 78, loss = 0.825474
I0122 16:46:40.762148 58065 caffe_interface.cpp:125] Batch 78, top-1 = 0.8
I0122 16:46:40.762153 58065 caffe_interface.cpp:125] Batch 78, top-5 = 0.98
I0122 16:46:40.768021 58065 caffe_interface.cpp:125] Batch 79, loss = 1.07207
I0122 16:46:40.768038 58065 caffe_interface.cpp:125] Batch 79, top-1 = 0.66
I0122 16:46:40.768040 58065 caffe_interface.cpp:125] Batch 79, top-5 = 0.98
I0122 16:46:40.778261 58065 caffe_interface.cpp:125] Batch 80, loss = 0.793972
I0122 16:46:40.778270 58065 caffe_interface.cpp:125] Batch 80, top-1 = 0.7
I0122 16:46:40.778272 58065 caffe_interface.cpp:125] Batch 80, top-5 = 0.98
I0122 16:46:40.788528 58065 caffe_interface.cpp:125] Batch 81, loss = 1.27994
I0122 16:46:40.788537 58065 caffe_interface.cpp:125] Batch 81, top-1 = 0.68
I0122 16:46:40.788539 58065 caffe_interface.cpp:125] Batch 81, top-5 = 0.92
I0122 16:46:40.804415 58065 caffe_interface.cpp:125] Batch 82, loss = 1.29629
I0122 16:46:40.804424 58065 caffe_interface.cpp:125] Batch 82, top-1 = 0.64
I0122 16:46:40.804427 58065 caffe_interface.cpp:125] Batch 82, top-5 = 0.98
I0122 16:46:40.814494 58065 caffe_interface.cpp:125] Batch 83, loss = 0.976237
I0122 16:46:40.814502 58065 caffe_interface.cpp:125] Batch 83, top-1 = 0.78
I0122 16:46:40.814505 58065 caffe_interface.cpp:125] Batch 83, top-5 = 1
I0122 16:46:40.815711 58065 caffe_interface.cpp:125] Batch 84, loss = 0.985523
I0122 16:46:40.815718 58065 caffe_interface.cpp:125] Batch 84, top-1 = 0.64
I0122 16:46:40.815721 58065 caffe_interface.cpp:125] Batch 84, top-5 = 0.96
I0122 16:46:40.816939 58065 caffe_interface.cpp:125] Batch 85, loss = 0.794591
I0122 16:46:40.816947 58065 caffe_interface.cpp:125] Batch 85, top-1 = 0.78
I0122 16:46:40.816951 58065 caffe_interface.cpp:125] Batch 85, top-5 = 0.96
I0122 16:46:40.818162 58065 caffe_interface.cpp:125] Batch 86, loss = 1.06521
I0122 16:46:40.818169 58065 caffe_interface.cpp:125] Batch 86, top-1 = 0.76
I0122 16:46:40.818174 58065 caffe_interface.cpp:125] Batch 86, top-5 = 0.96
I0122 16:46:40.820279 58065 caffe_interface.cpp:125] Batch 87, loss = 0.663353
I0122 16:46:40.820286 58065 caffe_interface.cpp:125] Batch 87, top-1 = 0.8
I0122 16:46:40.820289 58065 caffe_interface.cpp:125] Batch 87, top-5 = 0.98
I0122 16:46:40.821573 58065 caffe_interface.cpp:125] Batch 88, loss = 0.977538
I0122 16:46:40.821581 58065 caffe_interface.cpp:125] Batch 88, top-1 = 0.72
I0122 16:46:40.821584 58065 caffe_interface.cpp:125] Batch 88, top-5 = 0.94
I0122 16:46:40.822968 58065 caffe_interface.cpp:125] Batch 89, loss = 0.667445
I0122 16:46:40.822976 58065 caffe_interface.cpp:125] Batch 89, top-1 = 0.8
I0122 16:46:40.822979 58065 caffe_interface.cpp:125] Batch 89, top-5 = 1
I0122 16:46:40.824192 58065 caffe_interface.cpp:125] Batch 90, loss = 1.10633
I0122 16:46:40.824199 58065 caffe_interface.cpp:125] Batch 90, top-1 = 0.74
I0122 16:46:40.824203 58065 caffe_interface.cpp:125] Batch 90, top-5 = 0.96
I0122 16:46:40.825424 58065 caffe_interface.cpp:125] Batch 91, loss = 1.07462
I0122 16:46:40.825433 58065 caffe_interface.cpp:125] Batch 91, top-1 = 0.66
I0122 16:46:40.825435 58065 caffe_interface.cpp:125] Batch 91, top-5 = 0.98
I0122 16:46:40.826653 58065 caffe_interface.cpp:125] Batch 92, loss = 1.37489
I0122 16:46:40.826660 58065 caffe_interface.cpp:125] Batch 92, top-1 = 0.68
I0122 16:46:40.826664 58065 caffe_interface.cpp:125] Batch 92, top-5 = 0.96
I0122 16:46:40.827885 58065 caffe_interface.cpp:125] Batch 93, loss = 0.788607
I0122 16:46:40.827893 58065 caffe_interface.cpp:125] Batch 93, top-1 = 0.78
I0122 16:46:40.827896 58065 caffe_interface.cpp:125] Batch 93, top-5 = 0.98
I0122 16:46:40.829099 58065 caffe_interface.cpp:125] Batch 94, loss = 2.03082
I0122 16:46:40.829107 58065 caffe_interface.cpp:125] Batch 94, top-1 = 0.52
I0122 16:46:40.829110 58065 caffe_interface.cpp:125] Batch 94, top-5 = 0.94
I0122 16:46:40.830334 58065 caffe_interface.cpp:125] Batch 95, loss = 0.956458
I0122 16:46:40.830343 58065 caffe_interface.cpp:125] Batch 95, top-1 = 0.76
I0122 16:46:40.830345 58065 caffe_interface.cpp:125] Batch 95, top-5 = 0.98
I0122 16:46:40.831571 58065 caffe_interface.cpp:125] Batch 96, loss = 1.09397
I0122 16:46:40.831579 58065 caffe_interface.cpp:125] Batch 96, top-1 = 0.74
I0122 16:46:40.831583 58065 caffe_interface.cpp:125] Batch 96, top-5 = 0.98
I0122 16:46:40.832798 58065 caffe_interface.cpp:125] Batch 97, loss = 0.528506
I0122 16:46:40.832805 58065 caffe_interface.cpp:125] Batch 97, top-1 = 0.78
I0122 16:46:40.832808 58065 caffe_interface.cpp:125] Batch 97, top-5 = 0.98
I0122 16:46:40.834035 58065 caffe_interface.cpp:125] Batch 98, loss = 0.912603
I0122 16:46:40.834043 58065 caffe_interface.cpp:125] Batch 98, top-1 = 0.74
I0122 16:46:40.834048 58065 caffe_interface.cpp:125] Batch 98, top-5 = 1
I0122 16:46:40.835249 58065 caffe_interface.cpp:125] Batch 99, loss = 1.0065
I0122 16:46:40.835258 58065 caffe_interface.cpp:125] Batch 99, top-1 = 0.72
I0122 16:46:40.835260 58065 caffe_interface.cpp:125] Batch 99, top-5 = 1
I0122 16:46:40.836520 58065 caffe_interface.cpp:125] Batch 100, loss = 0.945765
I0122 16:46:40.836527 58065 caffe_interface.cpp:125] Batch 100, top-1 = 0.78
I0122 16:46:40.836530 58065 caffe_interface.cpp:125] Batch 100, top-5 = 0.96
I0122 16:46:40.837749 58065 caffe_interface.cpp:125] Batch 101, loss = 0.480934
I0122 16:46:40.837759 58065 caffe_interface.cpp:125] Batch 101, top-1 = 0.86
I0122 16:46:40.837761 58065 caffe_interface.cpp:125] Batch 101, top-5 = 1
I0122 16:46:40.838974 58065 caffe_interface.cpp:125] Batch 102, loss = 1.14407
I0122 16:46:40.838982 58065 caffe_interface.cpp:125] Batch 102, top-1 = 0.7
I0122 16:46:40.838986 58065 caffe_interface.cpp:125] Batch 102, top-5 = 1
I0122 16:46:40.840184 58065 caffe_interface.cpp:125] Batch 103, loss = 0.596823
I0122 16:46:40.840191 58065 caffe_interface.cpp:125] Batch 103, top-1 = 0.82
I0122 16:46:40.840195 58065 caffe_interface.cpp:125] Batch 103, top-5 = 0.98
I0122 16:46:40.841401 58065 caffe_interface.cpp:125] Batch 104, loss = 0.90853
I0122 16:46:40.841408 58065 caffe_interface.cpp:125] Batch 104, top-1 = 0.7
I0122 16:46:40.841413 58065 caffe_interface.cpp:125] Batch 104, top-5 = 0.98
I0122 16:46:40.842622 58065 caffe_interface.cpp:125] Batch 105, loss = 0.876959
I0122 16:46:40.842631 58065 caffe_interface.cpp:125] Batch 105, top-1 = 0.74
I0122 16:46:40.842635 58065 caffe_interface.cpp:125] Batch 105, top-5 = 1
I0122 16:46:40.843847 58065 caffe_interface.cpp:125] Batch 106, loss = 0.883244
I0122 16:46:40.843854 58065 caffe_interface.cpp:125] Batch 106, top-1 = 0.74
I0122 16:46:40.843858 58065 caffe_interface.cpp:125] Batch 106, top-5 = 0.98
I0122 16:46:40.845064 58065 caffe_interface.cpp:125] Batch 107, loss = 1.10049
I0122 16:46:40.845072 58065 caffe_interface.cpp:125] Batch 107, top-1 = 0.72
I0122 16:46:40.845077 58065 caffe_interface.cpp:125] Batch 107, top-5 = 0.98
I0122 16:46:40.846292 58065 caffe_interface.cpp:125] Batch 108, loss = 0.685486
I0122 16:46:40.846299 58065 caffe_interface.cpp:125] Batch 108, top-1 = 0.82
I0122 16:46:40.846303 58065 caffe_interface.cpp:125] Batch 108, top-5 = 1
I0122 16:46:40.847514 58065 caffe_interface.cpp:125] Batch 109, loss = 1.00584
I0122 16:46:40.847522 58065 caffe_interface.cpp:125] Batch 109, top-1 = 0.66
I0122 16:46:40.847527 58065 caffe_interface.cpp:125] Batch 109, top-5 = 1
I0122 16:46:40.848749 58065 caffe_interface.cpp:125] Batch 110, loss = 0.995727
I0122 16:46:40.848763 58065 caffe_interface.cpp:125] Batch 110, top-1 = 0.68
I0122 16:46:40.848767 58065 caffe_interface.cpp:125] Batch 110, top-5 = 0.98
I0122 16:46:40.849993 58065 caffe_interface.cpp:125] Batch 111, loss = 1.28213
I0122 16:46:40.850001 58065 caffe_interface.cpp:125] Batch 111, top-1 = 0.6
I0122 16:46:40.850005 58065 caffe_interface.cpp:125] Batch 111, top-5 = 0.98
I0122 16:46:40.851222 58065 caffe_interface.cpp:125] Batch 112, loss = 0.594952
I0122 16:46:40.851228 58065 caffe_interface.cpp:125] Batch 112, top-1 = 0.84
I0122 16:46:40.851233 58065 caffe_interface.cpp:125] Batch 112, top-5 = 0.98
I0122 16:46:40.853381 58065 caffe_interface.cpp:125] Batch 113, loss = 0.921423
I0122 16:46:40.853389 58065 caffe_interface.cpp:125] Batch 113, top-1 = 0.7
I0122 16:46:40.853394 58065 caffe_interface.cpp:125] Batch 113, top-5 = 1
I0122 16:46:40.854720 58065 caffe_interface.cpp:125] Batch 114, loss = 1.47721
I0122 16:46:40.854735 58065 caffe_interface.cpp:125] Batch 114, top-1 = 0.54
I0122 16:46:40.854748 58065 caffe_interface.cpp:125] Batch 114, top-5 = 0.98
I0122 16:46:40.856142 58065 caffe_interface.cpp:125] Batch 115, loss = 0.573719
I0122 16:46:40.856149 58065 caffe_interface.cpp:125] Batch 115, top-1 = 0.82
I0122 16:46:40.856153 58065 caffe_interface.cpp:125] Batch 115, top-5 = 1
I0122 16:46:40.857378 58065 caffe_interface.cpp:125] Batch 116, loss = 1.13351
I0122 16:46:40.857384 58065 caffe_interface.cpp:125] Batch 116, top-1 = 0.64
I0122 16:46:40.857388 58065 caffe_interface.cpp:125] Batch 116, top-5 = 1
I0122 16:46:40.858595 58065 caffe_interface.cpp:125] Batch 117, loss = 0.939312
I0122 16:46:40.858603 58065 caffe_interface.cpp:125] Batch 117, top-1 = 0.72
I0122 16:46:40.858608 58065 caffe_interface.cpp:125] Batch 117, top-5 = 1
I0122 16:46:40.859828 58065 caffe_interface.cpp:125] Batch 118, loss = 0.894557
I0122 16:46:40.859836 58065 caffe_interface.cpp:125] Batch 118, top-1 = 0.72
I0122 16:46:40.859839 58065 caffe_interface.cpp:125] Batch 118, top-5 = 0.94
I0122 16:46:40.861061 58065 caffe_interface.cpp:125] Batch 119, loss = 1.47285
I0122 16:46:40.861069 58065 caffe_interface.cpp:125] Batch 119, top-1 = 0.72
I0122 16:46:40.861073 58065 caffe_interface.cpp:125] Batch 119, top-5 = 0.92
I0122 16:46:40.866782 58065 caffe_interface.cpp:125] Batch 120, loss = 1.53752
I0122 16:46:40.866789 58065 caffe_interface.cpp:125] Batch 120, top-1 = 0.62
I0122 16:46:40.866792 58065 caffe_interface.cpp:125] Batch 120, top-5 = 1
I0122 16:46:40.875031 58065 caffe_interface.cpp:125] Batch 121, loss = 1.20594
I0122 16:46:40.875041 58065 caffe_interface.cpp:125] Batch 121, top-1 = 0.7
I0122 16:46:40.875043 58065 caffe_interface.cpp:125] Batch 121, top-5 = 0.96
I0122 16:46:40.881773 58065 caffe_interface.cpp:125] Batch 122, loss = 0.876503
I0122 16:46:40.881781 58065 caffe_interface.cpp:125] Batch 122, top-1 = 0.82
I0122 16:46:40.881786 58065 caffe_interface.cpp:125] Batch 122, top-5 = 1
I0122 16:46:40.896472 58065 caffe_interface.cpp:125] Batch 123, loss = 0.695081
I0122 16:46:40.896479 58065 caffe_interface.cpp:125] Batch 123, top-1 = 0.8
I0122 16:46:40.896481 58065 caffe_interface.cpp:125] Batch 123, top-5 = 1
I0122 16:46:40.913336 58065 caffe_interface.cpp:125] Batch 124, loss = 1.06296
I0122 16:46:40.913343 58065 caffe_interface.cpp:125] Batch 124, top-1 = 0.72
I0122 16:46:40.913345 58065 caffe_interface.cpp:125] Batch 124, top-5 = 0.98
I0122 16:46:40.914563 58065 caffe_interface.cpp:125] Batch 125, loss = 0.99463
I0122 16:46:40.914572 58065 caffe_interface.cpp:125] Batch 125, top-1 = 0.72
I0122 16:46:40.914575 58065 caffe_interface.cpp:125] Batch 125, top-5 = 0.98
I0122 16:46:40.915782 58065 caffe_interface.cpp:125] Batch 126, loss = 1.3901
I0122 16:46:40.915789 58065 caffe_interface.cpp:125] Batch 126, top-1 = 0.64
I0122 16:46:40.915793 58065 caffe_interface.cpp:125] Batch 126, top-5 = 0.96
I0122 16:46:40.917006 58065 caffe_interface.cpp:125] Batch 127, loss = 1.41232
I0122 16:46:40.917021 58065 caffe_interface.cpp:125] Batch 127, top-1 = 0.6
I0122 16:46:40.917024 58065 caffe_interface.cpp:125] Batch 127, top-5 = 0.98
I0122 16:46:40.918236 58065 caffe_interface.cpp:125] Batch 128, loss = 0.754334
I0122 16:46:40.918244 58065 caffe_interface.cpp:125] Batch 128, top-1 = 0.74
I0122 16:46:40.918248 58065 caffe_interface.cpp:125] Batch 128, top-5 = 0.98
I0122 16:46:40.919456 58065 caffe_interface.cpp:125] Batch 129, loss = 0.679883
I0122 16:46:40.919464 58065 caffe_interface.cpp:125] Batch 129, top-1 = 0.78
I0122 16:46:40.919467 58065 caffe_interface.cpp:125] Batch 129, top-5 = 1
I0122 16:46:40.921723 58065 caffe_interface.cpp:125] Batch 130, loss = 1.3064
I0122 16:46:40.921731 58065 caffe_interface.cpp:125] Batch 130, top-1 = 0.64
I0122 16:46:40.921736 58065 caffe_interface.cpp:125] Batch 130, top-5 = 0.98
I0122 16:46:40.922996 58065 caffe_interface.cpp:125] Batch 131, loss = 1.49326
I0122 16:46:40.923003 58065 caffe_interface.cpp:125] Batch 131, top-1 = 0.58
I0122 16:46:40.923007 58065 caffe_interface.cpp:125] Batch 131, top-5 = 0.96
I0122 16:46:40.924392 58065 caffe_interface.cpp:125] Batch 132, loss = 1.19904
I0122 16:46:40.924414 58065 caffe_interface.cpp:125] Batch 132, top-1 = 0.68
I0122 16:46:40.924417 58065 caffe_interface.cpp:125] Batch 132, top-5 = 0.98
I0122 16:46:40.925639 58065 caffe_interface.cpp:125] Batch 133, loss = 1.39128
I0122 16:46:40.925647 58065 caffe_interface.cpp:125] Batch 133, top-1 = 0.68
I0122 16:46:40.925652 58065 caffe_interface.cpp:125] Batch 133, top-5 = 0.98
I0122 16:46:40.926856 58065 caffe_interface.cpp:125] Batch 134, loss = 0.898068
I0122 16:46:40.926863 58065 caffe_interface.cpp:125] Batch 134, top-1 = 0.66
I0122 16:46:40.926867 58065 caffe_interface.cpp:125] Batch 134, top-5 = 1
I0122 16:46:40.928076 58065 caffe_interface.cpp:125] Batch 135, loss = 1.26726
I0122 16:46:40.928084 58065 caffe_interface.cpp:125] Batch 135, top-1 = 0.62
I0122 16:46:40.928087 58065 caffe_interface.cpp:125] Batch 135, top-5 = 0.98
I0122 16:46:40.929308 58065 caffe_interface.cpp:125] Batch 136, loss = 1.24798
I0122 16:46:40.929316 58065 caffe_interface.cpp:125] Batch 136, top-1 = 0.72
I0122 16:46:40.929318 58065 caffe_interface.cpp:125] Batch 136, top-5 = 0.98
I0122 16:46:40.930529 58065 caffe_interface.cpp:125] Batch 137, loss = 0.845152
I0122 16:46:40.930537 58065 caffe_interface.cpp:125] Batch 137, top-1 = 0.7
I0122 16:46:40.930541 58065 caffe_interface.cpp:125] Batch 137, top-5 = 1
I0122 16:46:40.931746 58065 caffe_interface.cpp:125] Batch 138, loss = 0.956352
I0122 16:46:40.931752 58065 caffe_interface.cpp:125] Batch 138, top-1 = 0.72
I0122 16:46:40.931756 58065 caffe_interface.cpp:125] Batch 138, top-5 = 1
I0122 16:46:40.932961 58065 caffe_interface.cpp:125] Batch 139, loss = 1.13297
I0122 16:46:40.932976 58065 caffe_interface.cpp:125] Batch 139, top-1 = 0.76
I0122 16:46:40.932979 58065 caffe_interface.cpp:125] Batch 139, top-5 = 0.96
I0122 16:46:40.934195 58065 caffe_interface.cpp:125] Batch 140, loss = 1.05376
I0122 16:46:40.934201 58065 caffe_interface.cpp:125] Batch 140, top-1 = 0.72
I0122 16:46:40.934204 58065 caffe_interface.cpp:125] Batch 140, top-5 = 0.96
I0122 16:46:40.935410 58065 caffe_interface.cpp:125] Batch 141, loss = 0.89403
I0122 16:46:40.935418 58065 caffe_interface.cpp:125] Batch 141, top-1 = 0.76
I0122 16:46:40.935421 58065 caffe_interface.cpp:125] Batch 141, top-5 = 0.98
I0122 16:46:40.936894 58065 caffe_interface.cpp:125] Batch 142, loss = 0.635987
I0122 16:46:40.936902 58065 caffe_interface.cpp:125] Batch 142, top-1 = 0.82
I0122 16:46:40.936905 58065 caffe_interface.cpp:125] Batch 142, top-5 = 0.98
I0122 16:46:40.938110 58065 caffe_interface.cpp:125] Batch 143, loss = 1.10537
I0122 16:46:40.938118 58065 caffe_interface.cpp:125] Batch 143, top-1 = 0.66
I0122 16:46:40.938122 58065 caffe_interface.cpp:125] Batch 143, top-5 = 0.94
I0122 16:46:40.939324 58065 caffe_interface.cpp:125] Batch 144, loss = 0.883345
I0122 16:46:40.939332 58065 caffe_interface.cpp:125] Batch 144, top-1 = 0.78
I0122 16:46:40.939335 58065 caffe_interface.cpp:125] Batch 144, top-5 = 1
I0122 16:46:40.940553 58065 caffe_interface.cpp:125] Batch 145, loss = 0.55877
I0122 16:46:40.940562 58065 caffe_interface.cpp:125] Batch 145, top-1 = 0.82
I0122 16:46:40.940564 58065 caffe_interface.cpp:125] Batch 145, top-5 = 1
I0122 16:46:40.941766 58065 caffe_interface.cpp:125] Batch 146, loss = 0.847331
I0122 16:46:40.941772 58065 caffe_interface.cpp:125] Batch 146, top-1 = 0.72
I0122 16:46:40.941776 58065 caffe_interface.cpp:125] Batch 146, top-5 = 0.98
I0122 16:46:40.942976 58065 caffe_interface.cpp:125] Batch 147, loss = 1.30982
I0122 16:46:40.942984 58065 caffe_interface.cpp:125] Batch 147, top-1 = 0.64
I0122 16:46:40.942988 58065 caffe_interface.cpp:125] Batch 147, top-5 = 0.98
I0122 16:46:40.944190 58065 caffe_interface.cpp:125] Batch 148, loss = 0.787106
I0122 16:46:40.944197 58065 caffe_interface.cpp:125] Batch 148, top-1 = 0.78
I0122 16:46:40.944200 58065 caffe_interface.cpp:125] Batch 148, top-5 = 1
I0122 16:46:40.945405 58065 caffe_interface.cpp:125] Batch 149, loss = 0.934255
I0122 16:46:40.945411 58065 caffe_interface.cpp:125] Batch 149, top-1 = 0.78
I0122 16:46:40.945415 58065 caffe_interface.cpp:125] Batch 149, top-5 = 0.96
I0122 16:46:40.946631 58065 caffe_interface.cpp:125] Batch 150, loss = 1.10088
I0122 16:46:40.946640 58065 caffe_interface.cpp:125] Batch 150, top-1 = 0.72
I0122 16:46:40.946643 58065 caffe_interface.cpp:125] Batch 150, top-5 = 0.96
I0122 16:46:40.947849 58065 caffe_interface.cpp:125] Batch 151, loss = 1.19224
I0122 16:46:40.947856 58065 caffe_interface.cpp:125] Batch 151, top-1 = 0.68
I0122 16:46:40.947860 58065 caffe_interface.cpp:125] Batch 151, top-5 = 1
I0122 16:46:40.949071 58065 caffe_interface.cpp:125] Batch 152, loss = 0.794252
I0122 16:46:40.949084 58065 caffe_interface.cpp:125] Batch 152, top-1 = 0.8
I0122 16:46:40.949087 58065 caffe_interface.cpp:125] Batch 152, top-5 = 0.98
I0122 16:46:40.950292 58065 caffe_interface.cpp:125] Batch 153, loss = 0.952556
I0122 16:46:40.950299 58065 caffe_interface.cpp:125] Batch 153, top-1 = 0.74
I0122 16:46:40.950302 58065 caffe_interface.cpp:125] Batch 153, top-5 = 0.96
I0122 16:46:40.951508 58065 caffe_interface.cpp:125] Batch 154, loss = 1.11791
I0122 16:46:40.951516 58065 caffe_interface.cpp:125] Batch 154, top-1 = 0.7
I0122 16:46:40.951520 58065 caffe_interface.cpp:125] Batch 154, top-5 = 1
I0122 16:46:40.952718 58065 caffe_interface.cpp:125] Batch 155, loss = 0.626066
I0122 16:46:40.952725 58065 caffe_interface.cpp:125] Batch 155, top-1 = 0.8
I0122 16:46:40.952728 58065 caffe_interface.cpp:125] Batch 155, top-5 = 1
I0122 16:46:40.954857 58065 caffe_interface.cpp:125] Batch 156, loss = 1.55054
I0122 16:46:40.954865 58065 caffe_interface.cpp:125] Batch 156, top-1 = 0.58
I0122 16:46:40.954869 58065 caffe_interface.cpp:125] Batch 156, top-5 = 0.96
I0122 16:46:40.956180 58065 caffe_interface.cpp:125] Batch 157, loss = 0.874293
I0122 16:46:40.956188 58065 caffe_interface.cpp:125] Batch 157, top-1 = 0.7
I0122 16:46:40.956192 58065 caffe_interface.cpp:125] Batch 157, top-5 = 0.96
I0122 16:46:40.957517 58065 caffe_interface.cpp:125] Batch 158, loss = 1.06961
I0122 16:46:40.957525 58065 caffe_interface.cpp:125] Batch 158, top-1 = 0.72
I0122 16:46:40.957528 58065 caffe_interface.cpp:125] Batch 158, top-5 = 0.96
I0122 16:46:40.958741 58065 caffe_interface.cpp:125] Batch 159, loss = 1.39624
I0122 16:46:40.958748 58065 caffe_interface.cpp:125] Batch 159, top-1 = 0.6
I0122 16:46:40.958752 58065 caffe_interface.cpp:125] Batch 159, top-5 = 0.98
I0122 16:46:40.959961 58065 caffe_interface.cpp:125] Batch 160, loss = 0.623602
I0122 16:46:40.959969 58065 caffe_interface.cpp:125] Batch 160, top-1 = 0.9
I0122 16:46:40.959971 58065 caffe_interface.cpp:125] Batch 160, top-5 = 0.98
I0122 16:46:40.965485 58065 caffe_interface.cpp:125] Batch 161, loss = 1.11999
I0122 16:46:40.965492 58065 caffe_interface.cpp:125] Batch 161, top-1 = 0.7
I0122 16:46:40.965495 58065 caffe_interface.cpp:125] Batch 161, top-5 = 0.98
I0122 16:46:40.975827 58065 caffe_interface.cpp:125] Batch 162, loss = 0.856527
I0122 16:46:40.975836 58065 caffe_interface.cpp:125] Batch 162, top-1 = 0.72
I0122 16:46:40.975838 58065 caffe_interface.cpp:125] Batch 162, top-5 = 1
I0122 16:46:40.982707 58065 caffe_interface.cpp:125] Batch 163, loss = 0.836485
I0122 16:46:40.982715 58065 caffe_interface.cpp:125] Batch 163, top-1 = 0.74
I0122 16:46:40.982718 58065 caffe_interface.cpp:125] Batch 163, top-5 = 0.96
I0122 16:46:40.997455 58065 caffe_interface.cpp:125] Batch 164, loss = 1.33243
I0122 16:46:40.997462 58065 caffe_interface.cpp:125] Batch 164, top-1 = 0.64
I0122 16:46:40.997464 58065 caffe_interface.cpp:125] Batch 164, top-5 = 0.98
I0122 16:46:41.012116 58065 caffe_interface.cpp:125] Batch 165, loss = 0.906893
I0122 16:46:41.012125 58065 caffe_interface.cpp:125] Batch 165, top-1 = 0.74
I0122 16:46:41.012127 58065 caffe_interface.cpp:125] Batch 165, top-5 = 0.98
I0122 16:46:41.013360 58065 caffe_interface.cpp:125] Batch 166, loss = 0.379142
I0122 16:46:41.013366 58065 caffe_interface.cpp:125] Batch 166, top-1 = 0.82
I0122 16:46:41.013370 58065 caffe_interface.cpp:125] Batch 166, top-5 = 1
I0122 16:46:41.014577 58065 caffe_interface.cpp:125] Batch 167, loss = 1.15686
I0122 16:46:41.014585 58065 caffe_interface.cpp:125] Batch 167, top-1 = 0.7
I0122 16:46:41.014598 58065 caffe_interface.cpp:125] Batch 167, top-5 = 0.96
I0122 16:46:41.015815 58065 caffe_interface.cpp:125] Batch 168, loss = 0.7449
I0122 16:46:41.015821 58065 caffe_interface.cpp:125] Batch 168, top-1 = 0.8
I0122 16:46:41.015825 58065 caffe_interface.cpp:125] Batch 168, top-5 = 1
I0122 16:46:41.017037 58065 caffe_interface.cpp:125] Batch 169, loss = 0.934589
I0122 16:46:41.017045 58065 caffe_interface.cpp:125] Batch 169, top-1 = 0.64
I0122 16:46:41.017048 58065 caffe_interface.cpp:125] Batch 169, top-5 = 0.98
I0122 16:46:41.018272 58065 caffe_interface.cpp:125] Batch 170, loss = 0.838117
I0122 16:46:41.018280 58065 caffe_interface.cpp:125] Batch 170, top-1 = 0.7
I0122 16:46:41.018285 58065 caffe_interface.cpp:125] Batch 170, top-5 = 1
I0122 16:46:41.019484 58065 caffe_interface.cpp:125] Batch 171, loss = 0.945991
I0122 16:46:41.019491 58065 caffe_interface.cpp:125] Batch 171, top-1 = 0.7
I0122 16:46:41.019495 58065 caffe_interface.cpp:125] Batch 171, top-5 = 0.96
I0122 16:46:41.021737 58065 caffe_interface.cpp:125] Batch 172, loss = 1.82944
I0122 16:46:41.021744 58065 caffe_interface.cpp:125] Batch 172, top-1 = 0.66
I0122 16:46:41.021747 58065 caffe_interface.cpp:125] Batch 172, top-5 = 0.9
I0122 16:46:41.023072 58065 caffe_interface.cpp:125] Batch 173, loss = 0.844273
I0122 16:46:41.023079 58065 caffe_interface.cpp:125] Batch 173, top-1 = 0.76
I0122 16:46:41.023082 58065 caffe_interface.cpp:125] Batch 173, top-5 = 1
I0122 16:46:41.024477 58065 caffe_interface.cpp:125] Batch 174, loss = 0.822846
I0122 16:46:41.024487 58065 caffe_interface.cpp:125] Batch 174, top-1 = 0.74
I0122 16:46:41.024492 58065 caffe_interface.cpp:125] Batch 174, top-5 = 1
I0122 16:46:41.025699 58065 caffe_interface.cpp:125] Batch 175, loss = 1.32634
I0122 16:46:41.025707 58065 caffe_interface.cpp:125] Batch 175, top-1 = 0.64
I0122 16:46:41.025709 58065 caffe_interface.cpp:125] Batch 175, top-5 = 0.98
I0122 16:46:41.026927 58065 caffe_interface.cpp:125] Batch 176, loss = 1.20173
I0122 16:46:41.026934 58065 caffe_interface.cpp:125] Batch 176, top-1 = 0.66
I0122 16:46:41.026938 58065 caffe_interface.cpp:125] Batch 176, top-5 = 0.98
I0122 16:46:41.028146 58065 caffe_interface.cpp:125] Batch 177, loss = 0.628455
I0122 16:46:41.028153 58065 caffe_interface.cpp:125] Batch 177, top-1 = 0.78
I0122 16:46:41.028156 58065 caffe_interface.cpp:125] Batch 177, top-5 = 0.98
I0122 16:46:41.029374 58065 caffe_interface.cpp:125] Batch 178, loss = 0.661319
I0122 16:46:41.029381 58065 caffe_interface.cpp:125] Batch 178, top-1 = 0.8
I0122 16:46:41.029386 58065 caffe_interface.cpp:125] Batch 178, top-5 = 0.98
I0122 16:46:41.030597 58065 caffe_interface.cpp:125] Batch 179, loss = 0.650088
I0122 16:46:41.030604 58065 caffe_interface.cpp:125] Batch 179, top-1 = 0.72
I0122 16:46:41.030607 58065 caffe_interface.cpp:125] Batch 179, top-5 = 1
I0122 16:46:41.030611 58065 caffe_interface.cpp:130] Loss: 1.00932
I0122 16:46:41.030617 58065 caffe_interface.cpp:142] loss = 1.00932 (* 1 = 1.00932 loss)
I0122 16:46:41.030622 58065 caffe_interface.cpp:142] top-1 = 0.723889
I0122 16:46:41.030627 58065 caffe_interface.cpp:142] top-5 = 0.976
I0122 16:46:41.177410 58065 pruning_runner.cpp:306] pruning done, output model: cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/sparse.caffemodel
I0122 16:46:41.177436 58065 pruning_runner.cpp:320] summary of REGULAR compression with rate 0.7:
+-------------------------------------------------------------------+
| Item           | Baseline       | Compressed     | Delta          |
+-------------------------------------------------------------------+
| Accuracy       | 0.862666428    | 0.723888934    | -0.138777494   |
+-------------------------------------------------------------------+
| Weights        | 68389          | 22761          | -66.7183304%   |
+-------------------------------------------------------------------+
| Operations     | 49053696       | 14835712       | -69.7561798%   |
+-------------------------------------------------------------------+
To fine-tune the compressed model, please run:
deephi_compress finetune -config cifar10/deephi/miniVggNet/pruning/config7.prototxt
## fine-tuning: 7-th run
${PRUNE_ROOT}/deephi_compress finetune -config ${WORK_DIR}/config7.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_finetune7_miniVggNet.txt
I0122 16:46:41.416851 58312 deephi_compress.cpp:236] cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0122 16:46:41.599483 58312 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:46:41.600018 58312 gpu_memory.cpp:55] Total memory: 25620447232, Free: 23005560832, dev_info[0]: total=25620447232 free=23005560832
I0122 16:46:41.600029 58312 caffe_interface.cpp:493] Using GPUs 0
I0122 16:46:41.600298 58312 caffe_interface.cpp:498] GPU 0: Quadro P6000
I0122 16:46:42.351171 58312 solver.cpp:51] Initializing solver from parameters: 
test_iter: 180
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 20000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 20000
snapshot_prefix: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/"
solver_mode: GPU
device_id: 0
random_seed: 1201
net: "cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt"
type: "SGD"
I0122 16:46:42.351289 58312 solver.cpp:99] Creating training net from net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0122 16:46:42.351524 58312 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0122 16:46:42.351555 58312 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top1
I0122 16:46:42.351559 58312 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy-top5
I0122 16:46:42.351722 58312 net.cpp:52] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
I0122 16:46:42.351797 58312 layer_factory.hpp:77] Creating layer data
I0122 16:46:42.351891 58312 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:42.352509 58312 net.cpp:94] Creating Layer data
I0122 16:46:42.352522 58312 net.cpp:409] data -> data
I0122 16:46:42.352545 58312 net.cpp:409] data -> label
I0122 16:46:42.354089 58349 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/train_lmdb
I0122 16:46:42.354148 58349 data_reader.cpp:117] TRAIN: reading data using 1 channel(s)
I0122 16:46:42.354254 58312 data_layer.cpp:78] ReshapePrefetch 128, 3, 32, 32
I0122 16:46:42.354339 58312 data_layer.cpp:83] output data size: 128,3,32,32
I0122 16:46:42.376992 58312 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:42.377058 58312 net.cpp:144] Setting up data
I0122 16:46:42.377068 58312 net.cpp:151] Top shape: 128 3 32 32 (393216)
I0122 16:46:42.377070 58312 net.cpp:151] Top shape: 128 (128)
I0122 16:46:42.377074 58312 net.cpp:159] Memory required for data: 1573376
I0122 16:46:42.377079 58312 layer_factory.hpp:77] Creating layer conv1
I0122 16:46:42.377092 58312 net.cpp:94] Creating Layer conv1
I0122 16:46:42.377096 58312 net.cpp:435] conv1 <- data
I0122 16:46:42.377113 58312 net.cpp:409] conv1 -> conv1
I0122 16:46:42.378201 58312 net.cpp:144] Setting up conv1
I0122 16:46:42.378211 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.378213 58312 net.cpp:159] Memory required for data: 18350592
I0122 16:46:42.378228 58312 layer_factory.hpp:77] Creating layer bn1
I0122 16:46:42.378237 58312 net.cpp:94] Creating Layer bn1
I0122 16:46:42.378238 58312 net.cpp:435] bn1 <- conv1
I0122 16:46:42.378243 58312 net.cpp:409] bn1 -> scale1
I0122 16:46:42.379772 58312 net.cpp:144] Setting up bn1
I0122 16:46:42.379778 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.379781 58312 net.cpp:159] Memory required for data: 35127808
I0122 16:46:42.379789 58312 layer_factory.hpp:77] Creating layer relu1
I0122 16:46:42.379812 58312 net.cpp:94] Creating Layer relu1
I0122 16:46:42.379814 58312 net.cpp:435] relu1 <- scale1
I0122 16:46:42.379819 58312 net.cpp:409] relu1 -> relu1
I0122 16:46:42.380386 58312 net.cpp:144] Setting up relu1
I0122 16:46:42.380393 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.380396 58312 net.cpp:159] Memory required for data: 51905024
I0122 16:46:42.380399 58312 layer_factory.hpp:77] Creating layer conv2
I0122 16:46:42.380409 58312 net.cpp:94] Creating Layer conv2
I0122 16:46:42.380414 58312 net.cpp:435] conv2 <- relu1
I0122 16:46:42.380419 58312 net.cpp:409] conv2 -> conv2
I0122 16:46:42.381484 58312 net.cpp:144] Setting up conv2
I0122 16:46:42.381494 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.381496 58312 net.cpp:159] Memory required for data: 68682240
I0122 16:46:42.381505 58312 layer_factory.hpp:77] Creating layer bn2
I0122 16:46:42.381510 58312 net.cpp:94] Creating Layer bn2
I0122 16:46:42.381513 58312 net.cpp:435] bn2 <- conv2
I0122 16:46:42.381520 58312 net.cpp:409] bn2 -> scale2
I0122 16:46:42.382570 58312 net.cpp:144] Setting up bn2
I0122 16:46:42.382577 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.382580 58312 net.cpp:159] Memory required for data: 85459456
I0122 16:46:42.382589 58312 layer_factory.hpp:77] Creating layer relu2
I0122 16:46:42.382594 58312 net.cpp:94] Creating Layer relu2
I0122 16:46:42.382597 58312 net.cpp:435] relu2 <- scale2
I0122 16:46:42.382601 58312 net.cpp:409] relu2 -> relu2
I0122 16:46:42.382726 58312 net.cpp:144] Setting up relu2
I0122 16:46:42.382732 58312 net.cpp:151] Top shape: 128 32 32 32 (4194304)
I0122 16:46:42.382736 58312 net.cpp:159] Memory required for data: 102236672
I0122 16:46:42.382738 58312 layer_factory.hpp:77] Creating layer pool1
I0122 16:46:42.382746 58312 net.cpp:94] Creating Layer pool1
I0122 16:46:42.382750 58312 net.cpp:435] pool1 <- relu2
I0122 16:46:42.382753 58312 net.cpp:409] pool1 -> pool1
I0122 16:46:42.382788 58312 net.cpp:144] Setting up pool1
I0122 16:46:42.382794 58312 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:46:42.382797 58312 net.cpp:159] Memory required for data: 106430976
I0122 16:46:42.382800 58312 layer_factory.hpp:77] Creating layer drop1
I0122 16:46:42.382805 58312 net.cpp:94] Creating Layer drop1
I0122 16:46:42.382809 58312 net.cpp:435] drop1 <- pool1
I0122 16:46:42.382824 58312 net.cpp:409] drop1 -> drop1
I0122 16:46:42.382869 58312 net.cpp:144] Setting up drop1
I0122 16:46:42.382874 58312 net.cpp:151] Top shape: 128 32 16 16 (1048576)
I0122 16:46:42.382879 58312 net.cpp:159] Memory required for data: 110625280
I0122 16:46:42.382880 58312 layer_factory.hpp:77] Creating layer conv3
I0122 16:46:42.382889 58312 net.cpp:94] Creating Layer conv3
I0122 16:46:42.382894 58312 net.cpp:435] conv3 <- drop1
I0122 16:46:42.382899 58312 net.cpp:409] conv3 -> conv3
I0122 16:46:42.383886 58312 net.cpp:144] Setting up conv3
I0122 16:46:42.383898 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.383900 58312 net.cpp:159] Memory required for data: 119013888
I0122 16:46:42.383908 58312 layer_factory.hpp:77] Creating layer bn3
I0122 16:46:42.383914 58312 net.cpp:94] Creating Layer bn3
I0122 16:46:42.383921 58312 net.cpp:435] bn3 <- conv3
I0122 16:46:42.383927 58312 net.cpp:409] bn3 -> scale3
I0122 16:46:42.384541 58312 net.cpp:144] Setting up bn3
I0122 16:46:42.384547 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.384551 58312 net.cpp:159] Memory required for data: 127402496
I0122 16:46:42.384562 58312 layer_factory.hpp:77] Creating layer relu3
I0122 16:46:42.384569 58312 net.cpp:94] Creating Layer relu3
I0122 16:46:42.384572 58312 net.cpp:435] relu3 <- scale3
I0122 16:46:42.384577 58312 net.cpp:409] relu3 -> relu3
I0122 16:46:42.384594 58312 net.cpp:144] Setting up relu3
I0122 16:46:42.384601 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.384604 58312 net.cpp:159] Memory required for data: 135791104
I0122 16:46:42.384608 58312 layer_factory.hpp:77] Creating layer conv4
I0122 16:46:42.384614 58312 net.cpp:94] Creating Layer conv4
I0122 16:46:42.384618 58312 net.cpp:435] conv4 <- relu3
I0122 16:46:42.384624 58312 net.cpp:409] conv4 -> conv4
I0122 16:46:42.385040 58312 net.cpp:144] Setting up conv4
I0122 16:46:42.385046 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.385051 58312 net.cpp:159] Memory required for data: 144179712
I0122 16:46:42.385056 58312 layer_factory.hpp:77] Creating layer bn4
I0122 16:46:42.385061 58312 net.cpp:94] Creating Layer bn4
I0122 16:46:42.385064 58312 net.cpp:435] bn4 <- conv4
I0122 16:46:42.385069 58312 net.cpp:409] bn4 -> scale4
I0122 16:46:42.385736 58312 net.cpp:144] Setting up bn4
I0122 16:46:42.385743 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.385746 58312 net.cpp:159] Memory required for data: 152568320
I0122 16:46:42.385756 58312 layer_factory.hpp:77] Creating layer relu4
I0122 16:46:42.385759 58312 net.cpp:94] Creating Layer relu4
I0122 16:46:42.385762 58312 net.cpp:435] relu4 <- scale4
I0122 16:46:42.385767 58312 net.cpp:409] relu4 -> relu4
I0122 16:46:42.385798 58312 net.cpp:144] Setting up relu4
I0122 16:46:42.385804 58312 net.cpp:151] Top shape: 128 64 16 16 (2097152)
I0122 16:46:42.385807 58312 net.cpp:159] Memory required for data: 160956928
I0122 16:46:42.385809 58312 layer_factory.hpp:77] Creating layer pool2
I0122 16:46:42.385815 58312 net.cpp:94] Creating Layer pool2
I0122 16:46:42.385818 58312 net.cpp:435] pool2 <- relu4
I0122 16:46:42.385823 58312 net.cpp:409] pool2 -> pool2
I0122 16:46:42.385850 58312 net.cpp:144] Setting up pool2
I0122 16:46:42.385857 58312 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:46:42.385860 58312 net.cpp:159] Memory required for data: 163054080
I0122 16:46:42.385862 58312 layer_factory.hpp:77] Creating layer drop2
I0122 16:46:42.385869 58312 net.cpp:94] Creating Layer drop2
I0122 16:46:42.385872 58312 net.cpp:435] drop2 <- pool2
I0122 16:46:42.385876 58312 net.cpp:409] drop2 -> drop2
I0122 16:46:42.385912 58312 net.cpp:144] Setting up drop2
I0122 16:46:42.385918 58312 net.cpp:151] Top shape: 128 64 8 8 (524288)
I0122 16:46:42.385922 58312 net.cpp:159] Memory required for data: 165151232
I0122 16:46:42.385926 58312 layer_factory.hpp:77] Creating layer fc1
I0122 16:46:42.385932 58312 net.cpp:94] Creating Layer fc1
I0122 16:46:42.385936 58312 net.cpp:435] fc1 <- drop2
I0122 16:46:42.385941 58312 net.cpp:409] fc1 -> fc1
I0122 16:46:42.400333 58312 net.cpp:144] Setting up fc1
I0122 16:46:42.400352 58312 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:46:42.400354 58312 net.cpp:159] Memory required for data: 165413376
I0122 16:46:42.400362 58312 layer_factory.hpp:77] Creating layer bn5
I0122 16:46:42.400372 58312 net.cpp:94] Creating Layer bn5
I0122 16:46:42.400374 58312 net.cpp:435] bn5 <- fc1
I0122 16:46:42.400382 58312 net.cpp:409] bn5 -> scale5
I0122 16:46:42.400892 58312 net.cpp:144] Setting up bn5
I0122 16:46:42.400899 58312 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:46:42.400903 58312 net.cpp:159] Memory required for data: 165675520
I0122 16:46:42.400915 58312 layer_factory.hpp:77] Creating layer relu5
I0122 16:46:42.400925 58312 net.cpp:94] Creating Layer relu5
I0122 16:46:42.400928 58312 net.cpp:435] relu5 <- scale5
I0122 16:46:42.400933 58312 net.cpp:409] relu5 -> relu5
I0122 16:46:42.400950 58312 net.cpp:144] Setting up relu5
I0122 16:46:42.400957 58312 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:46:42.400961 58312 net.cpp:159] Memory required for data: 165937664
I0122 16:46:42.400964 58312 layer_factory.hpp:77] Creating layer drop3
I0122 16:46:42.400969 58312 net.cpp:94] Creating Layer drop3
I0122 16:46:42.400972 58312 net.cpp:435] drop3 <- relu5
I0122 16:46:42.400976 58312 net.cpp:409] drop3 -> drop3
I0122 16:46:42.401007 58312 net.cpp:144] Setting up drop3
I0122 16:46:42.401012 58312 net.cpp:151] Top shape: 128 512 (65536)
I0122 16:46:42.401016 58312 net.cpp:159] Memory required for data: 166199808
I0122 16:46:42.401018 58312 layer_factory.hpp:77] Creating layer fc2
I0122 16:46:42.401024 58312 net.cpp:94] Creating Layer fc2
I0122 16:46:42.401029 58312 net.cpp:435] fc2 <- drop3
I0122 16:46:42.401034 58312 net.cpp:409] fc2 -> fc2
I0122 16:46:42.401170 58312 net.cpp:144] Setting up fc2
I0122 16:46:42.401176 58312 net.cpp:151] Top shape: 128 10 (1280)
I0122 16:46:42.401178 58312 net.cpp:159] Memory required for data: 166204928
I0122 16:46:42.401183 58312 layer_factory.hpp:77] Creating layer loss
I0122 16:46:42.401190 58312 net.cpp:94] Creating Layer loss
I0122 16:46:42.401191 58312 net.cpp:435] loss <- fc2
I0122 16:46:42.401196 58312 net.cpp:435] loss <- label
I0122 16:46:42.401201 58312 net.cpp:409] loss -> loss
I0122 16:46:42.401207 58312 layer_factory.hpp:77] Creating layer loss
I0122 16:46:42.401978 58312 net.cpp:144] Setting up loss
I0122 16:46:42.401988 58312 net.cpp:151] Top shape: (1)
I0122 16:46:42.401991 58312 net.cpp:154]     with loss weight 1
I0122 16:46:42.402000 58312 net.cpp:159] Memory required for data: 166204932
I0122 16:46:42.402004 58312 net.cpp:220] loss needs backward computation.
I0122 16:46:42.402019 58312 net.cpp:220] fc2 needs backward computation.
I0122 16:46:42.402022 58312 net.cpp:220] drop3 needs backward computation.
I0122 16:46:42.402025 58312 net.cpp:220] relu5 needs backward computation.
I0122 16:46:42.402027 58312 net.cpp:220] bn5 needs backward computation.
I0122 16:46:42.402030 58312 net.cpp:220] fc1 needs backward computation.
I0122 16:46:42.402034 58312 net.cpp:220] drop2 needs backward computation.
I0122 16:46:42.402037 58312 net.cpp:220] pool2 needs backward computation.
I0122 16:46:42.402040 58312 net.cpp:220] relu4 needs backward computation.
I0122 16:46:42.402043 58312 net.cpp:220] bn4 needs backward computation.
I0122 16:46:42.402047 58312 net.cpp:220] conv4 needs backward computation.
I0122 16:46:42.402050 58312 net.cpp:220] relu3 needs backward computation.
I0122 16:46:42.402052 58312 net.cpp:220] bn3 needs backward computation.
I0122 16:46:42.402056 58312 net.cpp:220] conv3 needs backward computation.
I0122 16:46:42.402060 58312 net.cpp:220] drop1 needs backward computation.
I0122 16:46:42.402062 58312 net.cpp:220] pool1 needs backward computation.
I0122 16:46:42.402065 58312 net.cpp:220] relu2 needs backward computation.
I0122 16:46:42.402068 58312 net.cpp:220] bn2 needs backward computation.
I0122 16:46:42.402072 58312 net.cpp:220] conv2 needs backward computation.
I0122 16:46:42.402076 58312 net.cpp:220] relu1 needs backward computation.
I0122 16:46:42.402091 58312 net.cpp:220] bn1 needs backward computation.
I0122 16:46:42.402094 58312 net.cpp:220] conv1 needs backward computation.
I0122 16:46:42.402098 58312 net.cpp:222] data does not need backward computation.
I0122 16:46:42.402101 58312 net.cpp:264] This network produces output loss
I0122 16:46:42.402120 58312 net.cpp:284] Network initialization done.
I0122 16:46:42.402427 58312 solver.cpp:189] Creating test net (#0) specified by net file: cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/net_finetune.prototxt
I0122 16:46:42.402460 58312 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:46:42.402648 58312 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
    compression: PRUNE
  }
  param {
    lr_mult: 2
    decay_mult: 0
    compression: PRUNE
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 1
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  param {
    lr_mult: 0
    decay_mult: 0
    compression: PRUNE
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:46:42.402743 58312 layer_factory.hpp:77] Creating layer data
I0122 16:46:42.402783 58312 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:42.403620 58312 net.cpp:94] Creating Layer data
I0122 16:46:42.403628 58312 net.cpp:409] data -> data
I0122 16:46:42.403637 58312 net.cpp:409] data -> label
I0122 16:46:42.404538 58379 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:46:42.404572 58379 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:46:42.404659 58312 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:46:42.404747 58312 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:46:42.408411 58312 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:46:42.408466 58312 net.cpp:144] Setting up data
I0122 16:46:42.408473 58312 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:46:42.408478 58312 net.cpp:151] Top shape: 50 (50)
I0122 16:46:42.408483 58312 net.cpp:159] Memory required for data: 614600
I0122 16:46:42.408488 58312 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:46:42.408495 58312 net.cpp:94] Creating Layer label_data_1_split
I0122 16:46:42.408499 58312 net.cpp:435] label_data_1_split <- label
I0122 16:46:42.408506 58312 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:46:42.408525 58312 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:46:42.408531 58312 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:46:42.408643 58312 net.cpp:144] Setting up label_data_1_split
I0122 16:46:42.408648 58312 net.cpp:151] Top shape: 50 (50)
I0122 16:46:42.408653 58312 net.cpp:151] Top shape: 50 (50)
I0122 16:46:42.408655 58312 net.cpp:151] Top shape: 50 (50)
I0122 16:46:42.408658 58312 net.cpp:159] Memory required for data: 615200
I0122 16:46:42.408661 58312 layer_factory.hpp:77] Creating layer conv1
I0122 16:46:42.408671 58312 net.cpp:94] Creating Layer conv1
I0122 16:46:42.408674 58312 net.cpp:435] conv1 <- data
I0122 16:46:42.408680 58312 net.cpp:409] conv1 -> conv1
I0122 16:46:42.409039 58312 net.cpp:144] Setting up conv1
I0122 16:46:42.409045 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.409049 58312 net.cpp:159] Memory required for data: 7168800
I0122 16:46:42.409059 58312 layer_factory.hpp:77] Creating layer bn1
I0122 16:46:42.409066 58312 net.cpp:94] Creating Layer bn1
I0122 16:46:42.409070 58312 net.cpp:435] bn1 <- conv1
I0122 16:46:42.409075 58312 net.cpp:409] bn1 -> scale1
I0122 16:46:42.409714 58312 net.cpp:144] Setting up bn1
I0122 16:46:42.409721 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.409724 58312 net.cpp:159] Memory required for data: 13722400
I0122 16:46:42.409735 58312 layer_factory.hpp:77] Creating layer relu1
I0122 16:46:42.409744 58312 net.cpp:94] Creating Layer relu1
I0122 16:46:42.409746 58312 net.cpp:435] relu1 <- scale1
I0122 16:46:42.409750 58312 net.cpp:409] relu1 -> relu1
I0122 16:46:42.409768 58312 net.cpp:144] Setting up relu1
I0122 16:46:42.409773 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.409777 58312 net.cpp:159] Memory required for data: 20276000
I0122 16:46:42.409780 58312 layer_factory.hpp:77] Creating layer conv2
I0122 16:46:42.409787 58312 net.cpp:94] Creating Layer conv2
I0122 16:46:42.409792 58312 net.cpp:435] conv2 <- relu1
I0122 16:46:42.409796 58312 net.cpp:409] conv2 -> conv2
I0122 16:46:42.410387 58312 net.cpp:144] Setting up conv2
I0122 16:46:42.410395 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.410398 58312 net.cpp:159] Memory required for data: 26829600
I0122 16:46:42.410405 58312 layer_factory.hpp:77] Creating layer bn2
I0122 16:46:42.410411 58312 net.cpp:94] Creating Layer bn2
I0122 16:46:42.410414 58312 net.cpp:435] bn2 <- conv2
I0122 16:46:42.410420 58312 net.cpp:409] bn2 -> scale2
I0122 16:46:42.411198 58312 net.cpp:144] Setting up bn2
I0122 16:46:42.411206 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.411209 58312 net.cpp:159] Memory required for data: 33383200
I0122 16:46:42.411217 58312 layer_factory.hpp:77] Creating layer relu2
I0122 16:46:42.411221 58312 net.cpp:94] Creating Layer relu2
I0122 16:46:42.411226 58312 net.cpp:435] relu2 <- scale2
I0122 16:46:42.411231 58312 net.cpp:409] relu2 -> relu2
I0122 16:46:42.411252 58312 net.cpp:144] Setting up relu2
I0122 16:46:42.411257 58312 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:46:42.411260 58312 net.cpp:159] Memory required for data: 39936800
I0122 16:46:42.411263 58312 layer_factory.hpp:77] Creating layer pool1
I0122 16:46:42.411268 58312 net.cpp:94] Creating Layer pool1
I0122 16:46:42.411272 58312 net.cpp:435] pool1 <- relu2
I0122 16:46:42.411276 58312 net.cpp:409] pool1 -> pool1
I0122 16:46:42.411309 58312 net.cpp:144] Setting up pool1
I0122 16:46:42.411324 58312 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:46:42.411327 58312 net.cpp:159] Memory required for data: 41575200
I0122 16:46:42.411330 58312 layer_factory.hpp:77] Creating layer drop1
I0122 16:46:42.411335 58312 net.cpp:94] Creating Layer drop1
I0122 16:46:42.411339 58312 net.cpp:435] drop1 <- pool1
I0122 16:46:42.411343 58312 net.cpp:409] drop1 -> drop1
I0122 16:46:42.411378 58312 net.cpp:144] Setting up drop1
I0122 16:46:42.411383 58312 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:46:42.411387 58312 net.cpp:159] Memory required for data: 43213600
I0122 16:46:42.411389 58312 layer_factory.hpp:77] Creating layer conv3
I0122 16:46:42.411397 58312 net.cpp:94] Creating Layer conv3
I0122 16:46:42.411401 58312 net.cpp:435] conv3 <- drop1
I0122 16:46:42.411406 58312 net.cpp:409] conv3 -> conv3
I0122 16:46:42.411809 58312 net.cpp:144] Setting up conv3
I0122 16:46:42.411818 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.411820 58312 net.cpp:159] Memory required for data: 46490400
I0122 16:46:42.411825 58312 layer_factory.hpp:77] Creating layer bn3
I0122 16:46:42.411831 58312 net.cpp:94] Creating Layer bn3
I0122 16:46:42.411835 58312 net.cpp:435] bn3 <- conv3
I0122 16:46:42.411840 58312 net.cpp:409] bn3 -> scale3
I0122 16:46:42.412494 58312 net.cpp:144] Setting up bn3
I0122 16:46:42.412501 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.412504 58312 net.cpp:159] Memory required for data: 49767200
I0122 16:46:42.412515 58312 layer_factory.hpp:77] Creating layer relu3
I0122 16:46:42.412523 58312 net.cpp:94] Creating Layer relu3
I0122 16:46:42.412526 58312 net.cpp:435] relu3 <- scale3
I0122 16:46:42.412530 58312 net.cpp:409] relu3 -> relu3
I0122 16:46:42.412547 58312 net.cpp:144] Setting up relu3
I0122 16:46:42.412554 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.412555 58312 net.cpp:159] Memory required for data: 53044000
I0122 16:46:42.412559 58312 layer_factory.hpp:77] Creating layer conv4
I0122 16:46:42.412566 58312 net.cpp:94] Creating Layer conv4
I0122 16:46:42.412569 58312 net.cpp:435] conv4 <- relu3
I0122 16:46:42.412575 58312 net.cpp:409] conv4 -> conv4
I0122 16:46:42.413028 58312 net.cpp:144] Setting up conv4
I0122 16:46:42.413034 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.413038 58312 net.cpp:159] Memory required for data: 56320800
I0122 16:46:42.413043 58312 layer_factory.hpp:77] Creating layer bn4
I0122 16:46:42.413049 58312 net.cpp:94] Creating Layer bn4
I0122 16:46:42.413053 58312 net.cpp:435] bn4 <- conv4
I0122 16:46:42.413058 58312 net.cpp:409] bn4 -> scale4
I0122 16:46:42.413753 58312 net.cpp:144] Setting up bn4
I0122 16:46:42.413759 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.413763 58312 net.cpp:159] Memory required for data: 59597600
I0122 16:46:42.413770 58312 layer_factory.hpp:77] Creating layer relu4
I0122 16:46:42.413775 58312 net.cpp:94] Creating Layer relu4
I0122 16:46:42.413779 58312 net.cpp:435] relu4 <- scale4
I0122 16:46:42.413784 58312 net.cpp:409] relu4 -> relu4
I0122 16:46:42.413800 58312 net.cpp:144] Setting up relu4
I0122 16:46:42.413806 58312 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:46:42.413810 58312 net.cpp:159] Memory required for data: 62874400
I0122 16:46:42.413813 58312 layer_factory.hpp:77] Creating layer pool2
I0122 16:46:42.413818 58312 net.cpp:94] Creating Layer pool2
I0122 16:46:42.413822 58312 net.cpp:435] pool2 <- relu4
I0122 16:46:42.413827 58312 net.cpp:409] pool2 -> pool2
I0122 16:46:42.413859 58312 net.cpp:144] Setting up pool2
I0122 16:46:42.413864 58312 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:46:42.413867 58312 net.cpp:159] Memory required for data: 63693600
I0122 16:46:42.413870 58312 layer_factory.hpp:77] Creating layer drop2
I0122 16:46:42.413875 58312 net.cpp:94] Creating Layer drop2
I0122 16:46:42.413878 58312 net.cpp:435] drop2 <- pool2
I0122 16:46:42.413882 58312 net.cpp:409] drop2 -> drop2
I0122 16:46:42.413965 58312 net.cpp:144] Setting up drop2
I0122 16:46:42.413971 58312 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:46:42.413983 58312 net.cpp:159] Memory required for data: 64512800
I0122 16:46:42.413986 58312 layer_factory.hpp:77] Creating layer fc1
I0122 16:46:42.413993 58312 net.cpp:94] Creating Layer fc1
I0122 16:46:42.413997 58312 net.cpp:435] fc1 <- drop2
I0122 16:46:42.414001 58312 net.cpp:409] fc1 -> fc1
I0122 16:46:42.427992 58312 net.cpp:144] Setting up fc1
I0122 16:46:42.428012 58312 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:42.428015 58312 net.cpp:159] Memory required for data: 64615200
I0122 16:46:42.428023 58312 layer_factory.hpp:77] Creating layer bn5
I0122 16:46:42.428032 58312 net.cpp:94] Creating Layer bn5
I0122 16:46:42.428036 58312 net.cpp:435] bn5 <- fc1
I0122 16:46:42.428043 58312 net.cpp:409] bn5 -> scale5
I0122 16:46:42.428656 58312 net.cpp:144] Setting up bn5
I0122 16:46:42.428663 58312 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:42.428666 58312 net.cpp:159] Memory required for data: 64717600
I0122 16:46:42.428678 58312 layer_factory.hpp:77] Creating layer relu5
I0122 16:46:42.428685 58312 net.cpp:94] Creating Layer relu5
I0122 16:46:42.428689 58312 net.cpp:435] relu5 <- scale5
I0122 16:46:42.428694 58312 net.cpp:409] relu5 -> relu5
I0122 16:46:42.428712 58312 net.cpp:144] Setting up relu5
I0122 16:46:42.428719 58312 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:42.428721 58312 net.cpp:159] Memory required for data: 64820000
I0122 16:46:42.428725 58312 layer_factory.hpp:77] Creating layer drop3
I0122 16:46:42.428730 58312 net.cpp:94] Creating Layer drop3
I0122 16:46:42.428732 58312 net.cpp:435] drop3 <- relu5
I0122 16:46:42.428736 58312 net.cpp:409] drop3 -> drop3
I0122 16:46:42.428764 58312 net.cpp:144] Setting up drop3
I0122 16:46:42.428769 58312 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:46:42.428772 58312 net.cpp:159] Memory required for data: 64922400
I0122 16:46:42.428774 58312 layer_factory.hpp:77] Creating layer fc2
I0122 16:46:42.428781 58312 net.cpp:94] Creating Layer fc2
I0122 16:46:42.428784 58312 net.cpp:435] fc2 <- drop3
I0122 16:46:42.428789 58312 net.cpp:409] fc2 -> fc2
I0122 16:46:42.428930 58312 net.cpp:144] Setting up fc2
I0122 16:46:42.428936 58312 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:42.428941 58312 net.cpp:159] Memory required for data: 64924400
I0122 16:46:42.428944 58312 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:46:42.428951 58312 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:46:42.428953 58312 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:46:42.428957 58312 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:46:42.428964 58312 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:46:42.428972 58312 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:46:42.429013 58312 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:46:42.429018 58312 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:42.429021 58312 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:42.429024 58312 net.cpp:151] Top shape: 50 10 (500)
I0122 16:46:42.429028 58312 net.cpp:159] Memory required for data: 64930400
I0122 16:46:42.429029 58312 layer_factory.hpp:77] Creating layer loss
I0122 16:46:42.429035 58312 net.cpp:94] Creating Layer loss
I0122 16:46:42.429039 58312 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:46:42.429044 58312 net.cpp:435] loss <- label_data_1_split_0
I0122 16:46:42.429049 58312 net.cpp:409] loss -> loss
I0122 16:46:42.429059 58312 layer_factory.hpp:77] Creating layer loss
I0122 16:46:42.429131 58312 net.cpp:144] Setting up loss
I0122 16:46:42.429136 58312 net.cpp:151] Top shape: (1)
I0122 16:46:42.429141 58312 net.cpp:154]     with loss weight 1
I0122 16:46:42.429149 58312 net.cpp:159] Memory required for data: 64930404
I0122 16:46:42.429152 58312 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:46:42.429158 58312 net.cpp:94] Creating Layer accuracy-top1
I0122 16:46:42.429164 58312 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:46:42.429167 58312 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:46:42.429172 58312 net.cpp:409] accuracy-top1 -> top-1
I0122 16:46:42.429193 58312 net.cpp:144] Setting up accuracy-top1
I0122 16:46:42.429198 58312 net.cpp:151] Top shape: (1)
I0122 16:46:42.429199 58312 net.cpp:159] Memory required for data: 64930408
I0122 16:46:42.429203 58312 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:46:42.429208 58312 net.cpp:94] Creating Layer accuracy-top5
I0122 16:46:42.429211 58312 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:46:42.429214 58312 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:46:42.429219 58312 net.cpp:409] accuracy-top5 -> top-5
I0122 16:46:42.429226 58312 net.cpp:144] Setting up accuracy-top5
I0122 16:46:42.429229 58312 net.cpp:151] Top shape: (1)
I0122 16:46:42.429232 58312 net.cpp:159] Memory required for data: 64930412
I0122 16:46:42.429235 58312 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:46:42.429240 58312 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:46:42.429244 58312 net.cpp:220] loss needs backward computation.
I0122 16:46:42.429247 58312 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:46:42.429250 58312 net.cpp:220] fc2 needs backward computation.
I0122 16:46:42.429255 58312 net.cpp:220] drop3 needs backward computation.
I0122 16:46:42.429257 58312 net.cpp:220] relu5 needs backward computation.
I0122 16:46:42.429260 58312 net.cpp:220] bn5 needs backward computation.
I0122 16:46:42.429263 58312 net.cpp:220] fc1 needs backward computation.
I0122 16:46:42.429267 58312 net.cpp:220] drop2 needs backward computation.
I0122 16:46:42.429270 58312 net.cpp:220] pool2 needs backward computation.
I0122 16:46:42.429272 58312 net.cpp:220] relu4 needs backward computation.
I0122 16:46:42.429276 58312 net.cpp:220] bn4 needs backward computation.
I0122 16:46:42.429280 58312 net.cpp:220] conv4 needs backward computation.
I0122 16:46:42.429283 58312 net.cpp:220] relu3 needs backward computation.
I0122 16:46:42.429286 58312 net.cpp:220] bn3 needs backward computation.
I0122 16:46:42.429289 58312 net.cpp:220] conv3 needs backward computation.
I0122 16:46:42.429293 58312 net.cpp:220] drop1 needs backward computation.
I0122 16:46:42.429296 58312 net.cpp:220] pool1 needs backward computation.
I0122 16:46:42.429299 58312 net.cpp:220] relu2 needs backward computation.
I0122 16:46:42.429302 58312 net.cpp:220] bn2 needs backward computation.
I0122 16:46:42.429306 58312 net.cpp:220] conv2 needs backward computation.
I0122 16:46:42.429308 58312 net.cpp:220] relu1 needs backward computation.
I0122 16:46:42.429311 58312 net.cpp:220] bn1 needs backward computation.
I0122 16:46:42.429314 58312 net.cpp:220] conv1 needs backward computation.
I0122 16:46:42.429319 58312 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:46:42.429322 58312 net.cpp:222] data does not need backward computation.
I0122 16:46:42.429325 58312 net.cpp:264] This network produces output loss
I0122 16:46:42.429328 58312 net.cpp:264] This network produces output top-1
I0122 16:46:42.429332 58312 net.cpp:264] This network produces output top-5
I0122 16:46:42.429353 58312 net.cpp:284] Network initialization done.
I0122 16:46:42.429458 58312 solver.cpp:63] Solver scaffolding done.
I0122 16:46:42.430613 58312 caffe_interface.cpp:93] Finetuning from cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/sparse.caffemodel
I0122 16:46:42.489207 58312 caffe_interface.cpp:527] Starting Optimization
I0122 16:46:42.489228 58312 solver.cpp:335] Solving 
I0122 16:46:42.489230 58312 solver.cpp:336] Learning Rate Policy: poly
I0122 16:46:42.490484 58312 solver.cpp:418] Iteration 0, Testing net (#0)
I0122 16:46:42.898044 58312 solver.cpp:517]     Test net output #0: loss = 1.00932 (* 1 = 1.00932 loss)
I0122 16:46:42.898066 58312 solver.cpp:517]     Test net output #1: top-1 = 0.723889
I0122 16:46:42.898072 58312 solver.cpp:517]     Test net output #2: top-5 = 0.976
I0122 16:46:42.918915 58312 solver.cpp:266] Iteration 0 (0 iter/s, 0.429625s/100 iter), loss = 0.76743
I0122 16:46:42.918941 58312 solver.cpp:285]     Train net output #0: loss = 0.76743 (* 1 = 0.76743 loss)
I0122 16:46:42.918967 58312 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0122 16:46:44.359012 58312 solver.cpp:266] Iteration 100 (69.4441 iter/s, 1.44001s/100 iter), loss = 0.530221
I0122 16:46:44.359042 58312 solver.cpp:285]     Train net output #0: loss = 0.530221 (* 1 = 0.530221 loss)
I0122 16:46:44.361256 58312 sgd_solver.cpp:106] Iteration 100, lr = 0.00995
I0122 16:46:45.808549 58312 solver.cpp:266] Iteration 200 (69.0973 iter/s, 1.44724s/100 iter), loss = 0.330664
I0122 16:46:45.808579 58312 solver.cpp:285]     Train net output #0: loss = 0.330664 (* 1 = 0.330664 loss)
I0122 16:46:45.808585 58312 sgd_solver.cpp:106] Iteration 200, lr = 0.0099
I0122 16:46:47.233423 58312 solver.cpp:266] Iteration 300 (70.186 iter/s, 1.42479s/100 iter), loss = 0.278995
I0122 16:46:47.233453 58312 solver.cpp:285]     Train net output #0: loss = 0.278995 (* 1 = 0.278995 loss)
I0122 16:46:47.233459 58312 sgd_solver.cpp:106] Iteration 300, lr = 0.00985
I0122 16:46:48.662551 58312 solver.cpp:266] Iteration 400 (69.9772 iter/s, 1.42904s/100 iter), loss = 0.385726
I0122 16:46:48.662581 58312 solver.cpp:285]     Train net output #0: loss = 0.385726 (* 1 = 0.385726 loss)
I0122 16:46:48.662588 58312 sgd_solver.cpp:106] Iteration 400, lr = 0.0098
I0122 16:46:50.112143 58312 solver.cpp:266] Iteration 500 (68.9895 iter/s, 1.4495s/100 iter), loss = 0.313135
I0122 16:46:50.112174 58312 solver.cpp:285]     Train net output #0: loss = 0.313135 (* 1 = 0.313135 loss)
I0122 16:46:50.112181 58312 sgd_solver.cpp:106] Iteration 500, lr = 0.00975
I0122 16:46:51.535087 58312 solver.cpp:266] Iteration 600 (70.2813 iter/s, 1.42285s/100 iter), loss = 0.253755
I0122 16:46:51.535117 58312 solver.cpp:285]     Train net output #0: loss = 0.253755 (* 1 = 0.253755 loss)
I0122 16:46:51.535123 58312 sgd_solver.cpp:106] Iteration 600, lr = 0.0097
I0122 16:46:52.958874 58312 solver.cpp:266] Iteration 700 (70.2397 iter/s, 1.4237s/100 iter), loss = 0.543433
I0122 16:46:52.958914 58312 solver.cpp:285]     Train net output #0: loss = 0.543433 (* 1 = 0.543433 loss)
I0122 16:46:52.958921 58312 sgd_solver.cpp:106] Iteration 700, lr = 0.00965
I0122 16:46:54.411420 58312 solver.cpp:266] Iteration 800 (68.8491 iter/s, 1.45245s/100 iter), loss = 0.351883
I0122 16:46:54.411451 58312 solver.cpp:285]     Train net output #0: loss = 0.351883 (* 1 = 0.351883 loss)
I0122 16:46:54.411456 58312 sgd_solver.cpp:106] Iteration 800, lr = 0.0096
I0122 16:46:55.834237 58312 solver.cpp:266] Iteration 900 (70.2875 iter/s, 1.42273s/100 iter), loss = 0.308681
I0122 16:46:55.834270 58312 solver.cpp:285]     Train net output #0: loss = 0.308681 (* 1 = 0.308681 loss)
I0122 16:46:55.834275 58312 sgd_solver.cpp:106] Iteration 900, lr = 0.00955
I0122 16:46:57.245929 58312 solver.cpp:418] Iteration 1000, Testing net (#0)
I0122 16:46:57.657801 58312 solver.cpp:517]     Test net output #0: loss = 0.938955 (* 1 = 0.938955 loss)
I0122 16:46:57.657819 58312 solver.cpp:517]     Test net output #1: top-1 = 0.740556
I0122 16:46:57.657822 58312 solver.cpp:517]     Test net output #2: top-5 = 0.981556
I0122 16:46:57.680773 58312 solver.cpp:266] Iteration 1000 (54.1585 iter/s, 1.84643s/100 iter), loss = 0.310347
I0122 16:46:57.680794 58312 solver.cpp:285]     Train net output #0: loss = 0.310347 (* 1 = 0.310347 loss)
I0122 16:46:57.683023 58312 sgd_solver.cpp:106] Iteration 1000, lr = 0.0095
I0122 16:46:59.122097 58312 solver.cpp:266] Iteration 1100 (69.4919 iter/s, 1.43902s/100 iter), loss = 0.312954
I0122 16:46:59.122128 58312 solver.cpp:285]     Train net output #0: loss = 0.312954 (* 1 = 0.312954 loss)
I0122 16:46:59.122134 58312 sgd_solver.cpp:106] Iteration 1100, lr = 0.00945
I0122 16:47:00.541785 58312 solver.cpp:266] Iteration 1200 (70.4425 iter/s, 1.4196s/100 iter), loss = 0.32649
I0122 16:47:00.541813 58312 solver.cpp:285]     Train net output #0: loss = 0.32649 (* 1 = 0.32649 loss)
I0122 16:47:00.541819 58312 sgd_solver.cpp:106] Iteration 1200, lr = 0.0094
I0122 16:47:01.971976 58312 solver.cpp:266] Iteration 1300 (69.925 iter/s, 1.4301s/100 iter), loss = 0.333099
I0122 16:47:01.972007 58312 solver.cpp:285]     Train net output #0: loss = 0.333099 (* 1 = 0.333099 loss)
I0122 16:47:01.972033 58312 sgd_solver.cpp:106] Iteration 1300, lr = 0.00935
I0122 16:47:03.417685 58312 solver.cpp:266] Iteration 1400 (69.1745 iter/s, 1.44562s/100 iter), loss = 0.315005
I0122 16:47:03.417708 58312 solver.cpp:285]     Train net output #0: loss = 0.315005 (* 1 = 0.315005 loss)
I0122 16:47:03.417713 58312 sgd_solver.cpp:106] Iteration 1400, lr = 0.0093
I0122 16:47:04.839507 58312 solver.cpp:266] Iteration 1500 (70.3364 iter/s, 1.42174s/100 iter), loss = 0.342585
I0122 16:47:04.839550 58312 solver.cpp:285]     Train net output #0: loss = 0.342585 (* 1 = 0.342585 loss)
I0122 16:47:04.839556 58312 sgd_solver.cpp:106] Iteration 1500, lr = 0.00925
I0122 16:47:06.259665 58312 solver.cpp:266] Iteration 1600 (70.4197 iter/s, 1.42006s/100 iter), loss = 0.364308
I0122 16:47:06.259697 58312 solver.cpp:285]     Train net output #0: loss = 0.364308 (* 1 = 0.364308 loss)
I0122 16:47:06.259704 58312 sgd_solver.cpp:106] Iteration 1600, lr = 0.0092
I0122 16:47:07.698143 58312 solver.cpp:266] Iteration 1700 (69.5225 iter/s, 1.43838s/100 iter), loss = 0.306775
I0122 16:47:07.698173 58312 solver.cpp:285]     Train net output #0: loss = 0.306775 (* 1 = 0.306775 loss)
I0122 16:47:07.698179 58312 sgd_solver.cpp:106] Iteration 1700, lr = 0.00915
I0122 16:47:09.130154 58312 solver.cpp:266] Iteration 1800 (69.8362 iter/s, 1.43192s/100 iter), loss = 0.328329
I0122 16:47:09.130184 58312 solver.cpp:285]     Train net output #0: loss = 0.328329 (* 1 = 0.328329 loss)
I0122 16:47:09.130190 58312 sgd_solver.cpp:106] Iteration 1800, lr = 0.0091
I0122 16:47:10.570122 58312 solver.cpp:266] Iteration 1900 (69.4503 iter/s, 1.43988s/100 iter), loss = 0.259856
I0122 16:47:10.570152 58312 solver.cpp:285]     Train net output #0: loss = 0.259856 (* 1 = 0.259856 loss)
I0122 16:47:10.570197 58312 sgd_solver.cpp:106] Iteration 1900, lr = 0.00905
I0122 16:47:12.003250 58312 solver.cpp:418] Iteration 2000, Testing net (#0)
I0122 16:47:12.402424 58312 solver.cpp:517]     Test net output #0: loss = 0.583655 (* 1 = 0.583655 loss)
I0122 16:47:12.402444 58312 solver.cpp:517]     Test net output #1: top-1 = 0.815556
I0122 16:47:12.402448 58312 solver.cpp:517]     Test net output #2: top-5 = 0.988334
I0122 16:47:12.410527 58312 solver.cpp:266] Iteration 2000 (54.3401 iter/s, 1.84026s/100 iter), loss = 0.326125
I0122 16:47:12.410547 58312 solver.cpp:285]     Train net output #0: loss = 0.326125 (* 1 = 0.326125 loss)
I0122 16:47:12.410554 58312 sgd_solver.cpp:106] Iteration 2000, lr = 0.009
I0122 16:47:13.826377 58312 solver.cpp:266] Iteration 2100 (70.6329 iter/s, 1.41577s/100 iter), loss = 0.348969
I0122 16:47:13.826407 58312 solver.cpp:285]     Train net output #0: loss = 0.348969 (* 1 = 0.348969 loss)
I0122 16:47:13.826414 58312 sgd_solver.cpp:106] Iteration 2100, lr = 0.00895
I0122 16:47:15.245630 58312 solver.cpp:266] Iteration 2200 (70.4641 iter/s, 1.41916s/100 iter), loss = 0.255484
I0122 16:47:15.245659 58312 solver.cpp:285]     Train net output #0: loss = 0.255484 (* 1 = 0.255484 loss)
I0122 16:47:15.245666 58312 sgd_solver.cpp:106] Iteration 2200, lr = 0.0089
I0122 16:47:16.699653 58312 solver.cpp:266] Iteration 2300 (68.7791 iter/s, 1.45393s/100 iter), loss = 0.332692
I0122 16:47:16.699685 58312 solver.cpp:285]     Train net output #0: loss = 0.332692 (* 1 = 0.332692 loss)
I0122 16:47:16.699692 58312 sgd_solver.cpp:106] Iteration 2300, lr = 0.00885
I0122 16:47:18.122738 58312 solver.cpp:266] Iteration 2400 (70.2743 iter/s, 1.42299s/100 iter), loss = 0.426664
I0122 16:47:18.122768 58312 solver.cpp:285]     Train net output #0: loss = 0.426664 (* 1 = 0.426664 loss)
I0122 16:47:18.122774 58312 sgd_solver.cpp:106] Iteration 2400, lr = 0.0088
I0122 16:47:19.550601 58312 solver.cpp:266] Iteration 2500 (70.0392 iter/s, 1.42777s/100 iter), loss = 0.288346
I0122 16:47:19.550631 58312 solver.cpp:285]     Train net output #0: loss = 0.288346 (* 1 = 0.288346 loss)
I0122 16:47:19.550637 58312 sgd_solver.cpp:106] Iteration 2500, lr = 0.00875
I0122 16:47:20.994462 58312 solver.cpp:266] Iteration 2600 (69.2631 iter/s, 1.44377s/100 iter), loss = 0.290778
I0122 16:47:20.994493 58312 solver.cpp:285]     Train net output #0: loss = 0.290778 (* 1 = 0.290778 loss)
I0122 16:47:20.994498 58312 sgd_solver.cpp:106] Iteration 2600, lr = 0.0087
I0122 16:47:22.412672 58312 solver.cpp:266] Iteration 2700 (70.5158 iter/s, 1.41812s/100 iter), loss = 0.462006
I0122 16:47:22.412704 58312 solver.cpp:285]     Train net output #0: loss = 0.462006 (* 1 = 0.462006 loss)
I0122 16:47:22.412710 58312 sgd_solver.cpp:106] Iteration 2700, lr = 0.00865
I0122 16:47:23.836820 58312 solver.cpp:266] Iteration 2800 (70.2219 iter/s, 1.42406s/100 iter), loss = 0.278369
I0122 16:47:23.836849 58312 solver.cpp:285]     Train net output #0: loss = 0.278369 (* 1 = 0.278369 loss)
I0122 16:47:23.836856 58312 sgd_solver.cpp:106] Iteration 2800, lr = 0.0086
I0122 16:47:25.265533 58312 solver.cpp:266] Iteration 2900 (69.9975 iter/s, 1.42862s/100 iter), loss = 0.286695
I0122 16:47:25.265563 58312 solver.cpp:285]     Train net output #0: loss = 0.286695 (* 1 = 0.286695 loss)
I0122 16:47:25.265568 58312 sgd_solver.cpp:106] Iteration 2900, lr = 0.00855
I0122 16:47:26.706269 58312 solver.cpp:418] Iteration 3000, Testing net (#0)
I0122 16:47:27.107686 58312 solver.cpp:517]     Test net output #0: loss = 0.585626 (* 1 = 0.585626 loss)
I0122 16:47:27.107705 58312 solver.cpp:517]     Test net output #1: top-1 = 0.815222
I0122 16:47:27.107709 58312 solver.cpp:517]     Test net output #2: top-5 = 0.985778
I0122 16:47:27.115824 58312 solver.cpp:266] Iteration 3000 (54.0485 iter/s, 1.85019s/100 iter), loss = 0.199321
I0122 16:47:27.115844 58312 solver.cpp:285]     Train net output #0: loss = 0.199321 (* 1 = 0.199321 loss)
I0122 16:47:27.115850 58312 sgd_solver.cpp:106] Iteration 3000, lr = 0.0085
I0122 16:47:28.512302 58312 solver.cpp:266] Iteration 3100 (71.6128 iter/s, 1.3964s/100 iter), loss = 0.262232
I0122 16:47:28.512334 58312 solver.cpp:285]     Train net output #0: loss = 0.262232 (* 1 = 0.262232 loss)
I0122 16:47:28.512341 58312 sgd_solver.cpp:106] Iteration 3100, lr = 0.00845
I0122 16:47:29.553453 58312 solver.cpp:266] Iteration 3200 (96.0548 iter/s, 1.04107s/100 iter), loss = 0.306909
I0122 16:47:29.553486 58312 solver.cpp:285]     Train net output #0: loss = 0.306909 (* 1 = 0.306909 loss)
I0122 16:47:29.553529 58312 sgd_solver.cpp:106] Iteration 3200, lr = 0.0084
I0122 16:47:30.957442 58312 solver.cpp:266] Iteration 3300 (71.2324 iter/s, 1.40385s/100 iter), loss = 0.193087
I0122 16:47:30.957471 58312 solver.cpp:285]     Train net output #0: loss = 0.193087 (* 1 = 0.193087 loss)
I0122 16:47:30.957478 58312 sgd_solver.cpp:106] Iteration 3300, lr = 0.00835
I0122 16:47:32.380865 58312 solver.cpp:266] Iteration 3400 (70.2576 iter/s, 1.42333s/100 iter), loss = 0.423326
I0122 16:47:32.380895 58312 solver.cpp:285]     Train net output #0: loss = 0.423326 (* 1 = 0.423326 loss)
I0122 16:47:32.380901 58312 sgd_solver.cpp:106] Iteration 3400, lr = 0.0083
I0122 16:47:33.840721 58312 solver.cpp:266] Iteration 3500 (68.5042 iter/s, 1.45976s/100 iter), loss = 0.317476
I0122 16:47:33.840752 58312 solver.cpp:285]     Train net output #0: loss = 0.317476 (* 1 = 0.317476 loss)
I0122 16:47:33.840759 58312 sgd_solver.cpp:106] Iteration 3500, lr = 0.00825
I0122 16:47:35.259124 58312 solver.cpp:266] Iteration 3600 (70.5063 iter/s, 1.41831s/100 iter), loss = 0.325519
I0122 16:47:35.259156 58312 solver.cpp:285]     Train net output #0: loss = 0.325519 (* 1 = 0.325519 loss)
I0122 16:47:35.259162 58312 sgd_solver.cpp:106] Iteration 3600, lr = 0.0082
I0122 16:47:36.680428 58312 solver.cpp:266] Iteration 3700 (70.3624 iter/s, 1.42121s/100 iter), loss = 0.346314
I0122 16:47:36.680459 58312 solver.cpp:285]     Train net output #0: loss = 0.346314 (* 1 = 0.346314 loss)
I0122 16:47:36.680464 58312 sgd_solver.cpp:106] Iteration 3700, lr = 0.00815
I0122 16:47:38.139197 58312 solver.cpp:266] Iteration 3800 (68.5552 iter/s, 1.45868s/100 iter), loss = 0.36633
I0122 16:47:38.139230 58312 solver.cpp:285]     Train net output #0: loss = 0.36633 (* 1 = 0.36633 loss)
I0122 16:47:38.139235 58312 sgd_solver.cpp:106] Iteration 3800, lr = 0.0081
I0122 16:47:39.559396 58312 solver.cpp:266] Iteration 3900 (70.4172 iter/s, 1.42011s/100 iter), loss = 0.200626
I0122 16:47:39.559427 58312 solver.cpp:285]     Train net output #0: loss = 0.200626 (* 1 = 0.200626 loss)
I0122 16:47:39.559432 58312 sgd_solver.cpp:106] Iteration 3900, lr = 0.00805
I0122 16:47:40.968325 58312 solver.cpp:418] Iteration 4000, Testing net (#0)
I0122 16:47:41.387349 58312 solver.cpp:517]     Test net output #0: loss = 0.5531 (* 1 = 0.5531 loss)
I0122 16:47:41.387365 58312 solver.cpp:517]     Test net output #1: top-1 = 0.821666
I0122 16:47:41.387379 58312 solver.cpp:517]     Test net output #2: top-5 = 0.988667
I0122 16:47:41.422955 58312 solver.cpp:266] Iteration 4000 (53.6636 iter/s, 1.86346s/100 iter), loss = 0.329511
I0122 16:47:41.422976 58312 solver.cpp:285]     Train net output #0: loss = 0.329511 (* 1 = 0.329511 loss)
I0122 16:47:41.422982 58312 sgd_solver.cpp:106] Iteration 4000, lr = 0.008
I0122 16:47:42.837451 58312 solver.cpp:266] Iteration 4100 (70.7005 iter/s, 1.41442s/100 iter), loss = 0.295614
I0122 16:47:42.837563 58312 solver.cpp:285]     Train net output #0: loss = 0.295614 (* 1 = 0.295614 loss)
I0122 16:47:42.837569 58312 sgd_solver.cpp:106] Iteration 4100, lr = 0.00795
I0122 16:47:44.253707 58312 solver.cpp:266] Iteration 4200 (70.6171 iter/s, 1.41609s/100 iter), loss = 0.326627
I0122 16:47:44.253738 58312 solver.cpp:285]     Train net output #0: loss = 0.326627 (* 1 = 0.326627 loss)
I0122 16:47:44.253743 58312 sgd_solver.cpp:106] Iteration 4200, lr = 0.0079
I0122 16:47:45.693394 58312 solver.cpp:266] Iteration 4300 (69.4639 iter/s, 1.4396s/100 iter), loss = 0.379523
I0122 16:47:45.693426 58312 solver.cpp:285]     Train net output #0: loss = 0.379523 (* 1 = 0.379523 loss)
I0122 16:47:45.695641 58312 sgd_solver.cpp:106] Iteration 4300, lr = 0.00785
I0122 16:47:47.131682 58312 solver.cpp:266] Iteration 4400 (69.6387 iter/s, 1.43598s/100 iter), loss = 0.325291
I0122 16:47:47.131712 58312 solver.cpp:285]     Train net output #0: loss = 0.325291 (* 1 = 0.325291 loss)
I0122 16:47:47.131718 58312 sgd_solver.cpp:106] Iteration 4400, lr = 0.0078
I0122 16:47:48.540371 58312 solver.cpp:266] Iteration 4500 (70.9925 iter/s, 1.4086s/100 iter), loss = 0.360811
I0122 16:47:48.540401 58312 solver.cpp:285]     Train net output #0: loss = 0.360811 (* 1 = 0.360811 loss)
I0122 16:47:48.540406 58312 sgd_solver.cpp:106] Iteration 4500, lr = 0.00775
I0122 16:47:49.986109 58312 solver.cpp:266] Iteration 4600 (69.1732 iter/s, 1.44565s/100 iter), loss = 0.316999
I0122 16:47:49.986137 58312 solver.cpp:285]     Train net output #0: loss = 0.316999 (* 1 = 0.316999 loss)
I0122 16:47:49.988355 58312 sgd_solver.cpp:106] Iteration 4600, lr = 0.0077
I0122 16:47:51.427049 58312 solver.cpp:266] Iteration 4700 (69.5103 iter/s, 1.43864s/100 iter), loss = 0.467512
I0122 16:47:51.427080 58312 solver.cpp:285]     Train net output #0: loss = 0.467512 (* 1 = 0.467512 loss)
I0122 16:47:51.427086 58312 sgd_solver.cpp:106] Iteration 4700, lr = 0.00765
I0122 16:47:52.846803 58312 solver.cpp:266] Iteration 4800 (70.4393 iter/s, 1.41966s/100 iter), loss = 0.268442
I0122 16:47:52.846833 58312 solver.cpp:285]     Train net output #0: loss = 0.268442 (* 1 = 0.268442 loss)
I0122 16:47:52.846839 58312 sgd_solver.cpp:106] Iteration 4800, lr = 0.0076
I0122 16:47:54.299011 58312 solver.cpp:266] Iteration 4900 (68.8651 iter/s, 1.45211s/100 iter), loss = 0.456482
I0122 16:47:54.299041 58312 solver.cpp:285]     Train net output #0: loss = 0.456482 (* 1 = 0.456482 loss)
I0122 16:47:54.299047 58312 sgd_solver.cpp:106] Iteration 4900, lr = 0.00755
I0122 16:47:55.709297 58312 solver.cpp:418] Iteration 5000, Testing net (#0)
I0122 16:47:56.110698 58312 solver.cpp:517]     Test net output #0: loss = 0.519797 (* 1 = 0.519797 loss)
I0122 16:47:56.110718 58312 solver.cpp:517]     Test net output #1: top-1 = 0.831222
I0122 16:47:56.110721 58312 solver.cpp:517]     Test net output #2: top-5 = 0.989889
I0122 16:47:56.118780 58312 solver.cpp:266] Iteration 5000 (54.9551 iter/s, 1.81967s/100 iter), loss = 0.321727
I0122 16:47:56.118800 58312 solver.cpp:285]     Train net output #0: loss = 0.321727 (* 1 = 0.321727 loss)
I0122 16:47:56.118808 58312 sgd_solver.cpp:106] Iteration 5000, lr = 0.0075
I0122 16:47:57.528702 58312 solver.cpp:266] Iteration 5100 (70.9299 iter/s, 1.40984s/100 iter), loss = 0.285196
I0122 16:47:57.528733 58312 solver.cpp:285]     Train net output #0: loss = 0.285196 (* 1 = 0.285196 loss)
I0122 16:47:57.528738 58312 sgd_solver.cpp:106] Iteration 5100, lr = 0.00745
I0122 16:47:58.960904 58312 solver.cpp:266] Iteration 5200 (69.8269 iter/s, 1.43211s/100 iter), loss = 0.298008
I0122 16:47:58.960933 58312 solver.cpp:285]     Train net output #0: loss = 0.298008 (* 1 = 0.298008 loss)
I0122 16:47:58.963143 58312 sgd_solver.cpp:106] Iteration 5200, lr = 0.0074
I0122 16:48:00.410823 58312 solver.cpp:266] Iteration 5300 (69.0788 iter/s, 1.44762s/100 iter), loss = 0.324297
I0122 16:48:00.410854 58312 solver.cpp:285]     Train net output #0: loss = 0.324297 (* 1 = 0.324297 loss)
I0122 16:48:00.410861 58312 sgd_solver.cpp:106] Iteration 5300, lr = 0.00735
I0122 16:48:01.824249 58312 solver.cpp:266] Iteration 5400 (70.7546 iter/s, 1.41334s/100 iter), loss = 0.31724
I0122 16:48:01.824299 58312 solver.cpp:285]     Train net output #0: loss = 0.31724 (* 1 = 0.31724 loss)
I0122 16:48:01.824306 58312 sgd_solver.cpp:106] Iteration 5400, lr = 0.0073
I0122 16:48:03.280450 58312 solver.cpp:266] Iteration 5500 (68.6771 iter/s, 1.45609s/100 iter), loss = 0.274158
I0122 16:48:03.280481 58312 solver.cpp:285]     Train net output #0: loss = 0.274158 (* 1 = 0.274158 loss)
I0122 16:48:03.280488 58312 sgd_solver.cpp:106] Iteration 5500, lr = 0.00725
I0122 16:48:04.696779 58312 solver.cpp:266] Iteration 5600 (70.6096 iter/s, 1.41624s/100 iter), loss = 0.264274
I0122 16:48:04.696805 58312 solver.cpp:285]     Train net output #0: loss = 0.264274 (* 1 = 0.264274 loss)
I0122 16:48:04.696810 58312 sgd_solver.cpp:106] Iteration 5600, lr = 0.0072
I0122 16:48:06.125277 58312 solver.cpp:266] Iteration 5700 (70.0078 iter/s, 1.42841s/100 iter), loss = 0.316816
I0122 16:48:06.125305 58312 solver.cpp:285]     Train net output #0: loss = 0.316816 (* 1 = 0.316816 loss)
I0122 16:48:06.125311 58312 sgd_solver.cpp:106] Iteration 5700, lr = 0.00715
I0122 16:48:07.561856 58312 solver.cpp:266] Iteration 5800 (69.6141 iter/s, 1.43649s/100 iter), loss = 0.333464
I0122 16:48:07.561885 58312 solver.cpp:285]     Train net output #0: loss = 0.333464 (* 1 = 0.333464 loss)
I0122 16:48:07.564098 58312 sgd_solver.cpp:106] Iteration 5800, lr = 0.0071
I0122 16:48:09.003885 58312 solver.cpp:266] Iteration 5900 (69.4575 iter/s, 1.43973s/100 iter), loss = 0.268907
I0122 16:48:09.003916 58312 solver.cpp:285]     Train net output #0: loss = 0.268907 (* 1 = 0.268907 loss)
I0122 16:48:09.003922 58312 sgd_solver.cpp:106] Iteration 5900, lr = 0.00705
I0122 16:48:10.410159 58312 solver.cpp:418] Iteration 6000, Testing net (#0)
I0122 16:48:10.809567 58312 solver.cpp:517]     Test net output #0: loss = 0.542629 (* 1 = 0.542629 loss)
I0122 16:48:10.809584 58312 solver.cpp:517]     Test net output #1: top-1 = 0.830111
I0122 16:48:10.809588 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:48:10.824760 58312 solver.cpp:266] Iteration 6000 (54.9217 iter/s, 1.82077s/100 iter), loss = 0.37067
I0122 16:48:10.824782 58312 solver.cpp:285]     Train net output #0: loss = 0.37067 (* 1 = 0.37067 loss)
I0122 16:48:10.824790 58312 sgd_solver.cpp:106] Iteration 6000, lr = 0.007
I0122 16:48:12.270165 58312 solver.cpp:266] Iteration 6100 (69.1887 iter/s, 1.44532s/100 iter), loss = 0.29498
I0122 16:48:12.270196 58312 solver.cpp:285]     Train net output #0: loss = 0.29498 (* 1 = 0.29498 loss)
I0122 16:48:12.270202 58312 sgd_solver.cpp:106] Iteration 6100, lr = 0.00695
I0122 16:48:13.776548 58312 solver.cpp:266] Iteration 6200 (66.3883 iter/s, 1.50629s/100 iter), loss = 0.245993
I0122 16:48:13.776631 58312 solver.cpp:285]     Train net output #0: loss = 0.245993 (* 1 = 0.245993 loss)
I0122 16:48:13.776638 58312 sgd_solver.cpp:106] Iteration 6200, lr = 0.0069
I0122 16:48:15.297286 58312 solver.cpp:266] Iteration 6300 (65.7638 iter/s, 1.52059s/100 iter), loss = 0.220188
I0122 16:48:15.297314 58312 solver.cpp:285]     Train net output #0: loss = 0.220188 (* 1 = 0.220188 loss)
I0122 16:48:15.297320 58312 sgd_solver.cpp:106] Iteration 6300, lr = 0.00685
I0122 16:48:16.816567 58312 solver.cpp:266] Iteration 6400 (65.8245 iter/s, 1.51919s/100 iter), loss = 0.248188
I0122 16:48:16.816598 58312 solver.cpp:285]     Train net output #0: loss = 0.248188 (* 1 = 0.248188 loss)
I0122 16:48:16.816604 58312 sgd_solver.cpp:106] Iteration 6400, lr = 0.0068
I0122 16:48:18.356133 58312 solver.cpp:266] Iteration 6500 (64.9574 iter/s, 1.53947s/100 iter), loss = 0.294874
I0122 16:48:18.356165 58312 solver.cpp:285]     Train net output #0: loss = 0.294874 (* 1 = 0.294874 loss)
I0122 16:48:18.358374 58312 sgd_solver.cpp:106] Iteration 6500, lr = 0.00675
I0122 16:48:19.905648 58312 solver.cpp:266] Iteration 6600 (64.6324 iter/s, 1.54721s/100 iter), loss = 0.317285
I0122 16:48:19.905689 58312 solver.cpp:285]     Train net output #0: loss = 0.317285 (* 1 = 0.317285 loss)
I0122 16:48:19.905695 58312 sgd_solver.cpp:106] Iteration 6600, lr = 0.0067
I0122 16:48:21.428934 58312 solver.cpp:266] Iteration 6700 (65.652 iter/s, 1.52318s/100 iter), loss = 0.245738
I0122 16:48:21.428964 58312 solver.cpp:285]     Train net output #0: loss = 0.245738 (* 1 = 0.245738 loss)
I0122 16:48:21.428970 58312 sgd_solver.cpp:106] Iteration 6700, lr = 0.00665
I0122 16:48:22.951807 58312 solver.cpp:266] Iteration 6800 (65.6694 iter/s, 1.52278s/100 iter), loss = 0.375939
I0122 16:48:22.951836 58312 solver.cpp:285]     Train net output #0: loss = 0.375939 (* 1 = 0.375939 loss)
I0122 16:48:22.951843 58312 sgd_solver.cpp:106] Iteration 6800, lr = 0.0066
I0122 16:48:24.433056 58312 solver.cpp:266] Iteration 6900 (67.5148 iter/s, 1.48116s/100 iter), loss = 0.259578
I0122 16:48:24.433097 58312 solver.cpp:285]     Train net output #0: loss = 0.259578 (* 1 = 0.259578 loss)
I0122 16:48:24.433104 58312 sgd_solver.cpp:106] Iteration 6900, lr = 0.00655
I0122 16:48:25.853214 58312 solver.cpp:418] Iteration 7000, Testing net (#0)
I0122 16:48:26.298113 58312 solver.cpp:517]     Test net output #0: loss = 0.520141 (* 1 = 0.520141 loss)
I0122 16:48:26.298130 58312 solver.cpp:517]     Test net output #1: top-1 = 0.829555
I0122 16:48:26.298143 58312 solver.cpp:517]     Test net output #2: top-5 = 0.988556
I0122 16:48:26.306236 58312 solver.cpp:266] Iteration 7000 (53.3884 iter/s, 1.87307s/100 iter), loss = 0.406141
I0122 16:48:26.306254 58312 solver.cpp:285]     Train net output #0: loss = 0.406141 (* 1 = 0.406141 loss)
I0122 16:48:26.306260 58312 sgd_solver.cpp:106] Iteration 7000, lr = 0.0065
I0122 16:48:27.726423 58312 solver.cpp:266] Iteration 7100 (70.417 iter/s, 1.42011s/100 iter), loss = 0.325279
I0122 16:48:27.726454 58312 solver.cpp:285]     Train net output #0: loss = 0.325279 (* 1 = 0.325279 loss)
I0122 16:48:27.726459 58312 sgd_solver.cpp:106] Iteration 7100, lr = 0.00645
I0122 16:48:29.137079 58312 solver.cpp:266] Iteration 7200 (70.8936 iter/s, 1.41056s/100 iter), loss = 0.34838
I0122 16:48:29.137109 58312 solver.cpp:285]     Train net output #0: loss = 0.34838 (* 1 = 0.34838 loss)
I0122 16:48:29.137115 58312 sgd_solver.cpp:106] Iteration 7200, lr = 0.0064
I0122 16:48:30.578670 58312 solver.cpp:266] Iteration 7300 (69.3724 iter/s, 1.4415s/100 iter), loss = 0.360001
I0122 16:48:30.578699 58312 solver.cpp:285]     Train net output #0: loss = 0.360001 (* 1 = 0.360001 loss)
I0122 16:48:30.580915 58312 sgd_solver.cpp:106] Iteration 7300, lr = 0.00635
I0122 16:48:32.012105 58312 solver.cpp:266] Iteration 7400 (69.8747 iter/s, 1.43113s/100 iter), loss = 0.228007
I0122 16:48:32.012136 58312 solver.cpp:285]     Train net output #0: loss = 0.228007 (* 1 = 0.228007 loss)
I0122 16:48:32.012141 58312 sgd_solver.cpp:106] Iteration 7400, lr = 0.0063
I0122 16:48:33.430613 58312 solver.cpp:266] Iteration 7500 (70.5011 iter/s, 1.41842s/100 iter), loss = 0.290923
I0122 16:48:33.430677 58312 solver.cpp:285]     Train net output #0: loss = 0.290923 (* 1 = 0.290923 loss)
I0122 16:48:33.430685 58312 sgd_solver.cpp:106] Iteration 7500, lr = 0.00625
I0122 16:48:34.856586 58312 solver.cpp:266] Iteration 7600 (70.1336 iter/s, 1.42585s/100 iter), loss = 0.233588
I0122 16:48:34.856629 58312 solver.cpp:285]     Train net output #0: loss = 0.233588 (* 1 = 0.233588 loss)
I0122 16:48:34.856637 58312 sgd_solver.cpp:106] Iteration 7600, lr = 0.0062
I0122 16:48:36.311689 58312 solver.cpp:266] Iteration 7700 (68.7285 iter/s, 1.455s/100 iter), loss = 0.19444
I0122 16:48:36.311719 58312 solver.cpp:285]     Train net output #0: loss = 0.19444 (* 1 = 0.19444 loss)
I0122 16:48:36.311740 58312 sgd_solver.cpp:106] Iteration 7700, lr = 0.00615
I0122 16:48:37.730077 58312 solver.cpp:266] Iteration 7800 (70.507 iter/s, 1.4183s/100 iter), loss = 0.23877
I0122 16:48:37.730106 58312 solver.cpp:285]     Train net output #0: loss = 0.23877 (* 1 = 0.23877 loss)
I0122 16:48:37.730113 58312 sgd_solver.cpp:106] Iteration 7800, lr = 0.0061
I0122 16:48:39.167552 58312 solver.cpp:266] Iteration 7900 (69.5708 iter/s, 1.43739s/100 iter), loss = 0.270279
I0122 16:48:39.167584 58312 solver.cpp:285]     Train net output #0: loss = 0.270279 (* 1 = 0.270279 loss)
I0122 16:48:39.169800 58312 sgd_solver.cpp:106] Iteration 7900, lr = 0.00605
I0122 16:48:40.594193 58312 solver.cpp:418] Iteration 8000, Testing net (#0)
I0122 16:48:40.992202 58312 solver.cpp:517]     Test net output #0: loss = 0.551165 (* 1 = 0.551165 loss)
I0122 16:48:40.992220 58312 solver.cpp:517]     Test net output #1: top-1 = 0.823111
I0122 16:48:40.992224 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991334
I0122 16:48:41.000300 58312 solver.cpp:266] Iteration 8000 (54.6319 iter/s, 1.83043s/100 iter), loss = 0.223896
I0122 16:48:41.000320 58312 solver.cpp:285]     Train net output #0: loss = 0.223896 (* 1 = 0.223896 loss)
I0122 16:48:41.000327 58312 sgd_solver.cpp:106] Iteration 8000, lr = 0.006
I0122 16:48:42.420756 58312 solver.cpp:266] Iteration 8100 (70.404 iter/s, 1.42037s/100 iter), loss = 0.26518
I0122 16:48:42.420786 58312 solver.cpp:285]     Train net output #0: loss = 0.26518 (* 1 = 0.26518 loss)
I0122 16:48:42.420792 58312 sgd_solver.cpp:106] Iteration 8100, lr = 0.00595
I0122 16:48:43.886524 58312 solver.cpp:266] Iteration 8200 (68.2279 iter/s, 1.46568s/100 iter), loss = 0.115809
I0122 16:48:43.886694 58312 solver.cpp:285]     Train net output #0: loss = 0.115809 (* 1 = 0.115809 loss)
I0122 16:48:43.886703 58312 sgd_solver.cpp:106] Iteration 8200, lr = 0.0059
I0122 16:48:45.307691 58312 solver.cpp:266] Iteration 8300 (70.3759 iter/s, 1.42094s/100 iter), loss = 0.2026
I0122 16:48:45.307723 58312 solver.cpp:285]     Train net output #0: loss = 0.2026 (* 1 = 0.2026 loss)
I0122 16:48:45.307729 58312 sgd_solver.cpp:106] Iteration 8300, lr = 0.00585
I0122 16:48:46.725708 58312 solver.cpp:266] Iteration 8400 (70.5255 iter/s, 1.41793s/100 iter), loss = 0.269474
I0122 16:48:46.725741 58312 solver.cpp:285]     Train net output #0: loss = 0.269474 (* 1 = 0.269474 loss)
I0122 16:48:46.725747 58312 sgd_solver.cpp:106] Iteration 8400, lr = 0.0058
I0122 16:48:48.145905 58312 solver.cpp:266] Iteration 8500 (70.4174 iter/s, 1.4201s/100 iter), loss = 0.285371
I0122 16:48:48.145936 58312 solver.cpp:285]     Train net output #0: loss = 0.285371 (* 1 = 0.285371 loss)
I0122 16:48:48.145942 58312 sgd_solver.cpp:106] Iteration 8500, lr = 0.00575
I0122 16:48:49.593864 58312 solver.cpp:266] Iteration 8600 (69.0672 iter/s, 1.44786s/100 iter), loss = 0.249659
I0122 16:48:49.593895 58312 solver.cpp:285]     Train net output #0: loss = 0.249659 (* 1 = 0.249659 loss)
I0122 16:48:49.593901 58312 sgd_solver.cpp:106] Iteration 8600, lr = 0.0057
I0122 16:48:51.014866 58312 solver.cpp:266] Iteration 8700 (70.3773 iter/s, 1.42091s/100 iter), loss = 0.250168
I0122 16:48:51.014895 58312 solver.cpp:285]     Train net output #0: loss = 0.250168 (* 1 = 0.250168 loss)
I0122 16:48:51.014902 58312 sgd_solver.cpp:106] Iteration 8700, lr = 0.00565
I0122 16:48:52.433029 58312 solver.cpp:266] Iteration 8800 (70.5181 iter/s, 1.41808s/100 iter), loss = 0.363054
I0122 16:48:52.433056 58312 solver.cpp:285]     Train net output #0: loss = 0.363054 (* 1 = 0.363054 loss)
I0122 16:48:52.433063 58312 sgd_solver.cpp:106] Iteration 8800, lr = 0.0056
I0122 16:48:53.881484 58312 solver.cpp:266] Iteration 8900 (69.0432 iter/s, 1.44837s/100 iter), loss = 0.229036
I0122 16:48:53.881516 58312 solver.cpp:285]     Train net output #0: loss = 0.229036 (* 1 = 0.229036 loss)
I0122 16:48:53.881561 58312 sgd_solver.cpp:106] Iteration 8900, lr = 0.00555
I0122 16:48:55.297384 58312 solver.cpp:418] Iteration 9000, Testing net (#0)
I0122 16:48:55.721662 58312 solver.cpp:517]     Test net output #0: loss = 0.51053 (* 1 = 0.51053 loss)
I0122 16:48:55.721680 58312 solver.cpp:517]     Test net output #1: top-1 = 0.836333
I0122 16:48:55.721684 58312 solver.cpp:517]     Test net output #2: top-5 = 0.990444
I0122 16:48:55.729806 58312 solver.cpp:266] Iteration 9000 (54.1073 iter/s, 1.84818s/100 iter), loss = 0.28269
I0122 16:48:55.729826 58312 solver.cpp:285]     Train net output #0: loss = 0.282691 (* 1 = 0.282691 loss)
I0122 16:48:55.729832 58312 sgd_solver.cpp:106] Iteration 9000, lr = 0.0055
I0122 16:48:57.146585 58312 solver.cpp:266] Iteration 9100 (70.5865 iter/s, 1.4167s/100 iter), loss = 0.165615
I0122 16:48:57.146615 58312 solver.cpp:285]     Train net output #0: loss = 0.165615 (* 1 = 0.165615 loss)
I0122 16:48:57.146621 58312 sgd_solver.cpp:106] Iteration 9100, lr = 0.00545
I0122 16:48:58.561962 58312 solver.cpp:266] Iteration 9200 (70.6571 iter/s, 1.41529s/100 iter), loss = 0.311267
I0122 16:48:58.561993 58312 solver.cpp:285]     Train net output #0: loss = 0.311267 (* 1 = 0.311267 loss)
I0122 16:48:58.562000 58312 sgd_solver.cpp:106] Iteration 9200, lr = 0.0054
I0122 16:49:00.003438 58312 solver.cpp:266] Iteration 9300 (69.3779 iter/s, 1.44138s/100 iter), loss = 0.218344
I0122 16:49:00.003468 58312 solver.cpp:285]     Train net output #0: loss = 0.218344 (* 1 = 0.218344 loss)
I0122 16:49:00.005688 58312 sgd_solver.cpp:106] Iteration 9300, lr = 0.00535
I0122 16:49:01.435137 58312 solver.cpp:266] Iteration 9400 (69.9599 iter/s, 1.42939s/100 iter), loss = 0.320064
I0122 16:49:01.435168 58312 solver.cpp:285]     Train net output #0: loss = 0.320064 (* 1 = 0.320064 loss)
I0122 16:49:01.435174 58312 sgd_solver.cpp:106] Iteration 9400, lr = 0.0053
I0122 16:49:02.854909 58312 solver.cpp:266] Iteration 9500 (70.4383 iter/s, 1.41968s/100 iter), loss = 0.18121
I0122 16:49:02.854974 58312 solver.cpp:285]     Train net output #0: loss = 0.18121 (* 1 = 0.18121 loss)
I0122 16:49:02.854981 58312 sgd_solver.cpp:106] Iteration 9500, lr = 0.00525
I0122 16:49:04.295691 58312 solver.cpp:266] Iteration 9600 (69.4128 iter/s, 1.44066s/100 iter), loss = 0.354689
I0122 16:49:04.295722 58312 solver.cpp:285]     Train net output #0: loss = 0.354689 (* 1 = 0.354689 loss)
I0122 16:49:04.297942 58312 sgd_solver.cpp:106] Iteration 9600, lr = 0.0052
I0122 16:49:05.731505 58312 solver.cpp:266] Iteration 9700 (69.7591 iter/s, 1.4335s/100 iter), loss = 0.166016
I0122 16:49:05.731535 58312 solver.cpp:285]     Train net output #0: loss = 0.166016 (* 1 = 0.166016 loss)
I0122 16:49:05.731542 58312 sgd_solver.cpp:106] Iteration 9700, lr = 0.00515
I0122 16:49:07.152644 58312 solver.cpp:266] Iteration 9800 (70.3705 iter/s, 1.42105s/100 iter), loss = 0.322197
I0122 16:49:07.152674 58312 solver.cpp:285]     Train net output #0: loss = 0.322197 (* 1 = 0.322197 loss)
I0122 16:49:07.152680 58312 sgd_solver.cpp:106] Iteration 9800, lr = 0.0051
I0122 16:49:08.595185 58312 solver.cpp:266] Iteration 9900 (69.3265 iter/s, 1.44245s/100 iter), loss = 0.298505
I0122 16:49:08.595216 58312 solver.cpp:285]     Train net output #0: loss = 0.298505 (* 1 = 0.298505 loss)
I0122 16:49:08.595257 58312 sgd_solver.cpp:106] Iteration 9900, lr = 0.00505
I0122 16:49:09.952414 58312 solver.cpp:418] Iteration 10000, Testing net (#0)
I0122 16:49:10.247941 58312 solver.cpp:517]     Test net output #0: loss = 0.490215 (* 1 = 0.490215 loss)
I0122 16:49:10.247959 58312 solver.cpp:517]     Test net output #1: top-1 = 0.840111
I0122 16:49:10.247963 58312 solver.cpp:517]     Test net output #2: top-5 = 0.989445
I0122 16:49:10.256068 58312 solver.cpp:266] Iteration 10000 (60.2138 iter/s, 1.66075s/100 iter), loss = 0.258272
I0122 16:49:10.256096 58312 solver.cpp:285]     Train net output #0: loss = 0.258272 (* 1 = 0.258272 loss)
I0122 16:49:10.256104 58312 sgd_solver.cpp:106] Iteration 10000, lr = 0.005
I0122 16:49:11.452085 58312 solver.cpp:266] Iteration 10100 (83.6157 iter/s, 1.19595s/100 iter), loss = 0.261322
I0122 16:49:11.452127 58312 solver.cpp:285]     Train net output #0: loss = 0.261322 (* 1 = 0.261322 loss)
I0122 16:49:11.452134 58312 sgd_solver.cpp:106] Iteration 10100, lr = 0.00495
I0122 16:49:12.914784 58312 solver.cpp:266] Iteration 10200 (68.3717 iter/s, 1.46259s/100 iter), loss = 0.289293
I0122 16:49:12.914815 58312 solver.cpp:285]     Train net output #0: loss = 0.289293 (* 1 = 0.289293 loss)
I0122 16:49:12.914821 58312 sgd_solver.cpp:106] Iteration 10200, lr = 0.0049
I0122 16:49:14.329951 58312 solver.cpp:266] Iteration 10300 (70.6675 iter/s, 1.41508s/100 iter), loss = 0.311981
I0122 16:49:14.330117 58312 solver.cpp:285]     Train net output #0: loss = 0.311981 (* 1 = 0.311981 loss)
I0122 16:49:14.330124 58312 sgd_solver.cpp:106] Iteration 10300, lr = 0.00485
I0122 16:49:15.758951 58312 solver.cpp:266] Iteration 10400 (69.9899 iter/s, 1.42878s/100 iter), loss = 0.295789
I0122 16:49:15.758982 58312 solver.cpp:285]     Train net output #0: loss = 0.295789 (* 1 = 0.295789 loss)
I0122 16:49:15.758988 58312 sgd_solver.cpp:106] Iteration 10400, lr = 0.0048
I0122 16:49:17.190984 58312 solver.cpp:266] Iteration 10500 (69.8352 iter/s, 1.43194s/100 iter), loss = 0.173949
I0122 16:49:17.191013 58312 solver.cpp:285]     Train net output #0: loss = 0.173949 (* 1 = 0.173949 loss)
I0122 16:49:17.193219 58312 sgd_solver.cpp:106] Iteration 10500, lr = 0.00475
I0122 16:49:18.627723 58312 solver.cpp:266] Iteration 10600 (69.7133 iter/s, 1.43445s/100 iter), loss = 0.300829
I0122 16:49:18.627753 58312 solver.cpp:285]     Train net output #0: loss = 0.300829 (* 1 = 0.300829 loss)
I0122 16:49:18.627759 58312 sgd_solver.cpp:106] Iteration 10600, lr = 0.0047
I0122 16:49:20.040900 58312 solver.cpp:266] Iteration 10700 (70.767 iter/s, 1.41309s/100 iter), loss = 0.298509
I0122 16:49:20.040930 58312 solver.cpp:285]     Train net output #0: loss = 0.298509 (* 1 = 0.298509 loss)
I0122 16:49:20.040936 58312 sgd_solver.cpp:106] Iteration 10700, lr = 0.00465
I0122 16:49:21.459725 58312 solver.cpp:266] Iteration 10800 (70.4852 iter/s, 1.41874s/100 iter), loss = 0.241881
I0122 16:49:21.459756 58312 solver.cpp:285]     Train net output #0: loss = 0.241881 (* 1 = 0.241881 loss)
I0122 16:49:21.459762 58312 sgd_solver.cpp:106] Iteration 10800, lr = 0.0046
I0122 16:49:22.924427 58312 solver.cpp:266] Iteration 10900 (68.2776 iter/s, 1.46461s/100 iter), loss = 0.313129
I0122 16:49:22.924456 58312 solver.cpp:285]     Train net output #0: loss = 0.313129 (* 1 = 0.313129 loss)
I0122 16:49:22.924461 58312 sgd_solver.cpp:106] Iteration 10900, lr = 0.00455
I0122 16:49:24.331984 58312 solver.cpp:418] Iteration 11000, Testing net (#0)
I0122 16:49:24.731324 58312 solver.cpp:517]     Test net output #0: loss = 0.534782 (* 1 = 0.534782 loss)
I0122 16:49:24.731348 58312 solver.cpp:517]     Test net output #1: top-1 = 0.828222
I0122 16:49:24.731351 58312 solver.cpp:517]     Test net output #2: top-5 = 0.988778
I0122 16:49:24.739501 58312 solver.cpp:266] Iteration 11000 (55.0971 iter/s, 1.81498s/100 iter), loss = 0.332049
I0122 16:49:24.739519 58312 solver.cpp:285]     Train net output #0: loss = 0.33205 (* 1 = 0.33205 loss)
I0122 16:49:24.739526 58312 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0122 16:49:26.165722 58312 solver.cpp:266] Iteration 11100 (70.1192 iter/s, 1.42614s/100 iter), loss = 0.222754
I0122 16:49:26.165753 58312 solver.cpp:285]     Train net output #0: loss = 0.222754 (* 1 = 0.222754 loss)
I0122 16:49:26.165758 58312 sgd_solver.cpp:106] Iteration 11100, lr = 0.00445
I0122 16:49:27.597995 58312 solver.cpp:266] Iteration 11200 (69.8235 iter/s, 1.43218s/100 iter), loss = 0.218672
I0122 16:49:27.598024 58312 solver.cpp:285]     Train net output #0: loss = 0.218672 (* 1 = 0.218672 loss)
I0122 16:49:27.600239 58312 sgd_solver.cpp:106] Iteration 11200, lr = 0.0044
I0122 16:49:29.028728 58312 solver.cpp:266] Iteration 11300 (70.0069 iter/s, 1.42843s/100 iter), loss = 0.200466
I0122 16:49:29.028760 58312 solver.cpp:285]     Train net output #0: loss = 0.200466 (* 1 = 0.200466 loss)
I0122 16:49:29.028766 58312 sgd_solver.cpp:106] Iteration 11300, lr = 0.00435
I0122 16:49:30.473248 58312 solver.cpp:266] Iteration 11400 (69.2315 iter/s, 1.44443s/100 iter), loss = 0.30793
I0122 16:49:30.473278 58312 solver.cpp:285]     Train net output #0: loss = 0.30793 (* 1 = 0.30793 loss)
I0122 16:49:30.475491 58312 sgd_solver.cpp:106] Iteration 11400, lr = 0.0043
I0122 16:49:31.914939 58312 solver.cpp:266] Iteration 11500 (69.4739 iter/s, 1.43939s/100 iter), loss = 0.229089
I0122 16:49:31.914970 58312 solver.cpp:285]     Train net output #0: loss = 0.229089 (* 1 = 0.229089 loss)
I0122 16:49:31.914976 58312 sgd_solver.cpp:106] Iteration 11500, lr = 0.00425
I0122 16:49:33.339934 58312 solver.cpp:266] Iteration 11600 (70.1801 iter/s, 1.42491s/100 iter), loss = 0.250778
I0122 16:49:33.339963 58312 solver.cpp:285]     Train net output #0: loss = 0.250778 (* 1 = 0.250778 loss)
I0122 16:49:33.339970 58312 sgd_solver.cpp:106] Iteration 11600, lr = 0.0042
I0122 16:49:34.776561 58312 solver.cpp:266] Iteration 11700 (69.6119 iter/s, 1.43654s/100 iter), loss = 0.318375
I0122 16:49:34.776590 58312 solver.cpp:285]     Train net output #0: loss = 0.318375 (* 1 = 0.318375 loss)
I0122 16:49:34.778812 58312 sgd_solver.cpp:106] Iteration 11700, lr = 0.00415
I0122 16:49:36.209643 58312 solver.cpp:266] Iteration 11800 (69.8923 iter/s, 1.43077s/100 iter), loss = 0.326488
I0122 16:49:36.209673 58312 solver.cpp:285]     Train net output #0: loss = 0.326488 (* 1 = 0.326488 loss)
I0122 16:49:36.209679 58312 sgd_solver.cpp:106] Iteration 11800, lr = 0.0041
I0122 16:49:37.624979 58312 solver.cpp:266] Iteration 11900 (70.659 iter/s, 1.41525s/100 iter), loss = 0.199798
I0122 16:49:37.625008 58312 solver.cpp:285]     Train net output #0: loss = 0.199798 (* 1 = 0.199798 loss)
I0122 16:49:37.625015 58312 sgd_solver.cpp:106] Iteration 11900, lr = 0.00405
I0122 16:49:39.038949 58312 solver.cpp:418] Iteration 12000, Testing net (#0)
I0122 16:49:39.466850 58312 solver.cpp:517]     Test net output #0: loss = 0.543084 (* 1 = 0.543084 loss)
I0122 16:49:39.466868 58312 solver.cpp:517]     Test net output #1: top-1 = 0.830111
I0122 16:49:39.466872 58312 solver.cpp:517]     Test net output #2: top-5 = 0.988222
I0122 16:49:39.492658 58312 solver.cpp:266] Iteration 12000 (53.5452 iter/s, 1.86758s/100 iter), loss = 0.192966
I0122 16:49:39.492678 58312 solver.cpp:285]     Train net output #0: loss = 0.192966 (* 1 = 0.192966 loss)
I0122 16:49:39.492686 58312 sgd_solver.cpp:106] Iteration 12000, lr = 0.004
I0122 16:49:40.914398 58312 solver.cpp:266] Iteration 12100 (70.3403 iter/s, 1.42166s/100 iter), loss = 0.256862
I0122 16:49:40.914429 58312 solver.cpp:285]     Train net output #0: loss = 0.256862 (* 1 = 0.256862 loss)
I0122 16:49:40.914435 58312 sgd_solver.cpp:106] Iteration 12100, lr = 0.00395
I0122 16:49:42.324142 58312 solver.cpp:266] Iteration 12200 (70.9394 iter/s, 1.40965s/100 iter), loss = 0.380403
I0122 16:49:42.324172 58312 solver.cpp:285]     Train net output #0: loss = 0.380403 (* 1 = 0.380403 loss)
I0122 16:49:42.324177 58312 sgd_solver.cpp:106] Iteration 12200, lr = 0.0039
I0122 16:49:43.788034 58312 solver.cpp:266] Iteration 12300 (68.3153 iter/s, 1.4638s/100 iter), loss = 0.264265
I0122 16:49:43.788065 58312 solver.cpp:285]     Train net output #0: loss = 0.264265 (* 1 = 0.264265 loss)
I0122 16:49:43.788072 58312 sgd_solver.cpp:106] Iteration 12300, lr = 0.00385
I0122 16:49:45.197943 58312 solver.cpp:266] Iteration 12400 (70.931 iter/s, 1.40982s/100 iter), loss = 0.248951
I0122 16:49:45.198042 58312 solver.cpp:285]     Train net output #0: loss = 0.248951 (* 1 = 0.248951 loss)
I0122 16:49:45.198050 58312 sgd_solver.cpp:106] Iteration 12400, lr = 0.0038
I0122 16:49:46.625808 58312 solver.cpp:266] Iteration 12500 (70.0423 iter/s, 1.42771s/100 iter), loss = 0.169047
I0122 16:49:46.625839 58312 solver.cpp:285]     Train net output #0: loss = 0.169047 (* 1 = 0.169047 loss)
I0122 16:49:46.625845 58312 sgd_solver.cpp:106] Iteration 12500, lr = 0.00375
I0122 16:49:48.075760 58312 solver.cpp:266] Iteration 12600 (68.9722 iter/s, 1.44986s/100 iter), loss = 0.228187
I0122 16:49:48.075791 58312 solver.cpp:285]     Train net output #0: loss = 0.228187 (* 1 = 0.228187 loss)
I0122 16:49:48.075798 58312 sgd_solver.cpp:106] Iteration 12600, lr = 0.0037
I0122 16:49:49.489380 58312 solver.cpp:266] Iteration 12700 (70.7448 iter/s, 1.41353s/100 iter), loss = 0.279003
I0122 16:49:49.489411 58312 solver.cpp:285]     Train net output #0: loss = 0.279003 (* 1 = 0.279003 loss)
I0122 16:49:49.489416 58312 sgd_solver.cpp:106] Iteration 12700, lr = 0.00365
I0122 16:49:50.901175 58312 solver.cpp:266] Iteration 12800 (70.8363 iter/s, 1.41171s/100 iter), loss = 0.227125
I0122 16:49:50.901206 58312 solver.cpp:285]     Train net output #0: loss = 0.227125 (* 1 = 0.227125 loss)
I0122 16:49:50.901212 58312 sgd_solver.cpp:106] Iteration 12800, lr = 0.0036
I0122 16:49:52.367550 58312 solver.cpp:266] Iteration 12900 (68.1996 iter/s, 1.46628s/100 iter), loss = 0.216257
I0122 16:49:52.367580 58312 solver.cpp:285]     Train net output #0: loss = 0.216257 (* 1 = 0.216257 loss)
I0122 16:49:52.367586 58312 sgd_solver.cpp:106] Iteration 12900, lr = 0.00355
I0122 16:49:53.779124 58312 solver.cpp:418] Iteration 13000, Testing net (#0)
I0122 16:49:54.179684 58312 solver.cpp:517]     Test net output #0: loss = 0.50153 (* 1 = 0.50153 loss)
I0122 16:49:54.179703 58312 solver.cpp:517]     Test net output #1: top-1 = 0.836778
I0122 16:49:54.179708 58312 solver.cpp:517]     Test net output #2: top-5 = 0.989889
I0122 16:49:54.188055 58312 solver.cpp:266] Iteration 13000 (54.9328 iter/s, 1.82041s/100 iter), loss = 0.227225
I0122 16:49:54.188073 58312 solver.cpp:285]     Train net output #0: loss = 0.227225 (* 1 = 0.227225 loss)
I0122 16:49:54.188079 58312 sgd_solver.cpp:106] Iteration 13000, lr = 0.0035
I0122 16:49:55.608582 58312 solver.cpp:266] Iteration 13100 (70.4003 iter/s, 1.42045s/100 iter), loss = 0.207814
I0122 16:49:55.608614 58312 solver.cpp:285]     Train net output #0: loss = 0.207814 (* 1 = 0.207814 loss)
I0122 16:49:55.608620 58312 sgd_solver.cpp:106] Iteration 13100, lr = 0.00345
I0122 16:49:57.067575 58312 solver.cpp:266] Iteration 13200 (68.5448 iter/s, 1.4589s/100 iter), loss = 0.314548
I0122 16:49:57.067605 58312 solver.cpp:285]     Train net output #0: loss = 0.314548 (* 1 = 0.314548 loss)
I0122 16:49:57.067610 58312 sgd_solver.cpp:106] Iteration 13200, lr = 0.0034
I0122 16:49:58.482954 58312 solver.cpp:266] Iteration 13300 (70.6568 iter/s, 1.41529s/100 iter), loss = 0.241811
I0122 16:49:58.482985 58312 solver.cpp:285]     Train net output #0: loss = 0.241811 (* 1 = 0.241811 loss)
I0122 16:49:58.482991 58312 sgd_solver.cpp:106] Iteration 13300, lr = 0.00335
I0122 16:49:59.923086 58312 solver.cpp:266] Iteration 13400 (69.4424 iter/s, 1.44004s/100 iter), loss = 0.200293
I0122 16:49:59.923117 58312 solver.cpp:285]     Train net output #0: loss = 0.200293 (* 1 = 0.200293 loss)
I0122 16:49:59.923162 58312 sgd_solver.cpp:106] Iteration 13400, lr = 0.0033
I0122 16:50:01.371773 58312 solver.cpp:266] Iteration 13500 (69.0345 iter/s, 1.44855s/100 iter), loss = 0.192317
I0122 16:50:01.371801 58312 solver.cpp:285]     Train net output #0: loss = 0.192317 (* 1 = 0.192317 loss)
I0122 16:50:01.371806 58312 sgd_solver.cpp:106] Iteration 13500, lr = 0.00325
I0122 16:50:02.784536 58312 solver.cpp:266] Iteration 13600 (70.7876 iter/s, 1.41268s/100 iter), loss = 0.224959
I0122 16:50:02.784567 58312 solver.cpp:285]     Train net output #0: loss = 0.224959 (* 1 = 0.224959 loss)
I0122 16:50:02.784574 58312 sgd_solver.cpp:106] Iteration 13600, lr = 0.0032
I0122 16:50:04.213099 58312 solver.cpp:266] Iteration 13700 (70.0048 iter/s, 1.42847s/100 iter), loss = 0.1756
I0122 16:50:04.213130 58312 solver.cpp:285]     Train net output #0: loss = 0.1756 (* 1 = 0.1756 loss)
I0122 16:50:04.213137 58312 sgd_solver.cpp:106] Iteration 13700, lr = 0.00315
I0122 16:50:05.667631 58312 solver.cpp:266] Iteration 13800 (68.755 iter/s, 1.45444s/100 iter), loss = 0.286327
I0122 16:50:05.667661 58312 solver.cpp:285]     Train net output #0: loss = 0.286328 (* 1 = 0.286328 loss)
I0122 16:50:05.667666 58312 sgd_solver.cpp:106] Iteration 13800, lr = 0.0031
I0122 16:50:07.083369 58312 solver.cpp:266] Iteration 13900 (70.6389 iter/s, 1.41565s/100 iter), loss = 0.1514
I0122 16:50:07.083401 58312 solver.cpp:285]     Train net output #0: loss = 0.1514 (* 1 = 0.1514 loss)
I0122 16:50:07.083406 58312 sgd_solver.cpp:106] Iteration 13900, lr = 0.00305
I0122 16:50:08.493189 58312 solver.cpp:418] Iteration 14000, Testing net (#0)
I0122 16:50:08.892923 58312 solver.cpp:517]     Test net output #0: loss = 0.484259 (* 1 = 0.484259 loss)
I0122 16:50:08.892942 58312 solver.cpp:517]     Test net output #1: top-1 = 0.842889
I0122 16:50:08.892946 58312 solver.cpp:517]     Test net output #2: top-5 = 0.99
I0122 16:50:08.903712 58312 solver.cpp:266] Iteration 14000 (54.9378 iter/s, 1.82024s/100 iter), loss = 0.243597
I0122 16:50:08.903731 58312 solver.cpp:285]     Train net output #0: loss = 0.243597 (* 1 = 0.243597 loss)
I0122 16:50:08.903739 58312 sgd_solver.cpp:106] Iteration 14000, lr = 0.003
I0122 16:50:10.351506 58312 solver.cpp:266] Iteration 14100 (69.0746 iter/s, 1.44771s/100 iter), loss = 0.164905
I0122 16:50:10.351537 58312 solver.cpp:285]     Train net output #0: loss = 0.164905 (* 1 = 0.164905 loss)
I0122 16:50:10.351543 58312 sgd_solver.cpp:106] Iteration 14100, lr = 0.00295
I0122 16:50:11.784123 58312 solver.cpp:266] Iteration 14200 (69.8067 iter/s, 1.43253s/100 iter), loss = 0.265896
I0122 16:50:11.784152 58312 solver.cpp:285]     Train net output #0: loss = 0.265896 (* 1 = 0.265896 loss)
I0122 16:50:11.784158 58312 sgd_solver.cpp:106] Iteration 14200, lr = 0.0029
I0122 16:50:13.209571 58312 solver.cpp:266] Iteration 14300 (70.1577 iter/s, 1.42536s/100 iter), loss = 0.236601
I0122 16:50:13.209602 58312 solver.cpp:285]     Train net output #0: loss = 0.236601 (* 1 = 0.236601 loss)
I0122 16:50:13.209609 58312 sgd_solver.cpp:106] Iteration 14300, lr = 0.00285
I0122 16:50:14.655130 58312 solver.cpp:266] Iteration 14400 (69.1817 iter/s, 1.44547s/100 iter), loss = 0.146458
I0122 16:50:14.655161 58312 solver.cpp:285]     Train net output #0: loss = 0.146458 (* 1 = 0.146458 loss)
I0122 16:50:14.655167 58312 sgd_solver.cpp:106] Iteration 14400, lr = 0.0028
I0122 16:50:16.064996 58312 solver.cpp:266] Iteration 14500 (70.9332 iter/s, 1.40978s/100 iter), loss = 0.208312
I0122 16:50:16.065099 58312 solver.cpp:285]     Train net output #0: loss = 0.208312 (* 1 = 0.208312 loss)
I0122 16:50:16.065115 58312 sgd_solver.cpp:106] Iteration 14500, lr = 0.00275
I0122 16:50:17.485805 58312 solver.cpp:266] Iteration 14600 (70.3903 iter/s, 1.42065s/100 iter), loss = 0.125515
I0122 16:50:17.485836 58312 solver.cpp:285]     Train net output #0: loss = 0.125515 (* 1 = 0.125515 loss)
I0122 16:50:17.485842 58312 sgd_solver.cpp:106] Iteration 14600, lr = 0.0027
I0122 16:50:18.926343 58312 solver.cpp:266] Iteration 14700 (69.4229 iter/s, 1.44045s/100 iter), loss = 0.140416
I0122 16:50:18.926375 58312 solver.cpp:285]     Train net output #0: loss = 0.140416 (* 1 = 0.140416 loss)
I0122 16:50:18.928592 58312 sgd_solver.cpp:106] Iteration 14700, lr = 0.00265
I0122 16:50:20.356103 58312 solver.cpp:266] Iteration 14800 (70.0548 iter/s, 1.42745s/100 iter), loss = 0.231286
I0122 16:50:20.356134 58312 solver.cpp:285]     Train net output #0: loss = 0.231286 (* 1 = 0.231286 loss)
I0122 16:50:20.356140 58312 sgd_solver.cpp:106] Iteration 14800, lr = 0.0026
I0122 16:50:21.764721 58312 solver.cpp:266] Iteration 14900 (70.996 iter/s, 1.40853s/100 iter), loss = 0.191844
I0122 16:50:21.764753 58312 solver.cpp:285]     Train net output #0: loss = 0.191844 (* 1 = 0.191844 loss)
I0122 16:50:21.764758 58312 sgd_solver.cpp:106] Iteration 14900, lr = 0.00255
I0122 16:50:23.178556 58312 solver.cpp:418] Iteration 15000, Testing net (#0)
I0122 16:50:23.613584 58312 solver.cpp:517]     Test net output #0: loss = 0.492079 (* 1 = 0.492079 loss)
I0122 16:50:23.613603 58312 solver.cpp:517]     Test net output #1: top-1 = 0.842666
I0122 16:50:23.613607 58312 solver.cpp:517]     Test net output #2: top-5 = 0.989556
I0122 16:50:23.621711 58312 solver.cpp:266] Iteration 15000 (53.8535 iter/s, 1.85689s/100 iter), loss = 0.264445
I0122 16:50:23.621729 58312 solver.cpp:285]     Train net output #0: loss = 0.264445 (* 1 = 0.264445 loss)
I0122 16:50:23.621735 58312 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0122 16:50:25.043656 58312 solver.cpp:266] Iteration 15100 (70.3301 iter/s, 1.42187s/100 iter), loss = 0.166516
I0122 16:50:25.043686 58312 solver.cpp:285]     Train net output #0: loss = 0.166516 (* 1 = 0.166516 loss)
I0122 16:50:25.043692 58312 sgd_solver.cpp:106] Iteration 15100, lr = 0.00245
I0122 16:50:26.454686 58312 solver.cpp:266] Iteration 15200 (70.8746 iter/s, 1.41094s/100 iter), loss = 0.336871
I0122 16:50:26.454717 58312 solver.cpp:285]     Train net output #0: loss = 0.336871 (* 1 = 0.336871 loss)
I0122 16:50:26.454723 58312 sgd_solver.cpp:106] Iteration 15200, lr = 0.0024
I0122 16:50:27.922442 58312 solver.cpp:266] Iteration 15300 (68.1354 iter/s, 1.46767s/100 iter), loss = 0.262672
I0122 16:50:27.922472 58312 solver.cpp:285]     Train net output #0: loss = 0.262672 (* 1 = 0.262672 loss)
I0122 16:50:27.922477 58312 sgd_solver.cpp:106] Iteration 15300, lr = 0.00235
I0122 16:50:29.342628 58312 solver.cpp:266] Iteration 15400 (70.4176 iter/s, 1.4201s/100 iter), loss = 0.232212
I0122 16:50:29.342659 58312 solver.cpp:285]     Train net output #0: loss = 0.232212 (* 1 = 0.232212 loss)
I0122 16:50:29.342665 58312 sgd_solver.cpp:106] Iteration 15400, lr = 0.0023
I0122 16:50:30.763036 58312 solver.cpp:266] Iteration 15500 (70.4068 iter/s, 1.42032s/100 iter), loss = 0.244157
I0122 16:50:30.763065 58312 solver.cpp:285]     Train net output #0: loss = 0.244157 (* 1 = 0.244157 loss)
I0122 16:50:30.763072 58312 sgd_solver.cpp:106] Iteration 15500, lr = 0.00225
I0122 16:50:32.217494 58312 solver.cpp:266] Iteration 15600 (68.7584 iter/s, 1.45437s/100 iter), loss = 0.189668
I0122 16:50:32.217525 58312 solver.cpp:285]     Train net output #0: loss = 0.189668 (* 1 = 0.189668 loss)
I0122 16:50:32.217530 58312 sgd_solver.cpp:106] Iteration 15600, lr = 0.0022
I0122 16:50:33.626005 58312 solver.cpp:266] Iteration 15700 (71.0014 iter/s, 1.40842s/100 iter), loss = 0.198833
I0122 16:50:33.626036 58312 solver.cpp:285]     Train net output #0: loss = 0.198833 (* 1 = 0.198833 loss)
I0122 16:50:33.626041 58312 sgd_solver.cpp:106] Iteration 15700, lr = 0.00215
I0122 16:50:35.058892 58312 solver.cpp:266] Iteration 15800 (69.7935 iter/s, 1.4328s/100 iter), loss = 0.196284
I0122 16:50:35.058925 58312 solver.cpp:285]     Train net output #0: loss = 0.196284 (* 1 = 0.196284 loss)
I0122 16:50:35.058969 58312 sgd_solver.cpp:106] Iteration 15800, lr = 0.0021
I0122 16:50:36.491087 58312 solver.cpp:266] Iteration 15900 (69.8295 iter/s, 1.43206s/100 iter), loss = 0.352349
I0122 16:50:36.491117 58312 solver.cpp:285]     Train net output #0: loss = 0.352349 (* 1 = 0.352349 loss)
I0122 16:50:36.491124 58312 sgd_solver.cpp:106] Iteration 15900, lr = 0.00205
I0122 16:50:37.904881 58312 solver.cpp:418] Iteration 16000, Testing net (#0)
I0122 16:50:38.302060 58312 solver.cpp:517]     Test net output #0: loss = 0.496241 (* 1 = 0.496241 loss)
I0122 16:50:38.302078 58312 solver.cpp:517]     Test net output #1: top-1 = 0.840555
I0122 16:50:38.302083 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991111
I0122 16:50:38.310192 58312 solver.cpp:266] Iteration 16000 (54.9751 iter/s, 1.819s/100 iter), loss = 0.178285
I0122 16:50:38.310210 58312 solver.cpp:285]     Train net output #0: loss = 0.178285 (* 1 = 0.178285 loss)
I0122 16:50:38.310217 58312 sgd_solver.cpp:106] Iteration 16000, lr = 0.002
I0122 16:50:39.747462 58312 solver.cpp:266] Iteration 16100 (69.5802 iter/s, 1.43719s/100 iter), loss = 0.329562
I0122 16:50:39.747493 58312 solver.cpp:285]     Train net output #0: loss = 0.329563 (* 1 = 0.329563 loss)
I0122 16:50:39.747539 58312 sgd_solver.cpp:106] Iteration 16100, lr = 0.00195
I0122 16:50:41.186126 58312 solver.cpp:266] Iteration 16200 (69.5154 iter/s, 1.43853s/100 iter), loss = 0.239703
I0122 16:50:41.186156 58312 solver.cpp:285]     Train net output #0: loss = 0.239703 (* 1 = 0.239703 loss)
I0122 16:50:41.186162 58312 sgd_solver.cpp:106] Iteration 16200, lr = 0.0019
I0122 16:50:42.605223 58312 solver.cpp:266] Iteration 16300 (70.4717 iter/s, 1.41901s/100 iter), loss = 0.243043
I0122 16:50:42.605254 58312 solver.cpp:285]     Train net output #0: loss = 0.243043 (* 1 = 0.243043 loss)
I0122 16:50:42.605260 58312 sgd_solver.cpp:106] Iteration 16300, lr = 0.00185
I0122 16:50:44.018102 58312 solver.cpp:266] Iteration 16400 (70.7819 iter/s, 1.41279s/100 iter), loss = 0.169503
I0122 16:50:44.018132 58312 solver.cpp:285]     Train net output #0: loss = 0.169503 (* 1 = 0.169503 loss)
I0122 16:50:44.018138 58312 sgd_solver.cpp:106] Iteration 16400, lr = 0.0018
I0122 16:50:45.446853 58312 solver.cpp:266] Iteration 16500 (69.9956 iter/s, 1.42866s/100 iter), loss = 0.221716
I0122 16:50:45.446882 58312 solver.cpp:285]     Train net output #0: loss = 0.221716 (* 1 = 0.221716 loss)
I0122 16:50:45.449105 58312 sgd_solver.cpp:106] Iteration 16500, lr = 0.00175
I0122 16:50:46.879923 58312 solver.cpp:266] Iteration 16600 (69.893 iter/s, 1.43076s/100 iter), loss = 0.241011
I0122 16:50:46.880035 58312 solver.cpp:285]     Train net output #0: loss = 0.241011 (* 1 = 0.241011 loss)
I0122 16:50:46.880043 58312 sgd_solver.cpp:106] Iteration 16600, lr = 0.0017
I0122 16:50:48.299104 58312 solver.cpp:266] Iteration 16700 (70.4715 iter/s, 1.41901s/100 iter), loss = 0.221627
I0122 16:50:48.299134 58312 solver.cpp:285]     Train net output #0: loss = 0.221627 (* 1 = 0.221627 loss)
I0122 16:50:48.299141 58312 sgd_solver.cpp:106] Iteration 16700, lr = 0.00165
I0122 16:50:49.715683 58312 solver.cpp:266] Iteration 16800 (70.5971 iter/s, 1.41649s/100 iter), loss = 0.215592
I0122 16:50:49.715713 58312 solver.cpp:285]     Train net output #0: loss = 0.215592 (* 1 = 0.215592 loss)
I0122 16:50:49.715720 58312 sgd_solver.cpp:106] Iteration 16800, lr = 0.0016
I0122 16:50:51.085132 58312 solver.cpp:266] Iteration 16900 (73.0267 iter/s, 1.36936s/100 iter), loss = 0.373433
I0122 16:50:51.085161 58312 solver.cpp:285]     Train net output #0: loss = 0.373433 (* 1 = 0.373433 loss)
I0122 16:50:51.086516 58312 sgd_solver.cpp:106] Iteration 16900, lr = 0.00155
I0122 16:50:52.116015 58312 solver.cpp:418] Iteration 17000, Testing net (#0)
I0122 16:50:52.563536 58312 solver.cpp:517]     Test net output #0: loss = 0.501903 (* 1 = 0.501903 loss)
I0122 16:50:52.563555 58312 solver.cpp:517]     Test net output #1: top-1 = 0.839556
I0122 16:50:52.563558 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991333
I0122 16:50:52.572899 58312 solver.cpp:266] Iteration 17000 (67.28 iter/s, 1.48633s/100 iter), loss = 0.22854
I0122 16:50:52.572929 58312 solver.cpp:285]     Train net output #0: loss = 0.22854 (* 1 = 0.22854 loss)
I0122 16:50:52.572937 58312 sgd_solver.cpp:106] Iteration 17000, lr = 0.0015
I0122 16:50:54.038501 58312 solver.cpp:266] Iteration 17100 (68.2353 iter/s, 1.46552s/100 iter), loss = 0.245045
I0122 16:50:54.038532 58312 solver.cpp:285]     Train net output #0: loss = 0.245045 (* 1 = 0.245045 loss)
I0122 16:50:54.038537 58312 sgd_solver.cpp:106] Iteration 17100, lr = 0.00145
I0122 16:50:55.460747 58312 solver.cpp:266] Iteration 17200 (70.3157 iter/s, 1.42216s/100 iter), loss = 0.254387
I0122 16:50:55.460778 58312 solver.cpp:285]     Train net output #0: loss = 0.254387 (* 1 = 0.254387 loss)
I0122 16:50:55.460784 58312 sgd_solver.cpp:106] Iteration 17200, lr = 0.0014
I0122 16:50:56.876515 58312 solver.cpp:266] Iteration 17300 (70.6377 iter/s, 1.41567s/100 iter), loss = 0.285815
I0122 16:50:56.876545 58312 solver.cpp:285]     Train net output #0: loss = 0.285815 (* 1 = 0.285815 loss)
I0122 16:50:56.876551 58312 sgd_solver.cpp:106] Iteration 17300, lr = 0.00135
I0122 16:50:58.333384 58312 solver.cpp:266] Iteration 17400 (68.6449 iter/s, 1.45677s/100 iter), loss = 0.264549
I0122 16:50:58.333412 58312 solver.cpp:285]     Train net output #0: loss = 0.264549 (* 1 = 0.264549 loss)
I0122 16:50:58.333418 58312 sgd_solver.cpp:106] Iteration 17400, lr = 0.0013
I0122 16:50:59.755903 58312 solver.cpp:266] Iteration 17500 (70.3022 iter/s, 1.42243s/100 iter), loss = 0.203918
I0122 16:50:59.755944 58312 solver.cpp:285]     Train net output #0: loss = 0.203918 (* 1 = 0.203918 loss)
I0122 16:50:59.755950 58312 sgd_solver.cpp:106] Iteration 17500, lr = 0.00125
I0122 16:51:01.178480 58312 solver.cpp:266] Iteration 17600 (70.2999 iter/s, 1.42248s/100 iter), loss = 0.253467
I0122 16:51:01.178514 58312 solver.cpp:285]     Train net output #0: loss = 0.253467 (* 1 = 0.253467 loss)
I0122 16:51:01.178520 58312 sgd_solver.cpp:106] Iteration 17600, lr = 0.0012
I0122 16:51:02.623209 58312 solver.cpp:266] Iteration 17700 (69.2216 iter/s, 1.44464s/100 iter), loss = 0.255005
I0122 16:51:02.623239 58312 solver.cpp:285]     Train net output #0: loss = 0.255005 (* 1 = 0.255005 loss)
I0122 16:51:02.623245 58312 sgd_solver.cpp:106] Iteration 17700, lr = 0.00115
I0122 16:51:04.041278 58312 solver.cpp:266] Iteration 17800 (70.5229 iter/s, 1.41798s/100 iter), loss = 0.247999
I0122 16:51:04.041309 58312 solver.cpp:285]     Train net output #0: loss = 0.247999 (* 1 = 0.247999 loss)
I0122 16:51:04.041314 58312 sgd_solver.cpp:106] Iteration 17800, lr = 0.0011
I0122 16:51:05.470913 58312 solver.cpp:266] Iteration 17900 (69.9523 iter/s, 1.42955s/100 iter), loss = 0.167927
I0122 16:51:05.470944 58312 solver.cpp:285]     Train net output #0: loss = 0.167927 (* 1 = 0.167927 loss)
I0122 16:51:05.473153 58312 sgd_solver.cpp:106] Iteration 17900, lr = 0.00105
I0122 16:51:06.905187 58312 solver.cpp:418] Iteration 18000, Testing net (#0)
I0122 16:51:07.306358 58312 solver.cpp:517]     Test net output #0: loss = 0.469573 (* 1 = 0.469573 loss)
I0122 16:51:07.306376 58312 solver.cpp:517]     Test net output #1: top-1 = 0.845111
I0122 16:51:07.306381 58312 solver.cpp:517]     Test net output #2: top-5 = 0.990778
I0122 16:51:07.314445 58312 solver.cpp:266] Iteration 18000 (54.3117 iter/s, 1.84122s/100 iter), loss = 0.153297
I0122 16:51:07.314465 58312 solver.cpp:285]     Train net output #0: loss = 0.153297 (* 1 = 0.153297 loss)
I0122 16:51:07.314471 58312 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0122 16:51:08.730263 58312 solver.cpp:266] Iteration 18100 (70.6345 iter/s, 1.41574s/100 iter), loss = 0.172776
I0122 16:51:08.730291 58312 solver.cpp:285]     Train net output #0: loss = 0.172776 (* 1 = 0.172776 loss)
I0122 16:51:08.730298 58312 sgd_solver.cpp:106] Iteration 18100, lr = 0.00095
I0122 16:51:10.167186 58312 solver.cpp:266] Iteration 18200 (69.5975 iter/s, 1.43683s/100 iter), loss = 0.148929
I0122 16:51:10.167218 58312 solver.cpp:285]     Train net output #0: loss = 0.148929 (* 1 = 0.148929 loss)
I0122 16:51:10.167260 58312 sgd_solver.cpp:106] Iteration 18200, lr = 0.0009
I0122 16:51:11.614673 58312 solver.cpp:266] Iteration 18300 (69.0916 iter/s, 1.44735s/100 iter), loss = 0.288
I0122 16:51:11.614704 58312 solver.cpp:285]     Train net output #0: loss = 0.288 (* 1 = 0.288 loss)
I0122 16:51:11.614711 58312 sgd_solver.cpp:106] Iteration 18300, lr = 0.00085
I0122 16:51:13.030076 58312 solver.cpp:266] Iteration 18400 (70.6557 iter/s, 1.41531s/100 iter), loss = 0.217355
I0122 16:51:13.030105 58312 solver.cpp:285]     Train net output #0: loss = 0.217355 (* 1 = 0.217355 loss)
I0122 16:51:13.030112 58312 sgd_solver.cpp:106] Iteration 18400, lr = 0.0008
I0122 16:51:14.458530 58312 solver.cpp:266] Iteration 18500 (70.0101 iter/s, 1.42837s/100 iter), loss = 0.259655
I0122 16:51:14.458560 58312 solver.cpp:285]     Train net output #0: loss = 0.259655 (* 1 = 0.259655 loss)
I0122 16:51:14.460778 58312 sgd_solver.cpp:106] Iteration 18500, lr = 0.00075
I0122 16:51:15.900770 58312 solver.cpp:266] Iteration 18600 (69.4476 iter/s, 1.43993s/100 iter), loss = 0.170009
I0122 16:51:15.900800 58312 solver.cpp:285]     Train net output #0: loss = 0.170009 (* 1 = 0.170009 loss)
I0122 16:51:15.900806 58312 sgd_solver.cpp:106] Iteration 18600, lr = 0.0007
I0122 16:51:17.344951 58312 solver.cpp:266] Iteration 18700 (69.2477 iter/s, 1.44409s/100 iter), loss = 0.194333
I0122 16:51:17.345054 58312 solver.cpp:285]     Train net output #0: loss = 0.194333 (* 1 = 0.194333 loss)
I0122 16:51:17.347195 58312 sgd_solver.cpp:106] Iteration 18700, lr = 0.00065
I0122 16:51:18.771064 58312 solver.cpp:266] Iteration 18800 (70.2341 iter/s, 1.42381s/100 iter), loss = 0.171815
I0122 16:51:18.771095 58312 solver.cpp:285]     Train net output #0: loss = 0.171815 (* 1 = 0.171815 loss)
I0122 16:51:18.771100 58312 sgd_solver.cpp:106] Iteration 18800, lr = 0.0006
I0122 16:51:20.182659 58312 solver.cpp:266] Iteration 18900 (70.8462 iter/s, 1.41151s/100 iter), loss = 0.219778
I0122 16:51:20.182690 58312 solver.cpp:285]     Train net output #0: loss = 0.219778 (* 1 = 0.219778 loss)
I0122 16:51:20.182695 58312 sgd_solver.cpp:106] Iteration 18900, lr = 0.00055
I0122 16:51:21.599087 58312 solver.cpp:418] Iteration 19000, Testing net (#0)
I0122 16:51:22.033977 58312 solver.cpp:517]     Test net output #0: loss = 0.464872 (* 1 = 0.464872 loss)
I0122 16:51:22.033991 58312 solver.cpp:517]     Test net output #1: top-1 = 0.848
I0122 16:51:22.033995 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991556
I0122 16:51:22.042074 58312 solver.cpp:266] Iteration 19000 (53.7833 iter/s, 1.85931s/100 iter), loss = 0.321937
I0122 16:51:22.042093 58312 solver.cpp:285]     Train net output #0: loss = 0.321937 (* 1 = 0.321937 loss)
I0122 16:51:22.042100 58312 sgd_solver.cpp:106] Iteration 19000, lr = 0.0005
I0122 16:51:23.460806 58312 solver.cpp:266] Iteration 19100 (70.4894 iter/s, 1.41865s/100 iter), loss = 0.242233
I0122 16:51:23.460836 58312 solver.cpp:285]     Train net output #0: loss = 0.242233 (* 1 = 0.242233 loss)
I0122 16:51:23.460844 58312 sgd_solver.cpp:106] Iteration 19100, lr = 0.00045
I0122 16:51:24.904261 58312 solver.cpp:266] Iteration 19200 (69.2826 iter/s, 1.44336s/100 iter), loss = 0.254909
I0122 16:51:24.904292 58312 solver.cpp:285]     Train net output #0: loss = 0.254909 (* 1 = 0.254909 loss)
I0122 16:51:24.906504 58312 sgd_solver.cpp:106] Iteration 19200, lr = 0.0004
I0122 16:51:26.339450 58312 solver.cpp:266] Iteration 19300 (69.7891 iter/s, 1.43289s/100 iter), loss = 0.131365
I0122 16:51:26.339490 58312 solver.cpp:285]     Train net output #0: loss = 0.131365 (* 1 = 0.131365 loss)
I0122 16:51:26.339498 58312 sgd_solver.cpp:106] Iteration 19300, lr = 0.00035
I0122 16:51:27.750479 58312 solver.cpp:266] Iteration 19400 (70.8752 iter/s, 1.41093s/100 iter), loss = 0.280087
I0122 16:51:27.750511 58312 solver.cpp:285]     Train net output #0: loss = 0.280087 (* 1 = 0.280087 loss)
I0122 16:51:27.750517 58312 sgd_solver.cpp:106] Iteration 19400, lr = 0.0003
I0122 16:51:29.158006 58312 solver.cpp:266] Iteration 19500 (71.0513 iter/s, 1.40743s/100 iter), loss = 0.13227
I0122 16:51:29.158037 58312 solver.cpp:285]     Train net output #0: loss = 0.13227 (* 1 = 0.13227 loss)
I0122 16:51:29.158043 58312 sgd_solver.cpp:106] Iteration 19500, lr = 0.00025
I0122 16:51:30.614485 58312 solver.cpp:266] Iteration 19600 (68.6631 iter/s, 1.45639s/100 iter), loss = 0.320311
I0122 16:51:30.614518 58312 solver.cpp:285]     Train net output #0: loss = 0.320312 (* 1 = 0.320312 loss)
I0122 16:51:30.614526 58312 sgd_solver.cpp:106] Iteration 19600, lr = 0.0002
I0122 16:51:32.036463 58312 solver.cpp:266] Iteration 19700 (70.3291 iter/s, 1.42189s/100 iter), loss = 0.260796
I0122 16:51:32.036489 58312 solver.cpp:285]     Train net output #0: loss = 0.260796 (* 1 = 0.260796 loss)
I0122 16:51:32.036494 58312 sgd_solver.cpp:106] Iteration 19700, lr = 0.00015
I0122 16:51:33.440219 58312 solver.cpp:266] Iteration 19800 (71.2416 iter/s, 1.40367s/100 iter), loss = 0.231194
I0122 16:51:33.440248 58312 solver.cpp:285]     Train net output #0: loss = 0.231194 (* 1 = 0.231194 loss)
I0122 16:51:33.440254 58312 sgd_solver.cpp:106] Iteration 19800, lr = 9.99999e-05
I0122 16:51:34.899338 58312 solver.cpp:266] Iteration 19900 (68.5388 iter/s, 1.45903s/100 iter), loss = 0.16002
I0122 16:51:34.899368 58312 solver.cpp:285]     Train net output #0: loss = 0.16002 (* 1 = 0.16002 loss)
I0122 16:51:34.899374 58312 sgd_solver.cpp:106] Iteration 19900, lr = 5e-05
I0122 16:51:36.304375 58312 solver.cpp:929] Snapshotting to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/_iter_20000.caffemodel
I0122 16:51:36.371570 58312 sgd_solver.cpp:273] Snapshotting solver state to binary proto file cifar10/deephi/miniVggNet/pruning/regular_rate_0.7/snapshots/_iter_20000.solverstate
I0122 16:51:36.383988 58312 solver.cpp:378] Iteration 20000, loss = 0.0559362
I0122 16:51:36.384011 58312 solver.cpp:418] Iteration 20000, Testing net (#0)
I0122 16:51:36.784060 58312 solver.cpp:517]     Test net output #0: loss = 0.465885 (* 1 = 0.465885 loss)
I0122 16:51:36.784076 58312 solver.cpp:517]     Test net output #1: top-1 = 0.848111
I0122 16:51:36.784081 58312 solver.cpp:517]     Test net output #2: top-5 = 0.991
I0122 16:51:36.784085 58312 solver.cpp:386] Optimization Done (68.4128 iter/s).
I0122 16:51:36.784090 58312 caffe_interface.cpp:530] Optimization Done.

## etc


## last step: get the final output model
## note that it does not work if you used the "final.prototxt" as wrongly described by transform help
#
${PRUNE_ROOT}/deephi_compress transform -model ${WORK_DIR}/train_val.prototxt -weights ${WORK_DIR}/regular_rate_0.7/snapshots/_iter_20000.caffemodel 2>&1 | tee ${WORK_DIR}/rpt/logfile_transform_miniVggNet.txt
I0122 16:51:37.597736 58430 gpu_memory.cpp:53] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0122 16:51:37.598410 58430 gpu_memory.cpp:55] Total memory: 25620447232, Free: 22914793472, dev_info[0]: total=25620447232 free=22914793472
I0122 16:51:37.598417 58430 caffe_interface.cpp:66] Use GPU with device ID 0
I0122 16:51:37.598668 58430 caffe_interface.cpp:70] GPU device name: Quadro P6000
I0122 16:51:38.320811 58430 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:51:38.321008 58430 net.cpp:52] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "scale1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "scale2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "scale3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "scale4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "scale5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:51:38.321110 58430 layer_factory.hpp:77] Creating layer data
I0122 16:51:38.321182 58430 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:51:38.322376 58430 net.cpp:94] Creating Layer data
I0122 16:51:38.322388 58430 net.cpp:409] data -> data
I0122 16:51:38.322398 58430 net.cpp:409] data -> label
I0122 16:51:38.323597 58463 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:51:38.323638 58463 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:51:38.323722 58430 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:51:38.323799 58430 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:51:38.327241 58430 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:51:38.327280 58430 net.cpp:144] Setting up data
I0122 16:51:38.327288 58430 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:51:38.327292 58430 net.cpp:151] Top shape: 50 (50)
I0122 16:51:38.327294 58430 net.cpp:159] Memory required for data: 614600
I0122 16:51:38.327297 58430 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:51:38.327307 58430 net.cpp:94] Creating Layer label_data_1_split
I0122 16:51:38.327311 58430 net.cpp:435] label_data_1_split <- label
I0122 16:51:38.327330 58430 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:51:38.327342 58430 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:51:38.327363 58430 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:51:38.327448 58430 net.cpp:144] Setting up label_data_1_split
I0122 16:51:38.327455 58430 net.cpp:151] Top shape: 50 (50)
I0122 16:51:38.327469 58430 net.cpp:151] Top shape: 50 (50)
I0122 16:51:38.327473 58430 net.cpp:151] Top shape: 50 (50)
I0122 16:51:38.327476 58430 net.cpp:159] Memory required for data: 615200
I0122 16:51:38.327478 58430 layer_factory.hpp:77] Creating layer conv1
I0122 16:51:38.327488 58430 net.cpp:94] Creating Layer conv1
I0122 16:51:38.327493 58430 net.cpp:435] conv1 <- data
I0122 16:51:38.327502 58430 net.cpp:409] conv1 -> conv1
I0122 16:51:38.328418 58430 net.cpp:144] Setting up conv1
I0122 16:51:38.328428 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.328433 58430 net.cpp:159] Memory required for data: 7168800
I0122 16:51:38.328449 58430 layer_factory.hpp:77] Creating layer bn1
I0122 16:51:38.328460 58430 net.cpp:94] Creating Layer bn1
I0122 16:51:38.328464 58430 net.cpp:435] bn1 <- conv1
I0122 16:51:38.328470 58430 net.cpp:409] bn1 -> scale1
I0122 16:51:38.329062 58430 net.cpp:144] Setting up bn1
I0122 16:51:38.329069 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.329072 58430 net.cpp:159] Memory required for data: 13722400
I0122 16:51:38.329084 58430 layer_factory.hpp:77] Creating layer relu1
I0122 16:51:38.329092 58430 net.cpp:94] Creating Layer relu1
I0122 16:51:38.329094 58430 net.cpp:435] relu1 <- scale1
I0122 16:51:38.329100 58430 net.cpp:409] relu1 -> relu1
I0122 16:51:38.329119 58430 net.cpp:144] Setting up relu1
I0122 16:51:38.329125 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.329128 58430 net.cpp:159] Memory required for data: 20276000
I0122 16:51:38.329130 58430 layer_factory.hpp:77] Creating layer conv2
I0122 16:51:38.329141 58430 net.cpp:94] Creating Layer conv2
I0122 16:51:38.329145 58430 net.cpp:435] conv2 <- relu1
I0122 16:51:38.329150 58430 net.cpp:409] conv2 -> conv2
I0122 16:51:38.330672 58430 net.cpp:144] Setting up conv2
I0122 16:51:38.330683 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.330685 58430 net.cpp:159] Memory required for data: 26829600
I0122 16:51:38.330693 58430 layer_factory.hpp:77] Creating layer bn2
I0122 16:51:38.330703 58430 net.cpp:94] Creating Layer bn2
I0122 16:51:38.330706 58430 net.cpp:435] bn2 <- conv2
I0122 16:51:38.330713 58430 net.cpp:409] bn2 -> scale2
I0122 16:51:38.331483 58430 net.cpp:144] Setting up bn2
I0122 16:51:38.331490 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.331493 58430 net.cpp:159] Memory required for data: 33383200
I0122 16:51:38.331501 58430 layer_factory.hpp:77] Creating layer relu2
I0122 16:51:38.331507 58430 net.cpp:94] Creating Layer relu2
I0122 16:51:38.331511 58430 net.cpp:435] relu2 <- scale2
I0122 16:51:38.331516 58430 net.cpp:409] relu2 -> relu2
I0122 16:51:38.331560 58430 net.cpp:144] Setting up relu2
I0122 16:51:38.331567 58430 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:38.331569 58430 net.cpp:159] Memory required for data: 39936800
I0122 16:51:38.331573 58430 layer_factory.hpp:77] Creating layer pool1
I0122 16:51:38.331579 58430 net.cpp:94] Creating Layer pool1
I0122 16:51:38.331583 58430 net.cpp:435] pool1 <- relu2
I0122 16:51:38.331588 58430 net.cpp:409] pool1 -> pool1
I0122 16:51:38.331626 58430 net.cpp:144] Setting up pool1
I0122 16:51:38.331631 58430 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:51:38.331634 58430 net.cpp:159] Memory required for data: 41575200
I0122 16:51:38.331637 58430 layer_factory.hpp:77] Creating layer drop1
I0122 16:51:38.331643 58430 net.cpp:94] Creating Layer drop1
I0122 16:51:38.331648 58430 net.cpp:435] drop1 <- pool1
I0122 16:51:38.331653 58430 net.cpp:409] drop1 -> drop1
I0122 16:51:38.331717 58430 net.cpp:144] Setting up drop1
I0122 16:51:38.331724 58430 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:51:38.331727 58430 net.cpp:159] Memory required for data: 43213600
I0122 16:51:38.331729 58430 layer_factory.hpp:77] Creating layer conv3
I0122 16:51:38.331739 58430 net.cpp:94] Creating Layer conv3
I0122 16:51:38.331743 58430 net.cpp:435] conv3 <- drop1
I0122 16:51:38.331750 58430 net.cpp:409] conv3 -> conv3
I0122 16:51:38.332739 58430 net.cpp:144] Setting up conv3
I0122 16:51:38.332762 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.332765 58430 net.cpp:159] Memory required for data: 46490400
I0122 16:51:38.332772 58430 layer_factory.hpp:77] Creating layer bn3
I0122 16:51:38.332782 58430 net.cpp:94] Creating Layer bn3
I0122 16:51:38.332787 58430 net.cpp:435] bn3 <- conv3
I0122 16:51:38.332794 58430 net.cpp:409] bn3 -> scale3
I0122 16:51:38.333444 58430 net.cpp:144] Setting up bn3
I0122 16:51:38.333451 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.333454 58430 net.cpp:159] Memory required for data: 49767200
I0122 16:51:38.333467 58430 layer_factory.hpp:77] Creating layer relu3
I0122 16:51:38.333472 58430 net.cpp:94] Creating Layer relu3
I0122 16:51:38.333475 58430 net.cpp:435] relu3 <- scale3
I0122 16:51:38.333482 58430 net.cpp:409] relu3 -> relu3
I0122 16:51:38.333503 58430 net.cpp:144] Setting up relu3
I0122 16:51:38.333508 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.333511 58430 net.cpp:159] Memory required for data: 53044000
I0122 16:51:38.333513 58430 layer_factory.hpp:77] Creating layer conv4
I0122 16:51:38.333523 58430 net.cpp:94] Creating Layer conv4
I0122 16:51:38.333528 58430 net.cpp:435] conv4 <- relu3
I0122 16:51:38.333534 58430 net.cpp:409] conv4 -> conv4
I0122 16:51:38.334024 58430 net.cpp:144] Setting up conv4
I0122 16:51:38.334033 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.334036 58430 net.cpp:159] Memory required for data: 56320800
I0122 16:51:38.334041 58430 layer_factory.hpp:77] Creating layer bn4
I0122 16:51:38.334049 58430 net.cpp:94] Creating Layer bn4
I0122 16:51:38.334053 58430 net.cpp:435] bn4 <- conv4
I0122 16:51:38.334059 58430 net.cpp:409] bn4 -> scale4
I0122 16:51:38.334743 58430 net.cpp:144] Setting up bn4
I0122 16:51:38.334753 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.334755 58430 net.cpp:159] Memory required for data: 59597600
I0122 16:51:38.334764 58430 layer_factory.hpp:77] Creating layer relu4
I0122 16:51:38.334769 58430 net.cpp:94] Creating Layer relu4
I0122 16:51:38.334771 58430 net.cpp:435] relu4 <- scale4
I0122 16:51:38.334779 58430 net.cpp:409] relu4 -> relu4
I0122 16:51:38.334798 58430 net.cpp:144] Setting up relu4
I0122 16:51:38.334806 58430 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:38.334810 58430 net.cpp:159] Memory required for data: 62874400
I0122 16:51:38.334812 58430 layer_factory.hpp:77] Creating layer pool2
I0122 16:51:38.334820 58430 net.cpp:94] Creating Layer pool2
I0122 16:51:38.334823 58430 net.cpp:435] pool2 <- relu4
I0122 16:51:38.334830 58430 net.cpp:409] pool2 -> pool2
I0122 16:51:38.334910 58430 net.cpp:144] Setting up pool2
I0122 16:51:38.334915 58430 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:51:38.334918 58430 net.cpp:159] Memory required for data: 63693600
I0122 16:51:38.334920 58430 layer_factory.hpp:77] Creating layer drop2
I0122 16:51:38.334928 58430 net.cpp:94] Creating Layer drop2
I0122 16:51:38.334933 58430 net.cpp:435] drop2 <- pool2
I0122 16:51:38.334936 58430 net.cpp:409] drop2 -> drop2
I0122 16:51:38.334965 58430 net.cpp:144] Setting up drop2
I0122 16:51:38.334971 58430 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:51:38.334975 58430 net.cpp:159] Memory required for data: 64512800
I0122 16:51:38.334976 58430 layer_factory.hpp:77] Creating layer fc1
I0122 16:51:38.334982 58430 net.cpp:94] Creating Layer fc1
I0122 16:51:38.334985 58430 net.cpp:435] fc1 <- drop2
I0122 16:51:38.334991 58430 net.cpp:409] fc1 -> fc1
I0122 16:51:38.349169 58430 net.cpp:144] Setting up fc1
I0122 16:51:38.349187 58430 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:38.349190 58430 net.cpp:159] Memory required for data: 64615200
I0122 16:51:38.349196 58430 layer_factory.hpp:77] Creating layer bn5
I0122 16:51:38.349206 58430 net.cpp:94] Creating Layer bn5
I0122 16:51:38.349210 58430 net.cpp:435] bn5 <- fc1
I0122 16:51:38.349216 58430 net.cpp:409] bn5 -> scale5
I0122 16:51:38.349782 58430 net.cpp:144] Setting up bn5
I0122 16:51:38.349790 58430 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:38.349807 58430 net.cpp:159] Memory required for data: 64717600
I0122 16:51:38.349822 58430 layer_factory.hpp:77] Creating layer relu5
I0122 16:51:38.349828 58430 net.cpp:94] Creating Layer relu5
I0122 16:51:38.349831 58430 net.cpp:435] relu5 <- scale5
I0122 16:51:38.349838 58430 net.cpp:409] relu5 -> relu5
I0122 16:51:38.349858 58430 net.cpp:144] Setting up relu5
I0122 16:51:38.349862 58430 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:38.349864 58430 net.cpp:159] Memory required for data: 64820000
I0122 16:51:38.349867 58430 layer_factory.hpp:77] Creating layer drop3
I0122 16:51:38.349875 58430 net.cpp:94] Creating Layer drop3
I0122 16:51:38.349879 58430 net.cpp:435] drop3 <- relu5
I0122 16:51:38.349884 58430 net.cpp:409] drop3 -> drop3
I0122 16:51:38.349925 58430 net.cpp:144] Setting up drop3
I0122 16:51:38.349932 58430 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:38.349934 58430 net.cpp:159] Memory required for data: 64922400
I0122 16:51:38.349937 58430 layer_factory.hpp:77] Creating layer fc2
I0122 16:51:38.349944 58430 net.cpp:94] Creating Layer fc2
I0122 16:51:38.349947 58430 net.cpp:435] fc2 <- drop3
I0122 16:51:38.349953 58430 net.cpp:409] fc2 -> fc2
I0122 16:51:38.350087 58430 net.cpp:144] Setting up fc2
I0122 16:51:38.350092 58430 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:38.350096 58430 net.cpp:159] Memory required for data: 64924400
I0122 16:51:38.350100 58430 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:51:38.350106 58430 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:51:38.350109 58430 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:51:38.350116 58430 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:51:38.350124 58430 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:51:38.350129 58430 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:51:38.350173 58430 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:51:38.350181 58430 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:38.350185 58430 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:38.350188 58430 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:38.350191 58430 net.cpp:159] Memory required for data: 64930400
I0122 16:51:38.350196 58430 layer_factory.hpp:77] Creating layer loss
I0122 16:51:38.350203 58430 net.cpp:94] Creating Layer loss
I0122 16:51:38.350208 58430 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:51:38.350214 58430 net.cpp:435] loss <- label_data_1_split_0
I0122 16:51:38.350227 58430 net.cpp:409] loss -> loss
I0122 16:51:38.350239 58430 layer_factory.hpp:77] Creating layer loss
I0122 16:51:38.350312 58430 net.cpp:144] Setting up loss
I0122 16:51:38.350317 58430 net.cpp:151] Top shape: (1)
I0122 16:51:38.350320 58430 net.cpp:154]     with loss weight 1
I0122 16:51:38.350349 58430 net.cpp:159] Memory required for data: 64930404
I0122 16:51:38.350353 58430 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:51:38.350358 58430 net.cpp:94] Creating Layer accuracy-top1
I0122 16:51:38.350363 58430 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:51:38.350365 58430 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:51:38.350370 58430 net.cpp:409] accuracy-top1 -> top-1
I0122 16:51:38.350378 58430 net.cpp:144] Setting up accuracy-top1
I0122 16:51:38.350381 58430 net.cpp:151] Top shape: (1)
I0122 16:51:38.350384 58430 net.cpp:159] Memory required for data: 64930408
I0122 16:51:38.350387 58430 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:51:38.350392 58430 net.cpp:94] Creating Layer accuracy-top5
I0122 16:51:38.350396 58430 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:51:38.350399 58430 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:51:38.350404 58430 net.cpp:409] accuracy-top5 -> top-5
I0122 16:51:38.350410 58430 net.cpp:144] Setting up accuracy-top5
I0122 16:51:38.350414 58430 net.cpp:151] Top shape: (1)
I0122 16:51:38.350416 58430 net.cpp:159] Memory required for data: 64930412
I0122 16:51:38.350419 58430 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:51:38.350425 58430 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:51:38.350440 58430 net.cpp:220] loss needs backward computation.
I0122 16:51:38.350443 58430 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:51:38.350446 58430 net.cpp:220] fc2 needs backward computation.
I0122 16:51:38.350450 58430 net.cpp:220] drop3 needs backward computation.
I0122 16:51:38.350453 58430 net.cpp:220] relu5 needs backward computation.
I0122 16:51:38.350456 58430 net.cpp:220] bn5 needs backward computation.
I0122 16:51:38.350461 58430 net.cpp:220] fc1 needs backward computation.
I0122 16:51:38.350463 58430 net.cpp:220] drop2 needs backward computation.
I0122 16:51:38.350466 58430 net.cpp:220] pool2 needs backward computation.
I0122 16:51:38.350469 58430 net.cpp:220] relu4 needs backward computation.
I0122 16:51:38.350472 58430 net.cpp:220] bn4 needs backward computation.
I0122 16:51:38.350476 58430 net.cpp:220] conv4 needs backward computation.
I0122 16:51:38.350479 58430 net.cpp:220] relu3 needs backward computation.
I0122 16:51:38.350482 58430 net.cpp:220] bn3 needs backward computation.
I0122 16:51:38.350486 58430 net.cpp:220] conv3 needs backward computation.
I0122 16:51:38.350488 58430 net.cpp:220] drop1 needs backward computation.
I0122 16:51:38.350492 58430 net.cpp:220] pool1 needs backward computation.
I0122 16:51:38.350494 58430 net.cpp:220] relu2 needs backward computation.
I0122 16:51:38.350497 58430 net.cpp:220] bn2 needs backward computation.
I0122 16:51:38.350500 58430 net.cpp:220] conv2 needs backward computation.
I0122 16:51:38.350504 58430 net.cpp:220] relu1 needs backward computation.
I0122 16:51:38.350507 58430 net.cpp:220] bn1 needs backward computation.
I0122 16:51:38.350509 58430 net.cpp:220] conv1 needs backward computation.
I0122 16:51:38.350513 58430 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:51:38.350517 58430 net.cpp:222] data does not need backward computation.
I0122 16:51:38.350520 58430 net.cpp:264] This network produces output loss
I0122 16:51:38.350523 58430 net.cpp:264] This network produces output top-1
I0122 16:51:38.350527 58430 net.cpp:264] This network produces output top-5
I0122 16:51:38.350549 58430 net.cpp:284] Network initialization done.
I0122 16:51:38.353660 58430 model_transformer.cpp:80] layer: data
I0122 16:51:38.353679 58430 model_transformer.cpp:80] layer: conv1
I0122 16:51:38.353711 58430 model_transformer.cpp:80] layer: bn1
I0122 16:51:38.353735 58430 model_transformer.cpp:80] layer: relu1
I0122 16:51:38.353741 58430 model_transformer.cpp:80] layer: conv2
I0122 16:51:38.353826 58430 model_transformer.cpp:80] layer: bn2
I0122 16:51:38.353848 58430 model_transformer.cpp:80] layer: relu2
I0122 16:51:38.353855 58430 model_transformer.cpp:80] layer: pool1
I0122 16:51:38.353860 58430 model_transformer.cpp:80] layer: drop1
I0122 16:51:38.353864 58430 model_transformer.cpp:80] layer: conv3
I0122 16:51:38.353981 58430 model_transformer.cpp:80] layer: bn3
I0122 16:51:38.354005 58430 model_transformer.cpp:80] layer: relu3
I0122 16:51:38.354010 58430 model_transformer.cpp:80] layer: conv4
I0122 16:51:38.354231 58430 model_transformer.cpp:80] layer: bn4
I0122 16:51:38.354254 58430 model_transformer.cpp:80] layer: relu4
I0122 16:51:38.354261 58430 model_transformer.cpp:80] layer: pool2
I0122 16:51:38.354265 58430 model_transformer.cpp:80] layer: drop2
I0122 16:51:38.354270 58430 model_transformer.cpp:80] layer: fc1
I0122 16:51:38.405055 58430 model_transformer.cpp:80] layer: bn5
I0122 16:51:38.405105 58430 model_transformer.cpp:80] layer: relu5
I0122 16:51:38.405112 58430 model_transformer.cpp:80] layer: drop3
I0122 16:51:38.405117 58430 model_transformer.cpp:80] layer: fc2
I0122 16:51:38.405213 58430 model_transformer.cpp:80] layer: loss
Output transformed caffemodel: transformed.caffemodel
mv transformed.caffemodel ${WORK_DIR}

# get flops and the number of parameters of a model
${PRUNE_ROOT}/deephi_compress stat -model ${WORK_DIR}/train_val.prototxt 2>&1 | tee ${WORK_DIR}/rpt/logfile_stat_miniVggNet.txt
I0122 16:51:39.702880 58494 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0122 16:51:39.703292 58494 net.cpp:52] Initializing net from parameters: 
name: "miniVggNet on Cifar10 m3 NO-inPlace"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "cifar10/input/lmdb/valid_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "bn4"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "bn5"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "bn5"
  top: "scale5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0122 16:51:39.703406 58494 layer_factory.hpp:77] Creating layer data
I0122 16:51:39.704141 58494 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:51:39.704593 58494 net.cpp:94] Creating Layer data
I0122 16:51:39.704604 58494 net.cpp:409] data -> data
I0122 16:51:39.704619 58494 net.cpp:409] data -> label
I0122 16:51:39.706388 58528 db_lmdb.cpp:35] Opened lmdb cifar10/input/lmdb/valid_lmdb
I0122 16:51:39.706427 58528 data_reader.cpp:117] TEST: reading data using 1 channel(s)
I0122 16:51:39.706467 58494 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0122 16:51:39.706487 58494 data_layer.cpp:83] output data size: 50,3,32,32
I0122 16:51:39.708323 58494 internal_thread.cpp:27] Starting internal thread(s) on GPU 0
I0122 16:51:39.708348 58494 net.cpp:144] Setting up data
I0122 16:51:39.708357 58494 net.cpp:151] Top shape: 50 3 32 32 (153600)
I0122 16:51:39.708361 58494 net.cpp:151] Top shape: 50 (50)
I0122 16:51:39.708364 58494 net.cpp:159] Memory required for data: 614600
I0122 16:51:39.708371 58494 layer_factory.hpp:77] Creating layer label_data_1_split
I0122 16:51:39.708380 58494 net.cpp:94] Creating Layer label_data_1_split
I0122 16:51:39.708385 58494 net.cpp:435] label_data_1_split <- label
I0122 16:51:39.708400 58494 net.cpp:409] label_data_1_split -> label_data_1_split_0
I0122 16:51:39.708410 58494 net.cpp:409] label_data_1_split -> label_data_1_split_1
I0122 16:51:39.708415 58494 net.cpp:409] label_data_1_split -> label_data_1_split_2
I0122 16:51:39.708425 58494 net.cpp:144] Setting up label_data_1_split
I0122 16:51:39.708429 58494 net.cpp:151] Top shape: 50 (50)
I0122 16:51:39.708432 58494 net.cpp:151] Top shape: 50 (50)
I0122 16:51:39.708436 58494 net.cpp:151] Top shape: 50 (50)
I0122 16:51:39.708438 58494 net.cpp:159] Memory required for data: 615200
I0122 16:51:39.708441 58494 layer_factory.hpp:77] Creating layer conv1
I0122 16:51:39.708451 58494 net.cpp:94] Creating Layer conv1
I0122 16:51:39.708456 58494 net.cpp:435] conv1 <- data
I0122 16:51:39.708461 58494 net.cpp:409] conv1 -> conv1
I0122 16:51:39.708528 58494 net.cpp:144] Setting up conv1
I0122 16:51:39.708534 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708537 58494 net.cpp:159] Memory required for data: 7168800
I0122 16:51:39.708552 58494 layer_factory.hpp:77] Creating layer bn1
I0122 16:51:39.708559 58494 net.cpp:94] Creating Layer bn1
I0122 16:51:39.708562 58494 net.cpp:435] bn1 <- conv1
I0122 16:51:39.708567 58494 net.cpp:409] bn1 -> bn1
I0122 16:51:39.708598 58494 net.cpp:144] Setting up bn1
I0122 16:51:39.708614 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708617 58494 net.cpp:159] Memory required for data: 13722400
I0122 16:51:39.708628 58494 layer_factory.hpp:77] Creating layer scale1
I0122 16:51:39.708636 58494 net.cpp:94] Creating Layer scale1
I0122 16:51:39.708639 58494 net.cpp:435] scale1 <- bn1
I0122 16:51:39.708643 58494 net.cpp:409] scale1 -> scale1
I0122 16:51:39.708653 58494 layer_factory.hpp:77] Creating layer scale1
I0122 16:51:39.708669 58494 net.cpp:144] Setting up scale1
I0122 16:51:39.708674 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708676 58494 net.cpp:159] Memory required for data: 20276000
I0122 16:51:39.708683 58494 layer_factory.hpp:77] Creating layer relu1
I0122 16:51:39.708688 58494 net.cpp:94] Creating Layer relu1
I0122 16:51:39.708691 58494 net.cpp:435] relu1 <- scale1
I0122 16:51:39.708695 58494 net.cpp:409] relu1 -> relu1
I0122 16:51:39.708705 58494 net.cpp:144] Setting up relu1
I0122 16:51:39.708711 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708714 58494 net.cpp:159] Memory required for data: 26829600
I0122 16:51:39.708716 58494 layer_factory.hpp:77] Creating layer conv2
I0122 16:51:39.708724 58494 net.cpp:94] Creating Layer conv2
I0122 16:51:39.708727 58494 net.cpp:435] conv2 <- relu1
I0122 16:51:39.708732 58494 net.cpp:409] conv2 -> conv2
I0122 16:51:39.708804 58494 net.cpp:144] Setting up conv2
I0122 16:51:39.708811 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708813 58494 net.cpp:159] Memory required for data: 33383200
I0122 16:51:39.708817 58494 layer_factory.hpp:77] Creating layer bn2
I0122 16:51:39.708822 58494 net.cpp:94] Creating Layer bn2
I0122 16:51:39.708827 58494 net.cpp:435] bn2 <- conv2
I0122 16:51:39.708830 58494 net.cpp:409] bn2 -> bn2
I0122 16:51:39.708858 58494 net.cpp:144] Setting up bn2
I0122 16:51:39.708863 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708866 58494 net.cpp:159] Memory required for data: 39936800
I0122 16:51:39.708873 58494 layer_factory.hpp:77] Creating layer scale2
I0122 16:51:39.708878 58494 net.cpp:94] Creating Layer scale2
I0122 16:51:39.708881 58494 net.cpp:435] scale2 <- bn2
I0122 16:51:39.708886 58494 net.cpp:409] scale2 -> scale2
I0122 16:51:39.708894 58494 layer_factory.hpp:77] Creating layer scale2
I0122 16:51:39.708907 58494 net.cpp:144] Setting up scale2
I0122 16:51:39.708912 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708915 58494 net.cpp:159] Memory required for data: 46490400
I0122 16:51:39.708921 58494 layer_factory.hpp:77] Creating layer relu2
I0122 16:51:39.708925 58494 net.cpp:94] Creating Layer relu2
I0122 16:51:39.708928 58494 net.cpp:435] relu2 <- scale2
I0122 16:51:39.708932 58494 net.cpp:409] relu2 -> relu2
I0122 16:51:39.708938 58494 net.cpp:144] Setting up relu2
I0122 16:51:39.708942 58494 net.cpp:151] Top shape: 50 32 32 32 (1638400)
I0122 16:51:39.708945 58494 net.cpp:159] Memory required for data: 53044000
I0122 16:51:39.708948 58494 layer_factory.hpp:77] Creating layer pool1
I0122 16:51:39.708953 58494 net.cpp:94] Creating Layer pool1
I0122 16:51:39.708956 58494 net.cpp:435] pool1 <- relu2
I0122 16:51:39.708959 58494 net.cpp:409] pool1 -> pool1
I0122 16:51:39.708971 58494 net.cpp:144] Setting up pool1
I0122 16:51:39.708976 58494 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:51:39.708978 58494 net.cpp:159] Memory required for data: 54682400
I0122 16:51:39.708981 58494 layer_factory.hpp:77] Creating layer drop1
I0122 16:51:39.708986 58494 net.cpp:94] Creating Layer drop1
I0122 16:51:39.708988 58494 net.cpp:435] drop1 <- pool1
I0122 16:51:39.708992 58494 net.cpp:409] drop1 -> drop1
I0122 16:51:39.708998 58494 net.cpp:144] Setting up drop1
I0122 16:51:39.709002 58494 net.cpp:151] Top shape: 50 32 16 16 (409600)
I0122 16:51:39.709005 58494 net.cpp:159] Memory required for data: 56320800
I0122 16:51:39.709008 58494 layer_factory.hpp:77] Creating layer conv3
I0122 16:51:39.709014 58494 net.cpp:94] Creating Layer conv3
I0122 16:51:39.709018 58494 net.cpp:435] conv3 <- drop1
I0122 16:51:39.709028 58494 net.cpp:409] conv3 -> conv3
I0122 16:51:39.709200 58494 net.cpp:144] Setting up conv3
I0122 16:51:39.709208 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709210 58494 net.cpp:159] Memory required for data: 59597600
I0122 16:51:39.709214 58494 layer_factory.hpp:77] Creating layer bn3
I0122 16:51:39.709221 58494 net.cpp:94] Creating Layer bn3
I0122 16:51:39.709224 58494 net.cpp:435] bn3 <- conv3
I0122 16:51:39.709229 58494 net.cpp:409] bn3 -> bn3
I0122 16:51:39.709257 58494 net.cpp:144] Setting up bn3
I0122 16:51:39.709262 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709265 58494 net.cpp:159] Memory required for data: 62874400
I0122 16:51:39.709272 58494 layer_factory.hpp:77] Creating layer scale3
I0122 16:51:39.709276 58494 net.cpp:94] Creating Layer scale3
I0122 16:51:39.709280 58494 net.cpp:435] scale3 <- bn3
I0122 16:51:39.709283 58494 net.cpp:409] scale3 -> scale3
I0122 16:51:39.709292 58494 layer_factory.hpp:77] Creating layer scale3
I0122 16:51:39.709306 58494 net.cpp:144] Setting up scale3
I0122 16:51:39.709311 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709313 58494 net.cpp:159] Memory required for data: 66151200
I0122 16:51:39.709317 58494 layer_factory.hpp:77] Creating layer relu3
I0122 16:51:39.709321 58494 net.cpp:94] Creating Layer relu3
I0122 16:51:39.709324 58494 net.cpp:435] relu3 <- scale3
I0122 16:51:39.709328 58494 net.cpp:409] relu3 -> relu3
I0122 16:51:39.709334 58494 net.cpp:144] Setting up relu3
I0122 16:51:39.709338 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709342 58494 net.cpp:159] Memory required for data: 69428000
I0122 16:51:39.709343 58494 layer_factory.hpp:77] Creating layer conv4
I0122 16:51:39.709352 58494 net.cpp:94] Creating Layer conv4
I0122 16:51:39.709357 58494 net.cpp:435] conv4 <- relu3
I0122 16:51:39.709362 58494 net.cpp:409] conv4 -> conv4
I0122 16:51:39.709686 58494 net.cpp:144] Setting up conv4
I0122 16:51:39.709693 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709695 58494 net.cpp:159] Memory required for data: 72704800
I0122 16:51:39.709700 58494 layer_factory.hpp:77] Creating layer bn4
I0122 16:51:39.709707 58494 net.cpp:94] Creating Layer bn4
I0122 16:51:39.709710 58494 net.cpp:435] bn4 <- conv4
I0122 16:51:39.709714 58494 net.cpp:409] bn4 -> bn4
I0122 16:51:39.709746 58494 net.cpp:144] Setting up bn4
I0122 16:51:39.709751 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709755 58494 net.cpp:159] Memory required for data: 75981600
I0122 16:51:39.709765 58494 layer_factory.hpp:77] Creating layer scale4
I0122 16:51:39.709774 58494 net.cpp:94] Creating Layer scale4
I0122 16:51:39.709776 58494 net.cpp:435] scale4 <- bn4
I0122 16:51:39.709781 58494 net.cpp:409] scale4 -> scale4
I0122 16:51:39.709789 58494 layer_factory.hpp:77] Creating layer scale4
I0122 16:51:39.709803 58494 net.cpp:144] Setting up scale4
I0122 16:51:39.709808 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709811 58494 net.cpp:159] Memory required for data: 79258400
I0122 16:51:39.709816 58494 layer_factory.hpp:77] Creating layer relu4
I0122 16:51:39.709820 58494 net.cpp:94] Creating Layer relu4
I0122 16:51:39.709822 58494 net.cpp:435] relu4 <- scale4
I0122 16:51:39.709827 58494 net.cpp:409] relu4 -> relu4
I0122 16:51:39.709833 58494 net.cpp:144] Setting up relu4
I0122 16:51:39.709839 58494 net.cpp:151] Top shape: 50 64 16 16 (819200)
I0122 16:51:39.709841 58494 net.cpp:159] Memory required for data: 82535200
I0122 16:51:39.709846 58494 layer_factory.hpp:77] Creating layer pool2
I0122 16:51:39.709849 58494 net.cpp:94] Creating Layer pool2
I0122 16:51:39.709852 58494 net.cpp:435] pool2 <- relu4
I0122 16:51:39.709856 58494 net.cpp:409] pool2 -> pool2
I0122 16:51:39.709863 58494 net.cpp:144] Setting up pool2
I0122 16:51:39.709867 58494 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:51:39.709869 58494 net.cpp:159] Memory required for data: 83354400
I0122 16:51:39.709872 58494 layer_factory.hpp:77] Creating layer drop2
I0122 16:51:39.709885 58494 net.cpp:94] Creating Layer drop2
I0122 16:51:39.709887 58494 net.cpp:435] drop2 <- pool2
I0122 16:51:39.709892 58494 net.cpp:409] drop2 -> drop2
I0122 16:51:39.709898 58494 net.cpp:144] Setting up drop2
I0122 16:51:39.709903 58494 net.cpp:151] Top shape: 50 64 8 8 (204800)
I0122 16:51:39.709913 58494 net.cpp:159] Memory required for data: 84173600
I0122 16:51:39.709916 58494 layer_factory.hpp:77] Creating layer fc1
I0122 16:51:39.709923 58494 net.cpp:94] Creating Layer fc1
I0122 16:51:39.709928 58494 net.cpp:435] fc1 <- drop2
I0122 16:51:39.709934 58494 net.cpp:409] fc1 -> fc1
I0122 16:51:39.724983 58494 net.cpp:144] Setting up fc1
I0122 16:51:39.725008 58494 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:39.725013 58494 net.cpp:159] Memory required for data: 84276000
I0122 16:51:39.725020 58494 layer_factory.hpp:77] Creating layer bn5
I0122 16:51:39.725033 58494 net.cpp:94] Creating Layer bn5
I0122 16:51:39.725037 58494 net.cpp:435] bn5 <- fc1
I0122 16:51:39.725044 58494 net.cpp:409] bn5 -> bn5
I0122 16:51:39.725088 58494 net.cpp:144] Setting up bn5
I0122 16:51:39.725093 58494 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:39.725096 58494 net.cpp:159] Memory required for data: 84378400
I0122 16:51:39.725105 58494 layer_factory.hpp:77] Creating layer scale5
I0122 16:51:39.725108 58494 net.cpp:94] Creating Layer scale5
I0122 16:51:39.725111 58494 net.cpp:435] scale5 <- bn5
I0122 16:51:39.725116 58494 net.cpp:409] scale5 -> scale5
I0122 16:51:39.725126 58494 layer_factory.hpp:77] Creating layer scale5
I0122 16:51:39.725144 58494 net.cpp:144] Setting up scale5
I0122 16:51:39.725149 58494 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:39.725152 58494 net.cpp:159] Memory required for data: 84480800
I0122 16:51:39.725157 58494 layer_factory.hpp:77] Creating layer relu5
I0122 16:51:39.725162 58494 net.cpp:94] Creating Layer relu5
I0122 16:51:39.725165 58494 net.cpp:435] relu5 <- scale5
I0122 16:51:39.725169 58494 net.cpp:409] relu5 -> relu5
I0122 16:51:39.725175 58494 net.cpp:144] Setting up relu5
I0122 16:51:39.725181 58494 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:39.725184 58494 net.cpp:159] Memory required for data: 84583200
I0122 16:51:39.725186 58494 layer_factory.hpp:77] Creating layer drop3
I0122 16:51:39.725191 58494 net.cpp:94] Creating Layer drop3
I0122 16:51:39.725194 58494 net.cpp:435] drop3 <- relu5
I0122 16:51:39.725198 58494 net.cpp:409] drop3 -> drop3
I0122 16:51:39.725205 58494 net.cpp:144] Setting up drop3
I0122 16:51:39.725208 58494 net.cpp:151] Top shape: 50 512 (25600)
I0122 16:51:39.725211 58494 net.cpp:159] Memory required for data: 84685600
I0122 16:51:39.725214 58494 layer_factory.hpp:77] Creating layer fc2
I0122 16:51:39.725219 58494 net.cpp:94] Creating Layer fc2
I0122 16:51:39.725222 58494 net.cpp:435] fc2 <- drop3
I0122 16:51:39.725227 58494 net.cpp:409] fc2 -> fc2
I0122 16:51:39.725271 58494 net.cpp:144] Setting up fc2
I0122 16:51:39.725276 58494 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:39.725280 58494 net.cpp:159] Memory required for data: 84687600
I0122 16:51:39.725283 58494 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0122 16:51:39.725289 58494 net.cpp:94] Creating Layer fc2_fc2_0_split
I0122 16:51:39.725292 58494 net.cpp:435] fc2_fc2_0_split <- fc2
I0122 16:51:39.725297 58494 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0122 16:51:39.725302 58494 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0122 16:51:39.725312 58494 net.cpp:409] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0122 16:51:39.725320 58494 net.cpp:144] Setting up fc2_fc2_0_split
I0122 16:51:39.725324 58494 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:39.725327 58494 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:39.725330 58494 net.cpp:151] Top shape: 50 10 (500)
I0122 16:51:39.725333 58494 net.cpp:159] Memory required for data: 84693600
I0122 16:51:39.725337 58494 layer_factory.hpp:77] Creating layer loss
I0122 16:51:39.725340 58494 net.cpp:94] Creating Layer loss
I0122 16:51:39.725343 58494 net.cpp:435] loss <- fc2_fc2_0_split_0
I0122 16:51:39.725347 58494 net.cpp:435] loss <- label_data_1_split_0
I0122 16:51:39.725368 58494 net.cpp:409] loss -> loss
I0122 16:51:39.725376 58494 layer_factory.hpp:77] Creating layer loss
I0122 16:51:39.725389 58494 net.cpp:144] Setting up loss
I0122 16:51:39.725395 58494 net.cpp:151] Top shape: (1)
I0122 16:51:39.725399 58494 net.cpp:154]     with loss weight 1
I0122 16:51:39.725420 58494 net.cpp:159] Memory required for data: 84693604
I0122 16:51:39.725422 58494 layer_factory.hpp:77] Creating layer accuracy-top1
I0122 16:51:39.725427 58494 net.cpp:94] Creating Layer accuracy-top1
I0122 16:51:39.725431 58494 net.cpp:435] accuracy-top1 <- fc2_fc2_0_split_1
I0122 16:51:39.725435 58494 net.cpp:435] accuracy-top1 <- label_data_1_split_1
I0122 16:51:39.725440 58494 net.cpp:409] accuracy-top1 -> top-1
I0122 16:51:39.725445 58494 net.cpp:144] Setting up accuracy-top1
I0122 16:51:39.725451 58494 net.cpp:151] Top shape: (1)
I0122 16:51:39.725455 58494 net.cpp:159] Memory required for data: 84693608
I0122 16:51:39.725456 58494 layer_factory.hpp:77] Creating layer accuracy-top5
I0122 16:51:39.725461 58494 net.cpp:94] Creating Layer accuracy-top5
I0122 16:51:39.725463 58494 net.cpp:435] accuracy-top5 <- fc2_fc2_0_split_2
I0122 16:51:39.725466 58494 net.cpp:435] accuracy-top5 <- label_data_1_split_2
I0122 16:51:39.725472 58494 net.cpp:409] accuracy-top5 -> top-5
I0122 16:51:39.725477 58494 net.cpp:144] Setting up accuracy-top5
I0122 16:51:39.725483 58494 net.cpp:151] Top shape: (1)
I0122 16:51:39.725486 58494 net.cpp:159] Memory required for data: 84693612
I0122 16:51:39.725488 58494 net.cpp:222] accuracy-top5 does not need backward computation.
I0122 16:51:39.725495 58494 net.cpp:222] accuracy-top1 does not need backward computation.
I0122 16:51:39.725499 58494 net.cpp:220] loss needs backward computation.
I0122 16:51:39.725502 58494 net.cpp:220] fc2_fc2_0_split needs backward computation.
I0122 16:51:39.725505 58494 net.cpp:220] fc2 needs backward computation.
I0122 16:51:39.725508 58494 net.cpp:220] drop3 needs backward computation.
I0122 16:51:39.725512 58494 net.cpp:220] relu5 needs backward computation.
I0122 16:51:39.725514 58494 net.cpp:220] scale5 needs backward computation.
I0122 16:51:39.725517 58494 net.cpp:220] bn5 needs backward computation.
I0122 16:51:39.725520 58494 net.cpp:220] fc1 needs backward computation.
I0122 16:51:39.725523 58494 net.cpp:220] drop2 needs backward computation.
I0122 16:51:39.725527 58494 net.cpp:220] pool2 needs backward computation.
I0122 16:51:39.725529 58494 net.cpp:220] relu4 needs backward computation.
I0122 16:51:39.725533 58494 net.cpp:220] scale4 needs backward computation.
I0122 16:51:39.725535 58494 net.cpp:220] bn4 needs backward computation.
I0122 16:51:39.725538 58494 net.cpp:220] conv4 needs backward computation.
I0122 16:51:39.725541 58494 net.cpp:220] relu3 needs backward computation.
I0122 16:51:39.725544 58494 net.cpp:220] scale3 needs backward computation.
I0122 16:51:39.725548 58494 net.cpp:220] bn3 needs backward computation.
I0122 16:51:39.725550 58494 net.cpp:220] conv3 needs backward computation.
I0122 16:51:39.725553 58494 net.cpp:220] drop1 needs backward computation.
I0122 16:51:39.725556 58494 net.cpp:220] pool1 needs backward computation.
I0122 16:51:39.725561 58494 net.cpp:220] relu2 needs backward computation.
I0122 16:51:39.725564 58494 net.cpp:220] scale2 needs backward computation.
I0122 16:51:39.725566 58494 net.cpp:220] bn2 needs backward computation.
I0122 16:51:39.725569 58494 net.cpp:220] conv2 needs backward computation.
I0122 16:51:39.725572 58494 net.cpp:220] relu1 needs backward computation.
I0122 16:51:39.725575 58494 net.cpp:220] scale1 needs backward computation.
I0122 16:51:39.725579 58494 net.cpp:220] bn1 needs backward computation.
I0122 16:51:39.725581 58494 net.cpp:220] conv1 needs backward computation.
I0122 16:51:39.725585 58494 net.cpp:222] label_data_1_split does not need backward computation.
I0122 16:51:39.725589 58494 net.cpp:222] data does not need backward computation.
I0122 16:51:39.725591 58494 net.cpp:264] This network produces output loss
I0122 16:51:39.725594 58494 net.cpp:264] This network produces output top-1
I0122 16:51:39.725603 58494 net.cpp:264] This network produces output top-5
I0122 16:51:39.725630 58494 net.cpp:284] Network initialization done.
I0122 16:51:39.725730 58494 net_counter.cpp:58] Convolution layer conv1 ops: 1802240
I0122 16:51:39.725734 58494 net_counter.cpp:62] Convolution layer conv1 params: 896
I0122 16:51:39.725739 58494 net_counter.cpp:62] BatchNorm layer bn1 params: 129
I0122 16:51:39.725741 58494 net_counter.cpp:58] Convolution layer conv2 ops: 18907136
I0122 16:51:39.725744 58494 net_counter.cpp:62] Convolution layer conv2 params: 9248
I0122 16:51:39.725745 58494 net_counter.cpp:62] BatchNorm layer bn2 params: 129
I0122 16:51:39.725749 58494 net_counter.cpp:58] Convolution layer conv3 ops: 9453568
I0122 16:51:39.725751 58494 net_counter.cpp:62] Convolution layer conv3 params: 18496
I0122 16:51:39.725754 58494 net_counter.cpp:62] BatchNorm layer bn3 params: 257
I0122 16:51:39.725756 58494 net_counter.cpp:58] Convolution layer conv4 ops: 18890752
I0122 16:51:39.725759 58494 net_counter.cpp:62] Convolution layer conv4 params: 36928
I0122 16:51:39.725762 58494 net_counter.cpp:62] BatchNorm layer bn4 params: 257
I0122 16:51:39.725764 58494 net_counter.cpp:62] BatchNorm layer bn5 params: 2049
I0122 16:51:39.725767 58494 net_counter.cpp:68] Total operations: 49053696
I0122 16:51:39.725770 58494 net_counter.cpp:69] Total params: 68389

